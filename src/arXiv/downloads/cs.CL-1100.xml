<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.CL%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.CL&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/7BxYOsJ5U+ZSIgaaB+eT8jP6vr8</id>
  <updated>2025-03-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">80880</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0303002v2</id>
    <updated>2003-03-21T12:34:45Z</updated>
    <published>2003-03-05T08:50:42Z</published>
    <title>About compression of vocabulary in computer oriented languages</title>
    <summary>  The author uses the entropy of the ideal Bose-Einstein gas to minimize losses
in computer-oriented languages.
</summary>
    <author>
      <name>V. P. Maslov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex, 7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.4; H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0303007v1</id>
    <updated>2003-03-14T05:15:20Z</updated>
    <published>2003-03-14T05:15:20Z</published>
    <title>Glottochronology and problems of protolanguage reconstruction</title>
    <summary>  A method of languages genealogical trees construction is proposed.
</summary>
    <author>
      <name>Kromer Victor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures. In Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304024v1</id>
    <updated>2003-04-17T02:22:06Z</updated>
    <published>2003-04-17T02:22:06Z</published>
    <title>Glottochronologic Retrognostic of Language System</title>
    <summary>  A glottochronologic retrognostic of language system is proposed
</summary>
    <author>
      <name>Kromer Victor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures. In Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0304024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1415v1</id>
    <updated>2008-01-09T12:34:40Z</updated>
    <published>2008-01-09T12:34:40Z</published>
    <title>The emerging field of language dynamics</title>
    <summary>  A simple review by a linguist, citing many articles by physicists:
Quantitative methods, agent-based computer simulations, language dynamics,
language typology, historical linguistics
</summary>
    <author>
      <name>S. Wichmann</name>
    </author>
    <link href="http://arxiv.org/abs/0801.1415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.5973v1</id>
    <updated>2011-06-27T20:13:07Z</updated>
    <published>2011-06-27T20:13:07Z</published>
    <title>Entropy of Telugu</title>
    <summary>  This paper presents an investigation of the entropy of the Telugu script.
Since this script is syllabic, and not alphabetic, the computation of entropy
is somewhat complicated.
</summary>
    <author>
      <name>Venkata Ravinder Paruchuri</name>
    </author>
    <link href="http://arxiv.org/abs/1106.5973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.5973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3011v2</id>
    <updated>2016-10-08T22:26:41Z</updated>
    <published>2013-11-13T03:58:38Z</published>
    <title>Cornell SPF: Cornell Semantic Parsing Framework</title>
    <summary>  The Cornell Semantic Parsing Framework (SPF) is a learning and inference
framework for mapping natural language to formal representation of its meaning.
</summary>
    <author>
      <name>Yoav Artzi</name>
    </author>
    <link href="http://arxiv.org/abs/1311.3011v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3011v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09772v2</id>
    <updated>2018-08-30T17:44:54Z</updated>
    <published>2018-08-29T12:58:45Z</published>
    <title>Notes on Deep Learning for NLP</title>
    <summary>  My notes on Deep Learning for NLP.
</summary>
    <author>
      <name>Antoine J. -P. Tixier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09772v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09772v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10361v1</id>
    <updated>2020-11-20T12:03:41Z</updated>
    <published>2020-11-20T12:03:41Z</published>
    <title>1st AfricaNLP Workshop Proceedings, 2020</title>
    <summary>  Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR
2020, Virtual Conference, Formerly Addis Ababa Ethiopia.
</summary>
    <author>
      <name>Kathleen Siminyu</name>
    </author>
    <author>
      <name>Laura Martinus</name>
    </author>
    <author>
      <name>Vukosi Marivate</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.05467v1</id>
    <updated>2019-12-11T17:05:18Z</updated>
    <published>2019-12-11T17:05:18Z</published>
    <title>MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low
  Resource Machine Translation</title>
    <summary>  Manipulating training data leads to robust neural models for MT.
</summary>
    <author>
      <name>Rumeng Li</name>
    </author>
    <author>
      <name>Xun Wang</name>
    </author>
    <author>
      <name>Hong Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/aaai.v34i05.6339</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/aaai.v34i05.6339" rel="related"/>
    <link href="http://arxiv.org/abs/1912.05467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.05467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.06554v1</id>
    <updated>2024-11-10T18:32:24Z</updated>
    <published>2024-11-10T18:32:24Z</published>
    <title>The KIPARLA Forest treebank of spoken Italian: an overview of initial
  design choices</title>
    <summary>  The paper presents an overview of initial design choices discussed towards
the creation of a treebank for the Italian KIParla corpus
</summary>
    <author>
      <name>Ludovica Pannitto</name>
    </author>
    <author>
      <name>Caterina Mauri</name>
    </author>
    <link href="http://arxiv.org/abs/2411.06554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.06554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0002017v1</id>
    <updated>2000-02-27T04:09:59Z</updated>
    <published>2000-02-27T04:09:59Z</published>
    <title>An Usage Measure Based on Psychophysical Relations</title>
    <summary>  A new word usage measure is proposed. It is based on psychophysical relations
and allows to reveal words by its degree of "importance" for making basic
dictionaries of sublanguages.
</summary>
    <author>
      <name>V. Kromer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0002017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0002017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607088v1</id>
    <updated>2006-07-18T07:43:07Z</updated>
    <published>2006-07-18T07:43:07Z</published>
    <title>Using Answer Set Programming in an Inference-Based approach to Natural
  Language Semantics</title>
    <summary>  Using Answer Set Programming in an Inference-Based approach to Natural
Language Semantics
</summary>
    <author>
      <name>Farid Nouioua</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Nicolas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LERIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Inference in Computational Semantics ICoS-5, France (2006) 77-86</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0607088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610124v1</id>
    <updated>2006-10-20T11:48:38Z</updated>
    <published>2006-10-20T11:48:38Z</published>
    <title>Dependency Treebanks: Methods, Annotation Schemes and Tools</title>
    <summary>  In this paper, current dependencybased treebanks are introduced and analyzed.
The methods used for building the resources, the annotation schemes applied,
and the tools used (such as POS taggers, parsers and annotation software) are
discussed.
</summary>
    <author>
      <name>Tuomo Kakkonen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 15th Nordic Conference of Computational
  Linguistics (NODALIDA 2005), pp. 94-104. Joensuu, Finland, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0610124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3197v1</id>
    <updated>2007-11-20T19:57:23Z</updated>
    <published>2007-11-20T19:57:23Z</published>
    <title>How to realize "a sense of humour" in computers ?</title>
    <summary>  Computer model of a "sense of humour" suggested previously [arXiv:0711.2058,
0711.2061, 0711.2270] is raised to the level of a realistic algorithm.
</summary>
    <author>
      <name>I. M. Suslov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">P.L.Kapitza Institute for Physical Problems, Moscow, Russia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures included</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.3197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3452v1</id>
    <updated>2007-11-21T20:38:29Z</updated>
    <published>2007-11-21T20:38:29Z</published>
    <title>In memoriam Maurice Gross</title>
    <summary>  Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural
language processing. This article is written in homage to his memory
</summary>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Archives of Control Sciences 15, 3 (2005) 257-278</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.3452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.4198v1</id>
    <updated>2008-02-28T12:58:49Z</updated>
    <published>2008-02-28T12:58:49Z</published>
    <title>Some properties of the Ukrainian writing system</title>
    <summary>  We investigate the grapheme-phoneme relation in Ukrainian and some properties
of the Ukrainian version of the Cyrillic alphabet.
</summary>
    <author>
      <name>Solomija Buk</name>
    </author>
    <author>
      <name>Ján Mačutek</name>
    </author>
    <author>
      <name>Andrij Rovenchak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Glottometrics 16, 63-79 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.4198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.4198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3845v1</id>
    <updated>2008-07-24T10:31:55Z</updated>
    <published>2008-07-24T10:31:55Z</published>
    <title>Formal semantics of language and the Richard-Berry paradox</title>
    <summary>  The classical logical antinomy known as Richard-Berry paradox is combined
with plausible assumptions about the size i.e. the descriptional complexity of
Turing machines formalizing certain sentences, to show that formalization of
language leads to contradiction.
</summary>
    <author>
      <name>Stefano Crespi Reghizzi</name>
    </author>
    <link href="http://arxiv.org/abs/0807.3845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5116v1</id>
    <updated>2009-11-26T16:27:58Z</updated>
    <published>2009-11-26T16:27:58Z</published>
    <title>Standardization of the formal representation of lexical information for
  NLP</title>
    <summary>  A survey of dictionary models and formats is presented as well as a
presentation of corresponding recent standardisation activities.
</summary>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Saclay - Ile de France, IDSL</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dictionarie. An International Encyclopedia of Lexicography.
  Supplementary volume: Recent developments with special focus on computational
  lexicography (2010) -</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.5116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5568v1</id>
    <updated>2009-11-30T08:07:16Z</updated>
    <published>2009-11-30T08:07:16Z</published>
    <title>Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau</title>
    <summary>  This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.
</summary>
    <author>
      <name>Cédric Messiant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Thierry Poibeau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.5568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3183v2</id>
    <updated>2011-07-01T17:08:12Z</updated>
    <published>2010-04-19T13:11:56Z</published>
    <title>Statistical Physics for Natural Language Processing</title>
    <summary>  This paper has been withdrawn by the author.
</summary>
    <author>
      <name>Juan-Manuel Torres Moreno</name>
    </author>
    <author>
      <name>Silvia Fernandez</name>
    </author>
    <author>
      <name>Eric SanJuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.3183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4697v3</id>
    <updated>2010-08-25T13:10:44Z</updated>
    <published>2010-05-25T20:36:09Z</published>
    <title>The Lambek-Grishin calculus is NP-complete</title>
    <summary>  The Lambek-Grishin calculus LG is the symmetric extension of the
non-associative Lambek calculus NL. In this paper we prove that the
derivability problem for LG is NP-complete.
</summary>
    <author>
      <name>Jeroen Bransen</name>
    </author>
    <link href="http://arxiv.org/abs/1005.4697v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4697v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1753v1</id>
    <updated>2011-07-09T00:40:06Z</updated>
    <published>2011-07-09T00:40:06Z</published>
    <title>Notes on Electronic Lexicography</title>
    <summary>  These notes are a continuation of topics covered by V. Selegej in his article
"Electronic Dictionaries and Computational lexicography". How can an electronic
dictionary have as its object the description of closely related languages?
Obviously, such a question allows multiple answers.
</summary>
    <author>
      <name>Yavor Parvanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.1753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.7917v1</id>
    <updated>2012-10-30T07:25:46Z</updated>
    <published>2012-10-30T07:25:46Z</published>
    <title>The Model of Semantic Concepts Lattice For Data Mining Of Microblogs</title>
    <summary>  The model of semantic concept lattice for data mining of microblogs has been
proposed in this work. It is shown that the use of this model is effective for
the semantic relations analysis and for the detection of associative rules of
key words.
</summary>
    <author>
      <name>Bohdan Pavlyshenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Ukrainian</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.7917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.7917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4674v1</id>
    <updated>2012-12-19T14:40:38Z</updated>
    <published>2012-12-19T14:40:38Z</published>
    <title>Natural Language Understanding Based on Semantic Relations between
  Sentences</title>
    <summary>  In this paper, we define event expression over sentences of natural language
and semantic relations between events. Based on this definition, we formally
consider text understanding process having events as basic unit.
</summary>
    <author>
      <name>Hyeok Kong</name>
    </author>
    <link href="http://arxiv.org/abs/1212.4674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1774v1</id>
    <updated>2014-08-08T07:39:13Z</updated>
    <published>2014-08-08T07:39:13Z</published>
    <title>Beyond description. Comment on "Approaching human language with complex
  networks" by Cong &amp; Liu</title>
    <summary>  Comment on "Approaching human language with complex networks" by Cong &amp; Liu
</summary>
    <author>
      <name>Ramon Ferrer-i-Cancho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.plrev.2014.07.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.plrev.2014.07.014" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics of Life Reviews 11 (4), 621-623 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.1774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03783v1</id>
    <updated>2016-01-15T00:09:52Z</updated>
    <published>2016-01-15T00:09:52Z</published>
    <title>Towards Turkish ASR: Anatomy of a rule-based Turkish g2p</title>
    <summary>  This paper describes the architecture and implementation of a rule-based
grapheme to phoneme converter for Turkish. The system accepts surface form as
input, outputs SAMPA mapping of the all parallel pronounciations according to
the morphological analysis together with stress positions. The system has been
implemented in Python
</summary>
    <author>
      <name>Duygu Altinok</name>
    </author>
    <link href="http://arxiv.org/abs/1601.03783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05467v1</id>
    <updated>2017-11-15T09:17:34Z</updated>
    <published>2017-11-15T09:17:34Z</published>
    <title>Aicyber's System for NLPCC 2017 Shared Task 2: Voting of Baselines</title>
    <summary>  This paper presents Aicyber's system for NLPCC 2017 shared task 2. It is
formed by a voting of three deep learning based system trained on
character-enhanced word vectors and a well known bag-of-word model.
</summary>
    <author>
      <name>Du Steven</name>
    </author>
    <author>
      <name>Xi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04127v1</id>
    <updated>2018-08-13T09:51:46Z</updated>
    <published>2018-08-13T09:51:46Z</published>
    <title>Learning Explanations from Language Data</title>
    <summary>  PatternAttribution is a recent method, introduced in the vision domain, that
explains classifications of deep neural networks. We demonstrate that it also
generates meaningful interpretations in the language domain.
</summary>
    <author>
      <name>David Harbecke</name>
    </author>
    <author>
      <name>Robert Schwarzenberg</name>
    </author>
    <author>
      <name>Christoph Alt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in 2018 EMNLP Workshop on Analyzing and Interpreting Neural
  Networks for NLP (BlackboxNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13494v1</id>
    <updated>2019-09-30T07:38:25Z</updated>
    <published>2019-09-30T07:38:25Z</published>
    <title>A Critique of the Smooth Inverse Frequency Sentence Embeddings</title>
    <summary>  We critically review the smooth inverse frequency sentence embedding method
of Arora, Liang, and Ma (2017), and show inconsistencies in its setup,
derivation, and evaluation.
</summary>
    <author>
      <name>Aidana Karipbayeva</name>
    </author>
    <author>
      <name>Alena Sorokina</name>
    </author>
    <author>
      <name>Zhenisbek Assylbekov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, Abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03872v1</id>
    <updated>2017-06-12T23:57:48Z</updated>
    <published>2017-06-12T23:57:48Z</published>
    <title>Six Challenges for Neural Machine Translation</title>
    <summary>  We explore six challenges for neural machine translation: domain mismatch,
amount of training data, rare words, long sentences, word alignment, and beam
search. We show both deficiencies and improvements over the quality of
phrase-based statistical machine translation.
</summary>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <author>
      <name>Rebecca Knowles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; First Workshop on Neural Machine Translation, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04872v1</id>
    <updated>2017-06-15T14:01:40Z</updated>
    <published>2017-06-15T14:01:40Z</published>
    <title>Towards a theory of word order. Comment on "Dependency distance: a new
  perspective on syntactic patterns in natural language" by Haitao Liu et al</title>
    <summary>  Comment on "Dependency distance: a new perspective on syntactic patterns in
natural language" by Haitao Liu et al
</summary>
    <author>
      <name>Ramon Ferrer-i-Cancho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.plrev.2017.06.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.plrev.2017.06.019" rel="related"/>
    <link href="http://arxiv.org/abs/1706.04872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08196v1</id>
    <updated>2017-09-24T13:48:36Z</updated>
    <published>2017-09-24T13:48:36Z</published>
    <title>Identifying Phrasemes via Interlingual Association Measures -- A
  Data-driven Approach on Dependency-parsed and Word-aligned Parallel Corpora</title>
    <summary>  This is a preprint of the article "Identifying Phrasemes via Interlingual
Association Measures" that was presented in February 2016 at the LeKo (Lexical
combinations and typified speech in a multilingual context) conference in
Innsbruck.
</summary>
    <author>
      <name>Johannes Graën</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08319v1</id>
    <updated>2019-01-24T10:02:45Z</updated>
    <published>2019-01-24T10:02:45Z</published>
    <title>A review of sentiment computation methods with R packages</title>
    <summary>  Four packages in R are analyzed to carry out sentiment analysis. All packages
allow to define custom dictionaries. Just one - Sentiment R - properly accounts
for the presence of negators.
</summary>
    <author>
      <name>Maurizio Naldi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1342v1</id>
    <updated>2014-12-03T14:37:36Z</updated>
    <published>2014-12-03T14:37:36Z</published>
    <title>A perspective on the advancement of natural language processing tasks
  via topological analysis of complex networks</title>
    <summary>  Comment on "Approaching human language with complex networks" by Cong and Liu
(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).
</summary>
    <author>
      <name>Diego R. Amancio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.plrev.2014.07.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.plrev.2014.07.010" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics of Life Reviews, v. 11, p. 641-643, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.1342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02905v2</id>
    <updated>2016-03-10T19:20:47Z</updated>
    <published>2016-03-09T14:56:44Z</published>
    <title>Lexical bundles in computational linguistics academic literature</title>
    <summary>  In this study we analyzed a corpus of 8 million words academic literature
from Computational lingustics' academic literature. the lexical bundles from
this corpus are categorized based on structures and functions.
</summary>
    <author>
      <name>Adel Rahimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.02905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3813v1</id>
    <updated>2014-09-12T18:37:35Z</updated>
    <published>2014-09-12T18:37:35Z</published>
    <title>Incorporating Semi-supervised Features into Discontinuous Easy-First
  Constituent Parsing</title>
    <summary>  This paper describes adaptations for EaFi, a parser for easy-first parsing of
discontinuous constituents, to adapt it to multiple languages as well as make
use of the unlabeled data that was provided as part of the SPMRL shared task
2014.
</summary>
    <author>
      <name>Yannick Versley</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05407v1</id>
    <updated>2019-05-14T06:18:35Z</updated>
    <published>2019-05-14T06:18:35Z</published>
    <title>On the number of k-skip-n-grams</title>
    <summary>  The paper proves that the number of k-skip-n-grams for a corpus of size $L$
is $$\frac{Ln + n + k' - n^2 - nk'}{n} \cdot \binom{n-1+k'}{n-1}$$ where $k' =
\min(L - n + 1, k)$.
</summary>
    <author>
      <name>Dmytro Krasnoshtan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.05407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01699v1</id>
    <updated>2019-08-05T15:45:39Z</updated>
    <published>2019-08-05T15:45:39Z</published>
    <title>Thoth: Improved Rapid Serial Visual Presentation using Natural Language
  Processing</title>
    <summary>  Thoth is a tool designed to combine many different types of speed reading
technology. The largest insight is using natural language parsing for more
optimal rapid serial visual presentation and more effective reading
information.
</summary>
    <author>
      <name>David Awad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.01699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08971v2</id>
    <updated>2019-09-01T02:39:09Z</updated>
    <published>2019-08-23T18:31:35Z</published>
    <title>Deploying Technology to Save Endangered Languages</title>
    <summary>  Computer scientists working on natural language processing, native speakers
of endangered languages, and field linguists to discuss ways to harness
Automatic Speech Recognition, especially neural networks, to automate
annotation, speech tagging, and text parsing on endangered languages.
</summary>
    <author>
      <name>Hilaria Cruz</name>
    </author>
    <author>
      <name>Joseph Waring</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08971v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08971v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.03163v2</id>
    <updated>2021-10-26T04:32:42Z</updated>
    <published>2021-10-07T03:36:02Z</published>
    <title>Transliteration of Foreign Words in Burmese</title>
    <summary>  This manuscript provides general descriptions on transliteration of foreign
words in the Burmese language. Phenomena caused by phonetic and orthographic
issues are discussed. Based on this work, we expect to gradually establish
prescriptive guidelines to normalize the transliteration on modern words in
Burmese.
</summary>
    <author>
      <name>Chenchen Ding</name>
    </author>
    <link href="http://arxiv.org/abs/2110.03163v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03163v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00693v1</id>
    <updated>2018-11-02T01:06:14Z</updated>
    <published>2018-11-02T01:06:14Z</published>
    <title>Meta-path Augmented Response Generation</title>
    <summary>  We propose a chatbot, namely Mocha to make good use of relevant entities when
generating responses. Augmented with meta-path information, Mocha is able to
mention proper entities following the conversation flow.
</summary>
    <author>
      <name>Yanran Li</name>
    </author>
    <author>
      <name>Wenjie Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.00693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07080v2</id>
    <updated>2019-07-17T02:49:55Z</updated>
    <published>2018-11-17T02:36:07Z</published>
    <title>Bilingual Dictionary Induction for Bantu Languages</title>
    <summary>  We present a method for learning bilingual translation dictionaries between
English and Bantu languages. We show that exploiting the grammatical structure
common to Bantu languages enables bilingual dictionary induction for languages
where training data is unavailable.
</summary>
    <author>
      <name>Ndapa Nakashole</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07080v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07080v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07098v2</id>
    <updated>2019-07-17T02:45:41Z</updated>
    <published>2018-11-17T04:24:14Z</published>
    <title>Sense Perception Common Sense Relationships</title>
    <summary>  Often missing in existing knowledge bases of facts, are relationships that
encode common sense knowledge about unnamed entities. In this paper, we propose
to extract novel, common sense relationships pertaining to sense perception
concepts such as sound and smell.
</summary>
    <author>
      <name>Ndapa Nakashole</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07098v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07098v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.11816v2</id>
    <updated>2019-04-30T20:31:59Z</updated>
    <published>2019-04-26T12:57:25Z</published>
    <title>Think Again Networks and the Delta Loss</title>
    <summary>  This short paper introduces an abstraction called Think Again Networks
(ThinkNet) which can be applied to any state-dependent function (such as a
recurrent neural network).
</summary>
    <author>
      <name>Alexandre Salle</name>
    </author>
    <author>
      <name>Marcelo Prates</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">redacted experiments on language modeling due to evaluation error</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.11816v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11816v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11608v2</id>
    <updated>2019-07-26T08:16:08Z</updated>
    <published>2019-06-27T13:15:12Z</published>
    <title>Simple Natural Language Processing Tools for Danish</title>
    <summary>  This technical note describes a set of baseline tools for automatic
processing of Danish text. The tools are machine-learning based, using natural
language processing models trained over previously annotated documents. They
are maintained at ITU Copenhagen and will always be freely available.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <link href="http://arxiv.org/abs/1906.11608v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11608v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12437v1</id>
    <updated>2019-07-29T13:59:02Z</updated>
    <published>2019-07-29T13:59:02Z</published>
    <title>A Baseline Neural Machine Translation System for Indian Languages</title>
    <summary>  We present a simple, yet effective, Neural Machine Translation system for
Indian languages. We demonstrate the feasibility for multiple language pairs,
and establish a strong baseline for further research.
</summary>
    <author>
      <name>Jerin Philip</name>
    </author>
    <author>
      <name>Vinay P. Namboodiri</name>
    </author>
    <author>
      <name>C. V. Jawahar</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.13413v1</id>
    <updated>2019-12-23T18:01:32Z</updated>
    <published>2019-12-23T18:01:32Z</published>
    <title>Semantics- and Syntax-related Subvectors in the Skip-gram Embeddings</title>
    <summary>  We show that the skip-gram embedding of any word can be decomposed into two
subvectors which roughly correspond to semantic and syntactic roles of the
word.
</summary>
    <author>
      <name>Maxat Tezekbayev</name>
    </author>
    <author>
      <name>Zhenisbek Assylbekov</name>
    </author>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, Student Abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.13413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.13413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04796v1</id>
    <updated>2020-12-09T00:10:57Z</updated>
    <published>2020-12-09T00:10:57Z</published>
    <title>On an Unknown Ancestor of Burrows' Delta Measure</title>
    <summary>  This article points out some surprising similarities between a 1944 study by
Georgy Udny Yule and modern approaches to authorship attribution.
</summary>
    <author>
      <name>Petr Plechac</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in: Interf\'erences litteraires/literaire interferenties;
  forthcoming special issue on "Literature and/as (the) Digital"</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.04796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07351v1</id>
    <updated>2021-09-15T15:03:27Z</updated>
    <published>2021-09-15T15:03:27Z</published>
    <title>The ELITR ECA Corpus</title>
    <summary>  We present the ELITR ECA corpus, a multilingual corpus derived from
publications of the European Court of Auditors. We use automatic translation
together with Bleualign to identify parallel sentence pairs in all 506
translation directions. The result is a corpus comprising 264k document pairs
and 41.9M sentence pairs.
</summary>
    <author>
      <name>Philip Williams</name>
    </author>
    <author>
      <name>Barry Haddow</name>
    </author>
    <link href="http://arxiv.org/abs/2109.07351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.05160v1</id>
    <updated>2023-03-09T10:33:31Z</updated>
    <published>2023-03-09T10:33:31Z</published>
    <title>$π$-augmented pregroups and applications to linguistics</title>
    <summary>  We enrich pregroups with a mapping which allows us to locally apply precyclic
permutations to designated substrings. We prove a normalisation theorem for
such algebraic structures and briefly formalise some known applications of
pregroups to the analysis of clitic pronouns in certain natural languages.
</summary>
    <author>
      <name>Valentin Boboc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.05160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.05160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.05834v1</id>
    <updated>2023-03-10T10:21:20Z</updated>
    <published>2023-03-10T10:21:20Z</published>
    <title>An algebraic approach to translating Japanese</title>
    <summary>  We use Lambek's pregroups and the framework of compositional distributional
models of language ("DisCoCat") to study translations from Japanese to English
as pairs of functors. Adding decorations to pregroups we show how to handle
word order changes between languages.
</summary>
    <author>
      <name>Valentin Boboc</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15398/jlm.v11i1.315</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15398/jlm.v11i1.315" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, multiple diagrams and glosses</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.05834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.05834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.17347v2</id>
    <updated>2024-06-05T02:14:37Z</updated>
    <published>2023-05-27T03:01:53Z</published>
    <title>CGELBank Annotation Manual v1.1</title>
    <summary>  CGELBank is a treebank and associated tools based on a syntactic formalism
for English derived from the Cambridge Grammar of the English Language. This
document lays out the particularities of the CGELBank annotation scheme.
</summary>
    <author>
      <name>Brett Reynolds</name>
    </author>
    <author>
      <name>Nathan Schneider</name>
    </author>
    <author>
      <name>Aryaman Arora</name>
    </author>
    <link href="http://arxiv.org/abs/2305.17347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.17347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.12282v1</id>
    <updated>2023-07-23T10:23:00Z</updated>
    <published>2023-07-23T10:23:00Z</published>
    <title>Milimili. Collecting Parallel Data via Crowdsourcing</title>
    <summary>  We present a methodology for gathering a parallel corpus through
crowdsourcing, which is more cost-effective than hiring professional
translators, albeit at the expense of quality. Additionally, we have made
available experimental parallel data collected for Chechen-Russian and
Fula-English language pairs.
</summary>
    <author>
      <name>Alexander Antonov</name>
    </author>
    <link href="http://arxiv.org/abs/2307.12282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.12282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.12026v1</id>
    <updated>2024-06-28T10:58:42Z</updated>
    <published>2024-06-28T10:58:42Z</published>
    <title>The Pitfalls of Publishing in the Age of LLMs: Strange and Surprising
  Adventures with a High-Impact NLP Journal</title>
    <summary>  We show the fraught side of the academic publishing realm and illustrate it
through a recent case study with an NLP journal.
</summary>
    <author>
      <name>Rakesh M. Verma</name>
    </author>
    <author>
      <name>Nachum Dershowitz</name>
    </author>
    <link href="http://arxiv.org/abs/2407.12026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.04589v1</id>
    <updated>2024-10-06T19:01:04Z</updated>
    <published>2024-10-06T19:01:04Z</published>
    <title>Towards the first UD Treebank of Spoken Italian: the KIParla forest</title>
    <summary>  The present project endeavors to enrich the linguistic resources available
for Italian by constructing a Universal Dependencies treebank for the KIParla
corpus (Mauri et al., 2019, Ballar\`e et al., 2020), an existing and well known
resource for spoken Italian.
</summary>
    <author>
      <name>Ludovica Pannitto</name>
    </author>
    <link href="http://arxiv.org/abs/2410.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.07623v1</id>
    <updated>2024-11-12T08:10:54Z</updated>
    <published>2024-11-12T08:10:54Z</published>
    <title>Annotating Constructions with UD: the experience of the Italian
  Constructicon</title>
    <summary>  The paper descirbes a first attempt of linking the Italian constructicon to
UD resources
</summary>
    <author>
      <name>Ludovica Pannitto</name>
    </author>
    <author>
      <name>Beatrice Bernasconi</name>
    </author>
    <author>
      <name>Lucia Busso</name>
    </author>
    <author>
      <name>Flavio Pisciotta</name>
    </author>
    <author>
      <name>Giulia Rambelli</name>
    </author>
    <author>
      <name>Francesca Masini</name>
    </author>
    <link href="http://arxiv.org/abs/2411.07623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.07623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17305v1</id>
    <updated>2025-02-24T16:41:38Z</updated>
    <published>2025-02-24T16:41:38Z</published>
    <title>`Generalization is hallucination' through the lens of tensor completions</title>
    <summary>  In this short position paper, we introduce tensor completions and artifacts
and make the case that they are a useful theoretical framework for
understanding certain types of hallucinations and generalizations in language
models.
</summary>
    <author>
      <name>Liang Ze Wong</name>
    </author>
    <link href="http://arxiv.org/abs/2502.17305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3119v2</id>
    <updated>2011-07-23T00:14:36Z</updated>
    <published>2011-07-15T18:06:37Z</published>
    <title>Experimenting with Transitive Verbs in a DisCoCat</title>
    <summary>  Formal and distributional semantic models offer complementary benefits in
modeling meaning. The categorical compositional distributional (DisCoCat) model
of meaning of Coecke et al. (arXiv:1003.4394v1 [cs.CL]) combines aspected of
both to provide a general framework in which meanings of words, obtained
distributionally, are composed using methods from the logical setting to form
sentence meaning. Concrete consequences of this general abstract setting and
applications to empirical data are under active study (Grefenstette et al.,
arxiv:1101.0309; Grefenstette and Sadrzadeh, arXiv:1106.4058v1 [cs.CL]). . In
this paper, we extend this study by examining transitive verbs, represented as
matrices in a DisCoCat. We discuss three ways of constructing such matrices,
and evaluate each method in a disambiguation task developed by Grefenstette and
Sadrzadeh (arXiv:1106.4058v1 [cs.CL]).
</summary>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, to be presented at GEMS 2011, as part of EMNLP'11 workshops</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.3119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.3; H.3.1; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902027v1</id>
    <updated>1999-02-16T15:34:02Z</updated>
    <published>1999-02-16T15:34:02Z</published>
    <title>Autocatalytic Theory of Meaning</title>
    <summary>  Recently it has been argued that autocatalytic theory could be applied to the
origin of culture. Here possible application to a theory of meaning in the
philosophy of language, called radical interpretation, is commented upon and
compared to previous applications.
</summary>
    <author>
      <name>Mark D. Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no diagrams, LaTex2e</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PSYCHOLOQUY.99.10.014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9902027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; J.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902030v1</id>
    <updated>1999-02-25T14:41:32Z</updated>
    <published>1999-02-25T14:41:32Z</published>
    <title>Is Word Sense Disambiguation just one more NLP task?</title>
    <summary>  This paper compares the tasks of part-of-speech (POS) tagging and
word-sense-tagging or disambiguation (WSD), and argues that the tasks are not
related by fineness of grain or anything like that, but are quite different
kinds of task, particularly becuase there is nothing in POS corresponding to
sense novelty. The paper also argues for the reintegration of sub-tasks that
are being separated for evaluation
</summary>
    <author>
      <name>Yorick Wilks</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9902030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906009v1</id>
    <updated>1999-06-06T17:36:34Z</updated>
    <published>1999-06-06T17:36:34Z</published>
    <title>Cascaded Markov Models</title>
    <summary>  This paper presents a new approach to partial parsing of context-free
structures. The approach is based on Markov Models. Each layer of the resulting
structure is represented by its own Markov Model, and output of a lower layer
is passed as input to the next higher layer. An empirical evaluation of the
method yields very good results for NP/PP chunking of German newspaper texts.
</summary>
    <author>
      <name>Thorsten Brants</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of EACL-99, Bergen, Norway</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9906009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906027v1</id>
    <updated>1999-06-25T11:44:42Z</updated>
    <published>1999-06-25T11:44:42Z</published>
    <title>Human-Computer Conversation</title>
    <summary>  The article surveys a little of the history of the technology, sets out the
main current theoretical approaches in brief, and discusses the on-going
opposition between theoretical and empirical approaches. It illustrates the
situation with some discussion of CONVERSE, a system that won the Loebner prize
in 1997 and which displays features of both approaches.
</summary>
    <author>
      <name>Yorick Wilks</name>
    </author>
    <author>
      <name>Roberta Catizone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9906027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7;H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906034v1</id>
    <updated>1999-06-30T23:06:09Z</updated>
    <published>1999-06-30T23:06:09Z</published>
    <title>A Unified Example-Based and Lexicalist Approach to Machine Translation</title>
    <summary>  We present an approach to Machine Translation that combines the ideas and
methodologies of the Example-Based and Lexicalist theoretical frameworks. The
approach has been implemented in a multilingual Machine Translation system.
</summary>
    <author>
      <name>Davide Turcato</name>
    </author>
    <author>
      <name>Paul McFetridge</name>
    </author>
    <author>
      <name>Fred Popowich</name>
    </author>
    <author>
      <name>Janine Toole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, to be presented at the 8th International Conference on
  Theoretical and Methodological Issues in Machine Translation (TMI-99)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9906034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907004v2</id>
    <updated>1999-10-14T00:31:39Z</updated>
    <published>1999-07-06T01:44:00Z</published>
    <title>MAP Lexicon is useful for segmentation and word discovery in
  child-directed speech</title>
    <summary>  Because of rather fundamental changes to the underlying model proposed in the
paper, it has been withdrawn from the archive.
</summary>
    <author>
      <name>Anand Venkataraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Because of rather fundamental changes to the underlying model
  proposed in the paper, it has been withdrawn from the archive.</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9907004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910011v1</id>
    <updated>1999-10-13T03:25:33Z</updated>
    <published>1999-10-13T03:25:33Z</published>
    <title>A statistical model for word discovery in child directed speech</title>
    <summary>  A statistical model for segmentation and word discovery in child directed
speech is presented. An incremental unsupervised learning algorithm to infer
word boundaries based on this model is described and results of empirical tests
showing that the algorithm is competitive with other models that have been used
for similar tasks are also presented.
</summary>
    <author>
      <name>Anand Venkataraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pgs, 10 figs</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9910011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9911006v1</id>
    <updated>1999-11-15T05:48:03Z</updated>
    <published>1999-11-15T05:48:03Z</published>
    <title>Question Answering System Using Syntactic Information</title>
    <summary>  Question answering task is now being done in TREC8 using English documents.
We examined question answering task in Japanese sentences. Our method selects
the answer by matching the question sentence with knowledge-based data written
in natural language. We use syntactic information to obtain highly accurate
answers.
</summary>
    <author>
      <name>M. Murata</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRL</arxiv:affiliation>
    </author>
    <author>
      <name>M. Utiyama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRL</arxiv:affiliation>
    </author>
    <author>
      <name>H. Isahara</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRL</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 0 figures. Computation and Language</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9911006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9911006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001012v1</id>
    <updated>2000-01-18T23:19:22Z</updated>
    <published>2000-01-18T23:19:22Z</published>
    <title>Measures of Distributional Similarity</title>
    <summary>  We study distributional similarity measures for the purpose of improving
probability estimation for unseen cooccurrences. Our contributions are
three-fold: an empirical comparison of a broad range of measures; a
classification of similarity functions based on the information that they
incorporate; and the introduction of a novel function that is superior at
evaluating potential proxy distributions.
</summary>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">37th Annual Meeting of the ACL, 1999, pp. 25-32</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0001012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003083v1</id>
    <updated>2000-03-30T16:56:02Z</updated>
    <published>2000-03-30T16:56:02Z</published>
    <title>Advances in domain independent linear text segmentation</title>
    <summary>  This paper describes a method for linear text segmentation which is twice as
accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).
Inter-sentence similarity is replaced by rank in the local context. Boundary
locations are discovered by divisive clustering.
</summary>
    <author>
      <name>Freddy Y. Y. Choi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Manchester</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures. To appear in Proceedings of NAACL00, Seattle.
  Software and experiment packages available from author's homepage:
  http://www.cs.man.ac.uk/~choif</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009008v1</id>
    <updated>2000-09-18T12:08:54Z</updated>
    <published>2000-09-18T12:08:54Z</published>
    <title>Introduction to the CoNLL-2000 Shared Task: Chunking</title>
    <summary>  We describe the CoNLL-2000 shared task: dividing text into syntactically
related non-overlapping groups of words, so-called text chunking. We give
background information on the data sets, present a general overview of the
systems that have taken part in the shared task and briefly discuss their
performance.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <author>
      <name>Sabine Buchholz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011040v1</id>
    <updated>2000-11-24T23:51:21Z</updated>
    <published>2000-11-24T23:51:21Z</published>
    <title>Do All Fragments Count?</title>
    <summary>  We aim at finding the minimal set of fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street
Journal treebank show that counts of almost arbitrary fragments within parse
trees are important, leading to improved parse accuracy over previous models
tested on this treebank. We isolate a number of dependency relations which
previous models neglect but which contribute to higher parse accuracy.
</summary>
    <author>
      <name>Rens Bod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report COMP-11-12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0011040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103007v1</id>
    <updated>2001-03-08T06:30:27Z</updated>
    <published>2001-03-08T06:30:27Z</published>
    <title>Two-parameter Model of Word Length "Language - Genre"</title>
    <summary>  A two-parameter model of word length measured by the number of syllables
comprising it is proposed. The first parameter is dependent on language type,
the second one - on text genre and reflects the degree of completion of
synergetic processes of language optimization.
</summary>
    <author>
      <name>Victor Kromer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure. In Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105035v1</id>
    <updated>2001-05-30T03:55:23Z</updated>
    <published>2001-05-30T03:55:23Z</published>
    <title>Historical Dynamics of Lexical System as Random Walk Process</title>
    <summary>  It is offered to consider word meanings changes in diachrony as
semicontinuous random walk with reflecting and swallowing screens. The basic
characteristics of word life cycle are defined. Verification of the model has
been realized on the data of Russian words distribution on various age periods.
</summary>
    <author>
      <name>Victor Kromer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 table, 3 figures. Submitted to conference "Language in
  Sinchrony and Diachrony", to be held in Petrozavodsk State Pedagogical
  University (Russia), 15-17 October, 2001. (In Russian)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0105035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107016v1</id>
    <updated>2001-07-15T12:51:01Z</updated>
    <published>2001-07-15T12:51:01Z</published>
    <title>Introduction to the CoNLL-2001 Shared Task: Clause Identification</title>
    <summary>  We describe the CoNLL-2001 shared task: dividing text into clauses. We give
background information on the data sets, present a general overview of the
systems that have taken part in the shared task and briefly discuss their
performance.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <author>
      <name>Herve Dejean</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Walter Daelemans and Remi Zajac (eds.), Proceedings of
  CoNLL-2001, Toulouse, France, 2001, pp. 53-57</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0107016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111064v1</id>
    <updated>2001-11-30T20:30:52Z</updated>
    <published>2001-11-30T20:30:52Z</published>
    <title>A procedure for unsupervised lexicon learning</title>
    <summary>  We describe an incremental unsupervised procedure to learn words from
transcribed continuous speech. The algorithm is based on a conservative and
traditional statistical model, and results of empirical tests show that it is
competitive with other algorithms that have been proposed recently for this
task.
</summary>
    <author>
      <name>Anand Venkataraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Expanded version of this paper appears in Computational Linguistics
  27(3)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the eighteenth international conference on machine
  learning, ICML-01, pp.569--576, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111065v1</id>
    <updated>2001-11-30T20:40:50Z</updated>
    <published>2001-11-30T20:40:50Z</published>
    <title>A Statistical Model for Word Discovery in Transcribed Speech</title>
    <summary>  A statistical model for segmentation and word discovery in continuous speech
is presented. An incremental unsupervised learning algorithm to infer word
boundaries based on this model is described. Results of empirical tests showing
that the algorithm is competitive with other models that have been used for
similar tasks are also presented.
</summary>
    <author>
      <name>Anand Venkataraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Expanded version of ICML-01 paper (pp.569--576)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Linguistics, 27(3), pp.352--372, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112005v1</id>
    <updated>2001-12-05T05:56:13Z</updated>
    <published>2001-12-05T05:56:13Z</published>
    <title>Universal Model for Paraphrasing -- Using Transformation Based on a
  Defined Criteria --</title>
    <summary>  This paper describes a universal model for paraphrasing that transforms
according to defined criteria. We showed that by using different criteria we
could construct different kinds of paraphrasing systems including one for
answering questions, one for compressing sentences, one for polishing up, and
one for transforming written language to spoken language.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Computation and Language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NLPRS'2001, Workshop on Automatic Paraphrasing: Theories and
  Applications</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0112005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204025v1</id>
    <updated>2002-04-11T11:22:43Z</updated>
    <published>2002-04-11T11:22:43Z</published>
    <title>Phonology</title>
    <summary>  Phonology is the systematic study of the sounds used in language, their
internal structure, and their composition into syllables, words and phrases.
Computational phonology is the application of formal and computational
techniques to the representation and processing of phonological information.
This chapter will present the fundamentals of descriptive phonology along with
a brief overview of computational phonology.
</summary>
    <author>
      <name>Steven Bird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Ruslan Mitkov (ed) (2002). Oxford Handbook of Computational
  Linguistics</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205026v1</id>
    <updated>2002-05-17T08:24:56Z</updated>
    <published>2002-05-17T08:24:56Z</published>
    <title>Monads for natural language semantics</title>
    <summary>  Accounts of semantic phenomena often involve extending types of meanings and
revising composition rules at the same time. The concept of monads allows many
such accounts -- for intensionality, variable binding, quantification and focus
-- to be stated uniformly and compositionally.
</summary>
    <author>
      <name>Chung-chieh Shan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Harvard University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2001 European Summer School in Logic, Language
  and Information student session, ed. Kristina Striegnitz, 285-298</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0205026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; D.3.1; F.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205067v1</id>
    <updated>2002-05-27T18:42:10Z</updated>
    <published>2002-05-27T18:42:10Z</published>
    <title>Evaluating the Effectiveness of Ensembles of Decision Trees in
  Disambiguating Senseval Lexical Samples</title>
    <summary>  This paper presents an evaluation of an ensemble--based system that
participated in the English and Spanish lexical sample tasks of Senseval-2. The
system combines decision trees of unigrams, bigrams, and co--occurrences into a
single classifier. The analysis is extended to include the Senseval-1 data.
</summary>
    <author>
      <name>Ted Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the Proceedings of the ACL-02 Workshop on Word Sense
  Disambiguation: Recent Successes and Future Directions, July 11, 2002,
  Philadelphia, PA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205068v1</id>
    <updated>2002-05-27T18:49:01Z</updated>
    <published>2002-05-27T18:49:01Z</published>
    <title>Assessing System Agreement and Instance Difficulty in the Lexical Sample
  Tasks of Senseval-2</title>
    <summary>  This paper presents a comparative evaluation among the systems that
participated in the Spanish and English lexical sample tasks of Senseval-2. The
focus is on pairwise comparisons among systems to assess the degree to which
they agree, and on measuring the difficulty of the test instances included in
these tasks.
</summary>
    <author>
      <name>Ted Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the Proceedings of the ACL-02 Workshop on Word Sense
  Disambiguation: Recent Successes and Future Directions, July 11, 2002,
  Philadelphia, PA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207005v1</id>
    <updated>2002-07-03T11:54:21Z</updated>
    <published>2002-07-03T11:54:21Z</published>
    <title>Efficient Deep Processing of Japanese</title>
    <summary>  We present a broad coverage Japanese grammar written in the HPSG formalism
with MRS semantics. The grammar is created for use in real world applications,
such that robustness and performance issues play an important role. It is
connected to a POS tagging and word segmentation tool. This grammar is being
developed in a multilingual context, requiring MRS structures that are easily
comparable across languages.
</summary>
    <author>
      <name>Melanie Siegel</name>
    </author>
    <author>
      <name>Emily M. Bender</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 19th International Conference on Computational
  Linguistics</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0207005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0209010v1</id>
    <updated>2002-09-05T19:03:06Z</updated>
    <published>2002-09-05T19:03:06Z</published>
    <title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named
  Entity Recognition</title>
    <summary>  We describe the CoNLL-2002 shared task: language-independent named entity
recognition. We give background information on the data sets and the evaluation
method, present a general overview of the systems that have taken part in the
task and discuss their performance.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dan Roth and Antal van den Bosch (eds.), Proceedings of
  CoNLL-2002, Taipei, Taiwan, 2002, pp. 155-158</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0209010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0209010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0302032v1</id>
    <updated>2003-02-22T23:37:26Z</updated>
    <published>2003-02-22T23:37:26Z</published>
    <title>Empirical Methods for Compound Splitting</title>
    <summary>  Compounded words are a challenge for NLP applications such as machine
translation (MT). We introduce methods to learn splitting rules from
monolingual and parallel corpora. We evaluate them against a gold standard and
measure their impact on performance of statistical MT systems. Results show
accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a
German-English noun phrase translation task.
</summary>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures. Published at EACL 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0302032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0302032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306050v1</id>
    <updated>2003-06-12T12:35:00Z</updated>
    <published>2003-06-12T12:35:00Z</published>
    <title>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named
  Entity Recognition</title>
    <summary>  We describe the CoNLL-2003 shared task: language-independent named entity
recognition. We give background information on the data sets (English and
German) and the evaluation method, present a general overview of the systems
that have taken part in the task and discuss their performance.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <author>
      <name>Fien De Meulder</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of CoNLL-2003, Edmonton, Canada, 2003, pp. 142-147</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0306050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307030v1</id>
    <updated>2003-07-11T15:42:51Z</updated>
    <published>2003-07-11T15:42:51Z</published>
    <title>Parsing and Generation with Tabulation and Compilation</title>
    <summary>  The standard tabulation techniques for logic programming presuppose fixed
order of computation. Some data-driven control should be introduced in order to
deal with diverse contexts. The present paper describes a data-driven method of
constraint transformation with a sort of compilation which subsumes
accessibility check and last-call optimization, which characterize standard
natural-language parsing techniques, semantic-head-driven generation, etc.
</summary>
    <author>
      <name>Koiti Hasida</name>
    </author>
    <author>
      <name>Takashi Miyata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, Proceedings of TAPD'98, pp.26-35</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; D.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311033v1</id>
    <updated>2003-11-21T19:48:17Z</updated>
    <published>2003-11-21T19:48:17Z</published>
    <title>The Rank-Frequency Analysis for the Functional Style Corpora in the
  Ukrainian Language</title>
    <summary>  We use the rank-frequency analysis for the estimation of Kernel Vocabulary
size within specific corpora of Ukrainian. The extrapolation of high-rank
behaviour is utilized for estimation of the total vocabulary size.
</summary>
    <author>
      <name>Solomija N. Buk</name>
    </author>
    <author>
      <name>Andrij A. Rovenchak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/0929617042000314912</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/0929617042000314912" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Quantitative Linguistics, Vol. 11, No. 3, P. 161-171
  (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0311033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404009v1</id>
    <updated>2004-04-05T11:51:43Z</updated>
    <published>2004-04-05T11:51:43Z</published>
    <title>Tabular Parsing</title>
    <summary>  This is a tutorial on tabular parsing, on the basis of tabulation of
nondeterministic push-down automata. Discussed are Earley's algorithm, the
Cocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parse
trees, and further issues.
</summary>
    <author>
      <name>Mark-Jan Nederhof</name>
    </author>
    <author>
      <name>Giorgio Satta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 14 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">M.-J. Nederhof and G. Satta. Tabular Parsing. In C. Martin-Vide,
  V. Mitrana, and G. Paun, editors, Formal Languages and Applications, Studies
  in Fuzziness and Soft Computing 148, pages 529-549. Springer, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405037v1</id>
    <updated>2004-05-10T16:05:23Z</updated>
    <published>2004-05-10T16:05:23Z</published>
    <title>A Probabilistic Model of Machine Translation</title>
    <summary>  A probabilistic model for computer-based generation of a machine translation
system on the basis of English-Russian parallel text corpora is suggested. The
model is trained using parallel text corpora with pre-aligned source and target
sentences. The training of the model results in a bilingual dictionary of words
and "word blocks" with relevant translation probability.
</summary>
    <author>
      <name>G. E. Miram</name>
    </author>
    <author>
      <name>V. K. Petrov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0405037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406054v1</id>
    <updated>2004-06-28T10:25:22Z</updated>
    <published>2004-06-28T10:25:22Z</published>
    <title>Building a linguistic corpus from bee dance data</title>
    <summary>  This paper discusses the problems and possibility of collecting bee dance
data in a linguistic \textit{corpus} and use linguistic instruments such as
Zipf's law and entropy statistics to decide on the question whether the dance
carries information of any kind. We describe this against the historical
background of attempts to analyse non-human communication systems.
</summary>
    <author>
      <name>J. J. Paijmans</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the first international congres of bioinformatics,
  Havana (Cuba), 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0406054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407046v1</id>
    <updated>2004-07-19T11:57:42Z</updated>
    <published>2004-07-19T11:57:42Z</published>
    <title>A Bimachine Compiler for Ranked Tagging Rules</title>
    <summary>  This paper describes a novel method of compiling ranked tagging rules into a
deterministic finite-state device called a bimachine. The rules are formulated
in the framework of regular rewrite operations and allow unrestricted regular
expressions in both left and right rule contexts. The compiler is illustrated
by an application within a speech synthesis system.
</summary>
    <author>
      <name>Wojciech Skut</name>
    </author>
    <author>
      <name>Stefan Ulrich</name>
    </author>
    <author>
      <name>Kathrine Hammervold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures Proceedings of COLING 2004 (to appear)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0407046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7;F.4.2;F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501078v1</id>
    <updated>2005-01-26T22:43:17Z</updated>
    <published>2005-01-26T22:43:17Z</published>
    <title>Multi-document Biography Summarization</title>
    <summary>  In this paper we describe a biography summarization system using sentence
classification and ideas from information retrieval. Although the individual
techniques are not new, assembling and applying them to generate multi-document
biographies is new. Our system was evaluated in DUC2004. It is among the top
performers in task 5-short summaries focused by person questions.
</summary>
    <author>
      <name>Liang Zhou</name>
    </author>
    <author>
      <name>Miruna Ticrea</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of EMNLP, pp. 434-441, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0501078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504022v1</id>
    <updated>2005-04-06T20:04:55Z</updated>
    <published>2005-04-06T20:04:55Z</published>
    <title>A Matter of Opinion: Sentiment Analysis and Business Intelligence
  (position paper)</title>
    <summary>  A general-audience introduction to the area of "sentiment analysis", the
computational treatment of subjective, opinion-oriented language (an example
application is determining whether a review is "thumbs up" or "thumbs down").
Some challenges, applications to business-intelligence tasks, and potential
future directions are described.
</summary>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the IBM Faculty Summit on the Architecture of
  On-Demand Business, May 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504074v1</id>
    <updated>2005-04-15T20:10:53Z</updated>
    <published>2005-04-15T20:10:53Z</published>
    <title>Metalinguistic Information Extraction for Terminology</title>
    <summary>  This paper describes and evaluates the Metalinguistic Operation Processor
(MOP) system for automatic compilation of metalinguistic information from
technical and scientific documents. This system is designed to extract
non-standard terminological resources that we have called Metalinguistic
Information Databases (or MIDs), in order to help update changing glossaries,
knowledge bases and ontologies, as well as to reflect the metastable dynamics
of special-domain knowledge.
</summary>
    <author>
      <name>Carlos Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CompuTerm 2004, COLING. Geneve</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0510015v1</id>
    <updated>2005-10-05T14:23:19Z</updated>
    <published>2005-10-05T14:23:19Z</published>
    <title>Word sense disambiguation criteria: a systematic study</title>
    <summary>  This article describes the results of a systematic in-depth study of the
criteria used for word sense disambiguation. Our study is based on 60 target
words: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line
with some practices in the field. For example, we show that omitting
non-content words decreases performance and that bigrams yield better results
than unigrams.
</summary>
    <author>
      <name>Laurent Audibert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DELIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">20th International Conference on Computational Linguistics
  (COLING-2004) (2004) pp. 910-916</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0510015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0510015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605076v1</id>
    <updated>2006-05-17T17:12:25Z</updated>
    <published>2006-05-17T17:12:25Z</published>
    <title>Numeration-automatic sequences</title>
    <summary>  We present a base class of automata that induce a numeration system and we
give an algorithm to give the n-th word in the language of the automaton when
the expansion of n in the induced numeration system is feeded to the automaton.
Furthermore we give some algorithms for reverse reading of this expansion and a
way to combine automata to other automata having the same properties.
</summary>
    <author>
      <name>J. F. J. Laros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609043v1</id>
    <updated>2006-09-08T14:01:57Z</updated>
    <published>2006-09-08T14:01:57Z</published>
    <title>Challenging the principle of compositionality in interpreting natural
  language texts</title>
    <summary>  The paper aims at emphasizing that, even relaxed, the hypothesis of
compositionality has to face many problems when used for interpreting natural
language texts. Rather than fixing these problems within the compositional
framework, we believe that a more radical change is necessary, and propose
another approach.
</summary>
    <author>
      <name>Françoise Gayral</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Kayser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>François Lévy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">conference on Compositionality, Concepts and Cognition, Allemagne
  (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0609043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3269v1</id>
    <updated>2007-07-22T15:24:48Z</updated>
    <published>2007-07-22T15:24:48Z</published>
    <title>International Standard for a Linguistic Annotation Framework</title>
    <summary>  This paper describes the Linguistic Annotation Framework under development
within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to
serve as a basis for harmonizing existing language resources as well as
developing new ones.
</summary>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Nancy Ide</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Language Engineering 10, 3-4 (09/2004) 211-225</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.3269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2444v1</id>
    <updated>2007-11-15T15:39:48Z</updated>
    <published>2007-11-15T15:39:48Z</published>
    <title>Proof nets for display logic</title>
    <summary>  This paper explores several extensions of proof nets for the Lambek calculus
in order to handle the different connectives of display logic in a natural way.
The new proof net calculus handles some recent additions to the Lambek
vocabulary such as Galois connections and Grishin interactions. It concludes
with an exploration of the generative capacity of the Lambek-Grishin calculus,
presenting an embedding of lexicalized tree adjoining grammars into the
Lambek-Grishin calculus.
</summary>
    <author>
      <name>Richard Moot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, Labri</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0711.2444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2904v1</id>
    <updated>2008-08-21T10:54:54Z</updated>
    <published>2008-08-21T10:54:54Z</published>
    <title>Investigation of the Zipf-plot of the extinct Meroitic language</title>
    <summary>  The ancient and extinct language Meroitic is investigated using Zipf's Law.
In particular, since Meroitic is still undeciphered, the Zipf law analysis
allows us to assess the quality of current texts and possible avenues for
future investigation using statistical techniques.
</summary>
    <author>
      <name>Reginald D. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Glottometrics 15, 2007, 53-61</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0808.2904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.3889v1</id>
    <updated>2008-08-28T11:59:34Z</updated>
    <published>2008-08-28T11:59:34Z</published>
    <title>Open architecture for multilingual parallel texts</title>
    <summary>  Multilingual parallel texts (abbreviated to parallel texts) are linguistic
versions of the same content ("translations"); e.g., the Maastricht Treaty in
English and Spanish are parallel texts. This document is about creating an open
architecture for the whole Authoring, Translation and Publishing Chain
(ATP-chain) for the processing of parallel texts.
</summary>
    <author>
      <name>M. T. Carrasco Benitez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages - for comments to the author and follow-ups go to
  http://dragoman.org/par</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.3889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.3889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.0200v1</id>
    <updated>2008-10-01T15:53:36Z</updated>
    <published>2008-10-01T15:53:36Z</published>
    <title>Distribution of complexities in the Vai script</title>
    <summary>  In the paper, we analyze the distribution of complexities in the Vai script,
an indigenous syllabic writing system from Liberia. It is found that the
uniformity hypothesis for complexities fails for this script. The models using
Poisson distribution for the number of components and hyper-Poisson
distribution for connections provide good fits in the case of the Vai script.
</summary>
    <author>
      <name>Andrij Rovenchak</name>
    </author>
    <author>
      <name>Ján Mačutek</name>
    </author>
    <author>
      <name>Charles Riley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Glottometrics 18, 1-12 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0810.0200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.0200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3416v1</id>
    <updated>2008-10-19T17:34:33Z</updated>
    <published>2008-10-19T17:34:33Z</published>
    <title>Text as Statistical Mechanics Object</title>
    <summary>  In this article we present a model of human written text based on statistical
mechanics approach by deriving the potential energy for different parts of the
text using large text corpus. We have checked the results numerically and found
that the specific heat parameter effectively separates the closed class words
from the specific terms used in the text.
</summary>
    <author>
      <name>K. Koroutchev</name>
    </author>
    <author>
      <name>E. Korutcheva</name>
    </author>
    <link href="http://arxiv.org/abs/0810.3416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5114v1</id>
    <updated>2009-06-28T02:32:53Z</updated>
    <published>2009-06-28T02:32:53Z</published>
    <title>Non-Parametric Bayesian Areal Linguistics</title>
    <summary>  We describe a statistical model over linguistic areas and phylogeny.
  Our model recovers known areas and identifies a plausible hierarchy of areal
features. The use of areas improves genetic reconstruction of languages both
qualitatively and quantitatively according to a variety of metrics. We model
linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman's
coalescent.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Conference of the North American Association
  for Computational Linguistics, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.5114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.4431v1</id>
    <updated>2009-08-30T23:20:41Z</updated>
    <published>2009-08-30T23:20:41Z</published>
    <title>An OLAC Extension for Dravidian Languages</title>
    <summary>  OLAC was founded in 2000 for creating online databases of language resources.
This paper intends to review the bottom-up distributed character of the project
and proposes an extension of the architecture for Dravidian languages. An
ontological structure is considered for effective natural language processing
(NLP) and its advantages over statistical methods are reviewed
</summary>
    <author>
      <name>B Prabhulla Chandran Pillai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.4431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.4431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1147v1</id>
    <updated>2009-09-07T06:29:51Z</updated>
    <published>2009-09-07T06:29:51Z</published>
    <title>Empowering OLAC Extension using Anusaaraka and Effective text processing
  using Double Byte coding</title>
    <summary>  The paper reviews the hurdles while trying to implement the OLAC extension
for Dravidian / Indian languages. The paper further explores the possibilities
which could minimise or solve these problems. In this context, the Chinese
system of text processing and the anusaaraka system are scrutinised.
</summary>
    <author>
      <name>B Prabhulla Chandran Pillai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2881v2</id>
    <updated>2009-12-16T08:02:57Z</updated>
    <published>2009-12-15T13:30:14Z</published>
    <title>Representing human and machine dictionaries in Markup languages</title>
    <summary>  In this chapter we present the main issues in representing machine readable
dictionaries in XML, and in particular according to the Text Encoding
Dictionary (TEI) guidelines.
</summary>
    <author>
      <name>Lothar Lemnitzer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Saclay - Ile de France, IDSL</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Saclay - Ile de France, IDSL</arxiv:affiliation>
    </author>
    <author>
      <name>Andreas Witt</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dictionaries. An International Encyclopedia of Lexicography.
  Supplementary volume: Recent developments with special focus on computational
  lexicography, Ulrich Heid (Ed.) (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.2881v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2881v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0904v1</id>
    <updated>2010-02-04T06:30:14Z</updated>
    <published>2010-02-04T06:30:14Z</published>
    <title>On Event Structure in the Torn Dress</title>
    <summary>  Using Pustejovsky's "The Syntax of Event Structure" and Fong's "On Mending a
Torn Dress" we give a glimpse of a Pustejovsky-like analysis to some example
sentences in Fong. We attempt to give a framework for semantics to the noun
phrases and adverbs as appropriate as well as the lexical entries for all words
in the examples and critique both papers in light of our findings and
difficulties.
</summary>
    <author>
      <name>Serguei A. Mokhov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages; a 2003 report</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.0904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4181v1</id>
    <updated>2010-04-23T17:00:18Z</updated>
    <published>2010-04-23T17:00:18Z</published>
    <title>Displacement Calculus</title>
    <summary>  The Lambek calculus provides a foundation for categorial grammar in the form
of a logic of concatenation. But natural language is characterized by
dependencies which may also be discontinuous. In this paper we introduce the
displacement calculus, a generalization of Lambek calculus, which preserves its
good proof-theoretic properties while embracing discontinuiity and subsuming
it. We illustrate linguistic applications and prove Cut-elimination, the
subformula property, and decidability
</summary>
    <author>
      <name>Glyn Morrill</name>
    </author>
    <author>
      <name>Oriol Valentín</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3902v1</id>
    <updated>2010-05-21T08:12:12Z</updated>
    <published>2010-05-21T08:12:12Z</published>
    <title>Morphonette: a morphological network of French</title>
    <summary>  This paper describes in details the first version of Morphonette, a new
French morphological resource and a new radically lexeme-based method of
morphological analysis. This research is grounded in a paradigmatic conception
of derivational morphology where the morphological structure is a structure of
the entire lexicon and not one of the individual words it contains. The
discovery of this structure relies on a measure of morphological similarity
between words, on formal analogy and on the properties of two morphological
paradigms:
</summary>
    <author>
      <name>Nabil Hathout</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLLE</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1005.3902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5016v1</id>
    <updated>2011-08-25T06:07:06Z</updated>
    <published>2011-08-25T06:07:06Z</published>
    <title>Une analyse basée sur la S-DRT pour la modélisation de dialogues
  pathologiques</title>
    <summary>  In this article, we present a corpus of dialogues between a schizophrenic
speaker and an interlocutor who drives the dialogue. We had identified specific
discontinuities for paranoid schizophrenics. We propose a modeling of these
discontinuities with S-DRT (its pragmatic part)
</summary>
    <author>
      <name>Maxime Amblard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Musiol Michel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LABPSYLOR</arxiv:affiliation>
    </author>
    <author>
      <name>Rebuschi Manuel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LHSP</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Traitement Automatique des Langues, Montpellier : France (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.6553v1</id>
    <updated>2011-11-28T19:17:57Z</updated>
    <published>2011-11-28T19:17:57Z</published>
    <title>Exploring Twitter Hashtags</title>
    <summary>  Twitter messages often contain so-called hashtags to denote keywords related
to them. Using a dataset of 29 million messages, I explore relations among
these hashtags with respect to co-occurrences. Furthermore, I present an
attempt to classify hashtags into five intuitive classes, using a
machine-learning approach. The overall outcome is an interactive Web
application to explore Twitter hashtags.
</summary>
    <author>
      <name>Jan Pöschko</name>
    </author>
    <link href="http://arxiv.org/abs/1111.6553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.6553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1192v1</id>
    <updated>2012-01-05T15:22:05Z</updated>
    <published>2012-01-05T15:22:05Z</published>
    <title>Formalization of semantic network of image constructions in electronic
  content</title>
    <summary>  A formal theory based on a binary operator of directional associative
relation is constructed in the article and an understanding of an associative
normal form of image constructions is introduced. A model of a commutative
semigroup, which provides a presentation of a sentence as three components of
an interrogative linguistic image construction, is considered.
</summary>
    <author>
      <name>Oleg Bisikalo</name>
    </author>
    <author>
      <name>Irina Kravchuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.1192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6735v1</id>
    <updated>2012-06-28T15:43:57Z</updated>
    <published>2012-06-28T15:43:57Z</published>
    <title>Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing</title>
    <summary>  We present a novel technique to remove spurious ambiguity from transition
systems for dependency parsing. Our technique chooses a canonical sequence of
transition operations (computation) for a given dependency tree. Our technique
can be applied to a large class of bottom-up transition systems, including for
instance Nivre (2004) and Attardi (2006).
</summary>
    <author>
      <name>Shay B. Cohen</name>
    </author>
    <author>
      <name>Carlos Gómez-Rodríguez</name>
    </author>
    <author>
      <name>Giorgio Satta</name>
    </author>
    <link href="http://arxiv.org/abs/1206.6735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2265v1</id>
    <updated>2012-07-10T08:48:13Z</updated>
    <published>2012-07-10T08:48:13Z</published>
    <title>Challenges for Distributional Compositional Semantics</title>
    <summary>  This paper summarises the current state-of-the art in the study of
compositionality in distributional semantics, and major challenges for this
area. We single out generalised quantifiers and intensional semantics as areas
on which to focus attention for the development of the theory. Once suitable
theories have been developed, algorithms will be needed to apply the theory to
tasks. Evaluation is a major problem; we single out application to recognising
textual entailment and machine translation for this purpose.
</summary>
    <author>
      <name>Daoud Clarke</name>
    </author>
    <link href="http://arxiv.org/abs/1207.2265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2777v1</id>
    <updated>2012-08-14T03:25:33Z</updated>
    <published>2012-08-14T03:25:33Z</published>
    <title>A Method for Selecting Noun Sense using Co-occurrence Relation in
  English-Korean Translation</title>
    <summary>  The sense analysis is still critical problem in machine translation system,
especially such as English-Korean translation which the syntactical different
between source and target languages is very great. We suggest a method for
selecting the noun sense using contextual feature in English-Korean
Translation.
</summary>
    <author>
      <name>Hyonil Kim</name>
    </author>
    <author>
      <name>Changil Choe</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Serdica Journal of Computing, Vol.'6 No.4, 2012, pp. 401-408</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.2777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4503v1</id>
    <updated>2012-08-22T14:11:07Z</updated>
    <published>2012-08-22T14:11:07Z</published>
    <title>Introduction of the weight edition errors in the Levenshtein distance</title>
    <summary>  In this paper, we present a new approach dedicated to correcting the spelling
errors of the Arabic language. This approach corrects typographical errors like
inserting, deleting, and permutation. Our method is inspired from the
Levenshtein algorithm, and allows a finer and better scheduling than
Levenshtein. The results obtained are very satisfactory and encouraging, which
shows the interest of our new approach.
</summary>
    <author>
      <name>Gueddah Hicham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures; International Journal of Advanced Research in
  Artificial Intelligence (IJARAI)2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.4503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.0249v1</id>
    <updated>2012-09-03T06:00:04Z</updated>
    <published>2012-09-03T06:00:04Z</published>
    <title>Robopinion: Opinion Mining Framework Inspired by Autonomous Robot
  Navigation</title>
    <summary>  Data association methods are used by autonomous robots to find matches
between the current landmarks and the new set of observed features. We seek a
framework for opinion mining to benefit from advancements in autonomous robot
navigation in both research and development
</summary>
    <author>
      <name>M. A. El-Dosuky</name>
    </author>
    <author>
      <name>M. Z. Rashad</name>
    </author>
    <author>
      <name>T. T. Hamza</name>
    </author>
    <author>
      <name>A. H. EL-Bassiouny</name>
    </author>
    <link href="http://arxiv.org/abs/1209.0249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.0249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4471v1</id>
    <updated>2012-09-20T09:21:29Z</updated>
    <published>2012-09-20T09:21:29Z</published>
    <title>Stemmer for Serbian language</title>
    <summary>  In linguistic morphology and information retrieval, stemming is the process
for reducing inflected (or sometimes derived) words to their stem, base or root
form; generally a written word form. In this work is presented suffix stripping
stemmer for Serbian language, one of the highly inflectional languages.
</summary>
    <author>
      <name>Nikola Milošević</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures, code included</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0252v2</id>
    <updated>2014-02-15T11:25:46Z</updated>
    <published>2012-09-30T22:55:45Z</published>
    <title>A Linguistic Model for Terminology Extraction based Conditional Random
  Fields</title>
    <summary>  In this paper, we show the possibility of using a linear Conditional Random
Fields (CRF) for terminology extraction from a specialized text corpus.
</summary>
    <author>
      <name>Fethi Fkih</name>
    </author>
    <author>
      <name>Mohamed Nazih Omri</name>
    </author>
    <author>
      <name>Imen Toumia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to the poor
  readability and the low quality of the English</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.0252v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0252v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5785v1</id>
    <updated>2013-05-24T16:34:22Z</updated>
    <published>2013-05-24T16:34:22Z</published>
    <title>An Inventory of Preposition Relations</title>
    <summary>  We describe an inventory of semantic relations that are expressed by
prepositions. We define these relations by building on the word sense
disambiguation task for prepositions and propose a mapping from preposition
senses to the relation labels by collapsing semantically related senses across
prepositions.
</summary>
    <author>
      <name>Vivek Srikumar</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material for Srikumar and Roth, 2013. Modeling Semantic
  Relations Expressed by Prepositions, TACL</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.5785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6238v1</id>
    <updated>2013-05-27T14:36:53Z</updated>
    <published>2013-05-27T14:36:53Z</published>
    <title>Extended Lambek calculi and first-order linear logic</title>
    <summary>  First-order multiplicative intuitionistic linear logic (MILL1) can be seen as
an extension of the Lambek calculus. In addition to the fragment of MILL1 which
corresponds to the Lambek calculus (of Moot &amp; Piazza 2001), I will show
fragments of MILL1 which generate the multiple context-free languages and which
correspond to the Displacement calculus of Morrilll e.a.
</summary>
    <author>
      <name>Richard Moot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Logic and Language, Allemagne (2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.6238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5499v1</id>
    <updated>2013-08-26T07:18:17Z</updated>
    <published>2013-08-26T07:18:17Z</published>
    <title>Linear models and linear mixed effects models in R with linguistic
  applications</title>
    <summary>  This text is a conceptual introduction to mixed effects modeling with
linguistic applications, using the R programming environment. The reader is
introduced to linear modeling and assumptions, as well as to mixed
effects/multilevel modeling, including a discussion of random intercepts,
random slopes and likelihood ratio tests. The example used throughout the text
focuses on the phonetic analysis of voice pitch data.
</summary>
    <author>
      <name>Bodo Winter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.5499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0543v1</id>
    <updated>2014-02-03T23:09:28Z</updated>
    <published>2014-02-03T23:09:28Z</published>
    <title>How Does Latent Semantic Analysis Work? A Visualisation Approach</title>
    <summary>  By using a small example, an analogy to photographic compression, and a
simple visualization using heatmaps, we show that latent semantic analysis
(LSA) is able to extract what appears to be semantic meaning of words from a
set of documents by blurring the distinctions between the words.
</summary>
    <author>
      <name>Jan Koeman</name>
    </author>
    <author>
      <name>William Rea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.0543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2796v1</id>
    <updated>2014-02-12T11:55:31Z</updated>
    <published>2014-02-12T11:55:31Z</published>
    <title>PR2: A Language Independent Unsupervised Tool for Personality
  Recognition from Text</title>
    <summary>  We present PR2, a personality recognition system available online, that
performs instance-based classification of Big5 personality types from
unstructured text, using language-independent features. It has been tested on
English and Italian, achieving performances up to f=.68.
</summary>
    <author>
      <name>Fabio Celli</name>
    </author>
    <author>
      <name>Massimo Poesio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, peer reviewed</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.2796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.7265v1</id>
    <updated>2014-02-28T14:49:31Z</updated>
    <published>2014-02-28T14:49:31Z</published>
    <title>Semantics, Modelling, and the Problem of Representation of Meaning -- a
  Brief Survey of Recent Literature</title>
    <summary>  Over the past 50 years many have debated what representation should be used
to capture the meaning of natural language utterances. Recently new needs of
such representations have been raised in research. Here I survey some of the
interesting representations suggested to answer for these new needs.
</summary>
    <author>
      <name>Yarin Gal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.7265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.7265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1872v1</id>
    <updated>2014-04-07T18:12:08Z</updated>
    <published>2014-04-07T18:12:08Z</published>
    <title>Intégration des données d'un lexique syntaxique dans un analyseur
  syntaxique probabiliste</title>
    <summary>  This article reports the evaluation of the integration of data from a
syntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic
parser. We show that by changing the set of labels for verbs and predicational
nouns, we can improve the performance on French of a non-lexicalized
probabilistic parser.
</summary>
    <author>
      <name>Anthony Sigogne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Matthieu Constant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Penser le Lexique-Grammaire. Perspectives actuelles, Fryni
  Kakoyianni-Doa (Ed.) (2014) 505-516</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0947v1</id>
    <updated>2014-05-05T16:24:09Z</updated>
    <published>2014-05-05T16:24:09Z</published>
    <title>Learning Bilingual Word Representations by Marginalizing Alignments</title>
    <summary>  We present a probabilistic model that simultaneously learns alignments and
distributed representations for bilingual data. By marginalizing over word
alignments the model captures a larger semantic context than prior work relying
on hard alignments. The advantage of this approach is demonstrated in a
cross-lingual classification task, where we outperform the prior published
state of the art.
</summary>
    <author>
      <name>Tomáš Kočiský</name>
    </author>
    <author>
      <name>Karl Moritz Hermann</name>
    </author>
    <author>
      <name>Phil Blunsom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACL 2014 (Short Papers)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6068v1</id>
    <updated>2014-05-23T14:05:45Z</updated>
    <published>2014-05-23T14:05:45Z</published>
    <title>Building of Networks of Natural Hierarchies of Terms Based on Analysis
  of Texts Corpora</title>
    <summary>  The technique of building of networks of hierarchies of terms based on the
analysis of chosen text corpora is offered. The technique is based on the
methodology of horizontal visibility graphs. Constructed and investigated
language network, formed on the basis of electronic preprints arXiv on topics
of information retrieval.
</summary>
    <author>
      <name>Dmitry Lande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6682v1</id>
    <updated>2014-05-26T18:51:06Z</updated>
    <published>2014-05-26T18:51:06Z</published>
    <title>Optimality Theory as a Framework for Lexical Acquisition</title>
    <summary>  This paper re-investigates a lexical acquisition system initially developed
for French.We show that, interestingly, the architecture of the system
reproduces and implements the main components of Optimality Theory. However, we
formulate the hypothesis that some of its limitations are mainly due to a poor
representation of the constraints used. Finally, we show how a better
representation of the constraints used would yield better results.
</summary>
    <author>
      <name>Thierry Poibeau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaTTICe</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">15th International Conference on Intelligent Text Processing and
  Computational Linguistics, Nepal (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.6682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2903v2</id>
    <updated>2014-06-12T01:39:49Z</updated>
    <published>2014-06-11T13:47:22Z</published>
    <title>A Brief State of the Art for Ontology Authoring</title>
    <summary>  One of the main challenges for building the Semantic web is Ontology
Authoring. Controlled Natural Languages CNLs offer a user friendly means for
non-experts to author ontologies. This paper provides a snapshot of the
state-of-the-art for the core CNLs for ontology authoring and reviews their
respective evaluations.
</summary>
    <author>
      <name>Hazem Safwat</name>
    </author>
    <author>
      <name>Brian Davis</name>
    </author>
    <link href="http://arxiv.org/abs/1406.2903v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2903v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3287v3</id>
    <updated>2015-02-18T19:44:24Z</updated>
    <published>2014-06-12T17:01:10Z</published>
    <title>A Clustering Analysis of Tweet Length and its Relation to Sentiment</title>
    <summary>  Sentiment analysis of Twitter data is performed. The researcher has made the
following contributions via this paper: (1) an innovative method for deriving
sentiment score dictionaries using an existing sentiment dictionary as seed
words is explored, and (2) an analysis of clustered tweet sentiment scores
based on tweet length is performed.
</summary>
    <author>
      <name>Matthew Mayo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3287v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3287v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1933v1</id>
    <updated>2014-07-08T02:53:29Z</updated>
    <published>2014-07-08T02:53:29Z</published>
    <title>Lexpresso: a Controlled Natural Language</title>
    <summary>  This paper presents an overview of `Lexpresso', a Controlled Natural Language
developed at the Defence Science &amp; Technology Organisation as a bidirectional
natural language interface to a high-level information fusion system. The paper
describes Lexpresso's main features including lexical coverage, expressiveness
and range of linguistic syntactic and semantic structures. It also touches on
its tight integration with a formal semantic formalism and tentatively
classifies it against the PENS system.
</summary>
    <author>
      <name>Adam Saulwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 4th Workshop on Controlled Natural Language 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.1933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6027v1</id>
    <updated>2014-07-22T20:25:40Z</updated>
    <published>2014-07-22T20:25:40Z</published>
    <title>Modeling languages from graph networks</title>
    <summary>  We model and compute the probability distribution of the letters in random
generated words in a language by using the theory of set partitions, Young
tableaux and graph theoretical representation methods. This has been of
interest for several application areas such as network systems, bioinformatics,
internet search, data mining and computacional linguistics.
</summary>
    <author>
      <name>Alberto Besana</name>
    </author>
    <author>
      <name>Cristina Martínez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03659v1</id>
    <updated>2015-04-14T18:48:58Z</updated>
    <published>2015-04-14T18:48:58Z</published>
    <title>Temporal ordering of clinical events</title>
    <summary>  This report describes a minimalistic set of methods engineered to anchor
clinical events onto a temporal space. Specifically, we describe methods to
extract clinical events (e.g., Problems, Treatments and Tests), temporal
expressions (i.e., time, date, duration, and frequency), and temporal links
(e.g., Before, After, Overlap) between events and temporal entities. These
methods are developed and validated using high quality datasets.
</summary>
    <author>
      <name>Azad Dehghan</name>
    </author>
    <link href="http://arxiv.org/abs/1504.03659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00639v1</id>
    <updated>2015-07-02T15:58:25Z</updated>
    <published>2015-07-02T15:58:25Z</published>
    <title>Simple, Fast Semantic Parsing with a Tensor Kernel</title>
    <summary>  We describe a simple approach to semantic parsing based on a tensor product
kernel. We extract two feature vectors: one for the query and one for each
candidate logical form. We then train a classifier using the tensor product of
the two vectors. Using very simple features for both, our system achieves an
average F1 score of 40.1% on the WebQuestions dataset. This is comparable to
more complex systems but is simpler to implement and runs faster.
</summary>
    <author>
      <name>Daoud Clarke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in CICLing 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.00639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02045v2</id>
    <updated>2015-08-16T18:06:17Z</updated>
    <published>2015-07-08T06:52:50Z</published>
    <title>What Your Username Says About You</title>
    <summary>  Usernames are ubiquitous on the Internet, and they are often suggestive of
user demographics. This work looks at the degree to which gender and language
can be inferred from a username alone by making use of unsupervised morphology
induction to decompose usernames into sub-units. Experimental results on the
two tasks demonstrate the effectiveness of the proposed morphological features
compared to a character n-gram baseline.
</summary>
    <author>
      <name>Aaron Jaech</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06028v1</id>
    <updated>2015-07-22T01:27:05Z</updated>
    <published>2015-07-22T01:27:05Z</published>
    <title>The challenges of SVM optimization using Adaboost on a phoneme
  recognition problem</title>
    <summary>  The use of digital technology is growing at a very fast pace which led to the
emergence of systems based on the cognitive infocommunications. The expansion
of this sector impose the use of combining methods in order to ensure the
robustness in cognitive systems.
</summary>
    <author>
      <name>Rimah Amami</name>
    </author>
    <author>
      <name>Dorra Ben Ayed</name>
    </author>
    <author>
      <name>Noureddine Ellouze</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CogInfoCom.2013.6719292</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CogInfoCom.2013.6719292" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE 4th International Conference on Cognitive Infocommunications
  (CogInfoCom), Budapest 2-5 Dec. 2013, pgs 463-468</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.06028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06837v1</id>
    <updated>2015-07-24T13:21:59Z</updated>
    <published>2015-07-24T13:21:59Z</published>
    <title>YARBUS : Yet Another Rule Based belief Update System</title>
    <summary>  We introduce a new rule based system for belief tracking in dialog systems.
Despite the simplicity of the rules being considered, the proposed belief
tracker ranks favourably compared to the previous submissions on the second and
third Dialog State Tracking challenges. The results of this simple tracker
allows to reconsider the performances of previous submissions using more
elaborate techniques.
</summary>
    <author>
      <name>Jeremy Fix</name>
    </author>
    <author>
      <name>Herve Frezza-buet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code at : https://github.com/jeremyfix/dstc</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04515v1</id>
    <updated>2015-08-19T03:26:05Z</updated>
    <published>2015-08-19T03:26:05Z</published>
    <title>Exploring Metaphorical Senses and Word Representations for Identifying
  Metonyms</title>
    <summary>  A metonym is a word with a figurative meaning, similar to a metaphor. Because
metonyms are closely related to metaphors, we apply features that are used
successfully for metaphor recognition to the task of detecting metonyms. On the
ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system
achieved 86.45% accuracy on the location metonyms. Our code can be found on
GitHub.
</summary>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Judith Gelernter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 pages content</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.04515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00710v1</id>
    <updated>2016-01-05T00:49:22Z</updated>
    <published>2016-01-05T00:49:22Z</published>
    <title>Multi-Source Neural Translation</title>
    <summary>  We build a multi-source machine translation model and train it to maximize
the probability of a target English string given French and German sources.
Using the neural encoder-decoder framework, we explore several combination
methods and report up to +4.8 Bleu increases on top of a very strong
attention-based neural translation model.
</summary>
    <author>
      <name>Barret Zoph</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.05472v1</id>
    <updated>2016-01-20T23:31:58Z</updated>
    <published>2016-01-20T23:31:58Z</published>
    <title>Hierarchical Latent Word Clustering</title>
    <summary>  This paper presents a new Bayesian non-parametric model by extending the
usage of Hierarchical Dirichlet Allocation to extract tree structured word
clusters from text data. The inference algorithm of the model collects words in
a cluster if they share similar distribution over documents. In our
experiments, we observed meaningful hierarchical structures on NIPS corpus and
radiology reports collected from public repositories.
</summary>
    <author>
      <name>Halid Ziya Yerebakan</name>
    </author>
    <author>
      <name>Fitsum Reda</name>
    </author>
    <author>
      <name>Yiqiang Zhan</name>
    </author>
    <author>
      <name>Yoshihisa Shinagawa</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07435v2</id>
    <updated>2016-02-08T08:18:24Z</updated>
    <published>2016-01-27T16:37:08Z</published>
    <title>Co-Occurrence Patterns in the Voynich Manuscript</title>
    <summary>  The Voynich Manuscript is a medieval book written in an unknown script. This
paper studies the distribution of similarly spelled words in the Voynich
Manuscript. It shows that the distribution of words within the manuscript is
not compatible with natural languages.
</summary>
    <author>
      <name>Torsten Timm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; tables for sections of the VMS added; 'The Towneley plays'
  as example for English poetry added; revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07435v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07435v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07618v1</id>
    <updated>2016-02-22T16:17:54Z</updated>
    <published>2016-02-22T16:17:54Z</published>
    <title>From quantum foundations via natural language meaning to a theory of
  everything</title>
    <summary>  In this paper we argue for a paradigmatic shift from `reductionism' to
`togetherness'. In particular, we show how interaction between systems in
quantum theory naturally carries over to modelling how word meanings interact
in natural language. Since meaning in natural language, depending on the
subject domain, encompasses discussions within any scientific discipline, we
obtain a template for theories such as social interaction, animal behaviour,
and many others.
</summary>
    <author>
      <name>Bob Coecke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited contribution to: `The Incomputable'</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01404v1</id>
    <updated>2016-08-04T00:36:57Z</updated>
    <published>2016-08-04T00:36:57Z</published>
    <title>Quantifier Scope in Categorical Compositional Distributional Semantics</title>
    <summary>  In previous work with J. Hedges, we formalised a generalised quantifiers
theory of natural language in categorical compositional distributional
semantics with the help of bialgebras. In this paper, we show how quantifier
scope ambiguity can be represented in that setting and how this representation
can be generalised to branching quantifiers.
</summary>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Queen Mary University of London</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.221.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.221.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SLPCS 2016, arXiv:1608.01018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 221, 2016, pp. 49-57</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01448v2</id>
    <updated>2016-09-29T12:43:43Z</updated>
    <published>2016-08-04T07:37:54Z</published>
    <title>Word Segmentation on Micro-blog Texts with External Lexicon and
  Heterogeneous Data</title>
    <summary>  This paper describes our system designed for the NLPCC 2016 shared task on
word segmentation on micro-blog texts.
</summary>
    <author>
      <name>Qingrong Xia</name>
    </author>
    <author>
      <name>Zhenghua Li</name>
    </author>
    <author>
      <name>Jiayuan Chao</name>
    </author>
    <author>
      <name>Min Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, Proceedings of 5th {CCF} Conference on Natural
  Language Processing and Chinese Computing (2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01448v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01448v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02094v1</id>
    <updated>2016-08-06T10:47:05Z</updated>
    <published>2016-08-06T10:47:05Z</published>
    <title>Desiderata for Vector-Space Word Representations</title>
    <summary>  A plethora of vector-space representations for words is currently available,
which is growing. These consist of fixed-length vectors containing real values,
which represent a word. The result is a representation upon which the power of
many conventional information processing and data mining techniques can be
brought to bear, as long as the representations are designed with some
forethought and fit certain constraints. This paper details desiderata for the
design of vector space representations of words.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03030v1</id>
    <updated>2016-08-10T03:05:26Z</updated>
    <published>2016-08-10T03:05:26Z</published>
    <title>Hierarchical Character-Word Models for Language Identification</title>
    <summary>  Social media messages' brevity and unconventional spelling pose a challenge
to language identification. We introduce a hierarchical model that learns
character and contextualized word-level representations for language
identification. Our method performs well against strong base- lines, and can
also reveal code-switching.
</summary>
    <author>
      <name>Aaron Jaech</name>
    </author>
    <author>
      <name>George Mulcaire</name>
    </author>
    <author>
      <name>Shobhit Hathi</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1608.03030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04485v1</id>
    <updated>2016-08-16T05:04:46Z</updated>
    <published>2016-08-16T05:04:46Z</published>
    <title>Authorship clustering using multi-headed recurrent neural networks</title>
    <summary>  A recurrent neural network that has been trained to separately model the
language of several documents by unknown authors is used to measure similarity
between the documents. It is able to find clues of common authorship even when
the documents are very short and about disparate topics. While it is easy to
make statistically significant predictions regarding authorship, it is
difficult to group documents into definite clusters with high accuracy.
</summary>
    <author>
      <name>Douglas Bagnall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures; notebook for PAN@CLEF 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06204v2</id>
    <updated>2017-04-13T08:33:33Z</updated>
    <published>2016-09-20T14:53:05Z</published>
    <title>Italy goes to Stanford: a collection of CoreNLP modules for Italian</title>
    <summary>  In this we paper present Tint, an easy-to-use set of fast, accurate and
extendable Natural Language Processing modules for Italian. It is based on
Stanford CoreNLP and is freely available as a standalone software or a library
that can be integrated in an existing project.
</summary>
    <author>
      <name>Alessio Palmero Aprosio</name>
    </author>
    <author>
      <name>Giovanni Moretti</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06239v1</id>
    <updated>2016-09-20T16:19:54Z</updated>
    <published>2016-09-20T16:19:54Z</published>
    <title>Generating Politically-Relevant Event Data</title>
    <summary>  Automatically generated political event data is an important part of the
social science data ecosystem. The approaches for generating this data, though,
have remained largely the same for two decades. During this time, the field of
computational linguistics has progressed tremendously. This paper presents an
overview of political event data, including methods and ontologies, and a set
of experiments to determine the applicability of deep neural networks to the
extraction of political events from news text.
</summary>
    <author>
      <name>John Beieler</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08389v2</id>
    <updated>2018-12-31T11:44:47Z</updated>
    <published>2016-09-27T12:55:10Z</published>
    <title>A Hackathon for Classical Tibetan</title>
    <summary>  We describe the course of a hackathon dedicated to the development of
linguistic tools for Tibetan Buddhist studies. Over a period of five days, a
group of seventeen scholars, scientists, and students developed and compared
algorithms for intertextual alignment and text classification, along with some
basic language tools, including a stemmer and word segmenter.
</summary>
    <author>
      <name>Orna Almogi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UHH</arxiv:affiliation>
    </author>
    <author>
      <name>Lena Dankin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAU-CS</arxiv:affiliation>
    </author>
    <author>
      <name>Nachum Dershowitz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAU-CS</arxiv:affiliation>
    </author>
    <author>
      <name>Lior Wolf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAU-CS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1609.08389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09007v1</id>
    <updated>2016-09-28T16:55:52Z</updated>
    <published>2016-09-28T16:55:52Z</published>
    <title>Unsupervised Neural Hidden Markov Models</title>
    <summary>  In this work, we present the first results for neuralizing an Unsupervised
Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach
outperforms existing generative models and is competitive with the
state-of-the-art though with a simpler model easily extended to include
additional context.
</summary>
    <author>
      <name>Ke Tran</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Ashish Vaswani</name>
    </author>
    <author>
      <name>Daniel Marcu</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at EMNLP 2016, Workshop on Structured Prediction for NLP.
  Oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03641v2</id>
    <updated>2017-02-27T18:38:56Z</updated>
    <published>2016-11-11T10:06:29Z</published>
    <title>Improving Reliability of Word Similarity Evaluation by Redesigning
  Annotation Task and Performance Measure</title>
    <summary>  We suggest a new method for creating and using gold-standard datasets for
word similarity evaluation. Our goal is to improve the reliability of the
evaluation, and we do this by redesigning the annotation task to achieve higher
inter-rater agreement, and by defining a performance measure which takes the
reliability of each annotation decision in the dataset into account.
</summary>
    <author>
      <name>Oded Avraham</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1611.03641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00168v1</id>
    <updated>2016-12-31T21:37:10Z</updated>
    <published>2016-12-31T21:37:10Z</published>
    <title>Social Media Argumentation Mining: The Quest for Deliberateness in
  Raucousness</title>
    <summary>  Argumentation mining from social media content has attracted increasing
attention. The task is both challenging and rewarding. The informal nature of
user-generated content makes the task dauntingly difficult. On the other hand,
the insights that could be gained by a large-scale analysis of social media
argumentation make it a very worthwhile task. In this position paper I discuss
the motivation for social media argumentation mining, as well as the tasks and
challenges involved.
</summary>
    <author>
      <name>Jan Šnajder</name>
    </author>
    <link href="http://arxiv.org/abs/1701.00168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00504v1</id>
    <updated>2017-01-02T19:23:55Z</updated>
    <published>2017-01-02T19:23:55Z</published>
    <title>Stance detection in online discussions</title>
    <summary>  This paper describes our system created to detect stance in online
discussions. The goal is to identify whether the author of a comment is in
favor of the given target or against. Our approach is based on a maximum
entropy classifier, which uses surface-level, sentiment and domain-specific
features. The system was originally developed to detect stance in English
tweets. We adapted it to process Czech news commentaries.
</summary>
    <author>
      <name>Peter Krejzl</name>
    </author>
    <author>
      <name>Barbora Hourová</name>
    </author>
    <author>
      <name>Josef Steinberger</name>
    </author>
    <link href="http://arxiv.org/abs/1701.00504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03092v1</id>
    <updated>2017-01-11T18:42:09Z</updated>
    <published>2017-01-11T18:42:09Z</published>
    <title>Job Detection in Twitter</title>
    <summary>  In this report, we propose a new application for twitter data called
\textit{job detection}. We identify people's job category based on their
tweets. As a preliminary work, we limited our task to identify only IT workers
from other job holders. We have used and compared both simple bag of words
model and a document representation based on Skip-gram model. Our results show
that the model based on Skip-gram, achieves a 76\% precision and 82\% recall.
</summary>
    <author>
      <name>Besat Kassaie</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01938v1</id>
    <updated>2017-04-06T17:07:40Z</updated>
    <published>2017-04-06T17:07:40Z</published>
    <title>The Interplay of Semantics and Morphology in Word Embeddings</title>
    <summary>  We explore the ability of word embeddings to capture both semantic and
morphological similarity, as affected by the different types of linguistic
properties (surface form, lemma, morphological tag) used to compose the
representation of each word. We train several models, where each uses a
different subset of these properties to compose its representations. By
evaluating the models on semantic and morphological measures, we reveal some
useful insights on the relationship between semantics and morphology.
</summary>
    <author>
      <name>Oded Avraham</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03693v1</id>
    <updated>2017-04-12T10:47:10Z</updated>
    <published>2017-04-12T10:47:10Z</published>
    <title>Trainable Referring Expression Generation using Overspecification
  Preferences</title>
    <summary>  Referring expression generation (REG) models that use speaker-dependent
information require a considerable amount of training data produced by every
individual speaker, or may otherwise perform poorly. In this work we present a
simple REG experiment that allows the use of larger training data sets by
grouping speakers according to their overspecification preferences. Intrinsic
evaluation shows that this method generally outperforms the personalised method
found in previous work.
</summary>
    <author>
      <name>Thiago castro Ferreira</name>
    </author>
    <author>
      <name>Ivandre Paraboni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06841v1</id>
    <updated>2017-04-22T19:39:32Z</updated>
    <published>2017-04-22T19:39:32Z</published>
    <title>Medical Text Classification using Convolutional Neural Networks</title>
    <summary>  We present an approach to automatically classify clinical text at a sentence
level. We are using deep convolutional neural networks to represent complex
features. We train the network on a dataset providing a broad categorization of
health information. Through a detailed evaluation, we demonstrate that our
method outperforms several approaches widely used in natural language
processing tasks by about 15%.
</summary>
    <author>
      <name>Mark Hughes</name>
    </author>
    <author>
      <name>Irene Li</name>
    </author>
    <author>
      <name>Spyros Kotoulas</name>
    </author>
    <author>
      <name>Toyotaro Suzumura</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08388v2</id>
    <updated>2017-04-28T01:16:07Z</updated>
    <published>2017-04-27T00:29:17Z</published>
    <title>Duluth at Semeval-2017 Task 7 : Puns upon a midnight dreary, Lexical
  Semantics for the weak and weary</title>
    <summary>  This paper describes the Duluth systems that participated in SemEval-2017
Task 7 : Detection and Interpretation of English Puns. The Duluth systems
participated in all three subtasks, and relied on methods that included word
sense disambiguation and measures of semantic relatedness.
</summary>
    <author>
      <name>Ted Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, to Appear in the Proceedings of the 11th International
  Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08388v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08388v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08531v1</id>
    <updated>2017-04-27T12:27:25Z</updated>
    <published>2017-04-27T12:27:25Z</published>
    <title>A Survey of Neural Network Techniques for Feature Extraction from Text</title>
    <summary>  This paper aims to catalyze the discussions about text feature extraction
techniques using neural network architectures. The research questions discussed
in the paper focus on the state-of-the-art neural network techniques that have
proven to be useful tools for language processing, language generation, text
classification and other computational linguistics tasks.
</summary>
    <author>
      <name>Vineet John</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00299v1</id>
    <updated>2017-07-02T14:39:21Z</updated>
    <published>2017-07-02T14:39:21Z</published>
    <title>Grammatical Error Correction with Neural Reinforcement Learning</title>
    <summary>  We propose a neural encoder-decoder model with reinforcement learning (NRL)
for grammatical error correction (GEC). Unlike conventional maximum likelihood
estimation (MLE), the model directly optimizes towards an objective that
considers a sentence-level, task-specific evaluation metric, avoiding the
exposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in
human and automated evaluation metrics, achieving the state-of-the-art on a
fluency-oriented GEC corpus.
</summary>
    <author>
      <name>Keisuke Sakaguchi</name>
    </author>
    <author>
      <name>Matt Post</name>
    </author>
    <author>
      <name>Benjamin Van Durme</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02063v1</id>
    <updated>2017-07-07T07:46:54Z</updated>
    <published>2017-07-07T07:46:54Z</published>
    <title>External Evaluation of Event Extraction Classifiers for Automatic
  Pathway Curation: An extended study of the mTOR pathway</title>
    <summary>  This paper evaluates the impact of various event extraction systems on
automatic pathway curation using the popular mTOR pathway. We quantify the
impact of training data sets as well as different machine learning classifiers
and show that some improve the quality of automatically extracted pathways.
</summary>
    <author>
      <name>Wojciech Kusa</name>
    </author>
    <author>
      <name>Michael Spranger</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03819v1</id>
    <updated>2017-07-12T10:09:32Z</updated>
    <published>2017-07-12T10:09:32Z</published>
    <title>A Critique of a Critique of Word Similarity Datasets: Sanity Check or
  Unnecessary Confusion?</title>
    <summary>  Critical evaluation of word similarity datasets is very important for
computational lexical semantics. This short report concerns the sanity check
proposed in Batchkarov et al. (2016) to evaluate several popular datasets such
as MC, RG and MEN -- the first two reportedly failed. I argue that this test is
unstable, offers no added insight, and needs major revision in order to fulfill
its purported goal.
</summary>
    <author>
      <name>Minh Le</name>
    </author>
    <link href="http://arxiv.org/abs/1707.03819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05850v3</id>
    <updated>2017-07-25T21:00:14Z</updated>
    <published>2017-07-18T20:38:42Z</published>
    <title>A Short Survey of Biomedical Relation Extraction Techniques</title>
    <summary>  Biomedical information is growing rapidly in the recent years and retrieving
useful data through information extraction system is getting more attention. In
the current research, we focus on different aspects of relation extraction
techniques in biomedical domain and briefly describe the state-of-the-art for
relation extraction between a variety of biological elements.
</summary>
    <author>
      <name>Elham Shahab</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">updated keywords and reference format</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05850v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05850v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06480v1</id>
    <updated>2017-07-20T12:46:09Z</updated>
    <published>2017-07-20T12:46:09Z</published>
    <title>Syllable-aware Neural Language Models: A Failure to Beat Character-aware
  Ones</title>
    <summary>  Syllabification does not seem to improve word-level RNN language modeling
quality when compared to character-based segmentation. However, our best
syllable-aware language model, achieving performance comparable to the
competitive character-aware model, has 18%-33% fewer parameters and is trained
1.2-2.2 times faster.
</summary>
    <author>
      <name>Zhenisbek Assylbekov</name>
    </author>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <author>
      <name>Bagdat Myrzakhmetov</name>
    </author>
    <author>
      <name>Jonathan N. Washington</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07755v2</id>
    <updated>2017-08-02T15:47:32Z</updated>
    <published>2017-07-24T21:33:21Z</published>
    <title>AMR Parsing using Stack-LSTMs</title>
    <summary>  We present a transition-based AMR parser that directly generates AMR parses
from plain text. We use Stack-LSTMs to represent our parser state and make
decisions greedily. In our experiments, we show that our parser achieves very
competitive scores on English using only AMR training data. Adding additional
information, such as POS tags and dependency trees, improves the results
further.
</summary>
    <author>
      <name>Miguel Ballesteros</name>
    </author>
    <author>
      <name>Yaser Al-Onaizan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09769v1</id>
    <updated>2017-07-31T08:56:32Z</updated>
    <published>2017-07-31T08:56:32Z</published>
    <title>Low-Resource Neural Headline Generation</title>
    <summary>  Recent neural headline generation models have shown great results, but are
generally trained on very large datasets. We focus our efforts on improving
headline quality on smaller datasets by the means of pretraining. We propose
new methods that enable pre-training all the parameters of the model and
utilize all available text, resulting in improvements by up to 32.4% relative
in perplexity and 2.84 points in ROUGE.
</summary>
    <author>
      <name>Ottokar Tilk</name>
    </author>
    <author>
      <name>Tanel Alumäe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2017 Workshop on New Frontiers in Summarization</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03699v1</id>
    <updated>2017-08-11T20:37:27Z</updated>
    <published>2017-08-11T20:37:27Z</published>
    <title>Improved Abusive Comment Moderation with User Embeddings</title>
    <summary>  Experimenting with a dataset of approximately 1.6M user comments from a Greek
news sports portal, we explore how a state of the art RNN-based moderation
method can be improved by adding user embeddings, user type embeddings, user
biases, or user type biases. We observe improvements in all cases, with user
embeddings leading to the biggest performance gains.
</summary>
    <author>
      <name>John Pavlopoulos</name>
    </author>
    <author>
      <name>Prodromos Malakasiotis</name>
    </author>
    <author>
      <name>Juli Bakagianni</name>
    </author>
    <author>
      <name>Ion Androutsopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06708v1</id>
    <updated>2017-08-19T18:18:22Z</updated>
    <published>2017-08-19T18:18:22Z</published>
    <title>A rule based algorithm for detecting negative words in Persian</title>
    <summary>  In this paper, we present a novel method for detecting negative words in
Persian. We first used an algorithm to an exceptions list which was later
modified by hand. We then used the mentioned lists and a Persian polarity
corpus in our rule based algorithm to detect negative words.
</summary>
    <author>
      <name>Reza Takhshid</name>
    </author>
    <author>
      <name>Adel Rahimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07722v3</id>
    <updated>2018-03-16T08:17:18Z</updated>
    <published>2017-08-24T04:59:53Z</published>
    <title>A dependency look at the reality of constituency</title>
    <summary>  A comment on "Neurophysiological dynamics of phrase-structure building during
sentence processing" by Nelson et al (2017), Proceedings of the National
Academy of Sciences USA 114(18), E3669-E3678.
</summary>
    <author>
      <name>Xinying Chen</name>
    </author>
    <author>
      <name>Carlos Gómez-Rodríguez</name>
    </author>
    <author>
      <name>Ramon Ferrer-i-Cancho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Glottometrics 40, 104-106 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.07722v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07722v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00803v1</id>
    <updated>2017-10-02T17:18:37Z</updated>
    <published>2017-10-02T17:18:37Z</published>
    <title>Compiling and Processing Historical and Contemporary Portuguese Corpora</title>
    <summary>  This technical report describes the framework used for processing three large
Portuguese corpora. Two corpora contain texts from newspapers, one published in
Brazil and the other published in Portugal. The third corpus is Colonia, a
historical Portuguese collection containing texts written between the 16th and
the early 20th century. The report presents pre-processing methods,
segmentation, and annotation of the corpora as well as indexing and querying
methods. Finally, it presents published research papers using the corpora.
</summary>
    <author>
      <name>Marcos Zampieri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02187v1</id>
    <updated>2017-10-05T19:24:07Z</updated>
    <published>2017-10-05T19:24:07Z</published>
    <title>BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages</title>
    <summary>  We present BPEmb, a collection of pre-trained subword unit embeddings in 275
languages, based on Byte-Pair Encoding (BPE). In an evaluation using
fine-grained entity typing as testbed, BPEmb performs competitively, and for
some languages bet- ter than alternative subword approaches, while requiring
vastly fewer resources and no tokenization. BPEmb is available at
https://github.com/bheinzerling/bpemb
</summary>
    <author>
      <name>Benjamin Heinzerling</name>
    </author>
    <author>
      <name>Michael Strube</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02925v1</id>
    <updated>2017-10-09T03:07:54Z</updated>
    <published>2017-10-09T03:07:54Z</published>
    <title>Natural Language Inference from Multiple Premises</title>
    <summary>  We define a novel textual entailment task that requires inference over
multiple premise sentences. We present a new dataset for this task that
minimizes trivial lexical inferences, emphasizes knowledge of everyday events,
and presents a more challenging setting for textual entailment. We evaluate
several strong neural baselines and analyze how the multiple premise task
differs from standard textual entailment.
</summary>
    <author>
      <name>Alice Lai</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Julia Hockenmaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IJCNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04515v1</id>
    <updated>2017-10-12T13:40:43Z</updated>
    <published>2017-10-12T13:40:43Z</published>
    <title>Convolutional Attention-based Seq2Seq Neural Network for End-to-End ASR</title>
    <summary>  This thesis introduces the sequence to sequence model with Luong's attention
mechanism for end-to-end ASR. It also describes various neural network
algorithms including Batch normalization, Dropout and Residual network which
constitute the convolutional attention-based seq2seq neural network. Finally
the proposed model proved its effectiveness for speech recognition achieving
15.8% phoneme error rate on TIMIT dataset.
</summary>
    <author>
      <name>Dan Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Masters thesis, Korea Univ</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05519v2</id>
    <updated>2018-02-21T10:45:32Z</updated>
    <published>2017-10-16T05:49:29Z</published>
    <title>BKTreebank: Building a Vietnamese Dependency Treebank</title>
    <summary>  Dependency treebank is an important resource in any language. In this paper,
we present our work on building BKTreebank, a dependency treebank for
Vietnamese. Important points on designing POS tagset, dependency relations, and
annotation guidelines are discussed. We describe experiments on POS tagging and
dependency parsing on the treebank. Experimental results show that the treebank
is a useful resource for Vietnamese language processing.
</summary>
    <author>
      <name>Kiem-Hieu Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for LREC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.05519v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05519v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08721v1</id>
    <updated>2017-10-24T12:11:07Z</updated>
    <published>2017-10-24T12:11:07Z</published>
    <title>Clickbait Identification using Neural Networks</title>
    <summary>  This paper presents the results of our participation in the Clickbait
Detection Challenge 2017. The system relies on a fusion of neural networks,
incorporating different types of available informations. It does not require
any linguistic preprocessing, and hence generalizes more easily to new domains
and languages. The final combined model achieves a mean squared error of
0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the
official evaluation metric the system ranked 6th of the 13 participating teams.
</summary>
    <author>
      <name>Philippe Thomas</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09589v1</id>
    <updated>2017-10-26T08:41:50Z</updated>
    <published>2017-10-26T08:41:50Z</published>
    <title>ALL-IN-1: Short Text Classification with One Model for All Languages</title>
    <summary>  We present ALL-IN-1, a simple model for multilingual text classification that
does not require any parallel data. It is based on a traditional Support Vector
Machine classifier exploiting multilingual word embeddings and character
n-grams. Our model is simple, easily extendable yet very effective, overall
ranking 1st (out of 12 teams) in the IJCNLP 2017 shared task on customer
feedback analysis in four languages: English, French, Japanese and Spanish.
</summary>
    <author>
      <name>Barbara Plank</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11350v3</id>
    <updated>2019-08-29T00:04:48Z</updated>
    <published>2017-10-31T07:09:14Z</published>
    <title>Grammar Induction for Minimalist Grammars using Variational Bayesian
  Inference : A Technical Report</title>
    <summary>  The following technical report presents a formal approach to probabilistic
minimalist grammar parameter estimation. We describe a formalization of a
minimalist grammar. We then present an algorithm for the application of
variational Bayesian inference to this formalization.
</summary>
    <author>
      <name>Eva Portelance</name>
    </author>
    <author>
      <name>Amelia Bruno</name>
    </author>
    <author>
      <name>Daniel Harasim</name>
    </author>
    <author>
      <name>Leon Bergen</name>
    </author>
    <author>
      <name>Timothy J. O'Donnell</name>
    </author>
    <link href="http://arxiv.org/abs/1710.11350v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11350v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01416v1</id>
    <updated>2017-11-04T09:11:18Z</updated>
    <published>2017-11-04T09:11:18Z</published>
    <title>Language as a matrix product state</title>
    <summary>  We propose a statistical model for natural language that begins by
considering language as a monoid, then representing it in complex matrices with
a compatible translation invariant probability measure. We interpret the
probability measure as arising via the Born rule from a translation invariant
matrix product state.
</summary>
    <author>
      <name>Vasily Pestun</name>
    </author>
    <author>
      <name>John Terilla</name>
    </author>
    <author>
      <name>Yiannis Vlassopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03967v1</id>
    <updated>2018-08-12T16:32:18Z</updated>
    <published>2018-08-12T16:32:18Z</published>
    <title>Augmenting word2vec with latent Dirichlet allocation within a clinical
  application</title>
    <summary>  This paper presents three hybrid models that directly combine latent
Dirichlet allocation and word embedding for distinguishing between speakers
with and without Alzheimer's disease from transcripts of picture descriptions.
Two of our models get F-scores over the current state-of-the-art using
automatic methods on the DementiaBank dataset.
</summary>
    <author>
      <name>Akshay Budhkar</name>
    </author>
    <author>
      <name>Frank Rudzicz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04164v1</id>
    <updated>2018-08-13T12:04:44Z</updated>
    <published>2018-08-13T12:04:44Z</published>
    <title>Automatic Reference-Based Evaluation of Pronoun Translation Misses the
  Point</title>
    <summary>  We compare the performance of the APT and AutoPRF metrics for pronoun
translation against a manually annotated dataset comprising human judgements as
to the correctness of translations of the PROTEST test suite. Although there is
some correlation with the human judgements, a range of issues limit the
performance of the automated metrics. Instead, we recommend the use of
semi-automatic metrics and test suites in place of fully automatic metrics.
</summary>
    <author>
      <name>Liane Guillou</name>
    </author>
    <author>
      <name>Christian Hardmeier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04365v1</id>
    <updated>2018-08-13T11:50:03Z</updated>
    <published>2018-08-13T11:50:03Z</published>
    <title>What is wrong with style transfer for texts?</title>
    <summary>  A number of recent machine learning papers work with an automated style
transfer for texts and, counter to intuition, demonstrate that there is no
consensus formulation of this NLP task. Different researchers propose different
algorithms, datasets and target metrics to address it. This short opinion paper
aims to discuss possible formalization of this NLP task in anticipation of a
further growing interest to it.
</summary>
    <author>
      <name>Alexey Tikhonov</name>
    </author>
    <author>
      <name>Ivan P. Yamshchikov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09733v1</id>
    <updated>2018-08-29T11:30:22Z</updated>
    <published>2018-08-29T11:30:22Z</published>
    <title>Distant Supervision from Disparate Sources for Low-Resource
  Part-of-Speech Tagging</title>
    <summary>  We introduce DsDs: a cross-lingual neural part-of-speech tagger that learns
from disparate sources of distant supervision, and realistically scales to
hundreds of low-resource languages. The model exploits annotation projection,
instance selection, tag dictionaries, morphological lexicons, and distributed
representations, all in a uniform framework. The approach is simple, yet
surprisingly effective, resulting in a new state of the art without access to
any gold annotated data.
</summary>
    <author>
      <name>Barbara Plank</name>
    </author>
    <author>
      <name>Željko Agić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00472v1</id>
    <updated>2018-09-30T21:53:47Z</updated>
    <published>2018-09-30T21:53:47Z</published>
    <title>Automatic Evaluation of Neural Personality-based Chatbots</title>
    <summary>  Stylistic variation is critical to render the utterances generated by
conversational agents natural and engaging. In this paper, we focus on
sequence-to-sequence models for open-domain dialogue response generation and
propose a new method to evaluate the extent to which such models are able to
generate responses that reflect different personality traits.
</summary>
    <author>
      <name>Yujie Xing</name>
    </author>
    <author>
      <name>Raquel Fernández</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 11th International Conference on
  Natural Language Generation (INLG-2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.00472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04528v1</id>
    <updated>2018-10-10T13:42:16Z</updated>
    <published>2018-10-10T13:42:16Z</published>
    <title>Is there Gender bias and stereotype in Portuguese Word Embeddings?</title>
    <summary>  In this work, we propose an analysis of the presence of gender bias
associated with professions in Portuguese word embeddings. The objective of
this work is to study gender implications related to stereotyped professions
for women and men in the context of the Portuguese language.
</summary>
    <author>
      <name>Brenda Salenave Santana</name>
    </author>
    <author>
      <name>Vinicius Woloszyn</name>
    </author>
    <author>
      <name>Leandro Krug Wives</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 13th edition of the International Conference on the
  Computational Processing of Portuguese (PROPOR 2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.04528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06233v1</id>
    <updated>2018-10-15T09:05:21Z</updated>
    <published>2018-10-15T09:05:21Z</published>
    <title>UMONS Submission for WMT18 Multimodal Translation Task</title>
    <summary>  This paper describes the UMONS solution for the Multimodal Machine
Translation Task presented at the third conference on machine translation
(WMT18). We explore a novel architecture, called deepGRU, based on recent
findings in the related task of Neural Image Captioning (NIC). The models
presented in the following sections lead to the best METEOR translation score
for both constrained (English, image) -&gt; German and (English, image) -&gt; French
sub-tasks.
</summary>
    <author>
      <name>Jean-Benoit Delbrouck</name>
    </author>
    <author>
      <name>Stéphane Dupont</name>
    </author>
    <link href="http://arxiv.org/abs/1810.06233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.09995v1</id>
    <updated>2018-10-23T17:56:23Z</updated>
    <published>2018-10-23T17:56:23Z</published>
    <title>Deep Graph Convolutional Encoders for Structured Data to Text Generation</title>
    <summary>  Most previous work on neural text generation from graph-structured data
relies on standard sequence-to-sequence methods. These approaches linearise the
input graph to be fed to a recurrent neural network. In this paper, we propose
an alternative encoder based on graph convolutional networks that directly
exploits the input structure. We report results on two graph-to-sequence
datasets that empirically show the benefits of explicitly encoding the input
graph structure.
</summary>
    <author>
      <name>Diego Marcheggiani</name>
    </author>
    <author>
      <name>Laura Perez-Beltrachini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INLG 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.09995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.09995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04393v1</id>
    <updated>2019-09-10T10:32:49Z</updated>
    <published>2019-09-10T10:32:49Z</published>
    <title>Extending the Service Composition Formalism with Relational Parameters</title>
    <summary>  Web Service Composition deals with the (re)use of Web Services to provide
complex functionality, inexistent in any single service. Over the
state-of-the-art, we introduce a new type of modeling, based on ontologies and
relations between objects, which allows us to extend the expressiveness of
problems that can be solved automatically.
</summary>
    <author>
      <name>Paul Diac</name>
    </author>
    <author>
      <name>Liana Tucar</name>
    </author>
    <author>
      <name>Radu Mereuta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.04393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06276v1</id>
    <updated>2019-09-13T14:59:51Z</updated>
    <published>2019-09-13T14:59:51Z</published>
    <title>Parameterized Convolutional Neural Networks for Aspect Level Sentiment
  Classification</title>
    <summary>  We introduce a novel parameterized convolutional neural network for aspect
level sentiment classification. Using parameterized filters and parameterized
gates, we incorporate aspect information into convolutional neural networks
(CNN). Experiments demonstrate that our parameterized filters and parameterized
gates effectively capture the aspect-specific features, and our CNN-based
models achieve excellent results on SemEval 2014 datasets.
</summary>
    <author>
      <name>Binxuan Huang</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08217v1</id>
    <updated>2019-09-18T05:54:32Z</updated>
    <published>2019-09-18T05:54:32Z</published>
    <title>Improving Natural Language Inference with a Pretrained Parser</title>
    <summary>  We introduce a novel approach to incorporate syntax into natural language
inference (NLI) models. Our method uses contextual token-level vector
representations from a pretrained dependency parser. Like other contextual
embedders, our method is broadly applicable to any neural model. We experiment
with four strong NLI models (decomposable attention model, ESIM, BERT, and
MT-DNN), and show consistent benefit to accuracy across three NLI benchmarks.
</summary>
    <author>
      <name>Deric Pang</name>
    </author>
    <author>
      <name>Lucy H. Lin</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1909.08217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13872v1</id>
    <updated>2019-09-30T17:54:50Z</updated>
    <published>2019-09-30T17:54:50Z</published>
    <title>Simple and Effective Paraphrastic Similarity from Parallel Translations</title>
    <summary>  We present a model and methodology for learning paraphrastic sentence
embeddings directly from bitext, removing the time-consuming intermediate step
of creating paraphrase corpora. Further, we show that the resulting model can
be applied to cross-lingual tasks where it both outperforms and is orders of
magnitude faster than more complex state-of-the-art baselines.
</summary>
    <author>
      <name>John Wieting</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Taylor Berg-Kirkpatrick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a short paper at ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.01338v1</id>
    <updated>2020-06-02T01:46:04Z</updated>
    <published>2020-06-02T01:46:04Z</published>
    <title>A Survey of Neural Networks and Formal Languages</title>
    <summary>  This report is a survey of the relationships between various state-of-the-art
neural network architectures and formal languages as, for example, structured
by the Chomsky Language Hierarchy. Of particular interest are the abilities of
a neural architecture to represent, recognize and generate words from a
specific language by learning from samples of the language.
</summary>
    <author>
      <name>Joshua Ackerman</name>
    </author>
    <author>
      <name>George Cybenko</name>
    </author>
    <link href="http://arxiv.org/abs/2006.01338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02767v1</id>
    <updated>2020-06-04T10:54:43Z</updated>
    <published>2020-06-04T10:54:43Z</published>
    <title>Seq2Seq AI Chatbot with Attention Mechanism</title>
    <summary>  Intelligent Conversational Agent development using Artificial Intelligence or
Machine Learning technique is an interesting problem in the field of Natural
Language Processing. With the rise of deep learning, these models were quickly
replaced by end to end trainable neural networks.
</summary>
    <author>
      <name>Abonia Sojasingarayar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages 8 Figures 4 Tables 5 Equations</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07264v1</id>
    <updated>2020-06-12T15:21:57Z</updated>
    <published>2020-06-12T15:21:57Z</published>
    <title>Low-resource Languages: A Review of Past Work and Future Challenges</title>
    <summary>  A current problem in NLP is massaging and processing low-resource languages
which lack useful training attributes such as supervised data, number of native
speakers or experts, etc. This review paper concisely summarizes previous
groundbreaking achievements made towards resolving this problem, and analyzes
potential improvements in the context of the overall future research direction.
</summary>
    <author>
      <name>Alexandre Magueresse</name>
    </author>
    <author>
      <name>Vincent Carles</name>
    </author>
    <author>
      <name>Evan Heetderks</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12234v2</id>
    <updated>2020-11-06T19:00:30Z</updated>
    <published>2020-06-22T13:30:35Z</published>
    <title>Shared Task on Evaluating Accuracy in Natural Language Generation</title>
    <summary>  We propose a shared task on methodologies and algorithms for evaluating the
accuracy of generated texts. Participants will measure the accuracy of
basketball game summaries produced by NLG systems from basketball box score
data.
</summary>
    <author>
      <name>Ehud Reiter</name>
    </author>
    <author>
      <name>Craig Thomson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in INLG 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.12234v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12234v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02551v1</id>
    <updated>2017-06-08T12:48:15Z</updated>
    <published>2017-06-08T12:48:15Z</published>
    <title>The Algorithmic Inflection of Russian and Generation of Grammatically
  Correct Text</title>
    <summary>  We present a deterministic algorithm for Russian inflection. This algorithm
is implemented in a publicly available web-service www.passare.ru which
provides functions for inflection of single words, word matching and synthesis
of grammatically correct Russian text. The inflectional functions have been
tested against the annotated corpus of Russian language OpenCorpora.
</summary>
    <author>
      <name>T. M. Sadykov</name>
    </author>
    <author>
      <name>T. A. Zhukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02883v1</id>
    <updated>2017-06-09T10:17:24Z</updated>
    <published>2017-06-09T10:17:24Z</published>
    <title>Overview of the NLPCC 2017 Shared Task: Chinese News Headline
  Categorization</title>
    <summary>  In this paper, we give an overview for the shared task at the CCF Conference
on Natural Language Processing \&amp; Chinese Computing (NLPCC 2017): Chinese News
Headline Categorization. The dataset of this shared task consists 18 classes,
12,000 short texts along with corresponded labels for each class. The dataset
and example code can be accessed at
https://github.com/FudanNLP/nlpcc2017_news_headline_categorization.
</summary>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Jingjing Gong</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09562v1</id>
    <updated>2017-06-29T03:19:39Z</updated>
    <published>2017-06-29T03:19:39Z</published>
    <title>Frame-Based Continuous Lexical Semantics through Exponential Family
  Tensor Factorization and Semantic Proto-Roles</title>
    <summary>  We study how different frame annotations complement one another when learning
continuous lexical semantics. We learn the representations from a tensorized
skip-gram model that consistently encodes syntactic-semantic content better,
with multiple 10% gains over baselines.
</summary>
    <author>
      <name>Francis Ferraro</name>
    </author>
    <author>
      <name>Adam Poliak</name>
    </author>
    <author>
      <name>Ryan Cotterell</name>
    </author>
    <author>
      <name>Benjamin Van Durme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Sixth Joint Conference on Lexical and Computational
  Semantics (*SEM). Association for Computational Linguistics, Vancouver,
  Canada. 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09856v1</id>
    <updated>2017-06-29T17:04:48Z</updated>
    <published>2017-06-29T17:04:48Z</published>
    <title>Automatic Mapping of French Discourse Connectives to PDTB Discourse
  Relations</title>
    <summary>  In this paper, we present an approach to exploit phrase tables generated by
statistical machine translation in order to map French discourse connectives to
discourse relations. Using this approach, we created ConcoLeDisCo, a lexicon of
French discourse connectives and their PDTB relations. When evaluated against
LEXCONN, ConcoLeDisCo achieves a recall of 0.81 and an Average Precision of
0.68 for the Concession and Condition relations.
</summary>
    <author>
      <name>Majid Laali</name>
    </author>
    <author>
      <name>Leila Kosseim</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00489v1</id>
    <updated>2017-09-01T21:38:28Z</updated>
    <published>2017-09-01T21:38:28Z</published>
    <title>Arc-Standard Spinal Parsing with Stack-LSTMs</title>
    <summary>  We present a neural transition-based parser for spinal trees, a dependency
representation of constituent trees. The parser uses Stack-LSTMs that compose
constituent nodes with dependency-based derivations. In experiments, we show
that this model adapts to different styles of dependency relations, but this
choice has little effect for predicting constituent structure, suggesting that
LSTMs induce useful states by themselves.
</summary>
    <author>
      <name>Miguel Ballesteros</name>
    </author>
    <author>
      <name>Xavier Carreras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IWPT 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.00489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03815v1</id>
    <updated>2017-09-12T12:58:07Z</updated>
    <published>2017-09-12T12:58:07Z</published>
    <title>OpenNMT: Open-source Toolkit for Neural Machine Translation</title>
    <summary>  We introduce an open-source toolkit for neural machine translation (NMT) to
support research into model architectures, feature representations, and source
modalities, while maintaining competitive performance, modularity and
reasonable training requirements.
</summary>
    <author>
      <name>Guillaume Klein</name>
    </author>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Yuntian Deng</name>
    </author>
    <author>
      <name>Josep Crego</name>
    </author>
    <author>
      <name>Jean Senellart</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in EAMT 2017 User Studies and Project/Product Descriptions</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05599v1</id>
    <updated>2017-09-17T02:56:40Z</updated>
    <published>2017-09-17T02:56:40Z</published>
    <title>Hierarchical Gated Recurrent Neural Tensor Network for Answer Triggering</title>
    <summary>  In this paper, we focus on the problem of answer triggering ad-dressed by
Yang et al. (2015), which is a critical component for a real-world question
answering system. We employ a hierarchical gated recurrent neural tensor
(HGRNT) model to capture both the context information and the deep
in-teractions between the candidate answers and the question. Our result on F
val-ue achieves 42.6%, which surpasses the baseline by over 10 %.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Yunfang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07809v1</id>
    <updated>2017-09-22T15:28:24Z</updated>
    <published>2017-09-22T15:28:24Z</published>
    <title>Neural Machine Translation</title>
    <summary>  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
</summary>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">100+ pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.10486v1</id>
    <updated>2017-09-29T16:36:53Z</updated>
    <published>2017-09-29T16:36:53Z</published>
    <title>Symbol, Conversational, and Societal Grounding with a Toy Robot</title>
    <summary>  Essential to meaningful interaction is grounding at the symbolic,
conversational, and societal levels. We present ongoing work with Anki's Cozmo
toy robot as a research platform where we leverage the recent
words-as-classifiers model of lexical semantics in interactive reference
resolution tasks for language grounding.
</summary>
    <author>
      <name>Casey Kennington</name>
    </author>
    <author>
      <name>Sarah Plane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.10486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.10486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05287v1</id>
    <updated>2019-01-16T14:01:15Z</updated>
    <published>2019-01-16T14:01:15Z</published>
    <title>Assessing BERT's Syntactic Abilities</title>
    <summary>  I assess the extent to which the recently introduced BERT model captures
English syntactic phenomena, using (1) naturally-occurring subject-verb
agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli,
in which content words in natural sentences are randomly replaced with words
sharing the same part-of-speech and inflection; and (3) manually crafted
stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT
model performs remarkably well on all cases.
</summary>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03724v1</id>
    <updated>2019-11-09T16:00:26Z</updated>
    <published>2019-11-09T16:00:26Z</published>
    <title>Error Analysis for Vietnamese Dependency Parsing</title>
    <summary>  Dependency parsing is needed in different applications of natural language
processing. In this paper, we present a thorough error analysis for dependency
parsing for the Vietnamese language, using two state-of-the-art parsers:
MSTParser and MaltParser. The error analysis results provide us insights in
order to improve the performance of dependency parsing for the Vietnamese
language.
</summary>
    <author>
      <name>Kiet Van Nguyen</name>
    </author>
    <author>
      <name>Ngan Luu-Thuy Nguyen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 Seventh International Conference on Knowledge and Systems
  Engineering (KSE)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.03724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08794v1</id>
    <updated>2019-11-20T09:52:23Z</updated>
    <published>2019-11-20T09:52:23Z</published>
    <title>Natural Language Generation Challenges for Explainable AI</title>
    <summary>  Good quality explanations of artificial intelligence (XAI) reasoning must be
written (and evaluated) for an explanatory purpose, targeted towards their
readers, have a good narrative and causal structure, and highlight where
uncertainty and data quality affect the AI output. I discuss these challenges
from a Natural Language Generation (NLG) perspective, and highlight four
specific NLG for XAI research challenges.
</summary>
    <author>
      <name>Ehud Reiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the NL4XAI workshop
  (https://sites.google.com/view/nl4xai2019/)</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.08794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4928v1</id>
    <updated>2014-03-19T19:59:49Z</updated>
    <published>2014-03-19T19:59:49Z</published>
    <title>Clinical TempEval</title>
    <summary>  We describe the Clinical TempEval task which is currently in preparation for
the SemEval-2015 evaluation exercise. This task involves identifying and
describing events, times and the relations between them in clinical text. Six
discrete subtasks are included, focusing on recognising mentions of times and
events, describing those mentions for both entity types, identifying the
relation between an event and the document creation time, and identifying
narrative container relations.
</summary>
    <author>
      <name>Steven Bethard</name>
    </author>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>James Pustejovsky</name>
    </author>
    <author>
      <name>Marc Verhagen</name>
    </author>
    <link href="http://arxiv.org/abs/1403.4928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3561v1</id>
    <updated>2014-11-13T14:44:00Z</updated>
    <published>2014-11-13T14:44:00Z</published>
    <title>A Text to Speech (TTS) System with English to Punjabi Conversion</title>
    <summary>  The paper aims to show how an application can be developed that converts the
English language into the Punjabi Language, and the same application can
convert the Text to Speech(TTS) i.e. pronounce the text. This application can
be really beneficial for those with special needs.
</summary>
    <author>
      <name>Prabhsimran Singh</name>
    </author>
    <author>
      <name>Amritpal Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Communication System
  Engineering, Volume 1, Issue 04, December 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.3561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2821v1</id>
    <updated>2014-12-09T00:39:16Z</updated>
    <published>2014-12-09T00:39:16Z</published>
    <title>Zipf's Law and the Frequency of Characters or Words of Oracles</title>
    <summary>  The article discusses the frequency of characters of Oracle,concluding that
the frequency and the rank of a word or character is fit to Zipf-Mandelboit Law
or Zipf's law with three parameters,and figuring out the parameters based on
the frequency,and pointing out that what some researchers of Oracle call the
assembling on the two ends is just a description by their impression about the
Oracle data.
</summary>
    <author>
      <name>Xiuli Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1412.2821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00528v1</id>
    <updated>2015-06-01T15:21:00Z</updated>
    <published>2015-06-01T15:21:00Z</published>
    <title>Medical Synonym Extraction with Concept Space Models</title>
    <summary>  In this paper, we present a novel approach for medical synonym extraction. We
aim to integrate the term embedding with the medical domain knowledge for
healthcare applications. One advantage of our method is that it is very
scalable. Experiments on a dataset with more than 1M term pairs show that the
proposed approach outperforms the baseline approaches by a large margin.
</summary>
    <author>
      <name>Chang Wang</name>
    </author>
    <author>
      <name>Liangliang Cao</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, to appear in IJCAI 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.00528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04229v1</id>
    <updated>2015-06-13T05:53:57Z</updated>
    <published>2015-06-13T05:53:57Z</published>
    <title>Evaluation of the Accuracy of the BGLemmatizer</title>
    <summary>  This paper reveals the results of an analysis of the accuracy of developed
software for automatic lemmatization for the Bulgarian language. This
lemmatization software is written entirely in Java and is distributed as a GATE
plugin. Certain statistical methods are used to define the accuracy of this
software. The results of the analysis show 95% lemmatization accuracy.
</summary>
    <author>
      <name>Elena Karashtranova</name>
    </author>
    <author>
      <name>Grigor Iliev</name>
    </author>
    <author>
      <name>Nadezhda Borisova</name>
    </author>
    <author>
      <name>Yana Chankova</name>
    </author>
    <author>
      <name>Irena Atanasova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Sixth International Scientific Conference - FMNS2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00502v1</id>
    <updated>2016-04-02T13:52:23Z</updated>
    <published>2016-04-02T13:52:23Z</published>
    <title>Online Updating of Word Representations for Part-of-Speech Tagging</title>
    <summary>  We propose online unsupervised domain adaptation (DA), which is performed
incrementally as data comes in and is applicable when batch DA is not possible.
In a part-of-speech (POS) tagging evaluation, we find that online unsupervised
DA performs as well as batch DA.
</summary>
    <author>
      <name>Wenpeng Yin</name>
    </author>
    <author>
      <name>Tobias Schnabel</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP'2015. Released POS tagger "FLORS" for online domain adaptation</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03357v1</id>
    <updated>2016-04-12T11:57:05Z</updated>
    <published>2016-04-12T11:57:05Z</published>
    <title>Improving sentence compression by learning to predict gaze</title>
    <summary>  We show how eye-tracking corpora can be used to improve sentence compression
models, presenting a novel multi-task learning algorithm based on multi-layer
LSTMs. We obtain performance competitive with or better than state-of-the-art
approaches.
</summary>
    <author>
      <name>Sigrid Klerke</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2016. Received Best Short Paper Award</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04873v1</id>
    <updated>2016-04-17T14:07:34Z</updated>
    <published>2016-04-17T14:07:34Z</published>
    <title>From Incremental Meaning to Semantic Unit (phrase by phrase)</title>
    <summary>  This paper describes an experimental approach to Detection of Minimal
Semantic Units and their Meaning (DiMSUM), explored within the framework of
SemEval 2016 Task 10. The approach is primarily based on a combination of word
embeddings and parserbased features, and employs unidirectional incremental
computation of compositional embeddings for multiword expressions.
</summary>
    <author>
      <name>Andreas Scherbakov</name>
    </author>
    <author>
      <name>Ekaterina Vylomova</name>
    </author>
    <author>
      <name>Fei Liu</name>
    </author>
    <author>
      <name>Timothy Baldwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, International Workshop on Semantic Evaluation
  (SemEval-2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05559v1</id>
    <updated>2016-04-18T18:51:18Z</updated>
    <published>2016-04-18T18:51:18Z</published>
    <title>Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts</title>
    <summary>  We show that an efficient and popular method for calculating bigram
frequencies is unsuitable for bodies of short texts and offer a simple
alternative. Our method has the same computational complexity as the old method
and offers an exact count instead of an approximation.
</summary>
    <author>
      <name>Melvyn Drag</name>
    </author>
    <author>
      <name>Gauthaman Vasudevan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06650v1</id>
    <updated>2016-04-22T13:33:08Z</updated>
    <published>2016-04-22T13:33:08Z</published>
    <title>Detecting state of aggression in sentences using CNN</title>
    <summary>  In this article we study verbal expression of aggression and its detection
using machine learning and neural networks methods. We test our results using
our corpora of messages from anonymous imageboards. We also compare Random
forest classifier with convolutional neural network for "Movie reviews with one
sentence per review" corpus.
</summary>
    <author>
      <name>Rodmonga Potapova</name>
    </author>
    <author>
      <name>Denis Gordeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted for SPECOM-2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04503v1</id>
    <updated>2016-06-14T19:00:59Z</updated>
    <published>2016-06-14T19:00:59Z</published>
    <title>Shallow Discourse Parsing Using Distributed Argument Representations and
  Bayesian Optimization</title>
    <summary>  This paper describes the Georgia Tech team's approach to the CoNLL-2016
supplementary evaluation on discourse relation sense classification. We use
long short-term memories (LSTM) to induce distributed representations of each
argument, and then combine these representations with surface features in a
neural network. The architecture of the neural network is determined by
Bayesian hyperparameter search.
</summary>
    <author>
      <name> Akanksha</name>
    </author>
    <author>
      <name>Jacob Eisenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">describes our system at the CoNLL 2016 shared task</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08270v1</id>
    <updated>2016-06-27T13:39:54Z</updated>
    <published>2016-06-27T13:39:54Z</published>
    <title>Evaluating Informal-Domain Word Representations With UrbanDictionary</title>
    <summary>  Existing corpora for intrinsic evaluation are not targeted towards tasks in
informal domains such as Twitter or news comment forums. We want to test
whether a representation of informal words fulfills the promise of eliding
explicit text normalization as a preprocessing step. One possible evaluation
metric for such domains is the proximity of spelling variants. We propose how
such a metric might be computed and how a spelling variant dataset can be
collected using UrbanDictionary.
</summary>
    <author>
      <name>Naomi Saphra</name>
    </author>
    <author>
      <name>Adam Lopez</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02801v1</id>
    <updated>2016-12-08T20:33:17Z</updated>
    <published>2016-12-08T20:33:17Z</published>
    <title>Discovering Conversational Dependencies between Messages in Dialogs</title>
    <summary>  We investigate the task of inferring conversational dependencies between
messages in one-on-one online chat, which has become one of the most popular
forms of customer service. We propose a novel probabilistic classifier that
leverages conversational, lexical and semantic information. The approach is
evaluated empirically on a set of customer service chat logs from a Chinese
e-commerce website. It outperforms heuristic baselines.
</summary>
    <author>
      <name>Wenchao Du</name>
    </author>
    <author>
      <name>Pascal Poupart</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI2017 student abstract camera-ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05202v1</id>
    <updated>2016-12-15T19:28:17Z</updated>
    <published>2016-12-15T19:28:17Z</published>
    <title>Building a robust sentiment lexicon with (almost) no resource</title>
    <summary>  Creating sentiment polarity lexicons is labor intensive. Automatically
translating them from resourceful languages requires in-domain machine
translation systems, which rely on large quantities of bi-texts. In this paper,
we propose to replace machine translation by transferring words from the
lexicon through word embeddings aligned across languages with a simple linear
transform. The approach leads to no degradation, compared to machine
translation, when tested on sentiment polarity classification on tweets from
four languages.
</summary>
    <author>
      <name>Mickael Rouvier</name>
    </author>
    <author>
      <name>Benoit Favre</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03091v1</id>
    <updated>2017-03-09T01:04:07Z</updated>
    <published>2017-03-09T01:04:07Z</published>
    <title>Deep Learning applied to NLP</title>
    <summary>  Convolutional Neural Network (CNNs) are typically associated with Computer
Vision. CNNs are responsible for major breakthroughs in Image Classification
and are the core of most Computer Vision systems today. More recently CNNs have
been applied to problems in Natural Language Processing and gotten some
interesting results. In this paper, we will try to explain the basics of CNNs,
its different variations and how they have been applied to NLP.
</summary>
    <author>
      <name>Marc Moreno Lopez</name>
    </author>
    <author>
      <name>Jugal Kalita</name>
    </author>
    <link href="http://arxiv.org/abs/1703.03091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05465v1</id>
    <updated>2017-03-16T03:15:22Z</updated>
    <published>2017-03-16T03:15:22Z</published>
    <title>Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity
  Model</title>
    <summary>  This paper describes a neural-network model which performed competitively
(top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS)
task. Our system employs an attention-based recurrent neural network model that
optimizes the sentence similarity. In this paper, we describe our participation
in the multilingual STS task which measures similarity across English, Spanish,
and Arabic.
</summary>
    <author>
      <name>Wenli Zhuang</name>
    </author>
    <author>
      <name>Ernie Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07438v2</id>
    <updated>2017-07-22T04:44:38Z</updated>
    <published>2017-03-21T21:36:28Z</published>
    <title>The NLTK FrameNet API: Designing for Discoverability with a Rich
  Linguistic Resource</title>
    <summary>  A new Python API, integrated within the NLTK suite, offers access to the
FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as
well as annotated sentences can be processed programatically, or browsed with
human-readable displays via the interactive Python prompt.
</summary>
    <author>
      <name>Nathan Schneider</name>
    </author>
    <author>
      <name>Chuck Wooters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017 Demo</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07438v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07438v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00902v1</id>
    <updated>2018-03-02T15:41:33Z</updated>
    <published>2018-03-02T15:41:33Z</published>
    <title>DEMorphy, German Language Morphological Analyzer</title>
    <summary>  DEMorphy is a morphological analyzer for German. It is built onto large,
compactified lexicons from German Morphological Dictionary. A guesser based on
German declension suffixed is also provided. For German, we provided a
state-of-art morphological analyzer. DEMorphy is implemented in Python with
ease of usability and accompanying documentation. The package is suitable for
both academic and commercial purposes wit a permissive licence.
</summary>
    <author>
      <name>Duygu Altinok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09103v1</id>
    <updated>2018-03-24T13:08:56Z</updated>
    <published>2018-03-24T13:08:56Z</published>
    <title>Machine Learning and Applied Linguistics</title>
    <summary>  This entry introduces the topic of machine learning and provides an overview
of its relevance for applied linguistics and language learning. The discussion
will focus on giving an introduction to the methods and applications of machine
learning in applied linguistics, and will provide references for further study.
</summary>
    <author>
      <name>Sowmya Vajjala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/9781405198431.wbeal1486</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/9781405198431.wbeal1486" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print version of the article that is accepted for publication in
  "Encyclopedia of Applied Linguistics"</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09901v1</id>
    <updated>2018-03-27T05:23:01Z</updated>
    <published>2018-03-27T05:23:01Z</published>
    <title>Mittens: An Extension of GloVe for Learning Domain-Specialized
  Representations</title>
    <summary>  We present a simple extension of the GloVe representation learning model that
begins with general-purpose representations and updates them based on data from
a specialized domain. We show that the resulting representations can lead to
faster learning and better results on a variety of tasks.
</summary>
    <author>
      <name>Nicholas Dingwall</name>
    </author>
    <author>
      <name>Christopher Potts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03710v1</id>
    <updated>2018-05-09T19:54:58Z</updated>
    <published>2018-05-09T19:54:58Z</published>
    <title>Incorporating Subword Information into Matrix Factorization Word
  Embeddings</title>
    <summary>  The positive effect of adding subword information to word embeddings has been
demonstrated for predictive models. In this paper we investigate whether
similar benefits can also be derived from incorporating subwords into counting
models. We evaluate the impact of different types of subwords (n-grams and
unsupervised morphemes), with results confirming the importance of subword
information in learning representations of rare and out-of-vocabulary words.
</summary>
    <author>
      <name>Alexandre Salle</name>
    </author>
    <author>
      <name>Aline Villavicencio</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03774v1</id>
    <updated>2018-05-10T01:27:58Z</updated>
    <published>2018-05-10T01:27:58Z</published>
    <title>The Evolution of Popularity and Images of Characters in Marvel Cinematic
  Universe Fanfictions</title>
    <summary>  This analysis proposes a new topic model to study the yearly trends in Marvel
Cinematic Universe fanfictions on three levels: character popularity, character
images/topics, and vocabulary pattern of topics. It is found that character
appearances in fanfictions have become more diverse over the years thanks to
constant introduction of new characters in feature films, and in the case of
Captain America, multi-dimensional character development is well-received by
the fanfiction world.
</summary>
    <author>
      <name>Fan Bu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06648v1</id>
    <updated>2018-05-17T08:29:09Z</updated>
    <published>2018-05-17T08:29:09Z</published>
    <title>Extrapolation in NLP</title>
    <summary>  We argue that extrapolation to examples outside the training space will often
be easier for models that capture global structures, rather than just maximise
their local fit to the training data. We show that this is true for two popular
models: the Decomposable Attention Model and word2vec.
</summary>
    <author>
      <name>Jeff Mitchell</name>
    </author>
    <author>
      <name>Pasquale Minervini</name>
    </author>
    <author>
      <name>Pontus Stenetorp</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11461v1</id>
    <updated>2018-05-28T16:17:27Z</updated>
    <published>2018-05-28T16:17:27Z</published>
    <title>Syntactic Dependency Representations in Neural Relation Classification</title>
    <summary>  We investigate the use of different syntactic dependency representations in a
neural relation classification task and compare the CoNLL, Stanford Basic and
Universal Dependencies schemes. We further compare with a syntax-agnostic
approach and perform an error analysis in order to gain a better understanding
of the results.
</summary>
    <author>
      <name>Farhad Nooralahzadeh</name>
    </author>
    <author>
      <name>Lilja Øvrelid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1804.08887</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11474v3</id>
    <updated>2021-03-12T20:16:34Z</updated>
    <published>2018-05-29T13:53:16Z</published>
    <title>Human vs Automatic Metrics: on the Importance of Correlation Design</title>
    <summary>  This paper discusses two existing approaches to the correlation analysis
between automatic evaluation metrics and human scores in the area of natural
language generation. Our experiments show that depending on the usage of a
system- or sentence-level correlation analysis, correlation results between
automatic scores and human judgments are inconsistent.
</summary>
    <author>
      <name>Anastasia Shimorina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for the WiNLP workshop at NAACL 2018; 3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11474v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11474v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04068v1</id>
    <updated>2018-06-11T15:50:13Z</updated>
    <published>2018-06-11T15:50:13Z</published>
    <title>A Co-Matching Model for Multi-choice Reading Comprehension</title>
    <summary>  Multi-choice reading comprehension is a challenging task, which involves the
matching between a passage and a question-answer pair. This paper proposes a
new co-matching approach to this problem, which jointly models whether a
passage can match both a question and a candidate answer. Experimental results
on the RACE dataset demonstrate that our approach achieves state-of-the-art
performance.
</summary>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Mo Yu</name>
    </author>
    <author>
      <name>Shiyu Chang</name>
    </author>
    <author>
      <name>Jing Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6, accepted ACL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09533v1</id>
    <updated>2018-06-22T15:37:35Z</updated>
    <published>2018-06-22T15:37:35Z</published>
    <title>Using NLP on news headlines to predict index trends</title>
    <summary>  This paper attempts to provide a state of the art in trend prediction using
news headlines. We present the research done on predicting DJIA trends using
Natural Language Processing. We will explain the different algorithms we have
used as well as the various embedding techniques attempted. We rely on
statistical and deep learning models in order to extract information from the
corpuses.
</summary>
    <author>
      <name>Marc Velay</name>
    </author>
    <author>
      <name>Fabrice Daniel</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11322v1</id>
    <updated>2018-06-29T09:51:26Z</updated>
    <published>2018-06-29T09:51:26Z</published>
    <title>Bias in Semantic and Discourse Interpretation</title>
    <summary>  In this paper, we show how game-theoretic work on conversation combined with
a theory of discourse structure provides a framework for studying interpretive
bias. Interpretive bias is an essential feature of learning and understanding
but also something that can be used to pervert or subvert the truth. The
framework we develop here provides tools for understanding and analyzing the
range of interpretive biases and the factors that contribute to them.
</summary>
    <author>
      <name>Nicholas Asher</name>
    </author>
    <author>
      <name>Soumya Paul</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01528v1</id>
    <updated>2020-07-03T07:31:14Z</updated>
    <published>2020-07-03T07:31:14Z</published>
    <title>On-The-Fly Information Retrieval Augmentation for Language Models</title>
    <summary>  Here we experiment with the use of information retrieval as an augmentation
for pre-trained language models. The text corpus used in information retrieval
can be viewed as form of episodic memory which grows over time. By augmenting
GPT 2.0 with information retrieval we achieve a zero shot 15% relative
reduction in perplexity on Gigaword corpus without any re-training. We also
validate our IR augmentation on an event co-reference task.
</summary>
    <author>
      <name>Hai Wang</name>
    </author>
    <author>
      <name>David McAllester</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2020 NUSE Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04247v1</id>
    <updated>2020-06-23T13:47:58Z</updated>
    <published>2020-06-23T13:47:58Z</published>
    <title>Neural relation extraction: a survey</title>
    <summary>  Neural relation extraction discovers semantic relations between entities from
unstructured text using deep learning methods. In this study, we present a
comprehensive review of methods on neural network based relation extraction. We
discuss advantageous and incompetent sides of existing studies and investigate
additional research directions and improvement ideas in this field.
</summary>
    <author>
      <name>Mehmet Aydar</name>
    </author>
    <author>
      <name>Ozge Bozal</name>
    </author>
    <author>
      <name>Furkan Ozbay</name>
    </author>
    <link href="http://arxiv.org/abs/2007.04247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.04968v1</id>
    <updated>2020-09-10T16:17:10Z</updated>
    <published>2020-09-10T16:17:10Z</published>
    <title>Modern Methods for Text Generation</title>
    <summary>  Synthetic text generation is challenging and has limited success. Recently, a
new architecture, called Transformers, allow machine learning models to
understand better sequential data, such as translation or summarization. BERT
and GPT-2, using Transformers in their cores, have shown a great performance in
tasks such as text classification, translation and NLI tasks. In this article,
we analyse both algorithms and compare their output quality in text generation
tasks.
</summary>
    <author>
      <name>Dimas Munoz Montesinos</name>
    </author>
    <link href="http://arxiv.org/abs/2009.04968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11340v2</id>
    <updated>2020-10-01T10:02:57Z</updated>
    <published>2020-09-23T19:03:58Z</published>
    <title>The importance of fillers for text representations of speech transcripts</title>
    <summary>  While being an essential component of spoken language, fillers (e.g."um" or
"uh") often remain overlooked in Spoken Language Understanding (SLU) tasks. We
explore the possibility of representing them with deep contextualised
embeddings, showing improvements on modelling spoken language and two
downstream tasks - predicting a speaker's stance and expressed confidence.
</summary>
    <author>
      <name>Tanvi Dinkar</name>
    </author>
    <author>
      <name>Pierre Colombo</name>
    </author>
    <author>
      <name>Matthieu Labeau</name>
    </author>
    <author>
      <name>Chloé Clavel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13602v1</id>
    <updated>2020-09-10T11:37:00Z</updated>
    <published>2020-09-10T11:37:00Z</published>
    <title>Non-Pharmaceutical Intervention Discovery with Topic Modeling</title>
    <summary>  We consider the task of discovering categories of non-pharmaceutical
interventions during the evolving COVID-19 pandemic. We explore topic modeling
on two corpora with national and international scope. These models discover
existing categories when compared with human intervention labels while reduced
human effort needed.
</summary>
    <author>
      <name>Jonathan Smith</name>
    </author>
    <author>
      <name>Borna Ghotbi</name>
    </author>
    <author>
      <name>Seungeun Yi</name>
    </author>
    <author>
      <name>Mahboobeh Parsapoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ML for Global Health (ICML 2020 Workshop)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14384v1</id>
    <updated>2020-09-30T01:52:00Z</updated>
    <published>2020-09-30T01:52:00Z</published>
    <title>Development of Word Embeddings for Uzbek Language</title>
    <summary>  In this paper, we share the process of developing word embeddings for the
Cyrillic variant of the Uzbek language. The result of our work is the first
publicly available set of word vectors trained on the word2vec, GloVe, and
fastText algorithms using a high-quality web crawl corpus developed in-house.
The developed word embeddings can be used in many natural language processing
downstream tasks.
</summary>
    <author>
      <name>B. Mansurov</name>
    </author>
    <author>
      <name>A. Mansurov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.14384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02269v2</id>
    <updated>2021-07-12T10:18:11Z</updated>
    <published>2021-03-03T09:11:18Z</published>
    <title>Lex2vec: making Explainable Word Embeddings via Lexical Resources</title>
    <summary>  In this technical report, we propose an algorithm, called Lex2vec that
exploits lexical resources to inject information into word embeddings and name
the embedding dimensions by means of knowledge bases. We evaluate the optimal
parameters to extract a number of informative labels that is readable and has a
good coverage for the embedding dimensions.
</summary>
    <author>
      <name>Fabio Celli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.02269v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02269v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14411v1</id>
    <updated>2021-03-26T11:41:52Z</updated>
    <published>2021-03-26T11:41:52Z</published>
    <title>Functorial Language Models</title>
    <summary>  We introduce functorial language models: a principled way to compute
probability distributions over word sequences given a monoidal functor from
grammar to meaning. This yields a method for training categorical compositional
distributional (DisCoCat) models on raw text data. We provide a
proof-of-concept implementation in DisCoPy, the Python toolbox for monoidal
categories.
</summary>
    <author>
      <name>Alexis Toumi</name>
    </author>
    <author>
      <name>Alex Koziell-Pipe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SemSpace 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14961v1</id>
    <updated>2021-03-27T18:28:33Z</updated>
    <published>2021-03-27T18:28:33Z</published>
    <title>Supersense and Sensibility: Proxy Tasks for Semantic Annotation of
  Prepositions</title>
    <summary>  Prepositional supersense annotation is time-consuming and requires expert
training. Here, we present two sensible methods for obtaining prepositional
supersense annotations by eliciting surface substitution and similarity
judgments. Four pilot studies suggest that both methods have potential for
producing prepositional supersense annotations that are comparable in quality
to expert annotations.
</summary>
    <author>
      <name>Luke Gessler</name>
    </author>
    <author>
      <name>Shira Wein</name>
    </author>
    <author>
      <name>Nathan Schneider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at LAW XIV in 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01052v1</id>
    <updated>2021-05-03T17:51:17Z</updated>
    <published>2021-05-03T17:51:17Z</published>
    <title>Applied Language Technology: NLP for the Humanities</title>
    <summary>  This contribution describes a two-course module that seeks to provide
humanities majors with a basic understanding of language technology and its
applications using Python. The learning materials consist of interactive
Jupyter Notebooks and accompanying YouTube videos, which are openly available
with a Creative Commons licence.
</summary>
    <author>
      <name>Tuomo Hiippala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/2021.teachingnlp-1.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/2021.teachingnlp-1.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Fifth Workshop on Teaching NLP, 2021, pp. 46-48</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.01052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06514v1</id>
    <updated>2021-05-13T19:09:22Z</updated>
    <published>2021-05-13T19:09:22Z</published>
    <title>Distilling BERT for low complexity network training</title>
    <summary>  This paper studies the efficiency of transferring BERT learnings to low
complexity models like BiLSTM, BiLSTM with attention and shallow CNNs using
sentiment analysis on SST-2 dataset. It also compares the complexity of
inference of the BERT model with these lower complexity models and underlines
the importance of these techniques in enabling high performance NLP models on
edge devices like mobiles, tablets and MCU development boards like Raspberry Pi
etc. and enabling exciting new applications.
</summary>
    <author>
      <name>Bansidhar Mangalwedhekar</name>
    </author>
    <link href="http://arxiv.org/abs/2105.06514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11197v1</id>
    <updated>2021-05-24T10:49:04Z</updated>
    <published>2021-05-24T10:49:04Z</published>
    <title>Towards Standard Criteria for human evaluation of Chatbots: A Survey</title>
    <summary>  Human evaluation is becoming a necessity to test the performance of Chatbots.
However, off-the-shelf settings suffer the severe reliability and replication
issues partly because of the extremely high diversity of criteria. It is high
time to come up with standard criteria and exact definitions. To this end, we
conduct a through investigation of 105 papers involving human evaluation for
Chatbots. Deriving from this, we propose five standard criteria along with
precise definitions.
</summary>
    <author>
      <name>Hongru Liang</name>
    </author>
    <author>
      <name>Huaqing Li</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11412v1</id>
    <updated>2021-05-24T17:02:24Z</updated>
    <published>2021-05-24T17:02:24Z</published>
    <title>Reproducibility Report: Contextualizing Hate Speech Classifiers with
  Post-hoc Explanation</title>
    <summary>  The presented report evaluates Contextualizing Hate Speech Classifiers with
Post-hoc Explanation paper within the scope of ML Reproducibility Challenge
2020. Our work focuses on both aspects constituting the paper: the method
itself and the validity of the stated results. In the following sections, we
have described the paper, related works, algorithmic frameworks, our
experiments and evaluations.
</summary>
    <author>
      <name>Kiran Purohit</name>
    </author>
    <author>
      <name>Owais Iqbal</name>
    </author>
    <author>
      <name>Ankan Mullick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.11412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00705v1</id>
    <updated>2015-09-02T13:54:57Z</updated>
    <published>2015-09-02T13:54:57Z</published>
    <title>Analysis of Communication Pattern with Scammers in Enron Corpus</title>
    <summary>  This paper is an exploratory analysis into fraud detection taking Enron email
corpus as the case study. The paper posits conclusions like strict servitude
and unquestionable faith among employees as breeding grounds for sham among
higher executives. We also try to infer on the nature of communication between
fraudulent employees and between non- fraudulent-fraudulent employees
</summary>
    <author>
      <name>Dinesh Balaji Sashikanth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.00705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03295v3</id>
    <updated>2016-02-27T07:58:19Z</updated>
    <published>2015-09-09T11:27:03Z</published>
    <title>Liberating language research from dogmas of the 20th century</title>
    <summary>  A commentary on the article "Large-scale evidence of dependency length
minimization in 37 languages" by Futrell, Mahowald &amp; Gibson (PNAS 2015 112 (33)
10336-10341).
</summary>
    <author>
      <name>Ramon Ferrer-i-Cancho</name>
    </author>
    <author>
      <name>Carlos Gómez-Rodríguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor corrections</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Liberating language research from dogmas of the 20th century.
  Glottometrics 33, 33-34 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.03295v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03295v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04513v1</id>
    <updated>2016-03-15T00:25:02Z</updated>
    <published>2016-03-15T00:25:02Z</published>
    <title>Multichannel Variable-Size Convolution for Sentence Classification</title>
    <summary>  We propose MVCNN, a convolution neural network (CNN) architecture for
sentence classification. It (i) combines diverse versions of pretrained word
embeddings and (ii) extracts features of multigranular phrases with
variable-size convolution filters. We also show that pretraining MVCNN is
critical for good performance. MVCNN achieves state-of-the-art performance on
four tasks: on small-scale binary, small-scale multi-class and largescale
Twitter sentiment prediction and on subjectivity classification.
</summary>
    <author>
      <name>Wenpeng Yin</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceeding of CoNLL2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02945v2</id>
    <updated>2016-05-11T17:20:26Z</updated>
    <published>2016-05-10T11:29:28Z</published>
    <title>The Yahoo Query Treebank, V. 1.0</title>
    <summary>  A description and annotation guidelines for the Yahoo Webscope release of
Query Treebank, Version 1.0, May 2016.
</summary>
    <author>
      <name>Yuval Pinter</name>
    </author>
    <author>
      <name>Roi Reichart</name>
    </author>
    <author>
      <name>Idan Szpektor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Co-released with the Webscope Dataset (L-28) and with Pinter et al.,
  Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02945v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02945v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05172v2</id>
    <updated>2016-07-02T12:29:08Z</updated>
    <published>2016-05-17T14:07:43Z</published>
    <title>Siamese convolutional networks based on phonetic features for cognate
  identification</title>
    <summary>  In this paper, we explore the use of convolutional networks (ConvNets) for
the purpose of cognate identification. We compare our architecture with binary
classifiers based on string similarity measures on different language families.
Our experiments show that convolutional networks achieve competitive results
across concepts and across language families at the task of cognate
identification.
</summary>
    <author>
      <name>Taraka Rama</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05172v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05172v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07346v1</id>
    <updated>2016-05-24T09:19:05Z</updated>
    <published>2016-05-24T09:19:05Z</published>
    <title>Multi-Level Analysis and Annotation of Arabic Corpora for Text-to-Sign
  Language MT</title>
    <summary>  In this paper, we present an ongoing effort in lexical semantic analysis and
annotation of Modern Standard Arabic (MSA) text, a semi automatic annotation
tool concerned with the morphologic, syntactic, and semantic levels of
description.
</summary>
    <author>
      <name>Abdelaziz Lakhfif</name>
    </author>
    <author>
      <name>Mohammed T. Laskri</name>
    </author>
    <author>
      <name>Eric Atwell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Second Workshop on Arabic Corpus Linguistics (WACL-2), 22nd July
  2013, Lancaster University, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04110v1</id>
    <updated>2016-07-14T12:45:07Z</updated>
    <published>2016-07-14T12:45:07Z</published>
    <title>Using Recurrent Neural Network for Learning Expressive Ontologies</title>
    <summary>  Recently, Neural Networks have been proven extremely effective in many
natural language processing tasks such as sentiment analysis, question
answering, or machine translation. Aiming to exploit such advantages in the
Ontology Learning process, in this technical report we present a detailed
description of a Recurrent Neural Network based system to be used to pursue
such goal.
</summary>
    <author>
      <name>Giulio Petrucci</name>
    </author>
    <author>
      <name>Chiara Ghidini</name>
    </author>
    <author>
      <name>Marco Rospocher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05408v1</id>
    <updated>2016-07-19T05:38:58Z</updated>
    <published>2016-07-19T05:38:58Z</published>
    <title>Discriminating between similar languages in Twitter using label
  propagation</title>
    <summary>  Identifying the language of social media messages is an important first step
in linguistic processing. Existing models for Twitter focus on content
analysis, which is successful for dissimilar language pairs. We propose a label
propagation approach that takes the social graph of tweet authors into account
as well as content to better tease apart similar languages. This results in
state-of-the-art shared task performance of $76.63\%$, $1.4\%$ higher than the
top system.
</summary>
    <author>
      <name>Will Radford</name>
    </author>
    <author>
      <name>Matthias Galle</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05650v2</id>
    <updated>2016-09-07T03:09:17Z</updated>
    <published>2016-07-13T06:57:38Z</published>
    <title>A Supervised Authorship Attribution Framework for Bengali Language</title>
    <summary>  Authorship Attribution is a long-standing problem in Natural Language
Processing. Several statistical and computational methods have been used to
find a solution to this problem. In this paper, we have proposed methods to
deal with the authorship attribution problem in Bengali.
</summary>
    <author>
      <name>Shanta Phani</name>
    </author>
    <author>
      <name>Shibamouli Lahiri</name>
    </author>
    <author>
      <name>Arindam Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors as the results need to
  be changed</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05650v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05650v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05755v4</id>
    <updated>2017-03-14T02:36:05Z</updated>
    <published>2016-07-10T19:14:00Z</published>
    <title>A New Bengali Readability Score</title>
    <summary>  In this paper we have proposed methods to analyze the readability of Bengali
language texts. We have got some exceptionally good results out of the
experiments.
</summary>
    <author>
      <name>Shanta Phani</name>
    </author>
    <author>
      <name>Shibamouli Lahiri</name>
    </author>
    <author>
      <name>Arindam Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author as the results need to be
  changed</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05755v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05755v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07931v1</id>
    <updated>2016-07-27T01:44:54Z</updated>
    <published>2016-07-27T01:44:54Z</published>
    <title>Synthetic Language Generation and Model Validation in BEAST2</title>
    <summary>  Generating synthetic languages aids in the testing and validation of future
computational linguistic models and methods. This thesis extends the BEAST2
phylogenetic framework to add linguistic sequence generation under multiple
models. The new plugin is then used to test the effects of the phenomena of
word borrowing on the inference process under two widely used phylolinguistic
models.
</summary>
    <author>
      <name>Stuart Bradley</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08592v1</id>
    <updated>2016-07-28T19:47:25Z</updated>
    <published>2016-07-28T19:47:25Z</published>
    <title>Modeling selectional restrictions in a relational type system</title>
    <summary>  Selectional restrictions are semantic constraints on forming certain complex
types in natural language. The paper gives an overview of modeling selectional
restrictions in a relational type system with morphological and syntactic
types. We discuss some foundations of the system and ways of formalizing
selectional restrictions.
  Keywords: type theory, selectional restrictions, syntax, morphology
</summary>
    <author>
      <name>Erkki Luuk</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02700v1</id>
    <updated>2017-05-07T21:16:35Z</updated>
    <published>2017-05-07T21:16:35Z</published>
    <title>Generating Memorable Mnemonic Encodings of Numbers</title>
    <summary>  The major system is a mnemonic system that can be used to memorize sequences
of numbers. In this work, we present a method to automatically generate
sentences that encode a given number. We propose several encoding models and
compare the most promising ones in a password memorability study. The results
of the study show that a model combining part-of-speech sentence templates with
an $n$-gram language model produces the most memorable password
representations.
</summary>
    <author>
      <name>Vincent Fiorentini</name>
    </author>
    <author>
      <name>Megan Shao</name>
    </author>
    <author>
      <name>Julie Medero</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03645v1</id>
    <updated>2017-05-10T08:05:44Z</updated>
    <published>2017-05-10T08:05:44Z</published>
    <title>A Survey of Deep Learning Methods for Relation Extraction</title>
    <summary>  Relation Extraction is an important sub-task of Information Extraction which
has the potential of employing deep learning (DL) models with the creation of
large datasets using distant supervision. In this review, we compare the
contributions and pitfalls of the various DL models that have been used for the
task, to help guide the path ahead.
</summary>
    <author>
      <name>Shantanu Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05437v1</id>
    <updated>2017-05-10T10:00:00Z</updated>
    <published>2017-05-10T10:00:00Z</published>
    <title>A Biomedical Information Extraction Primer for NLP Researchers</title>
    <summary>  Biomedical Information Extraction is an exciting field at the crossroads of
Natural Language Processing, Biology and Medicine. It encompasses a variety of
different tasks that require application of state-of-the-art NLP techniques,
such as NER and Relation Extraction. This paper provides an overview of the
problems in the field and discusses some of the techniques used for solving
them.
</summary>
    <author>
      <name>Surag Nair</name>
    </author>
    <link href="http://arxiv.org/abs/1705.05437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09755v1</id>
    <updated>2017-05-27T02:23:18Z</updated>
    <published>2017-05-27T02:23:18Z</published>
    <title>word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA</title>
    <summary>  We show that the skip-gram formulation of word2vec trained with negative
sampling is equivalent to a weighted logistic PCA. This connection allows us to
better understand the objective, compare it to other word embedding methods,
and extend it to higher dimensional models.
</summary>
    <author>
      <name>Andrew J. Landgraf</name>
    </author>
    <author>
      <name>Jeremy Bellay</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05785v2</id>
    <updated>2018-01-18T20:24:40Z</updated>
    <published>2017-12-15T18:41:53Z</published>
    <title>Sentiment Predictability for Stocks</title>
    <summary>  In this work, we present our findings and experiments for stock-market
prediction using various textual sentiment analysis tools, such as mood
analysis and event extraction, as well as prediction models, such as LSTMs and
specific convolutional architectures.
</summary>
    <author>
      <name>Jordan Prosky</name>
    </author>
    <author>
      <name>Xingyou Song</name>
    </author>
    <author>
      <name>Andrew Tan</name>
    </author>
    <author>
      <name>Michael Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05785v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05785v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07370v1</id>
    <updated>2018-02-20T23:08:19Z</updated>
    <published>2018-02-20T23:08:19Z</published>
    <title>SufiSent - Universal Sentence Representations Using Suffix Encodings</title>
    <summary>  Computing universal distributed representations of sentences is a fundamental
task in natural language processing. We propose a method to learn such
representations by encoding the suffixes of word sequences in a sentence and
training on the Stanford Natural Language Inference (SNLI) dataset. We
demonstrate the effectiveness of our approach by evaluating it on the SentEval
benchmark, improving on existing approaches on several transfer tasks.
</summary>
    <author>
      <name>Siddhartha Brahma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Submitted to ICLR 2018 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07374v1</id>
    <updated>2018-02-20T23:22:25Z</updated>
    <published>2018-02-20T23:22:25Z</published>
    <title>On the scaling of polynomial features for representation matching</title>
    <summary>  In many neural models, new features as polynomial functions of existing ones
are used to augment representations. Using the natural language inference task
as an example, we investigate the use of scaled polynomials of degree 2 and
above as matching features. We find that scaling degree 2 features has the
highest impact on performance, reducing classification error by 5% in the best
models.
</summary>
    <author>
      <name>Siddhartha Brahma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Submitted to ICLR 2018 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02286v1</id>
    <updated>2018-04-06T14:11:33Z</updated>
    <published>2018-04-06T14:11:33Z</published>
    <title>Chart Parsing Multimodal Grammars</title>
    <summary>  The short note describes the chart parser for multimodal type-logical
grammars which has been developed in conjunction with the type-logical treebank
for French. The chart parser presents an incomplete but fast implementation of
proof search for multimodal type-logical grammars using the "deductive parsing"
framework. Proofs found can be transformed to natural deduction proofs.
</summary>
    <author>
      <name>Richard Moot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNRS, LIRMM/INFO, UM</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1804.02286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02472v1</id>
    <updated>2018-04-06T22:11:17Z</updated>
    <published>2018-04-06T22:11:17Z</published>
    <title>Neural models of factuality</title>
    <summary>  We present two neural models for event factuality prediction, which yield
significant performance gains over previous models on three event factuality
datasets: FactBank, UW, and MEANTIME. We also present a substantial expansion
of the It Happened portion of the Universal Decompositional Semantics dataset,
yielding the largest event factuality dataset to date. We report model results
on this extended factuality dataset as well.
</summary>
    <author>
      <name>Rachel Rudinger</name>
    </author>
    <author>
      <name>Aaron Steven White</name>
    </author>
    <author>
      <name>Benjamin Van Durme</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05499v1</id>
    <updated>2018-04-16T04:03:52Z</updated>
    <published>2018-04-16T04:03:52Z</published>
    <title>Community Member Retrieval on Social Media using Textual Information</title>
    <summary>  This paper addresses the problem of community membership detection using only
text features in a scenario where a small number of positive labeled examples
defines the community. The solution introduces an unsupervised proxy task for
learning user embeddings: user re-identification. Experiments with 16 different
communities show that the resulting embeddings are more effective for community
membership identification than common unsupervised representations.
</summary>
    <author>
      <name>Aaron Jaech</name>
    </author>
    <author>
      <name>Shobhit Hathi</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06719v1</id>
    <updated>2018-04-14T18:19:38Z</updated>
    <published>2018-04-14T18:19:38Z</published>
    <title>Distribution-based Prediction of the Degree of Grammaticalization for
  German Prepositions</title>
    <summary>  We test the hypothesis that the degree of grammaticalization of German
prepositions correlates with their corpus-based contextual dispersion measured
by word entropy. We find that there is indeed a moderate correlation for
entropy, but a stronger correlation for frequency and number of context types.
</summary>
    <author>
      <name>Dominik Schlechtweg</name>
    </author>
    <author>
      <name>Sabine Schulte im Walde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, EvoLang</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08125v2</id>
    <updated>2018-12-13T14:59:33Z</updated>
    <published>2018-04-22T15:44:17Z</published>
    <title>Reduce, Reuse, Recycle: New uses for old QA resources</title>
    <summary>  We investigate applying repurposed generic QA data and models to a recently
proposed relation extraction task. We find that training on SQuAD produces
better zero-shot performance and more robust generalisation compared to the
task specific training set. We also show that standard QA architectures (e.g.
FastQA or BiDAF) can be applied to the slot filling queries without the need
for model modification.
</summary>
    <author>
      <name>Jeff Mitchell</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08166v1</id>
    <updated>2018-04-22T20:42:13Z</updated>
    <published>2018-04-22T20:42:13Z</published>
    <title>Word Embedding Perturbation for Sentence Classification</title>
    <summary>  In this technique report, we aim to mitigate the overfitting problem of
natural language by applying data augmentation methods. Specifically, we
attempt several types of noise to perturb the input word embedding, such as
Gaussian noise, Bernoulli noise, and adversarial noise, etc. We also apply
several constraints on different types of noise. By implementing these proposed
data augmentation methods, the baseline models can gain improvements on several
sentence classification tasks.
</summary>
    <author>
      <name>Dongxu Zhang</name>
    </author>
    <author>
      <name>Zhichao Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08426v1</id>
    <updated>2018-04-19T09:42:01Z</updated>
    <published>2018-04-19T09:42:01Z</published>
    <title>LightRel SemEval-2018 Task 7: Lightweight and Fast Relation
  Classification</title>
    <summary>  We present LightRel, a lightweight and fast relation classifier. Our goal is
to develop a high baseline for different relation extraction tasks. By defining
only very few data-internal, word-level features and external knowledge sources
in the form of word clusters and word embeddings, we train a fast and simple
linear classifier.
</summary>
    <author>
      <name>Tyler Renslow</name>
    </author>
    <author>
      <name>Günter Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SemEval-2018 task 7 Semantic Relation Extraction and Classification
  in Scientific Papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09301v1</id>
    <updated>2018-04-25T00:46:14Z</updated>
    <published>2018-04-25T00:46:14Z</published>
    <title>Gender Bias in Coreference Resolution</title>
    <summary>  We present an empirical study of gender bias in coreference resolution
systems. We first introduce a novel, Winograd schema-style set of minimal pair
sentences that differ only by pronoun gender. With these "Winogender schemas,"
we evaluate and confirm systematic gender bias in three publicly-available
coreference resolution systems, and correlate this bias with real-world and
textual gender statistics.
</summary>
    <author>
      <name>Rachel Rudinger</name>
    </author>
    <author>
      <name>Jason Naradowsky</name>
    </author>
    <author>
      <name>Brian Leonard</name>
    </author>
    <author>
      <name>Benjamin Van Durme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NAACL-HLT 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.09301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11324v1</id>
    <updated>2018-04-30T17:03:07Z</updated>
    <published>2018-04-30T17:03:07Z</published>
    <title>Accelerating NMT Batched Beam Decoding with LMBR Posteriors for
  Deployment</title>
    <summary>  We describe a batched beam decoding algorithm for NMT with LMBR n-gram
posteriors, showing that LMBR techniques still yield gains on top of the best
recently reported results with Transformers. We also discuss acceleration
strategies for deployment, and the effect of the beam size and batching on
memory and speed.
</summary>
    <author>
      <name>Gonzalo Iglesias</name>
    </author>
    <author>
      <name>William Tambellini</name>
    </author>
    <author>
      <name>Adrià De Gispert</name>
    </author>
    <author>
      <name>Eva Hasler</name>
    </author>
    <author>
      <name>Bill Byrne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NAACL-HLT 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.11324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03654v1</id>
    <updated>2018-07-10T14:02:35Z</updated>
    <published>2018-07-10T14:02:35Z</published>
    <title>Linguistic Characteristics of Censorable Language on SinaWeibo</title>
    <summary>  This paper investigates censorship from a linguistic perspective. We collect
a corpus of censored and uncensored posts on a number of topics, build a
classifier that predicts censorship decisions independent of discussion topics.
Our investigation reveals that the strongest linguistic indicator of censored
content of our corpus is its readability.
</summary>
    <author>
      <name>Kei Yin Ng</name>
    </author>
    <author>
      <name>Anna Feldman</name>
    </author>
    <author>
      <name>Jing Peng</name>
    </author>
    <author>
      <name>Chris Leberknight</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1st Workshop on NLP for Internet Freedom (NLP4IF-2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.03654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05519v1</id>
    <updated>2018-07-15T09:36:39Z</updated>
    <published>2018-07-15T09:36:39Z</published>
    <title>Concept-Based Embeddings for Natural Language Processing</title>
    <summary>  In this work, we focus on effectively leveraging and integrating information
from concept-level as well as word-level via projecting concepts and words into
a lower dimensional space while retaining most critical semantics. In a broad
context of opinion understanding system, we investigate the use of the fused
embedding for several core NLP tasks: named entity detection and
classification, automatic speech recognition reranking, and targeted sentiment
analysis.
</summary>
    <author>
      <name>Yukun Ma</name>
    </author>
    <author>
      <name>Erik Cambria</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01140v1</id>
    <updated>2020-01-04T23:27:59Z</updated>
    <published>2020-01-04T23:27:59Z</published>
    <title>Transformer-based language modeling and decoding for conversational
  speech recognition</title>
    <summary>  We propose a way to use a transformer-based language model in conversational
speech recognition. Specifically, we focus on decoding efficiently in a
weighted finite-state transducer framework. We showcase an approach to lattice
re-scoring that allows for longer range history captured by a transfomer-based
language model and takes advantage of a transformer's ability to avoid
computing sequentially.
</summary>
    <author>
      <name>Kareem Nassar</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03521v1</id>
    <updated>2020-01-10T15:45:59Z</updated>
    <published>2020-01-10T15:45:59Z</published>
    <title>Towards Minimal Supervision BERT-based Grammar Error Correction</title>
    <summary>  Current grammatical error correction (GEC) models typically consider the task
as sequence generation, which requires large amounts of annotated data and
limit the applications in data-limited settings. We try to incorporate
contextual information from pre-trained language model to leverage annotation
and benefit multilingual scenarios. Results show strong potential of
Bidirectional Encoder Representations from Transformers (BERT) in grammatical
error correction task.
</summary>
    <author>
      <name>Yiyuan Li</name>
    </author>
    <author>
      <name>Antonios Anastasopoulos</name>
    </author>
    <author>
      <name>Alan W Black</name>
    </author>
    <link href="http://arxiv.org/abs/2001.03521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11381v1</id>
    <updated>2020-01-17T15:42:14Z</updated>
    <published>2020-01-17T15:42:14Z</published>
    <title>Generación automática de frases literarias en español</title>
    <summary>  In this work we present a state of the art in the area of Computational
Creativity (CC). In particular, we address the automatic generation of literary
sentences in Spanish. We propose three models of text generation based mainly
on statistical algorithms and shallow parsing analysis. We also present some
rather encouraging preliminary results.
</summary>
    <author>
      <name>Luis-Gil Moreno-Jiménez</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Roseli S. Wedemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, in Spanish, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.11381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.02083v1</id>
    <updated>2020-04-05T03:30:01Z</updated>
    <published>2020-04-05T03:30:01Z</published>
    <title>A Resource for Studying Chatino Verbal Morphology</title>
    <summary>  We present the first resource focusing on the verbal inflectional morphology
of San Juan Quiahije Chatino, a tonal mesoamerican language spoken in Mexico.
We provide a collection of complete inflection tables of 198 lemmata, with
morphological tags based on the UniMorph schema. We also provide baseline
results on three core NLP tasks: morphological analysis, lemmatization, and
morphological inflection.
</summary>
    <author>
      <name>Hilaria Cruz</name>
    </author>
    <author>
      <name>Gregory Stump</name>
    </author>
    <author>
      <name>Antonios Anastasopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at LREC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.02083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08900v1</id>
    <updated>2020-04-19T16:28:35Z</updated>
    <published>2020-04-19T16:28:35Z</published>
    <title>The Cost of Training NLP Models: A Concise Overview</title>
    <summary>  We review the cost of training large-scale language models, and the drivers
of these costs. The intended audience includes engineers and scientists
budgeting their model-training experiments, as well as non-practitioners trying
to make sense of the economics of modern-day Natural Language Processing (NLP).
</summary>
    <author>
      <name>Or Sharir</name>
    </author>
    <author>
      <name>Barak Peleg</name>
    </author>
    <author>
      <name>Yoav Shoham</name>
    </author>
    <link href="http://arxiv.org/abs/2004.08900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10741v2</id>
    <updated>2020-08-05T20:15:30Z</updated>
    <published>2020-04-22T17:50:04Z</published>
    <title>Categories of Semantic Concepts</title>
    <summary>  Modelling concept representation is a foundational problem in the study of
cognition and linguistics. This work builds on the confluence of conceptual
tools from G\"ardenfors semantic spaces, categorical compositional linguistics,
and applied category theory to present a domain-independent and categorical
formalism of 'concept'.
</summary>
    <author>
      <name>James Hefford</name>
    </author>
    <author>
      <name>Vincent Wang</name>
    </author>
    <author>
      <name>Matthew Wilson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SemSpace 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10741v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10741v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12495v1</id>
    <updated>2020-04-26T22:47:29Z</updated>
    <published>2020-04-26T22:47:29Z</published>
    <title>Experiments with LVT and FRE for Transformer model</title>
    <summary>  In this paper, we experiment with Large Vocabulary Trick and Feature-rich
encoding applied to the Transformer model for Text Summarization. We could not
achieve better results, than the analogous RNN-based sequence-to-sequence
model, so we tried more models to find out, what improves the results and what
deteriorates them.
</summary>
    <author>
      <name>Ilshat Gibadullin</name>
    </author>
    <author>
      <name>Aidar Valeev</name>
    </author>
    <link href="http://arxiv.org/abs/2004.12495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00110v1</id>
    <updated>2020-04-30T21:15:19Z</updated>
    <published>2020-04-30T21:15:19Z</published>
    <title>On the Spontaneous Emergence of Discrete and Compositional Signals</title>
    <summary>  We propose a general framework to study language emergence through signaling
games with neural agents. Using a continuous latent space, we are able to (i)
train using backpropagation, (ii) show that discrete messages nonetheless
naturally emerge. We explore whether categorical perception effects follow and
show that the messages are not compositional.
</summary>
    <author>
      <name>Nur Geffen Lan</name>
    </author>
    <author>
      <name>Emmanuel Chemla</name>
    </author>
    <author>
      <name>Shane Steinert-Threlkeld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.00110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04929v1</id>
    <updated>2020-05-11T08:51:30Z</updated>
    <published>2020-05-11T08:51:30Z</published>
    <title>Towards logical negation for compositional distributional semantics</title>
    <summary>  The categorical compositional distributional model of meaning gives the
composition of words into phrases and sentences pride of place. However, it has
so far lacked a model of logical negation. This paper gives some steps towards
providing this operator, modelling it as a version of projection onto the
subspace orthogonal to a word. We give a small demonstration of the operators
performance in a sentence entailment task.
</summary>
    <author>
      <name>Martha Lewis</name>
    </author>
    <link href="http://arxiv.org/abs/2005.04929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06946v1</id>
    <updated>2020-04-02T10:17:55Z</updated>
    <published>2020-04-02T10:17:55Z</published>
    <title>4chan &amp; 8chan embeddings</title>
    <summary>  We have collected over 30M messages from the publicly available /pol/ message
boards on 4chan and 8chan, and compiled them into a model of toxic language
use. The trained word embeddings (0.4GB) are released for free and may be
useful for further study on toxic discourse or to boost hate speech detection
systems: https://textgain.com/8chan.
</summary>
    <author>
      <name>Pierre Voué</name>
    </author>
    <author>
      <name>Tom De Smedt</name>
    </author>
    <author>
      <name>Guy De Pauw</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Textgain Technical Reports 1 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.06946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08864v1</id>
    <updated>2020-05-18T16:39:16Z</updated>
    <published>2020-05-18T16:39:16Z</published>
    <title>Grammatical gender associations outweigh topical gender bias in
  crosslinguistic word embeddings</title>
    <summary>  Recent research has demonstrated that vector space models of semantics can
reflect undesirable biases in human culture. Our investigation of
crosslinguistic word embeddings reveals that topical gender bias interacts
with, and is surpassed in magnitude by, the effect of grammatical gender
associations, and both may be attenuated by corpus lemmatization. This finding
has implications for downstream applications such as machine translation.
</summary>
    <author>
      <name>Katherine McCurdy</name>
    </author>
    <author>
      <name>Oguz Serbetci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract presented at the WiNLP workshop, ACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09439v2</id>
    <updated>2021-01-26T00:09:14Z</updated>
    <published>2020-05-19T13:35:13Z</published>
    <title>Functorial Language Games for Question Answering</title>
    <summary>  We present some categorical investigations into Wittgenstein's
language-games, with applications to game-theoretic pragmatics and
question-answering in natural language processing.
</summary>
    <author>
      <name>Giovanni de Felice</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Elena Di Lavore</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tallinn University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Mario Román</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tallinn University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Alexis Toumi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oxford</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.333.21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.333.21" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ACT 2020, arXiv:2101.07888</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 333, 2021, pp. 311-321</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.09439v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09439v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11988v1</id>
    <updated>2020-05-25T09:12:37Z</updated>
    <published>2020-05-25T09:12:37Z</published>
    <title>Deep Learning Models for Automatic Summarization</title>
    <summary>  Text summarization is an NLP task which aims to convert a textual document
into a shorter one while keeping as much meaning as possible. This pedagogical
article reviews a number of recent Deep Learning architectures that have helped
to advance research in this field. We will discuss in particular applications
of pointer networks, hierarchical Transformers and Reinforcement Learning. We
assume basic knowledge of Seq2Seq architecture and Transformer networks within
NLP.
</summary>
    <author>
      <name>Pirmin Lemberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.11988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.00592v1</id>
    <updated>2020-11-01T18:46:53Z</updated>
    <published>2020-11-01T18:46:53Z</published>
    <title>Vec2Sent: Probing Sentence Embeddings with Natural Language Generation</title>
    <summary>  We introspect black-box sentence embeddings by conditionally generating from
them with the objective to retrieve the underlying discrete sentence. We
perceive of this as a new unsupervised probing task and show that it correlates
well with downstream task performance. We also illustrate how the language
generated from different encoders differs. We apply our approach to generate
sentence analogies from sentence embeddings.
</summary>
    <author>
      <name>Martin Kerscher</name>
    </author>
    <author>
      <name>Steffen Eger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in COLING 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.00592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.00592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.03492v2</id>
    <updated>2020-12-09T19:25:56Z</updated>
    <published>2020-11-06T17:57:18Z</published>
    <title>Practical and Ethical Considerations in the Effective use of Emotion and
  Sentiment Lexicons</title>
    <summary>  Lexicons of word-emotion associations are widely used in research and
real-world applications. As part of my research, I have created several such
lexicons (e.g., the NRC Emotion Lexicon). This paper outlines some practical
and ethical considerations involved in the effective use of these lexical
resources.
</summary>
    <author>
      <name>Saif M. Mohammad</name>
    </author>
    <link href="http://arxiv.org/abs/2011.03492v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.03492v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05788v1</id>
    <updated>2020-11-11T14:05:32Z</updated>
    <published>2020-11-11T14:05:32Z</published>
    <title>Assessment of text coherence based on the cohesion estimation</title>
    <summary>  In this paper, a graph-based coherence estimation method based on the
cohesion estimation is suggested. Our method uses a graph-based approach to
provide a user with an understanding of the evaluation process. Moreover, it
can be applied to different languages, therefore, the effectiveness of this
method is examined on the set of English, Chinese, and Arabic texts.
</summary>
    <author>
      <name>S. D. Pogorilyy</name>
    </author>
    <author>
      <name>A. A. Kramov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.06819v1</id>
    <updated>2020-11-13T09:19:48Z</updated>
    <published>2020-11-13T09:19:48Z</published>
    <title>diagNNose: A Library for Neural Activation Analysis</title>
    <summary>  In this paper we introduce diagNNose, an open source library for analysing
the activations of deep neural networks. diagNNose contains a wide array of
interpretability techniques that provide fundamental insights into the inner
workings of neural networks. We demonstrate the functionality of diagNNose with
a case study on subject-verb agreement within language models. diagNNose is
available at https://github.com/i-machine-think/diagnnose.
</summary>
    <author>
      <name>Jaap Jumelet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Third BlackboxNLP Workshop on Analyzing and
  Interpreting Neural Networks for NLP, EMNLP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.06819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.06819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09567v1</id>
    <updated>2020-11-18T22:33:09Z</updated>
    <published>2020-11-18T22:33:09Z</published>
    <title>Predicting metrical patterns in Spanish poetry with language models</title>
    <summary>  In this paper, we compare automated metrical pattern identification systems
available for Spanish against extensive experiments done by fine-tuning
language models trained on the same task. Despite being initially conceived as
a model suitable for semantic tasks, our results suggest that BERT-based models
retain enough structural information to perform reasonably well for Spanish
scansion.
</summary>
    <author>
      <name>Javier de la Rosa</name>
    </author>
    <author>
      <name>Salvador Ros</name>
    </author>
    <author>
      <name>Elena González-Blanco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LXAI Workshop @ NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.09567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13209v2</id>
    <updated>2022-04-26T08:34:57Z</updated>
    <published>2022-03-24T17:09:23Z</published>
    <title>Direct parsing to sentiment graphs</title>
    <summary>  This paper demonstrates how a graph-based semantic parser can be applied to
the task of structured sentiment analysis, directly predicting sentiment graphs
from text. We advance the state of the art on 4 out of 5 standard benchmark
sets. We release the source code, models and predictions.
</summary>
    <author>
      <name>David Samuel</name>
    </author>
    <author>
      <name>Jeremy Barnes</name>
    </author>
    <author>
      <name>Robin Kurtz</name>
    </author>
    <author>
      <name>Stephan Oepen</name>
    </author>
    <author>
      <name>Lilja Øvrelid</name>
    </author>
    <author>
      <name>Erik Velldal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5502v1</id>
    <updated>2014-09-19T02:50:04Z</updated>
    <published>2014-09-19T02:50:04Z</published>
    <title>Using crowdsourcing system for creating site-specific statistical
  machine translation engine</title>
    <summary>  A crowdsourcing translation approach is an effective tool for globalization
of site content, but it is also an important source of parallel linguistic
data. For the given site, processed with a crowdsourcing system, a
sentence-aligned corpus can be fetched, which covers a very narrow domain of
terminology and language patterns - a site-specific domain. These data can be
used for training and estimation of site-specific statistical machine
translation engine
</summary>
    <author>
      <name>Alexander Kalinin</name>
    </author>
    <author>
      <name>George Savchenko</name>
    </author>
    <link href="http://arxiv.org/abs/1409.5502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7619v1</id>
    <updated>2014-09-25T13:54:37Z</updated>
    <published>2014-09-25T13:54:37Z</published>
    <title>Generating Conceptual Metaphors from Proposition Stores</title>
    <summary>  Contemporary research on computational processing of linguistic metaphors is
divided into two main branches: metaphor recognition and metaphor
interpretation. We take a different line of research and present an automated
method for generating conceptual metaphors from linguistic data. Given the
generated conceptual metaphors, we find corresponding linguistic metaphors in
corpora. In this paper, we describe our approach and its evaluation using
English and Russian data.
</summary>
    <author>
      <name>Ekaterina Ovchinnikova</name>
    </author>
    <author>
      <name>Vladimir Zaytsev</name>
    </author>
    <author>
      <name>Suzanne Wertheim</name>
    </author>
    <author>
      <name>Ross Israel</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01886v1</id>
    <updated>2015-10-07T10:50:31Z</updated>
    <published>2015-10-07T10:50:31Z</published>
    <title>Using Ontology-Based Context in the Portuguese-English Translation of
  Homographs in Textual Dialogues</title>
    <summary>  This paper introduces a novel approach to tackle the existing gap on message
translations in dialogue systems. Currently, submitted messages to the dialogue
systems are considered as isolated sentences. Thus, missing context information
impede the disambiguation of homographs words in ambiguous sentences. Our
approach solves this disambiguation problem by using concepts over existing
ontologies.
</summary>
    <author>
      <name>Diego Moussallem</name>
    </author>
    <author>
      <name>Ricardo Choren</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2015.6502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2015.6502" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures, 2 tables in International journal of Artificial
  Intelligence &amp; Applications 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07851v1</id>
    <updated>2015-10-27T10:43:50Z</updated>
    <published>2015-10-27T10:43:50Z</published>
    <title>Standards for language resources in ISO -- Looking back at 13 fruitful
  years</title>
    <summary>  This paper provides an overview of the various projects carried out within
ISO committee TC 37/SC 4 dealing with the management of language (digital)
resources. On the basis of the technical experience gained in the committee and
the wider standardization landscape the paper identifies some possible trends
for the future.
</summary>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ALPAGE, CMB</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">edition - die Terminologiefachzeitschrift, Deutscher Terminologie-Tag
  e.V. (DTT), 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05962v1</id>
    <updated>2017-02-20T13:36:23Z</updated>
    <published>2017-02-20T13:36:23Z</published>
    <title>Latent Variable Dialogue Models and their Diversity</title>
    <summary>  We present a dialogue generation model that directly captures the variability
in possible responses to a given input, which reduces the `boring output' issue
of deterministic dialogue models. Experiments show that our model generates
more diverse outputs than baseline models, and also generates more consistently
acceptable output than sampling from a deterministic encoder-decoder model.
</summary>
    <author>
      <name>Kris Cao</name>
    </author>
    <author>
      <name>Stephen Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.05962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01355v2</id>
    <updated>2020-10-27T14:54:01Z</updated>
    <published>2019-08-04T14:46:35Z</published>
    <title>Separating Argument Structure from Logical Structure in AMR</title>
    <summary>  The AMR (Abstract Meaning Representation) formalism for representing meaning
of natural language sentences was not designed to deal with scope and
quantifiers. By extending AMR with indices for contexts and formulating
constraints on these contexts, a formalism is derived that makes correct
prediction for inferences involving negation and bound variables. The
attractive core predicate-argument structure of AMR is preserved. The resulting
framework is similar to that of Discourse Representation Theory.
</summary>
    <author>
      <name>Johan Bos</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02322v1</id>
    <updated>2019-08-06T18:50:45Z</updated>
    <published>2019-08-06T18:50:45Z</published>
    <title>DpgMedia2019: A Dutch News Dataset for Partisanship Detection</title>
    <summary>  We present a new Dutch news dataset with labeled partisanship. The dataset
contains more than 100K articles that are labeled on the publisher level and
776 articles that were crowdsourced using an internal survey platform and
labeled on the article level. In this paper, we document our original
motivation, the collection and annotation process, limitations, and
applications.
</summary>
    <author>
      <name>Chia-Lun Yeh</name>
    </author>
    <author>
      <name>Babak Loni</name>
    </author>
    <author>
      <name>Mariëlle Hendriks</name>
    </author>
    <author>
      <name>Henrike Reinhardt</name>
    </author>
    <author>
      <name>Anne Schuth</name>
    </author>
    <link href="http://arxiv.org/abs/1908.02322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03402v1</id>
    <updated>2019-08-09T10:40:52Z</updated>
    <published>2019-08-09T10:40:52Z</published>
    <title>UdS Submission for the WMT 19 Automatic Post-Editing Task</title>
    <summary>  In this paper, we describe our submission to the English-German APE shared
task at WMT 2019. We utilize and adapt an NMT architecture originally developed
for exploiting context information to APE, implement this in our own
transformer model and explore joint training of the APE task with a de-noising
encoder.
</summary>
    <author>
      <name>Hongfei Xu</name>
    </author>
    <author>
      <name>Qiuhui Liu</name>
    </author>
    <author>
      <name>Josef van Genabith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WMT 2019 Automatic Post-Editing Shared Task Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.03402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08113v1</id>
    <updated>2019-08-21T20:52:53Z</updated>
    <published>2019-08-21T20:52:53Z</published>
    <title>X-SQL: reinforce schema representation with context</title>
    <summary>  In this work, we present X-SQL, a new network architecture for the problem of
parsing natural language to SQL query. X-SQL proposes to enhance the structural
schema representation with the contextual output from BERT-style pre-training
model, and together with type information to learn a new schema representation
for down-stream tasks. We evaluated X-SQL on the WikiSQL dataset and show its
new state-of-the-art performance.
</summary>
    <author>
      <name>Pengcheng He</name>
    </author>
    <author>
      <name>Yi Mao</name>
    </author>
    <author>
      <name>Kaushik Chakrabarti</name>
    </author>
    <author>
      <name>Weizhu Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06450v1</id>
    <updated>2020-02-15T21:05:07Z</updated>
    <published>2020-02-15T21:05:07Z</published>
    <title>Supervised Phrase-boundary Embeddings</title>
    <summary>  We propose a new word embedding model, called SPhrase, that incorporates
supervised phrase information. Our method modifies traditional word embeddings
by ensuring that all target words in a phrase have exactly the same context. We
demonstrate that including this information within a context window produces
superior embeddings for both intrinsic evaluation tasks and downstream
extrinsic tasks.
</summary>
    <author>
      <name>Manni Singh</name>
    </author>
    <author>
      <name>David Weston</name>
    </author>
    <author>
      <name>Mark Levene</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12097v1</id>
    <updated>2020-02-27T14:02:31Z</updated>
    <published>2020-02-27T14:02:31Z</published>
    <title>Improving cross-lingual model transfer by chunking</title>
    <summary>  We present a shallow parser guided cross-lingual model transfer approach in
order to address the syntactic differences between source and target languages
more effectively. In this work, we assume the chunks or phrases in a sentence
as transfer units in order to address the syntactic differences between the
source and target languages arising due to the differences in ordering of words
in the phrases and the ordering of phrases in a sentence separately.
</summary>
    <author>
      <name>Ayan Das</name>
    </author>
    <author>
      <name>Sudeshna Sarkar</name>
    </author>
    <link href="http://arxiv.org/abs/2002.12097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00389v1</id>
    <updated>2020-10-01T13:26:58Z</updated>
    <published>2020-10-01T13:26:58Z</published>
    <title>A Survey on Explainability in Machine Reading Comprehension</title>
    <summary>  This paper presents a systematic review of benchmarks and approaches for
explainability in Machine Reading Comprehension (MRC). We present how the
representation and inference challenges evolved and the steps which were taken
to tackle these challenges. We also present the evaluation methodologies to
assess the performance of explainable systems. In addition, we identify
persisting open research questions and highlight critical directions for future
work.
</summary>
    <author>
      <name>Mokanarangan Thayaparan</name>
    </author>
    <author>
      <name>Marco Valentino</name>
    </author>
    <author>
      <name>André Freitas</name>
    </author>
    <link href="http://arxiv.org/abs/2010.00389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00633v1</id>
    <updated>2020-10-01T18:17:58Z</updated>
    <published>2020-10-01T18:17:58Z</published>
    <title>Discontinuous Constituent Parsing as Sequence Labeling</title>
    <summary>  This paper reduces discontinuous parsing to sequence labeling. It first shows
that existing reductions for constituent parsing as labeling do not support
discontinuities. Second, it fills this gap and proposes to encode tree
discontinuities as nearly ordered permutations of the input sequence. Third, it
studies whether such discontinuous representations are learnable. The
experiments show that despite the architectural simplicity, under the right
representation, the models are fast and accurate.
</summary>
    <author>
      <name>David Vilares</name>
    </author>
    <author>
      <name>Carlos Gómez-Rodríguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.05959v1</id>
    <updated>2020-10-12T18:39:07Z</updated>
    <published>2020-10-12T18:39:07Z</published>
    <title>Towards Induction of Structured Phoneme Inventories</title>
    <summary>  This extended abstract surveying the work on phonological typology was
prepared for "SIGTYP 2020: The Second Workshop on Computational Research in
Linguistic Typology" to be held at EMNLP 2020.
</summary>
    <author>
      <name>Alexander Gutkin</name>
    </author>
    <author>
      <name>Martin Jansche</name>
    </author>
    <author>
      <name>Lucy Skidmore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Second Workshop on Computational Research in
  Linguistic Typology (SIGTYP 2020) at EMNLP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.05959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12401v1</id>
    <updated>2020-10-23T13:45:33Z</updated>
    <published>2020-10-23T13:45:33Z</published>
    <title>Pretraining and Fine-Tuning Strategies for Sentiment Analysis of Latvian
  Tweets</title>
    <summary>  In this paper, we present various pre-training strategies that aid in
im-proving the accuracy of the sentiment classification task. We, at first,
pre-trainlanguage representation models using these strategies and then
fine-tune them onthe downstream task. Experimental results on a time-balanced
tweet evaluation setshow the improvement over the previous technique. We
achieve 76% accuracy forsentiment analysis on Latvian tweets, which is a
substantial improvement over pre-vious work
</summary>
    <author>
      <name>Gaurish Thakkar</name>
    </author>
    <author>
      <name>Marcis Pinnis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/FAIA200602</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/FAIA200602" rel="related"/>
    <link href="http://arxiv.org/abs/2010.12401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12912v1</id>
    <updated>2020-10-24T15:03:20Z</updated>
    <published>2020-10-24T15:03:20Z</published>
    <title>Word Embeddings for Chemical Patent Natural Language Processing</title>
    <summary>  We evaluate chemical patent word embeddings against known biomedical
embeddings and show that they outperform the latter extrinsically and
intrinsically. We also show that using contextualized embeddings can induce
predictive models of reasonable performance for this domain over a relatively
small gold standard.
</summary>
    <author>
      <name>Camilo Thorne</name>
    </author>
    <author>
      <name>Saber Akhondi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of an extended abstract presented (and reviewed) at
  the Latinx Workshop at ICML 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.12912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; J.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14806v2</id>
    <updated>2020-11-19T10:29:53Z</updated>
    <published>2020-10-28T08:08:12Z</published>
    <title>The Volctrans Machine Translation System for WMT20</title>
    <summary>  This paper describes our VolcTrans system on WMT20 shared news translation
task. We participated in 8 translation directions. Our basic systems are based
on Transformer, with several variants (wider or deeper Transformers, dynamic
convolutions). The final system includes text pre-process, data selection,
synthetic data generation, advanced model ensemble, and multilingual
pre-training.
</summary>
    <author>
      <name>Liwei Wu</name>
    </author>
    <author>
      <name>Xiao Pan</name>
    </author>
    <author>
      <name>Zehui Lin</name>
    </author>
    <author>
      <name>Yaoming Zhu</name>
    </author>
    <author>
      <name>Mingxuan Wang</name>
    </author>
    <author>
      <name>Lei Li</name>
    </author>
    <link href="http://arxiv.org/abs/2010.14806v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14806v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01799v1</id>
    <updated>2021-04-05T07:18:54Z</updated>
    <published>2021-04-05T07:18:54Z</published>
    <title>Deep Neural Networks for Relation Extraction</title>
    <summary>  Relation extraction from text is an important task for automatic knowledge
base population. In this thesis, we first propose a syntax-focused multi-factor
attention network model for finding the relation between two entities. Next, we
propose two joint entity and relation extraction frameworks based on
encoder-decoder architecture. Finally, we propose a hierarchical entity graph
convolutional network for relation extraction across documents.
</summary>
    <author>
      <name>Tapas Nayak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis, National University of Singapore (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.01799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02041v1</id>
    <updated>2021-04-05T17:46:10Z</updated>
    <published>2021-04-05T17:46:10Z</published>
    <title>Exploring Transformers in Emotion Recognition: a comparison of BERT,
  DistillBERT, RoBERTa, XLNet and ELECTRA</title>
    <summary>  This paper investigates how Natural Language Understanding (NLU) could be
applied in Emotion Recognition, a specific task in affective computing. We
finetuned different transformers language models (BERT, DistilBERT, RoBERTa,
XLNet, and ELECTRA) using a fine-grained emotion dataset and evaluating them in
terms of performance (f1-score) and time to complete.
</summary>
    <author>
      <name>Diogo Cortiz</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02310v2</id>
    <updated>2021-04-07T13:36:52Z</updated>
    <published>2021-04-06T06:26:58Z</published>
    <title>SERRANT: a syntactic classifier for English Grammatical Error Types</title>
    <summary>  SERRANT is a system and code for automatic classification of English
grammatical errors that combines SErCl and ERRANT. SERRANT uses ERRANT's
annotations when they are informative and those provided by SErCl otherwise.
</summary>
    <author>
      <name>Leshem Choshen</name>
    </author>
    <author>
      <name>Matanel Oren</name>
    </author>
    <author>
      <name>Dmitry Nikolaev</name>
    </author>
    <author>
      <name>Omri Abend</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code library in: https://github.com/matanel-oren/serrant</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.02310v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02310v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02756v1</id>
    <updated>2021-04-06T19:34:36Z</updated>
    <published>2021-04-06T19:34:36Z</published>
    <title>Efficient transfer learning for NLP with ELECTRA</title>
    <summary>  Clark et al. [2020] claims that the ELECTRA approach is highly efficient in
NLP performances relative to computation budget. As such, this reproducibility
study focus on this claim, summarized by the following question: Can we use
ELECTRA to achieve close to SOTA performances for NLP in low-resource settings,
in term of compute cost?
</summary>
    <author>
      <name>François Mercier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission for ML Reproducibility Challenge 2020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning Reproducibility Challenge 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.02756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04108v1</id>
    <updated>2021-04-08T23:01:17Z</updated>
    <published>2021-04-08T23:01:17Z</published>
    <title>XFORMAL: A Benchmark for Multilingual Formality Style Transfer</title>
    <summary>  We take the first step towards multilingual style transfer by creating and
releasing XFORMAL, a benchmark of multiple formal reformulations of informal
text in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest
that state-of-the-art style transfer approaches perform close to simple
baselines, indicating that style transfer is even more challenging when moving
multilingual.
</summary>
    <author>
      <name>Eleftheria Briakou</name>
    </author>
    <author>
      <name>Di Lu</name>
    </author>
    <author>
      <name>Ke Zhang</name>
    </author>
    <author>
      <name>Joel Tetreault</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04412v1</id>
    <updated>2021-04-09T15:02:56Z</updated>
    <published>2021-04-09T15:02:56Z</published>
    <title>Towards objectively evaluating the quality of generated medical
  summaries</title>
    <summary>  We propose a method for evaluating the quality of generated text by asking
evaluators to count facts, and computing precision, recall, f-score, and
accuracy from the raw counts. We believe this approach leads to a more
objective and easier to reproduce evaluation. We apply this to the task of
medical report summarisation, where measuring objective quality and accuracy is
of paramount importance.
</summary>
    <author>
      <name>Francesco Moramarco</name>
    </author>
    <author>
      <name>Damir Juric</name>
    </author>
    <author>
      <name>Aleksandar Savkov</name>
    </author>
    <author>
      <name>Ehud Reiter</name>
    </author>
    <link href="http://arxiv.org/abs/2104.04412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05745v1</id>
    <updated>2021-04-12T18:13:40Z</updated>
    <published>2021-04-12T18:13:40Z</published>
    <title>Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble</title>
    <summary>  This paper describes the TOKOFOU system, an ensemble model for misinformation
detection tasks based on six different transformer-based pre-trained encoders,
implemented in the context of the COVID-19 Infodemic Shared Task for English.
We fine tune each model on each of the task's questions and aggregate their
prediction scores using a majority voting approach. TOKOFOU obtains an overall
F1 score of 89.7%, ranking first.
</summary>
    <author>
      <name>Giorgos Tziafas</name>
    </author>
    <author>
      <name>Konstantinos Kogkalidis</name>
    </author>
    <author>
      <name>Tommaso Caselli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, NLP4IF 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06973v1</id>
    <updated>2021-04-14T16:56:14Z</updated>
    <published>2021-04-14T16:56:14Z</published>
    <title>[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias
  Mitigation</title>
    <summary>  Despite widespread use in natural language processing (NLP) tasks, word
embeddings have been criticized for inheriting unintended gender bias from
training corpora. programmer is more closely associated with man and homemaker
is more closely associated with woman. Such gender bias has also been shown to
propagate in downstream tasks.
</summary>
    <author>
      <name>Haswanth Aekula</name>
    </author>
    <author>
      <name>Sugam Garg</name>
    </author>
    <author>
      <name>Animesh Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review at ML Reproducibility Challenge 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13691v1</id>
    <updated>2021-04-28T10:28:35Z</updated>
    <published>2021-04-28T10:28:35Z</published>
    <title>SELF &amp; FEIL: Emotion and Intensity Lexicons for Finnish</title>
    <summary>  This paper introduces a Sentiment and Emotion Lexicon for Finnish (SELF) and
a Finnish Emotion Intensity Lexicon (FEIL). We describe the lexicon creation
process and evaluate the lexicon using some commonly available tools. The
lexicon uses annotations projected from the NRC Emotion Lexicon with carefully
edited translations. To our knowledge, this is the first comprehensive
sentiment and emotion lexicon for Finnish.
</summary>
    <author>
      <name>Emily Öhman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">unpublished short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.13691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.03376v1</id>
    <updated>2021-06-07T07:06:36Z</updated>
    <published>2021-06-07T07:06:36Z</published>
    <title>A Globally Normalized Neural Model for Semantic Parsing</title>
    <summary>  In this paper, we propose a globally normalized model for context-free
grammar (CFG)-based semantic parsing. Instead of predicting a probability, our
model predicts a real-valued score at each step and does not suffer from the
label bias problem. Experiments show that our approach outperforms locally
normalized models on small datasets, but it does not yield improvement on a
large dataset.
</summary>
    <author>
      <name>Chenyang Huang</name>
    </author>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Yanshuai Cao</name>
    </author>
    <author>
      <name>Osmar Zaïane</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <link href="http://arxiv.org/abs/2106.03376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.03376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.10955v1</id>
    <updated>2021-06-21T10:03:34Z</updated>
    <published>2021-06-21T10:03:34Z</published>
    <title>Extractive approach for text summarisation using graphs</title>
    <summary>  Natural language processing is an important discipline with the aim of
understanding text by its digital representation, that due to the diverse way
we write and speak, is often not accurate enough. Our paper explores different
graph-related algorithms that can be used in solving the text summarization
problem using an extractive approach. We consider two metrics: sentence overlap
and edit distance for measuring sentence similarity.
</summary>
    <author>
      <name>Kastriot Kadriu</name>
    </author>
    <author>
      <name>Milenko Obradovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.10955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07311v1</id>
    <updated>2022-01-13T23:45:24Z</updated>
    <published>2022-01-13T23:45:24Z</published>
    <title>Datasheet for the Pile</title>
    <summary>  This datasheet describes the Pile, a 825 GiB dataset of human-authored text
compiled by EleutherAI for use in large-scale language modeling. The Pile is
comprised of 22 different text sources, ranging from original scrapes done for
this project, to text data made available by the data owners, to third-party
scrapes available online.
</summary>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Kieran Bicheno</name>
    </author>
    <author>
      <name>Leo Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accompanies "The Pile: An 800GB Dataset of Diverse Text for Language
  Modeling" arXiv:2101.00027</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.07311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.01178v1</id>
    <updated>2022-01-29T20:05:28Z</updated>
    <published>2022-01-29T20:05:28Z</published>
    <title>Information Extraction through AI techniques: The KIDs use case at
  CONSOB</title>
    <summary>  In this paper we report on the initial activities carried out within a
collaboration between Consob and Sapienza University. We focus on Information
Extraction from documents describing financial instruments. We discuss how we
automate this task, via both rule-based and machine learning-based methods and
provide our first results.
</summary>
    <author>
      <name>Domenico Lembo</name>
    </author>
    <author>
      <name>Alessandra Limosani</name>
    </author>
    <author>
      <name>Francesca Medda</name>
    </author>
    <author>
      <name>Alessandra Monaco</name>
    </author>
    <author>
      <name>Federico Maria Scafoglieri</name>
    </author>
    <link href="http://arxiv.org/abs/2202.01178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.01178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04876v1</id>
    <updated>2022-02-10T07:29:05Z</updated>
    <published>2022-02-10T07:29:05Z</published>
    <title>Distilling Hypernymy Relations from Language Models: On the
  Effectiveness of Zero-Shot Taxonomy Induction</title>
    <summary>  In this paper, we analyze zero-shot taxonomy learning methods which are based
on distilling knowledge from language models via prompting and sentence
scoring. We show that, despite their simplicity, these methods outperform some
supervised strategies and are competitive with the current state-of-the-art
under adequate conditions. We also show that statistical and linguistic
properties of prompts dictate downstream performance.
</summary>
    <author>
      <name>Devansh Jain</name>
    </author>
    <author>
      <name>Luis Espinosa Anke</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.11669v1</id>
    <updated>2022-02-23T18:20:14Z</updated>
    <published>2022-02-23T18:20:14Z</published>
    <title>Refining the state-of-the-art in Machine Translation, optimizing NMT for
  the JA &lt;-&gt; EN language pair by leveraging personal domain expertise</title>
    <summary>  Documenting the construction of an NMT (Neural Machine Translation) system
for En/Ja based on the Transformer architecture leveraging the OpenNMT
framework. A systematic exploration of corpora pre-processing, hyperparameter
tuning and model architecture is carried out to obtain optimal performance. The
system is evaluated using standard auto-evaluation metrics such as BLEU, and my
subjective opinion as a Japanese linguist.
</summary>
    <author>
      <name>Matthew Bieda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.11669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.11669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12359v1</id>
    <updated>2022-02-23T18:34:45Z</updated>
    <published>2022-02-23T18:34:45Z</published>
    <title>UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training</title>
    <summary>  We present UnifiedQA-v2, a QA model built with the same process as UnifiedQA,
except that it utilizes more supervision -- roughly 3x the number of datasets
used for UnifiedQA. This generally leads to better in-domain and cross-domain
results.
</summary>
    <author>
      <name>Daniel Khashabi</name>
    </author>
    <author>
      <name>Yeganeh Kordi</name>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
    </author>
    <link href="http://arxiv.org/abs/2202.12359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4619v1</id>
    <updated>2013-02-19T14:32:17Z</updated>
    <published>2013-02-19T14:32:17Z</published>
    <title>Compactified Horizontal Visibility Graph for the Language Network</title>
    <summary>  A compactified horizontal visibility graph for the language network is
proposed. It was found that the networks constructed in such way are scale
free, and have a property that among the nodes with largest degrees there are
words that determine not only a text structure communication, but also its
informational structure.
</summary>
    <author>
      <name>D. V. Lande</name>
    </author>
    <author>
      <name>A. A. Snarskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 2 appendix tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.4619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1343v1</id>
    <updated>2013-06-06T08:56:32Z</updated>
    <published>2013-06-06T08:56:32Z</published>
    <title>The User Feedback on SentiWordNet</title>
    <summary>  With the release of SentiWordNet 3.0 the related Web interface has been
restyled and improved in order to allow users to submit feedback on the
SentiWordNet entries, in the form of the suggestion of alternative triplets of
values for an entry. This paper reports on the release of the user feedback
collected so far and on the plans for the future.
</summary>
    <author>
      <name>Andrea Esuli</name>
    </author>
    <link href="http://arxiv.org/abs/1306.1343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4134v1</id>
    <updated>2013-06-18T10:32:43Z</updated>
    <published>2013-06-18T10:32:43Z</published>
    <title>Dialogue System: A Brief Review</title>
    <summary>  A Dialogue System is a system which interacts with human in natural language.
At present many universities are developing the dialogue system in their
regional language. This paper will discuss about dialogue system, its
components, challenges and its evaluation. This paper helps the researchers for
getting info regarding dialogues system.
</summary>
    <author>
      <name>Suket Arora</name>
    </author>
    <author>
      <name>Kamaljeet Batra</name>
    </author>
    <author>
      <name>Sarabjit Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6944v1</id>
    <updated>2013-06-07T15:48:06Z</updated>
    <published>2013-06-07T15:48:06Z</published>
    <title>The DeLiVerMATH project - Text analysis in mathematics</title>
    <summary>  A high-quality content analysis is essential for retrieval functionalities
but the manual extraction of key phrases and classification is expensive.
Natural language processing provides a framework to automatize the process.
Here, a machine-based approach for the content analysis of mathematical texts
is described. A prototype for key phrase extraction and classification of
mathematical texts is presented.
</summary>
    <author>
      <name>Ulf Schöneberg</name>
    </author>
    <author>
      <name>Wolfram Sperber</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4869v1</id>
    <updated>2014-01-20T11:49:11Z</updated>
    <published>2014-01-20T11:49:11Z</published>
    <title>Does Syntactic Knowledge help English-Hindi SMT?</title>
    <summary>  In this paper we explore various parameter settings of the state-of-art
Statistical Machine Translation system to improve the quality of the
translation for a `distant' language pair like English-Hindi. We proposed new
techniques for efficient reordering. A slight improvement over the baseline is
reported using these techniques. We also show that a simple pre-processing step
can improve the quality of the translation significantly.
</summary>
    <author>
      <name>Taraka Rama</name>
    </author>
    <author>
      <name>Karthik Gali</name>
    </author>
    <author>
      <name>Avinesh PVS</name>
    </author>
    <link href="http://arxiv.org/abs/1401.4869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0291v2</id>
    <updated>2014-12-16T03:22:57Z</updated>
    <published>2014-10-01T17:03:18Z</published>
    <title>A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives</title>
    <summary>  We present an open source morphological analyzer for Japanese nouns, verbs
and adjectives. The system builds upon the morphological analyzing capabilities
of MeCab to incorporate finer details of classification such as politeness,
tense, mood and voice attributes. We implemented our analyzer in the form of a
finite state transducer using the open source finite state compiler FOMA
toolkit. The source code and tool is available at
https://bitbucket.org/skylander/yc-nlplab/.
</summary>
    <author>
      <name>Yanchuan Sim</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0291v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0291v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4639v3</id>
    <updated>2015-07-22T02:15:06Z</updated>
    <published>2014-10-17T05:19:12Z</published>
    <title>Dependent Types for Pragmatics</title>
    <summary>  This paper proposes the use of dependent types for pragmatic phenomena such
as pronoun binding and presupposition resolution as a type-theoretic
alternative to formalisms such as Discourse Representation Theory and Dynamic
Semantics.
</summary>
    <author>
      <name>Darryl McAdams</name>
    </author>
    <author>
      <name>Jonathan Sterling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version updates the paper for publication in LEUS</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.4639v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4639v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4966v1</id>
    <updated>2014-10-18T14:53:19Z</updated>
    <published>2014-10-18T14:53:19Z</published>
    <title>The Visualization of Change in Word Meaning over Time using Temporal
  Word Embeddings</title>
    <summary>  We describe a visualization tool that can be used to view the change in
meaning of words over time. The tool makes use of existing (static) word
embedding datasets together with a timestamped $n$-gram corpus to create {\em
temporal} word embeddings.
</summary>
    <author>
      <name>Chiraag Lala</name>
    </author>
    <author>
      <name>Shay B. Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/1410.4966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01666v1</id>
    <updated>2015-11-05T09:25:41Z</updated>
    <published>2015-11-05T09:25:41Z</published>
    <title>Comparing Writing Styles using Word Embedding and Dynamic Time Warping</title>
    <summary>  The development of plot or story in novels is reflected in the content and
the words used. The flow of sentiments, which is one aspect of writing style,
can be quantified by analyzing the flow of words. This study explores literary
works as signals in word embedding space and tries to compare writing styles of
popular classic novels using dynamic time warping.
</summary>
    <author>
      <name>Abhinav Tushar</name>
    </author>
    <author>
      <name>Abhinav Dahiya</name>
    </author>
    <link href="http://arxiv.org/abs/1511.01666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06312v1</id>
    <updated>2015-11-19T19:13:58Z</updated>
    <published>2015-11-19T19:13:58Z</published>
    <title>Good, Better, Best: Choosing Word Embedding Context</title>
    <summary>  We propose two methods of learning vector representations of words and
phrases that each combine sentence context with structural features extracted
from dependency trees. Using several variations of neural network classifier,
we show that these combined methods lead to improved performance when used as
input features for supervised term-matching.
</summary>
    <author>
      <name>James Cross</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00602v1</id>
    <updated>2016-10-03T15:40:08Z</updated>
    <published>2016-10-03T15:40:08Z</published>
    <title>Multimodal Semantic Simulations of Linguistically Underspecified Motion
  Events</title>
    <summary>  In this paper, we describe a system for generating three-dimensional visual
simulations of natural language motion expressions. We use a rich formal model
of events and their participants to generate simulations that satisfy the
minimal constraints entailed by the associated utterance, relying on semantic
knowledge of physical objects and motion events. This paper outlines technical
considerations and discusses implementing the aforementioned semantic models
into such a system.
</summary>
    <author>
      <name>Nikhil Krishnaswamy</name>
    </author>
    <author>
      <name>James Pustejovsky</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03246v1</id>
    <updated>2016-10-11T09:19:06Z</updated>
    <published>2016-10-11T09:19:06Z</published>
    <title>Toward a new instances of NELL</title>
    <summary>  We are developing the method to start new instances of NELL in various
languages and develop then NELL multilingualism. We base our method on our
experience on NELL Portuguese and NELL French. This reports explain our method
and develops some research perspectives.
</summary>
    <author>
      <name>Maisa C. Duarte</name>
    </author>
    <author>
      <name>Pierre Maret</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03759v2</id>
    <updated>2017-02-05T11:24:05Z</updated>
    <published>2016-10-12T15:53:02Z</published>
    <title>Language Models with Pre-Trained (GloVe) Word Embeddings</title>
    <summary>  In this work we implement a training of a Language Model (LM), using
Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by
Pennigton et al. in [1]. The implementation is following the general idea of
training RNNs for LM tasks presented in [2], but is rather using Gated
Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM
[4].
</summary>
    <author>
      <name>Victor Makarenkov</name>
    </author>
    <author>
      <name>Bracha Shapira</name>
    </author>
    <author>
      <name>Lior Rokach</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03759v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03759v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06053v1</id>
    <updated>2016-10-19T15:09:21Z</updated>
    <published>2016-10-19T15:09:21Z</published>
    <title>Chinese Restaurant Process for cognate clustering: A threshold free
  approach</title>
    <summary>  In this paper, we introduce a threshold free approach, motivated from Chinese
Restaurant Process, for the purpose of cognate clustering. We show that our
approach yields similar results to a linguistically motivated cognate
clustering system known as LexStat. Our Chinese Restaurant Process system is
fast and does not require any threshold and can be applied to any language
family of the world.
</summary>
    <author>
      <name>Taraka Rama</name>
    </author>
    <link href="http://arxiv.org/abs/1610.06053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07365v1</id>
    <updated>2016-10-24T11:30:22Z</updated>
    <published>2016-10-24T11:30:22Z</published>
    <title>Introduction: Cognitive Issues in Natural Language Processing</title>
    <summary>  This special issue is dedicated to get a better picture of the relationships
between computational linguistics and cognitive science. It specifically raises
two questions: "what is the potential contribution of computational language
modeling to cognitive science?" and conversely: "what is the influence of
cognitive science in contemporary computational linguistics?"
</summary>
    <author>
      <name>Thierry Poibeau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaTTICe</arxiv:affiliation>
    </author>
    <author>
      <name>Shravan Vasishth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1201/b21583-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1201/b21583-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Traitement Automatique des Langues, ATALA, 2014, Traitement
  Automatique des Langues et Sciences Cognitives, 55 (3), pp.7-19</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.07365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09565v1</id>
    <updated>2016-10-29T19:21:19Z</updated>
    <published>2016-10-29T19:21:19Z</published>
    <title>Sequence-to-sequence neural network models for transliteration</title>
    <summary>  Transliteration is a key component of machine translation systems and
software internationalization. This paper demonstrates that neural
sequence-to-sequence models obtain state of the art or close to state of the
art results on existing datasets. In an effort to make machine transliteration
accessible, we open source a new Arabic to English transliteration dataset and
our trained models.
</summary>
    <author>
      <name>Mihaela Rosca</name>
    </author>
    <author>
      <name>Thomas Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03564v1</id>
    <updated>2018-01-10T21:47:26Z</updated>
    <published>2018-01-10T21:47:26Z</published>
    <title>Unsupervised Part-of-Speech Induction</title>
    <summary>  Part-of-Speech (POS) tagging is an old and fundamental task in natural
language processing. While supervised POS taggers have shown promising
accuracy, it is not always feasible to use supervised methods due to lack of
labeled data. In this project, we attempt to unsurprisingly induce POS tags by
iteratively looking for a recurring pattern of words through a hierarchical
agglomerative clustering process. Our approach shows promising results when
compared to the tagging results of the state-of-the-art unsupervised POS
taggers.
</summary>
    <author>
      <name>Omid Kashefi</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01448v1</id>
    <updated>2018-09-05T11:55:05Z</updated>
    <published>2018-09-05T11:55:05Z</published>
    <title>Appendix - Recommended Statistical Significance Tests for NLP Tasks</title>
    <summary>  Statistical significance testing plays an important role when drawing
conclusions from experimental results in NLP papers. Particularly, it is a
valuable tool when one would like to establish the superiority of one algorithm
over another. This appendix complements the guide for testing statistical
significance in NLP presented in \cite{dror2018hitchhiker} by proposing valid
statistical tests for the common tasks and evaluation measures in the field.
</summary>
    <author>
      <name>Rotem Dror</name>
    </author>
    <author>
      <name>Roi Reichart</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03068v1</id>
    <updated>2018-09-10T00:34:34Z</updated>
    <published>2018-09-10T00:34:34Z</published>
    <title>A case for deep learning in semantics</title>
    <summary>  Pater's target article builds a persuasive case for establishing stronger
ties between theoretical linguistics and connectionism (deep learning). This
commentary extends his arguments to semantics, focusing in particular on issues
of learning, compositionality, and lexical meaning.
</summary>
    <author>
      <name>Christopher Potts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Commentary on Pater 2018, 'Generative linguistics and neural networks
  at 60: foundation, friction, and fusion', to appear in Language</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.03068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05219v1</id>
    <updated>2018-09-14T01:48:46Z</updated>
    <published>2018-09-14T01:48:46Z</published>
    <title>Automatic Catchphrase Extraction from Legal Case Documents via Scoring
  using Deep Neural Networks</title>
    <summary>  In this paper, we present a method of automatic catchphrase extracting from
legal case documents. We utilize deep neural networks for constructing scoring
model of our extraction system. We achieve comparable performance with systems
using corpus-wide and citation information which we do not use in our system.
</summary>
    <author>
      <name>Vu Tran</name>
    </author>
    <author>
      <name>Minh Le Nguyen</name>
    </author>
    <author>
      <name>Ken Satoh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MIning and REasoning with Legal text, MIREL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.05219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08935v1</id>
    <updated>2018-09-21T12:04:44Z</updated>
    <published>2018-09-21T12:04:44Z</published>
    <title>Lexical Bias In Essay Level Prediction</title>
    <summary>  Automatically predicting the level of non-native English speakers given their
written essays is an interesting machine learning problem. In this work I
present the system "balikasg" that achieved the state-of-the-art performance in
the CAp 2018 data science challenge among 14 systems. I detail the feature
extraction, feature engineering and model selection steps and I evaluate how
these decisions impact the system's performance. The paper concludes with
remarks for future work.
</summary>
    <author>
      <name>Georgios Balikas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CAp 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03554v1</id>
    <updated>2018-11-08T17:13:46Z</updated>
    <published>2018-11-08T17:13:46Z</published>
    <title>Implicit Argument Prediction as Reading Comprehension</title>
    <summary>  Implicit arguments, which cannot be detected solely through syntactic cues,
make it harder to extract predicate-argument tuples. We present a new model for
implicit argument prediction that draws on reading comprehension, casting the
predicate-argument tuple with the missing argument as a query. We also draw on
pointer networks and multi-hop computation. Our model shows good performance on
an argument cloze task as well as on a nominal implicit argument prediction
task.
</summary>
    <author>
      <name>Pengxiang Cheng</name>
    </author>
    <author>
      <name>Katrin Erk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AAAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.03554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07092v2</id>
    <updated>2019-07-17T02:48:04Z</updated>
    <published>2018-11-17T03:58:26Z</published>
    <title>Unnamed Entity Recognition of Sense Mentions</title>
    <summary>  We consider the problem of recognizing mentions of human senses in text. Our
contribution is a method for acquiring labeled data, and a learning method that
is trained on this data. Experiments show the effectiveness of our proposed
data labeling approach and our learning model on the task of sense recognition
in text.
</summary>
    <author>
      <name>Ndapa Nakashole</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.00555v1</id>
    <updated>2019-01-31T16:12:14Z</updated>
    <published>2019-01-31T16:12:14Z</published>
    <title>Riconoscimento ortografico per apostrofo ed espressioni polirematiche</title>
    <summary>  The work presents two algorithms of manipulation and comparison between
strings whose purpose is the orthographic recognition of the apostrophe and of
the compound expressions. The theory supporting general reasoning refers to the
basic concept of EditDistance, the improvements that ensure the achievement of
the objective are achieved with the aid of tools borrowed from the use of
techniques for processing large amounts of data on distributed platforms.
</summary>
    <author>
      <name>Massimiliano Polito</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Italian</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.00555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.07110v1</id>
    <updated>2019-02-19T16:00:38Z</updated>
    <published>2019-02-19T16:00:38Z</published>
    <title>A novel repetition normalized adversarial reward for headline generation</title>
    <summary>  While reinforcement learning can effectively improve language generation
models, it often suffers from generating incoherent and repetitive phrases
\cite{paulus2017deep}. In this paper, we propose a novel repetition normalized
adversarial reward to mitigate these problems. Our repetition penalized reward
can greatly reduce the repetition rate and adversarial training mitigates
generating incoherent phrases. Our model significantly outperforms the baseline
model on ROUGE-1\,(+3.24), ROUGE-L\,(+2.25), and a decreased repetition-rate
(-4.98\%).
</summary>
    <author>
      <name>Peng Xu</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.07110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07402v2</id>
    <updated>2020-03-20T12:21:06Z</updated>
    <published>2019-03-18T12:54:22Z</published>
    <title>Neutron: An Implementation of the Transformer Translation Model and its
  Variants</title>
    <summary>  The Transformer translation model is easier to parallelize and provides
better performance compared to recurrent seq2seq models, which makes it popular
among industry and research community. We implement the Neutron in this work,
including the Transformer model and its several variants from most recent
researches. It is highly optimized, easy to modify and provides comparable
performance with interesting features while keeping readability.
</summary>
    <author>
      <name>Hongfei Xu</name>
    </author>
    <author>
      <name>Qiuhui Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1903.07402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07917v1</id>
    <updated>2019-03-19T10:18:57Z</updated>
    <published>2019-03-19T10:18:57Z</published>
    <title>CVIT-MT Systems for WAT-2018</title>
    <summary>  This document describes the machine translation system used in the
submissions of IIIT-Hyderabad CVIT-MT for the WAT-2018 English-Hindi
translation task. Performance is evaluated on the associated corpus provided by
the organizers. We experimented with convolutional sequence to sequence
architectures. We also train with additional data obtained through
backtranslation.
</summary>
    <author>
      <name>Jerin Philip</name>
    </author>
    <author>
      <name>Vinay P. Namboodiri</name>
    </author>
    <author>
      <name>C. V. Jawahar</name>
    </author>
    <link href="http://arxiv.org/abs/1903.07917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10318v2</id>
    <updated>2019-09-05T15:35:21Z</updated>
    <published>2019-03-25T13:42:45Z</published>
    <title>Fine-tune BERT for Extractive Summarization</title>
    <summary>  BERT, a pre-trained Transformer model, has achieved ground-breaking
performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple
variant of BERT, for extractive summarization. Our system is the state of the
art on the CNN/Dailymail dataset, outperforming the previous best-performed
system by 1.65 on ROUGE-L. The codes to reproduce our results are available at
https://github.com/nlpyang/BertSum
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">fix figure 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10318v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10318v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00796v1</id>
    <updated>2019-03-29T05:03:15Z</updated>
    <published>2019-03-29T05:03:15Z</published>
    <title>Making Neural Machine Reading Comprehension Faster</title>
    <summary>  This study aims at solving the Machine Reading Comprehension problem where
questions have to be answered given a context passage. The challenge is to
develop a computationally faster model which will have improved inference time.
State of the art in many natural language understanding tasks, BERT model, has
been used and knowledge distillation method has been applied to train two
smaller models. The developed models are compared with other models which have
been developed with the same intention.
</summary>
    <author>
      <name>Debajyoti Chatterjee</name>
    </author>
    <link href="http://arxiv.org/abs/1904.00796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02293v1</id>
    <updated>2019-04-04T01:17:29Z</updated>
    <published>2019-04-04T01:17:29Z</published>
    <title>Generative Adversarial Networks for text using word2vec intermediaries</title>
    <summary>  Generative adversarial networks (GANs) have shown considerable success,
especially in the realistic generation of images. In this work, we apply
similar techniques for the generation of text. We propose a novel approach to
handle the discrete nature of text, during training, using word embeddings. Our
method is agnostic to vocabulary size and achieves competitive results relative
to methods with various discrete gradient estimators.
</summary>
    <author>
      <name>Akshay Budhkar</name>
    </author>
    <author>
      <name>Krishnapriya Vishnubhotla</name>
    </author>
    <author>
      <name>Safwan Hossain</name>
    </author>
    <author>
      <name>Frank Rudzicz</name>
    </author>
    <link href="http://arxiv.org/abs/1904.02293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.04398v1</id>
    <updated>2019-04-08T23:43:16Z</updated>
    <published>2019-04-08T23:43:16Z</published>
    <title>Disfluencies and Human Speech Transcription Errors</title>
    <summary>  This paper explores contexts associated with errors in transcrip-tion of
spontaneous speech, shedding light on human perceptionof disfluencies and other
conversational speech phenomena. Anew version of the Switchboard corpus is
provided with disfluency annotations for careful speech transcripts, together
with results showing the impact of transcription errors on evaluation of
automatic disfluency detection.
</summary>
    <author>
      <name>Vicky Zayats</name>
    </author>
    <author>
      <name>Trang Tran</name>
    </author>
    <author>
      <name>Richard Wright</name>
    </author>
    <author>
      <name>Courtney Mansfield</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to INTERSPEECH 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05529v1</id>
    <updated>2019-04-11T04:41:46Z</updated>
    <published>2019-04-11T04:41:46Z</published>
    <title>Frequency vs. Association for Constraint Selection in Usage-Based
  Construction Grammar</title>
    <summary>  A usage-based Construction Grammar (CxG) posits that slot-constraints
generalize from common exemplar constructions. But what is the best model of
constraint generalization? This paper evaluates competing frequency-based and
association-based models across eight languages using a metric derived from the
Minimum Description Length paradigm. The experiments show that
association-based models produce better generalizations across all languages by
a significant margin.
</summary>
    <author>
      <name>Jonathan Dunn</name>
    </author>
    <link href="http://arxiv.org/abs/1904.05529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09131v2</id>
    <updated>2020-11-24T17:50:32Z</updated>
    <published>2019-04-19T09:44:22Z</published>
    <title>OpenTapioca: Lightweight Entity Linking for Wikidata</title>
    <summary>  We propose a simple Named Entity Linking system that can be trained from
Wikidata only. This demonstrates the strengths and weaknesses of this data
source for this task and provides an easily reproducible baseline to compare
other systems against. Our model is lightweight to train, to run and to keep
synchronous with Wikidata in real time.
</summary>
    <author>
      <name>Antonin Delpeuch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in proceedings of the Wikidata Workshop 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09131v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09131v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04687v1</id>
    <updated>2019-06-11T16:39:11Z</updated>
    <published>2019-06-11T16:39:11Z</published>
    <title>Generating Summaries with Topic Templates and Structured Convolutional
  Decoders</title>
    <summary>  Existing neural generation approaches create multi-sentence text as a single
sequence. In this paper we propose a structured convolutional decoder that is
guided by the content structure of target summaries. We compare our model with
existing sequential decoders on three data sets representing different domains.
Automatic and human evaluation demonstrate that our summaries have better
content coverage.
</summary>
    <author>
      <name>Laura Perez-Beltrachini</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05786v1</id>
    <updated>2019-06-13T16:22:44Z</updated>
    <published>2019-06-13T16:22:44Z</published>
    <title>UCAM Biomedical translation at WMT19: Transfer learning multi-domain
  ensembles</title>
    <summary>  The 2019 WMT Biomedical translation task involved translating Medline
abstracts. We approached this using transfer learning to obtain a series of
strong neural models on distinct domains, and combining them into multi-domain
ensembles. We further experiment with an adaptive language-model ensemble
weighting scheme. Our submission achieved the best submitted results on both
directions of English-Spanish.
</summary>
    <author>
      <name>Danielle Saunders</name>
    </author>
    <author>
      <name>Felix Stahlberg</name>
    </author>
    <author>
      <name>Bill Byrne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at WMT19</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07555v1</id>
    <updated>2019-06-18T13:29:04Z</updated>
    <published>2019-06-18T13:29:04Z</published>
    <title>Automatic learner summary assessment for reading comprehension</title>
    <summary>  Automating the assessment of learner summaries provides a useful tool for
assessing learner reading comprehension. We present a summarization task for
evaluating non-native reading comprehension and propose three novel approaches
to automatically assess the learner summaries. We evaluate our models on two
datasets we created and show that our models outperform traditional approaches
that rely on exact word match on this task. Our best model produces quality
assessments close to professional examiners.
</summary>
    <author>
      <name>Menglin Xia</name>
    </author>
    <author>
      <name>Ekaterina Kochmar</name>
    </author>
    <author>
      <name>Ted Briscoe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.09532v1</id>
    <updated>2019-06-23T02:00:40Z</updated>
    <published>2019-06-23T02:00:40Z</published>
    <title>Smaller Text Classifiers with Discriminative Cluster Embeddings</title>
    <summary>  Word embedding parameters often dominate overall model sizes in neural
methods for natural language processing. We reduce deployed model sizes of text
classifiers by learning a hard word clustering in an end-to-end manner. We use
the Gumbel-Softmax distribution to maximize over the latent clustering while
minimizing the task loss. We propose variations that selectively assign
additional parameters to words, which further improves accuracy while still
remaining parameter-efficient.
</summary>
    <author>
      <name>Mingda Chen</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in NAACL 2018 short</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.09912v1</id>
    <updated>2019-06-17T06:32:24Z</updated>
    <published>2019-06-17T06:32:24Z</published>
    <title>KaWAT: A Word Analogy Task Dataset for Indonesian</title>
    <summary>  We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset
for Indonesian. We evaluated on it several existing pretrained Indonesian word
embeddings and embeddings trained on Indonesian online news corpus. We also
tested them on two downstream tasks and found that pretrained word embeddings
helped either by reducing the training epochs or yielding significant
performance gains.
</summary>
    <author>
      <name>Kemal Kurniawan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01686v1</id>
    <updated>2019-06-30T09:18:31Z</updated>
    <published>2019-06-30T09:18:31Z</published>
    <title>Machine Reading Comprehension: a Literature Review</title>
    <summary>  Machine reading comprehension aims to teach machines to understand a text
like a human and is a new challenging direction in Artificial Intelligence.
This article summarizes recent advances in MRC, mainly focusing on two aspects
(i.e., corpus and techniques). The specific characteristics of various MRC
corpus are listed and compared. The main ideas of some typical MRC techniques
are also described.
</summary>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>An Yang</name>
    </author>
    <author>
      <name>Sujian Li</name>
    </author>
    <author>
      <name>Yizhong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, preprint version</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04613v1</id>
    <updated>2019-07-10T10:50:22Z</updated>
    <published>2019-07-10T10:50:22Z</published>
    <title>Neural Networks as Explicit Word-Based Rules</title>
    <summary>  Filters of convolutional networks used in computer vision are often
visualized as image patches that maximize the response of the filter. We use
the same approach to interpret weight matrices in simple architectures for
natural language processing tasks. We interpret a convolutional network for
sentiment classification as word-based rules. Using the rule, we recover the
performance of the original model.
</summary>
    <author>
      <name>Jindřich Libovický</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages; extended abstract at BlackboxNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05839v1</id>
    <updated>2019-07-12T17:02:14Z</updated>
    <published>2019-07-12T17:02:14Z</published>
    <title>Equiprobable mappings in weighted constraint grammars</title>
    <summary>  We show that MaxEnt is so rich that it can distinguish between any two
different mappings: there always exists a nonnegative weight vector which
assigns them different MaxEnt probabilities. Stochastic HG instead does admit
equiprobable mappings and we give a complete formal characterization of them.
We compare these different predictions of the two frameworks on a test case of
Finnish stress.
</summary>
    <author>
      <name>Arto Anttila</name>
    </author>
    <author>
      <name>Scott Borgeson</name>
    </author>
    <author>
      <name>Giorgio Magri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; Proceedings of ACL Sigmorphon 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.05839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.06860v1</id>
    <updated>2019-07-16T06:43:34Z</updated>
    <published>2019-07-16T06:43:34Z</published>
    <title>A generic rule-based system for clinical trial patient selection</title>
    <summary>  The n2c2 2018 Challenge task 1 aimed to identify patients who meet lists of
heterogeneous inclusion/exclusion criteria for a hypothetical clinical trial.
We demonstrate a generic rule-based natural language pipeline can support this
task with decent performance (the average F1 score on the test set is 0.89,
ranked the 8th out of 45 teams ).
</summary>
    <author>
      <name>Jianlin Shi</name>
    </author>
    <author>
      <name>Kevin Graves</name>
    </author>
    <author>
      <name>John F. Hurdle</name>
    </author>
    <link href="http://arxiv.org/abs/1907.06860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12679v1</id>
    <updated>2019-07-29T22:53:59Z</updated>
    <published>2019-07-29T22:53:59Z</published>
    <title>Machine Translation Evaluation with BERT Regressor</title>
    <summary>  We introduce the metric using BERT (Bidirectional Encoder Representations
from Transformers) (Devlin et al., 2019) for automatic machine translation
evaluation. The experimental results of the WMT-2017 Metrics Shared Task
dataset show that our metric achieves state-of-the-art performance in
segment-level metrics task for all to-English language pairs.
</summary>
    <author>
      <name>Hiroki Shimanaka</name>
    </author>
    <author>
      <name>Tomoyuki Kajiwara</name>
    </author>
    <author>
      <name>Mamoru Komachi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08249v1</id>
    <updated>2019-10-18T03:54:58Z</updated>
    <published>2019-10-18T03:54:58Z</published>
    <title>Relational Graph Representation Learning for Open-Domain Question
  Answering</title>
    <summary>  We introduce a relational graph neural network with bi-directional attention
mechanism and hierarchical representation learning for open-domain question
answering task. Our model can learn contextual representation by jointly
learning and updating the query, knowledge graph, and document representations.
The experiments suggest that our model achieves state-of-the-art on the
WebQuestionsSP benchmark.
</summary>
    <author>
      <name>Salvatore Vivona</name>
    </author>
    <author>
      <name>Kaveh Hassani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2019 Workshop on Graph Representation Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.08249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11790v1</id>
    <updated>2019-10-25T15:15:44Z</updated>
    <published>2019-10-25T15:15:44Z</published>
    <title>Measuring Conversational Fluidity in Automated Dialogue Agents</title>
    <summary>  We present an automated evaluation method to measure fluidity in
conversational dialogue systems. The method combines various state of the art
Natural Language tools into a classifier, and human ratings on these dialogues
to train an automated judgment model. Our experiments show that the results are
an improvement on existing metrics for measuring fluidity.
</summary>
    <author>
      <name>Keith Vella</name>
    </author>
    <author>
      <name>Massimo Poesio</name>
    </author>
    <author>
      <name>Michael Sigamani</name>
    </author>
    <author>
      <name>Cihan Dogan</name>
    </author>
    <author>
      <name>Aimore Dutra</name>
    </author>
    <author>
      <name>Dimitrios Dimakopoulos</name>
    </author>
    <author>
      <name>Alfredo Gemma</name>
    </author>
    <author>
      <name>Ella Walters</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12477v1</id>
    <updated>2019-10-28T07:28:57Z</updated>
    <published>2019-10-28T07:28:57Z</published>
    <title>Multi-Module System for Open Domain Chinese Question Answering over
  Knowledge Base</title>
    <summary>  For the task of open domain Knowledge Based Question Answering in CCKS2019,
we propose a method combining information retrieval and semantic parsing. This
multi-module system extracts the topic entity and the most related relation
predicate from a question and transforms it into a Sparql query statement. Our
method obtained the F1 score of 70.45% on the test data.
</summary>
    <author>
      <name>Yiying Yang</name>
    </author>
    <author>
      <name>Xiahui He</name>
    </author>
    <author>
      <name>Kaijie Zhou</name>
    </author>
    <author>
      <name>Zhongyu Wei</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09558v2</id>
    <updated>2019-12-25T11:36:04Z</updated>
    <published>2019-12-19T21:47:08Z</published>
    <title>RIMAX: Ranking Semantic Rhymes by calculating Definition Similarity</title>
    <summary>  This paper presents RIMAX, a new system for detecting semantic rhymes, using
a Comprehensive Mexican Spanish Dictionary (DEM) and its Rhyming Dictionary
(REM). We use the Vector Space Model to calculate the similarity of the
definition of a query with the definitions corresponding to the assonant and
consonant rhymes of the query. The preliminary results using a manual
evaluation are very encouraging.
</summary>
    <author>
      <name>Alfonso Medina-Urrea</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.09558v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09558v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09723v3</id>
    <updated>2020-05-02T19:55:06Z</updated>
    <published>2019-12-20T09:44:42Z</published>
    <title>SberQuAD -- Russian Reading Comprehension Dataset: Description and
  Analysis</title>
    <summary>  SberQuAD -- a large scale analog of Stanford SQuAD in the Russian language -
is a valuable resource that has not been properly presented to the scientific
community. We fill this gap by providing a description, a thorough analysis,
and baseline experimental results.
</summary>
    <author>
      <name>Pavel Efimov</name>
    </author>
    <author>
      <name>Andrey Chertok</name>
    </author>
    <author>
      <name>Leonid Boytsov</name>
    </author>
    <author>
      <name>Pavel Braslavski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-58219-7_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-58219-7_1" rel="related"/>
    <link href="http://arxiv.org/abs/1912.09723v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09723v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00293v1</id>
    <updated>2020-08-01T16:28:47Z</updated>
    <published>2020-08-01T16:28:47Z</published>
    <title>The test set for the TransCoder system</title>
    <summary>  The TransCoder system translates source code between Java, C++, and Python 3.
The test set that was used to evaluate its quality is missing important
features of Java, including the ability to define and use classes and the
ability to call user-defined functions other than recursively. Therefore, the
accuracy of TransCoder over programs with those features remains unknown.
</summary>
    <author>
      <name>Ernest Davis</name>
    </author>
    <link href="http://arxiv.org/abs/2008.00293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00774v3</id>
    <updated>2020-09-15T09:39:13Z</updated>
    <published>2020-08-03T10:51:10Z</published>
    <title>Elsevier OA CC-By Corpus</title>
    <summary>  We introduce the Elsevier OA CC-BY corpus. This is the first open corpus of
Scientific Research papers which has a representative sample from across
scientific disciplines. This corpus not only includes the full text of the
article, but also the metadata of the documents, along with the bibliographic
information for each reference.
</summary>
    <author>
      <name>Daniel Kershaw</name>
    </author>
    <author>
      <name>Rob Koeling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.00774v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00774v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07189v1</id>
    <updated>2020-08-17T10:07:23Z</updated>
    <published>2020-08-17T10:07:23Z</published>
    <title>Comparison of Syntactic Parsers on Biomedical Texts</title>
    <summary>  Syntactic parsing is an important step in the automated text analysis which
aims at information extraction. Quality of the syntactic parsing determines to
a large extent the recall and precision of the text mining results. In this
paper we evaluate the performance of several popular syntactic parsers in
application to the biomedical text mining.
</summary>
    <author>
      <name>Maria Biryukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Most of this work was done in 2018 when the author was primarily
  affiliated with LCSB, Bioinformatic Core</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.07189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11053v1</id>
    <updated>2020-08-25T14:27:58Z</updated>
    <published>2020-08-25T14:27:58Z</published>
    <title>JokeMeter at SemEval-2020 Task 7: Convolutional humor</title>
    <summary>  This paper describes our system that was designed for Humor evaluation within
the SemEval-2020 Task 7. The system is based on convolutional neural network
architecture. We investigate the system on the official dataset, and we provide
more insight to model itself to see how the learned inner features look.
</summary>
    <author>
      <name>Martin Docekal</name>
    </author>
    <author>
      <name>Martin Fajcik</name>
    </author>
    <author>
      <name>Josef Jon</name>
    </author>
    <author>
      <name>Pavel Smrz</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11649v2</id>
    <updated>2020-10-15T14:37:59Z</updated>
    <published>2020-08-26T16:15:18Z</published>
    <title>Discrete Word Embedding for Logical Natural Language Understanding</title>
    <summary>  We propose an unsupervised neural model for learning a discrete embedding of
words. Unlike existing discrete embeddings, our binary embedding supports
vector arithmetic operations similar to continuous embeddings. Our embedding
represents each word as a set of propositional statements describing a
transition rule in classical/STRIPS planning formalism. This makes the
embedding directly compatible with symbolic, state of the art classical
planning solvers.
</summary>
    <author>
      <name>Masataro Asai</name>
    </author>
    <author>
      <name>Zilu Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">equal contribution</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11649v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11649v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11940v1</id>
    <updated>2020-08-27T06:42:18Z</updated>
    <published>2020-08-27T06:42:18Z</published>
    <title>Relation/Entity-Centric Reading Comprehension</title>
    <summary>  Constructing a machine that understands human language is one of the most
elusive and long-standing challenges in artificial intelligence. This thesis
addresses this challenge through studies of reading comprehension with a focus
on understanding entities and their relationships. More specifically, we focus
on question answering tasks designed to measure reading comprehension. We focus
on entities and relations because they are typically used to represent the
semantics of natural language.
</summary>
    <author>
      <name>Takeshi Onishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.10668v1</id>
    <updated>2020-12-19T11:55:46Z</updated>
    <published>2020-12-19T11:55:46Z</published>
    <title>FraCaS: Temporal Analysis</title>
    <summary>  In this paper, we propose an implementation of temporal semantics which is
suitable for inference problems. This implementation translates syntax trees to
logical formulas, suitable for consumption by the Coq proof assistant. We
support several phenomena including: temporal references, temporal adverbs,
aspectual classes and progressives. We apply these semantics to the complete
FraCaS testsuite. We obtain an accuracy of 81 percent overall and 73 percent
for problems explicitly marked as related to temporal reference.
</summary>
    <author>
      <name>Jean-Philippe Bernardy</name>
    </author>
    <author>
      <name>Stergios Chatzikyriakidis</name>
    </author>
    <link href="http://arxiv.org/abs/2012.10668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09015v1</id>
    <updated>2021-01-22T09:24:09Z</updated>
    <published>2021-01-22T09:24:09Z</published>
    <title>Unsupervised Technical Domain Terms Extraction using Term Extractor</title>
    <summary>  Terminology extraction, also known as term extraction, is a subtask of
information extraction. The goal of terminology extraction is to extract
relevant words or phrases from a given corpus automatically. This paper focuses
on the unsupervised automated domain term extraction method that considers
chunking, preprocessing, and ranking domain-specific terms using relevance and
cohesion functions for ICON 2020 shared task 2: TermTraction.
</summary>
    <author>
      <name>Suman Dowlagar</name>
    </author>
    <author>
      <name>Radhika Mamidi</name>
    </author>
    <link href="http://arxiv.org/abs/2101.09015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11433v2</id>
    <updated>2021-01-31T12:47:33Z</updated>
    <published>2021-01-21T07:11:21Z</published>
    <title>Analysis of Basic Emotions in Texts Based on BERT Vector Representation</title>
    <summary>  In the following paper the authors present a GAN-type model and the most
important stages of its development for the task of emotion recognition in
text. In particular, we propose an approach for generating a synthetic dataset
of all possible emotions combinations based on manually labelled incomplete
data.
</summary>
    <author>
      <name>A. Artemov</name>
    </author>
    <author>
      <name>A. Veselovskiy</name>
    </author>
    <author>
      <name>I. Khasenevich</name>
    </author>
    <author>
      <name>I. Bolokhov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11433v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11433v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11434v1</id>
    <updated>2021-01-20T23:45:33Z</updated>
    <published>2021-01-20T23:45:33Z</published>
    <title>Exploratory Arabic Offensive Language Dataset Analysis</title>
    <summary>  This paper adding more insights towards resources and datasets used in Arabic
offensive language research. The main goal of this paper is to guide
researchers in Arabic offensive language in selecting appropriate datasets
based on their content, and in creating new Arabic offensive language resources
to support and complement the available ones.
</summary>
    <author>
      <name>Fatemah Husain</name>
    </author>
    <author>
      <name>Ozlem Uzuner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">83 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00541v1</id>
    <updated>2021-01-31T21:31:11Z</updated>
    <published>2021-01-31T21:31:11Z</published>
    <title>Short Text Clustering with Transformers</title>
    <summary>  Recent techniques for the task of short text clustering often rely on word
embeddings as a transfer learning component. This paper shows that sentence
vector representations from Transformers in conjunction with different
clustering methods can be successfully applied to address the task.
Furthermore, we demonstrate that the algorithm of enhancement of clustering via
iterative classification can further improve initial clustering performance
with different classifiers, including those based on pre-trained Transformer
language models.
</summary>
    <author>
      <name>Leonid Pugachev</name>
    </author>
    <author>
      <name>Mikhail Burtsev</name>
    </author>
    <link href="http://arxiv.org/abs/2102.00541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03732v1</id>
    <updated>2021-02-07T07:37:07Z</updated>
    <published>2021-02-07T07:37:07Z</published>
    <title>Representation Learning for Natural Language Processing</title>
    <summary>  This book aims to review and present the recent advances of distributed
representation learning for NLP, including why representation learning can
improve NLP, how representation learning takes part in various important topics
of NLP, and what challenges are still not well addressed by distributed
representation.
</summary>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Yankai Lin</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-981-15-5573-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-981-15-5573-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.03732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12511v2</id>
    <updated>2021-03-04T12:40:58Z</updated>
    <published>2021-02-24T19:04:17Z</published>
    <title>References in Wikipedia: The Editors' Perspective</title>
    <summary>  References are an essential part of Wikipedia. Each statement in Wikipedia
should be referenced. In this paper, we explore the creation and collection of
references for new Wikipedia articles from an editors' perspective. We map out
the workflow of editors when creating a new article, emphasising how they
select references.
</summary>
    <author>
      <name>Lucie-Aimée Kaffee</name>
    </author>
    <author>
      <name>Hady Elsahar</name>
    </author>
    <link href="http://arxiv.org/abs/2102.12511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12634v1</id>
    <updated>2021-02-25T02:03:35Z</updated>
    <published>2021-02-25T02:03:35Z</published>
    <title>Automatic Story Generation: Challenges and Attempts</title>
    <summary>  The scope of this survey paper is to explore the challenges in automatic
story generation. We hope to contribute in the following ways: 1. Explore how
previous research in story generation addressed those challenges. 2. Discuss
future research directions and new technologies that may aid more advancements.
3. Shed light on emerging and often overlooked challenges such as creativity
and discourse.
</summary>
    <author>
      <name>Amal Alabdulkarim</name>
    </author>
    <author>
      <name>Siyan Li</name>
    </author>
    <author>
      <name>Xiangyu Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2102.12634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12843v1</id>
    <updated>2021-02-25T13:30:04Z</updated>
    <published>2021-02-25T13:30:04Z</published>
    <title>Spanish Biomedical and Clinical Language Embeddings</title>
    <summary>  We computed both Word and Sub-word Embeddings using FastText. For Sub-word
embeddings we selected Byte Pair Encoding (BPE) algorithm to represent the
sub-words. We evaluated the Biomedical Word Embeddings obtaining better results
than previous versions showing the implication that with more data, we obtain
better representations.
</summary>
    <author>
      <name>Asier Gutiérrez-Fandiño</name>
    </author>
    <author>
      <name>Jordi Armengol-Estapé</name>
    </author>
    <author>
      <name>Casimiro Pio Carrino</name>
    </author>
    <author>
      <name>Ona De Gibert</name>
    </author>
    <author>
      <name>Aitor Gonzalez-Agirre</name>
    </author>
    <author>
      <name>Marta Villegas</name>
    </author>
    <link href="http://arxiv.org/abs/2102.12843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07705v1</id>
    <updated>2021-07-16T04:47:50Z</updated>
    <published>2021-07-16T04:47:50Z</published>
    <title>Pseudo-labelling Enhanced Media Bias Detection</title>
    <summary>  Leveraging unlabelled data through weak or distant supervision is a
compelling approach to developing more effective text classification models.
This paper proposes a simple but effective data augmentation method, which
leverages the idea of pseudo-labelling to select samples from noisy distant
supervision annotation datasets. The result shows that the proposed method
improves the accuracy of biased news detection models.
</summary>
    <author>
      <name>Qin Ruan</name>
    </author>
    <author>
      <name>Brian Mac Namee</name>
    </author>
    <author>
      <name>Ruihai Dong</name>
    </author>
    <link href="http://arxiv.org/abs/2107.07705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.10127v1</id>
    <updated>2021-08-23T12:51:24Z</updated>
    <published>2021-08-23T12:51:24Z</published>
    <title>Legal Search in Case Law and Statute Law</title>
    <summary>  In this work we describe a method to identify document pairwise relevance in
the context of a typical legal document collection: limited resources, long
queries and long documents. We review the usage of generalized language models,
including supervised and unsupervised learning. We observe how our method,
while using text summaries, overperforms existing baselines based on full text,
and motivate potential improvement directions for future work.
</summary>
    <author>
      <name>Julien Rossi</name>
    </author>
    <author>
      <name>Evangelos Kanoulas</name>
    </author>
    <link href="http://arxiv.org/abs/2108.10127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04114v2</id>
    <updated>2021-09-17T11:54:37Z</updated>
    <published>2021-09-09T09:10:07Z</published>
    <title>Fixing exposure bias with imitation learning needs powerful oracles</title>
    <summary>  We apply imitation learning (IL) to tackle the NMT exposure bias problem with
error-correcting oracles, and evaluate an SMT lattice-based oracle which,
despite its excellent performance in an unconstrained oracle translation task,
turned out to be too pruned and idiosyncratic to serve as the oracle for IL.
</summary>
    <author>
      <name>Luca Hormann</name>
    </author>
    <author>
      <name>Artem Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04876v1</id>
    <updated>2021-09-10T13:44:15Z</updated>
    <published>2021-09-10T13:44:15Z</published>
    <title>Integrating Approaches to Word Representation</title>
    <summary>  The problem of representing the atomic elements of language in modern neural
learning systems is one of the central challenges of the field of natural
language processing. I present a survey of the distributional, compositional,
and relational approaches to addressing this task, and discuss various means of
integrating them into systems, with special emphasis on the word level and the
out-of-vocabulary phenomenon.
</summary>
    <author>
      <name>Yuval Pinter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Adapted dissertation introduction</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.04876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07017v1</id>
    <updated>2021-09-14T23:28:26Z</updated>
    <published>2021-09-14T23:28:26Z</published>
    <title>Written Justifications are Key to Aggregate Crowdsourced Forecasts</title>
    <summary>  This paper demonstrates that aggregating crowdsourced forecasts benefits from
modeling the written justifications provided by forecasters. Our experiments
show that the majority and weighted vote baselines are competitive, and that
the written justifications are beneficial to call a question throughout its
life except in the last quarter. We also conduct an error analysis shedding
light into the characteristics that make a justification unreliable.
</summary>
    <author>
      <name>Saketh Kotamraju</name>
    </author>
    <author>
      <name>Eduardo Blanco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of EMNLP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.07017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07230v1</id>
    <updated>2021-09-15T11:54:28Z</updated>
    <published>2021-09-15T11:54:28Z</published>
    <title>Learning Mathematical Properties of Integers</title>
    <summary>  Embedding words in high-dimensional vector spaces has proven valuable in many
natural language applications. In this work, we investigate whether
similarly-trained embeddings of integers can capture concepts that are useful
for mathematical applications. We probe the integer embeddings for mathematical
knowledge, apply them to a set of numerical reasoning tasks, and show that by
learning the representations from mathematical sequence data, we can
substantially improve over number embeddings learned from English text corpora.
</summary>
    <author>
      <name>Maria Ryskina</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BlackboxNLP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.07230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09420v1</id>
    <updated>2021-09-20T10:46:35Z</updated>
    <published>2021-09-20T10:46:35Z</published>
    <title>Crowdsourcing Diverse Paraphrases for Training Task-oriented Bots</title>
    <summary>  A prominent approach to build datasets for training task-oriented bots is
crowd-based paraphrasing. Current approaches, however, assume the crowd would
naturally provide diverse paraphrases or focus only on lexical diversity. In
this WiP we addressed an overlooked aspect of diversity, introducing an
approach for guiding the crowdsourcing process towards paraphrases that are
syntactically diverse.
</summary>
    <author>
      <name>Jorge Ramírez</name>
    </author>
    <author>
      <name>Auday Berro</name>
    </author>
    <author>
      <name>Marcos Baez</name>
    </author>
    <author>
      <name>Boualem Benatallah</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HCOMP 2021 Works-in-progress &amp; Demonstrations</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00808v1</id>
    <updated>2021-11-01T10:21:58Z</updated>
    <published>2021-11-01T10:21:58Z</published>
    <title>Unsupervised Discovery of Unaccusative and Unergative Verbs</title>
    <summary>  We present an unsupervised method to detect English unergative and
unaccusative verbs. These categories allow us to identify verbs participating
in the causative-inchoative alternation without knowing the semantic roles of
the verb. The method is based on the generation of intransitive sentence
variants of candidate verbs and probing a language model. We obtained results
on par with similar approaches, with the added benefit of not relying on
annotated resources.
</summary>
    <author>
      <name>Sharid Loáiciga</name>
    </author>
    <author>
      <name>Luca Bevacqua</name>
    </author>
    <author>
      <name>Christian Hardmeier</name>
    </author>
    <link href="http://arxiv.org/abs/2111.00808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01471v1</id>
    <updated>2021-11-02T10:14:17Z</updated>
    <published>2021-11-02T10:14:17Z</published>
    <title>Zero-Shot Translation using Diffusion Models</title>
    <summary>  In this work, we show a novel method for neural machine translation (NMT),
using a denoising diffusion probabilistic model (DDPM), adjusted for textual
data, following recent advances in the field. We show that it's possible to
translate sentences non-autoregressively using a diffusion model conditioned on
the source sentence. We also show that our model is able to translate between
pairs of languages unseen during training (zero-shot learning).
</summary>
    <author>
      <name>Eliya Nachmani</name>
    </author>
    <author>
      <name>Shaked Dovrat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.01471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07783v2</id>
    <updated>2024-05-30T00:26:06Z</updated>
    <published>2021-12-14T23:06:21Z</published>
    <title>Online antisemitism across platforms</title>
    <summary>  We created a fine-grained AI system for the detection of antisemitism. This
Explainable AI will identify English and German anti-Semitic expressions of
dehumanization, verbal aggression and conspiracies in online social media
messages across platforms, to support high-level decision making.
</summary>
    <author>
      <name>Tom De Smedt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Textgain Technical Reports 7 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.07783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04873v1</id>
    <updated>2022-04-11T05:32:14Z</updated>
    <published>2022-04-11T05:32:14Z</published>
    <title>Adapting BigScience Multilingual Model to Unseen Languages</title>
    <summary>  We benchmark different strategies of adding new languages (German and Korean)
into the BigScience's pretrained multilingual language model with 1.3 billion
parameters that currently supports 13 languages. We investigate the factors
that affect the language adaptability of the model and the trade-offs between
computational costs and expected performance.
</summary>
    <author>
      <name>Zheng-Xin Yong</name>
    </author>
    <author>
      <name>Vassilina Nikoulina</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09719v1</id>
    <updated>2022-04-08T05:49:04Z</updated>
    <published>2022-04-08T05:49:04Z</published>
    <title>Recent Progress in Conversational AI</title>
    <summary>  Conversational artificial intelligence (AI) is becoming an increasingly
popular topic among industry and academia. With the fast development of neural
network-based models, a lot of neural-based conversational AI system are
developed. We will provide a brief review of the recent progress in the
Conversational AI, including the commonly adopted techniques, notable works,
famous competitions from academia and industry and widely used datasets.
</summary>
    <author>
      <name>Zijun Xue</name>
    </author>
    <author>
      <name>Ruirui Li</name>
    </author>
    <author>
      <name>Mingda Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.11104v1</id>
    <updated>2022-04-23T16:47:48Z</updated>
    <published>2022-04-23T16:47:48Z</published>
    <title>WikiMulti: a Corpus for Cross-Lingual Summarization</title>
    <summary>  Cross-lingual summarization (CLS) is the task to produce a summary in one
particular language for a source document in a different language. We introduce
WikiMulti - a new dataset for cross-lingual summarization based on Wikipedia
articles in 15 languages. As a set of baselines for further studies, we
evaluate the performance of existing cross-lingual abstractive summarization
methods on our dataset. We make our dataset publicly available here:
https://github.com/tikhonovpavel/wikimulti
</summary>
    <author>
      <name>Pavel Tikhonov</name>
    </author>
    <author>
      <name>Valentin Malykh</name>
    </author>
    <link href="http://arxiv.org/abs/2204.11104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12481v1</id>
    <updated>2022-04-26T17:52:59Z</updated>
    <published>2022-04-26T17:52:59Z</published>
    <title>From Hyperbolic Geometry Back to Word Embeddings</title>
    <summary>  We choose random points in the hyperbolic disc and claim that these points
are already word representations. However, it is yet to be uncovered which
point corresponds to which word of the human language of interest. This
correspondence can be approximately established using a pointwise mutual
information between words and recent alignment techniques.
</summary>
    <author>
      <name>Sultan Nurmukhamedov</name>
    </author>
    <author>
      <name>Thomas Mach</name>
    </author>
    <author>
      <name>Arsen Sheverdin</name>
    </author>
    <author>
      <name>Zhenisbek Assylbekov</name>
    </author>
    <link href="http://arxiv.org/abs/2204.12481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13638v2</id>
    <updated>2022-06-08T18:46:51Z</updated>
    <published>2022-04-28T16:58:17Z</published>
    <title>Russian Texts Detoxification with Levenshtein Editing</title>
    <summary>  Text detoxification is a style transfer task of creating neutral versions of
toxic texts. In this paper, we use the concept of text editing to build a
two-step tagging-based detoxification model using a parallel corpus of Russian
texts. With this model, we achieved the best style transfer accuracy among all
models in the RUSSE Detox shared task, surpassing larger sequence-to-sequence
models.
</summary>
    <author>
      <name>Ilya Gusev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Dialogue 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13638v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13638v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06168v1</id>
    <updated>2022-05-12T15:45:10Z</updated>
    <published>2022-05-12T15:45:10Z</published>
    <title>Using dependency parsing for few-shot learning in distributional
  semantics</title>
    <summary>  In this work, we explore the novel idea of employing dependency parsing
information in the context of few-shot learning, the task of learning the
meaning of a rare word based on a limited amount of context sentences. Firstly,
we use dependency-based word embedding models as background spaces for few-shot
learning. Secondly, we introduce two few-shot learning methods which enhance
the additive baseline model by using dependencies.
</summary>
    <author>
      <name>Stefania Preda</name>
    </author>
    <author>
      <name>Guy Emerson</name>
    </author>
    <link href="http://arxiv.org/abs/2205.06168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07557v2</id>
    <updated>2022-05-17T08:09:51Z</updated>
    <published>2022-05-16T10:08:11Z</published>
    <title>Heroes, Villains, and Victims, and GPT-3: Automated Extraction of
  Character Roles Without Training Data</title>
    <summary>  This paper shows how to use large-scale pre-trained language models to
extract character roles from narrative texts without training data. Queried
with a zero-shot question-answering prompt, GPT-3 can identify the hero,
villain, and victim in diverse domains: newspaper articles, movie plot
summaries, and political speeches.
</summary>
    <author>
      <name>Dominik Stammbach</name>
    </author>
    <author>
      <name>Maria Antoniak</name>
    </author>
    <author>
      <name>Elliott Ash</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07634v1</id>
    <updated>2022-05-16T12:50:58Z</updated>
    <published>2022-05-16T12:50:58Z</published>
    <title>A Precis of Language Models are not Models of Language</title>
    <summary>  Natural Language Processing is one of the leading application areas in the
current resurgence of Artificial Intelligence, spearheaded by Artificial Neural
Networks. We show that despite their many successes at performing linguistic
tasks, Large Neural Language Models are ill-suited as comprehensive models of
natural language. The wider implication is that, in spite of the often
overbearing optimism about AI, modern neural models do not represent a
revolution in our understanding of cognition.
</summary>
    <author>
      <name>Csaba Veres</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11836v1</id>
    <updated>2022-05-24T06:58:07Z</updated>
    <published>2022-05-24T06:58:07Z</published>
    <title>Charon: a FrameNet Annotation Tool for Multimodal Corpora</title>
    <summary>  This paper presents Charon, a web tool for annotating multimodal corpora with
FrameNet categories. Annotation can be made for corpora containing both static
images and video sequences paired - or not - with text sequences. The pipeline
features, besides the annotation interface, corpus import and pre-processing
tools.
</summary>
    <author>
      <name>Frederico Belcavello</name>
    </author>
    <author>
      <name>Marcelo Viridiano</name>
    </author>
    <author>
      <name>Ely Edison Matos</name>
    </author>
    <author>
      <name>Tiago Timponi Torrent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted submission for the The Sixteenth Linguistic Annotation
  Workshop (LAW-XVI 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.11836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.13708v1</id>
    <updated>2022-05-27T01:55:59Z</updated>
    <published>2022-05-27T01:55:59Z</published>
    <title>HiJoNLP at SemEval-2022 Task 2: Detecting Idiomaticity of Multiword
  Expressions using Multilingual Pretrained Language Models</title>
    <summary>  This paper describes an approach to detect idiomaticity only from the
contextualized representation of a MWE over multilingual pretrained language
models. Our experiments find that larger models are usually more effective in
idiomaticity detection. However, using a higher layer of the model may not
guarantee a better performance. In multilingual scenarios, the convergence of
different languages are not consistent and rich-resource languages have big
advantages over other languages.
</summary>
    <author>
      <name>Minghuan Tan</name>
    </author>
    <link href="http://arxiv.org/abs/2205.13708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.13708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.05033v2</id>
    <updated>2022-06-19T12:56:28Z</updated>
    <published>2022-04-30T12:50:10Z</published>
    <title>Solution of DeBERTaV3 on CommonsenseQA</title>
    <summary>  We report the performance of DeBERTaV3 on CommonsenseQA in this report. We
simply formalize the answer selection as a text classification for DeBERTaV3.
The strong natural language inference ability of DeBERTaV3 helps its single and
ensemble model set the new (w/o external knowledge) state-of-the-art on
CommonsenseQA.
</summary>
    <author>
      <name>Letian Peng</name>
    </author>
    <author>
      <name>Zuchao Li</name>
    </author>
    <author>
      <name>Hai Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2206.05033v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05033v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.08727v1</id>
    <updated>2022-06-17T12:27:20Z</updated>
    <published>2022-06-17T12:27:20Z</published>
    <title>The ITU Faroese Pairs Dataset</title>
    <summary>  This article documents a dataset of sentence pairs between Faroese and
Danish, produced at ITU Copenhagen. The data covers tranlsation from both
source languages, and is intended for use as training data for machine
translation systems in this language pair.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>Annika Solveig Hedegaard Isfeldt</name>
    </author>
    <author>
      <name>Signhild Djurhuus</name>
    </author>
    <link href="http://arxiv.org/abs/2206.08727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.08727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02657v2</id>
    <updated>2022-09-25T09:13:35Z</updated>
    <published>2022-07-06T13:23:18Z</published>
    <title>A Challenge on Semi-Supervised and Reinforced Task-Oriented Dialog
  Systems</title>
    <summary>  A challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems,
Co-located with EMNLP2022 SereTOD Workshop.
</summary>
    <author>
      <name>Zhijian Ou</name>
    </author>
    <author>
      <name>Junlan Feng</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <author>
      <name>Yakun Li</name>
    </author>
    <author>
      <name>Hong Liu</name>
    </author>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Yi Huang</name>
    </author>
    <author>
      <name>Jiangjiang Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2.1</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.06161v2</id>
    <updated>2023-02-24T20:49:54Z</updated>
    <published>2022-08-12T08:15:34Z</published>
    <title>Sparse Probability of Agreement</title>
    <summary>  Measuring inter-annotator agreement is important for annotation tasks, but
many metrics require a fully-annotated set of data, where all annotators
annotate all samples. We define Sparse Probability of Agreement, SPA, which
estimates the probability of agreement when not all annotator-item-pairs are
available. We show that under certain conditions, SPA is an unbiased estimator,
and we provide multiple weighing schemes for handling data with various degrees
of annotation.
</summary>
    <author>
      <name>Jeppe Nørregaard</name>
    </author>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <link href="http://arxiv.org/abs/2208.06161v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06161v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10245v1</id>
    <updated>2022-07-26T17:18:24Z</updated>
    <published>2022-07-26T17:18:24Z</published>
    <title>When BERT Fails -- The Limits of EHR Classification</title>
    <summary>  Transformers are powerful text representation learners, useful for all kinds
of clinical decision support tasks. Although they outperform baselines on
readmission prediction, they are not infallible. Here, we look into one such
failure case, and report patterns that lead to inferior predictive performance.
</summary>
    <author>
      <name>Augusto Garcia-Agundez</name>
    </author>
    <author>
      <name>Carsten Eickhoff</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AMIA 2022 Annual Symposium</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.10245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11194v1</id>
    <updated>2022-08-23T21:07:42Z</updated>
    <published>2022-08-23T21:07:42Z</published>
    <title>Bitext Mining for Low-Resource Languages via Contrastive Learning</title>
    <summary>  Mining high-quality bitexts for low-resource languages is challenging. This
paper shows that sentence representation of language models fine-tuned with
multiple negatives ranking loss, a contrastive objective, helps retrieve clean
bitexts. Experiments show that parallel data mined from our approach
substantially outperform the previous state-of-the-art method on low resource
languages Khmer and Pashto.
</summary>
    <author>
      <name>Weiting Tan</name>
    </author>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <link href="http://arxiv.org/abs/2208.11194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01650v2</id>
    <updated>2022-09-20T20:34:02Z</updated>
    <published>2022-09-04T15:55:56Z</published>
    <title>ArgLegalSumm: Improving Abstractive Summarization of Legal Documents
  with Argument Mining</title>
    <summary>  A challenging task when generating summaries of legal documents is the
ability to address their argumentative nature. We introduce a simple technique
to capture the argumentative structure of legal documents by integrating
argument role labeling into the summarization process. Experiments with
pretrained language models show that our proposed approach improves performance
over strong baselines
</summary>
    <author>
      <name>Mohamed Elaraby</name>
    </author>
    <author>
      <name>Diane Litman</name>
    </author>
    <link href="http://arxiv.org/abs/2209.01650v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01650v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11016v1</id>
    <updated>2022-09-22T13:53:15Z</updated>
    <published>2022-09-22T13:53:15Z</published>
    <title>Approaching English-Polish Machine Translation Quality Assessment with
  Neural-based Methods</title>
    <summary>  This paper presents our contribution to the PolEval 2021 Task 2: Evaluation
of translation quality assessment metrics. We describe experiments with
pre-trained language models and state-of-the-art frameworks for translation
quality assessment in both nonblind and blind versions of the task. Our
solutions ranked second in the nonblind version and third in the blind version.
</summary>
    <author>
      <name>Artur Nowakowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PolEval 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the PolEval 2021 Workshop, 2021, 73-78</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.11016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.11016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12478v1</id>
    <updated>2022-10-22T15:33:38Z</updated>
    <published>2022-10-22T15:33:38Z</published>
    <title>DiscoSense: Commonsense Reasoning with Discourse Connectives</title>
    <summary>  We present DiscoSense, a benchmark for commonsense reasoning via
understanding a wide variety of discourse connectives. We generate compelling
distractors in DiscoSense using Conditional Adversarial Filtering, an extension
of Adversarial Filtering that employs conditional generation. We show that
state-of-the-art pre-trained language models struggle to perform well on
DiscoSense, which makes this dataset ideal for evaluating next-generation
commonsense reasoning systems.
</summary>
    <author>
      <name>Prajjwal Bhargava</name>
    </author>
    <author>
      <name>Vincent Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.14378v1</id>
    <updated>2022-10-25T23:09:20Z</updated>
    <published>2022-10-25T23:09:20Z</published>
    <title>Bilingual Lexicon Induction for Low-Resource Languages using Graph
  Matching via Optimal Transport</title>
    <summary>  Bilingual lexicons form a critical component of various natural language
processing applications, including unsupervised and semisupervised machine
translation and crosslingual information retrieval. We improve bilingual
lexicon induction performance across 40 language pairs with a graph-matching
method based on optimal transport. The method is especially strong with low
amounts of supervision.
</summary>
    <author>
      <name>Kelly Marchisio</name>
    </author>
    <author>
      <name>Ali Saad-Eldin</name>
    </author>
    <author>
      <name>Kevin Duh</name>
    </author>
    <author>
      <name>Carey Priebe</name>
    </author>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2022 Camera-Ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.14378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.14378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00513v1</id>
    <updated>2022-11-01T15:01:20Z</updated>
    <published>2022-11-01T15:01:20Z</published>
    <title>E2E Refined Dataset</title>
    <summary>  Although the well-known MR-to-text E2E dataset has been used by many
researchers, its MR-text pairs include many deletion/insertion/substitution
errors. Since such errors affect the quality of MR-to-text systems, they must
be fixed as much as possible. Therefore, we developed a refined dataset and
some python programs that convert the original E2E dataset into a refined
dataset.
</summary>
    <author>
      <name>Keisuke Toyama</name>
    </author>
    <author>
      <name>Katsuhito Sudoh</name>
    </author>
    <author>
      <name>Satoshi Nakamura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.06662v1</id>
    <updated>2022-11-12T13:42:49Z</updated>
    <published>2022-11-12T13:42:49Z</published>
    <title>Addressing Segmentation Ambiguity in Neural Linguistic Steganography</title>
    <summary>  Previous studies on neural linguistic steganography, except Ueoka et al.
(2021), overlook the fact that the sender must detokenize cover texts to avoid
arousing the eavesdropper's suspicion. In this paper, we demonstrate that
segmentation ambiguity indeed causes occasional decoding failures at the
receiver's side. With the near-ubiquity of subwords, this problem now affects
any language. We propose simple tricks to overcome this problem, which are even
applicable to languages without explicit word boundaries.
</summary>
    <author>
      <name>Jumon Nozaki</name>
    </author>
    <author>
      <name>Yugo Murawaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AACL-IJCNLP2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.06662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15022v1</id>
    <updated>2022-11-28T03:10:50Z</updated>
    <published>2022-11-28T03:10:50Z</published>
    <title>Summer: WeChat Neural Machine Translation Systems for the WMT22
  Biomedical Translation Task</title>
    <summary>  This paper introduces WeChat's participation in WMT 2022 shared biomedical
translation task on Chinese to English. Our systems are based on the
Transformer, and use several different Transformer structures to improve the
quality of translation. In our experiments, we employ data filtering, data
generation, several variants of Transformer, fine-tuning and model ensemble.
Our Chinese$\to$English system, named Summer, achieves the highest BLEU score
among all submissions.
</summary>
    <author>
      <name>Ernan Li</name>
    </author>
    <author>
      <name>Fandong Meng</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2211.15022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15568v1</id>
    <updated>2022-11-28T17:27:14Z</updated>
    <published>2022-11-28T17:27:14Z</published>
    <title>Automatically generating question-answer pairs for assessing basic
  reading comprehension in Swedish</title>
    <summary>  This paper presents an evaluation of the quality of automatically generated
reading comprehension questions from Swedish text, using the Quinductor method.
This method is a light-weight, data-driven but non-neural method for automatic
question generation (QG). The evaluation shows that Quinductor is a viable QG
method that can provide a strong baseline for neural-network-based QG methods.
</summary>
    <author>
      <name>Dmytro Kalpakchi</name>
    </author>
    <author>
      <name>Johan Boye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SLTC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15980v2</id>
    <updated>2022-12-03T13:31:50Z</updated>
    <published>2022-11-29T07:27:36Z</published>
    <title>End-to-End Neural Discourse Deixis Resolution in Dialogue</title>
    <summary>  We adapt Lee et al.'s (2018) span-based entity coreference model to the task
of end-to-end discourse deixis resolution in dialogue, specifically by
proposing extensions to their model that exploit task-specific characteristics.
The resulting model, dd-utt, achieves state-of-the-art results on the four
datasets in the CODI-CRAC 2021 shared task.
</summary>
    <author>
      <name>Shengjie Li</name>
    </author>
    <author>
      <name>Vincent Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a long paper to EMNLP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15980v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15980v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.02352v1</id>
    <updated>2022-12-05T15:35:10Z</updated>
    <published>2022-12-05T15:35:10Z</published>
    <title>Fake News and Hate Speech: Language in Common</title>
    <summary>  In this paper we raise the research question of whether fake news and hate
speech spreaders share common patterns in language. We compute a novel index,
the ingroup vs outgroup index, in three different datasets and we show that
both phenomena share an "us vs them" narrative.
</summary>
    <author>
      <name>Berta Chulvi</name>
    </author>
    <author>
      <name>Alejandro Toselli</name>
    </author>
    <author>
      <name>Paolo Rosso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.02352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03575v1</id>
    <updated>2022-12-07T11:23:43Z</updated>
    <published>2022-12-07T11:23:43Z</published>
    <title>Tag Embedding and Well-defined Intermediate Representation improve
  Auto-Formulation of Problem Description</title>
    <summary>  In this report, I address auto-formulation of problem description, the task
of converting an optimization problem into a canonical representation. I first
simplify the auto-formulation task by defining an intermediate representation,
then introduce entity tag embedding to utilize a given entity tag information.
The ablation study demonstrate the effectiveness of the proposed method, which
finally took second place in NeurIPS 2022 NL4Opt competition subtask 2.
</summary>
    <author>
      <name>Sanghwan Jang</name>
    </author>
    <link href="http://arxiv.org/abs/2212.03575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07219v1</id>
    <updated>2022-12-14T13:41:36Z</updated>
    <published>2022-12-14T13:41:36Z</published>
    <title>VTCC-NLP at NL4Opt competition subtask 1: An Ensemble Pre-trained
  language models for Named Entity Recognition</title>
    <summary>  We propose a combined three pre-trained language models (XLM-R, BART, and
DeBERTa-V3) as an empower of contextualized embedding for named entity
recognition. Our model achieves a 92.9% F1 score on the test set and ranks 5th
on the leaderboard at NL4Opt competition subtask 1.
</summary>
    <author>
      <name>Xuan-Dung Doan</name>
    </author>
    <link href="http://arxiv.org/abs/2212.07219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07649v1</id>
    <updated>2022-12-15T08:15:32Z</updated>
    <published>2022-12-15T08:15:32Z</published>
    <title>Improve Text Classification Accuracy with Intent Information</title>
    <summary>  Text classification, a core component of task-oriented dialogue systems,
attracts continuous research from both the research and industry community, and
has resulted in tremendous progress. However, existing method does not consider
the use of label information, which may weaken the performance of text
classification systems in some token-aware scenarios. To address the problem,
in this paper, we introduce the use of label information as label embedding for
the task of text classification and achieve remarkable performance on benchmark
dataset.
</summary>
    <author>
      <name>Yifeng Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2212.07649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.14648v1</id>
    <updated>2022-12-30T12:20:39Z</updated>
    <published>2022-12-30T12:20:39Z</published>
    <title>Distant Reading of the German Coalition Deal: Recognizing Policy
  Positions with BERT-based Text Classification</title>
    <summary>  Automated text analysis has become a widely used tool in political science.
In this research, we use a BERT model trained on German party manifestos to
identify the individual parties' contribution to the coalition agreement of
2021.
</summary>
    <author>
      <name>Michael Zylla</name>
    </author>
    <author>
      <name>Thomas Haider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Digital Humanities Conference 2022, Tokyo</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.14648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.14648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00399v1</id>
    <updated>2023-01-01T13:20:57Z</updated>
    <published>2023-01-01T13:20:57Z</published>
    <title>Semantic Operator Prediction and Applications</title>
    <summary>  In the present paper, semantic parsing challenges are briefly introduced and
QDMR formalism in semantic parsing is implemented using sequence to sequence
model with attention but uses only part of speech(POS) as a representation of
words of a sentence to make the training as simple and as fast as possible and
also avoiding curse of dimensionality as well as overfitting. It is shown how
semantic operator prediction could be augmented with other models like the
CopyNet model or the recursive neural net model.
</summary>
    <author>
      <name>Farshad Noravesh</name>
    </author>
    <link href="http://arxiv.org/abs/2301.00399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03559v1</id>
    <updated>2023-01-09T18:20:10Z</updated>
    <published>2023-01-09T18:20:10Z</published>
    <title>Color Me Intrigued: Quantifying Usage of Colors in Fiction</title>
    <summary>  We present preliminary results in quantitative analyses of color usage in
selected authors' works from LitBank. Using Glasgow Norms, human ratings on
5000+ words, we measure attributes of nouns dependent on color terms. Early
results demonstrate a significant increase in noun concreteness over time. We
also propose future research directions for computational literary color
analytics.
</summary>
    <author>
      <name>Siyan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted into the Creative AI Across Modalities workshop at AAAI2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.12206v1</id>
    <updated>2023-01-28T14:06:17Z</updated>
    <published>2023-01-28T14:06:17Z</published>
    <title>Semantic Tagging with LSTM-CRF</title>
    <summary>  In the present paper, two models are presented namely LSTM-CRF and
BERT-LSTM-CRF for semantic tagging of universal semantic tag dataset. The
experiments show that the first model is much easier to converge while the
second model that leverages BERT embedding, takes a long time to converge and
needs a big dataset for semtagging to be effective.
</summary>
    <author>
      <name>Farshad Noravesh</name>
    </author>
    <link href="http://arxiv.org/abs/2301.12206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.12206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.12417v1</id>
    <updated>2023-01-29T10:55:36Z</updated>
    <published>2023-01-29T10:55:36Z</published>
    <title>Syrupy Mouthfeel and Hints of Chocolate -- Predicting Coffee Review
  Scores using Text Based Sentiment</title>
    <summary>  This paper uses textual data contained in certified (q-graded) coffee reviews
to predict corresponding scores on a scale from 0-100. By transforming this
highly specialized and standardized textual data in a predictor space, we
construct regression models which accurately capture the patterns in
corresponding coffee bean scores.
</summary>
    <author>
      <name>Christopher Lohse</name>
    </author>
    <author>
      <name>Jeroen Lemsom</name>
    </author>
    <author>
      <name>Athanasios Kalogiratos</name>
    </author>
    <link href="http://arxiv.org/abs/2301.12417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.12417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13819v2</id>
    <updated>2023-02-06T12:03:38Z</updated>
    <published>2023-01-24T19:23:38Z</published>
    <title>Causal-Discovery Performance of ChatGPT in the context of Neuropathic
  Pain Diagnosis</title>
    <summary>  ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.
</summary>
    <author>
      <name>Ruibo Tu</name>
    </author>
    <author>
      <name>Chao Ma</name>
    </author>
    <author>
      <name>Cheng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2301.13819v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13819v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.04834v1</id>
    <updated>2023-02-09T18:41:04Z</updated>
    <published>2023-02-09T18:41:04Z</published>
    <title>FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning</title>
    <summary>  In this paper, we propose FrameBERT, a RoBERTa-based model that can
explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor
detection. FrameBERT not only achieves better or comparable performance to the
state-of-the-art, but also is more explainable and interpretable compared to
existing models, attributing to its ability of accounting for external
knowledge of FrameNet.
</summary>
    <author>
      <name>Yucheng Li</name>
    </author>
    <author>
      <name>Shun Wang</name>
    </author>
    <author>
      <name>Chenghua Lin</name>
    </author>
    <author>
      <name>Frank Guerin</name>
    </author>
    <author>
      <name>Loïc Barrault</name>
    </author>
    <link href="http://arxiv.org/abs/2302.04834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05120v1</id>
    <updated>2023-02-10T08:50:51Z</updated>
    <published>2023-02-10T08:50:51Z</published>
    <title>Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial
  Text Attacks</title>
    <summary>  We propose a novel gradient-based attack against transformer-based language
models that searches for an adversarial example in a continuous space of token
probabilities. Our algorithm mitigates the gap between adversarial loss for
continuous and discrete text representations by performing multi-step
quantization in a quantization-compensation loop. Experiments show that our
method significantly outperforms other approaches on various natural language
processing (NLP) tasks.
</summary>
    <author>
      <name>Piotr Gaiński</name>
    </author>
    <author>
      <name>Klaudia Bałazy</name>
    </author>
    <link href="http://arxiv.org/abs/2302.05120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.04093v1</id>
    <updated>2023-03-07T17:53:00Z</updated>
    <published>2023-03-07T17:53:00Z</published>
    <title>Marpa and nullable symbols</title>
    <summary>  The Marpa parser was intended to make the best results in the academic
literature on Earley's algorithm available as a practical general parser.
Earley-based parsers have had issues handling nullable symbols. Initially, we
dealt with nullable symbols by following the approach in Aycock and Horspool's
2002 paper. This paper reports our experience with the approach in that paper,
and the approach to handling nullables that we settled on in reaction to that
experience.
</summary>
    <author>
      <name>Jeffrey Kegler</name>
    </author>
    <link href="http://arxiv.org/abs/2303.04093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.04093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14234v1</id>
    <updated>2023-03-24T18:59:06Z</updated>
    <published>2023-03-24T18:59:06Z</published>
    <title>SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model</title>
    <summary>  Language documentation is a critical aspect of language preservation, often
including the creation of Interlinear Glossed Text (IGT). Creating IGT is
time-consuming and tedious, and automating the process can save valuable
annotator effort.
  This paper describes the baseline system for the SIGMORPHON 2023 Shared Task
of Interlinear Glossing. In our system, we utilize a transformer architecture
and treat gloss generation as a sequence labelling task.
</summary>
    <author>
      <name>Michael Ginn</name>
    </author>
    <link href="http://arxiv.org/abs/2303.14234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16742v1</id>
    <updated>2023-03-29T14:49:29Z</updated>
    <published>2023-03-29T14:49:29Z</published>
    <title>Evaluating NLG systems: A brief introduction</title>
    <summary>  This year the International Conference on Natural Language Generation (INLG)
will feature an award for the paper with the best evaluation. The purpose of
this award is to provide an incentive for NLG researchers to pay more attention
to the way they assess the output of their systems. This essay provides a short
introduction to evaluation in NLG, explaining key terms and distinctions.
</summary>
    <author>
      <name>Emiel van Miltenburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published on the INLG2023 conference website</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.16742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.06962v1</id>
    <updated>2023-04-14T07:07:42Z</updated>
    <published>2023-04-14T07:07:42Z</published>
    <title>Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning</title>
    <summary>  Prompt engineering and calibration make large language models excel at
reasoning tasks, including multiple choice commonsense reasoning. From a
practical perspective, we investigate and evaluate these strategies on smaller
language models. Through experiments on five commonsense reasoning benchmarks,
we find that each strategy favors certain models, but their joint effects are
mostly negative.
</summary>
    <author>
      <name>Chenkai Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the ICLR TinyPaper track</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.06962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.06962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14590v2</id>
    <updated>2023-06-06T00:46:49Z</updated>
    <published>2023-04-28T01:53:54Z</published>
    <title>A logical word embedding for learning grammar</title>
    <summary>  We introduce the logical grammar emdebbing (LGE), a model inspired by
pregroup grammars and categorial grammars to enable unsupervised inference of
lexical categories and syntactic rules from a corpus of text. LGE produces
comprehensible output summarizing its inferences, has a completely transparent
process for producing novel sentences, and can learn from as few as a hundred
sentences.
</summary>
    <author>
      <name>Sean Deyo</name>
    </author>
    <author>
      <name>Veit Elser</name>
    </author>
    <link href="http://arxiv.org/abs/2304.14590v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14590v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14780v1</id>
    <updated>2023-04-28T11:40:48Z</updated>
    <published>2023-04-28T11:40:48Z</published>
    <title>Training and Evaluation of a Multilingual Tokenizer for GPT-SW3</title>
    <summary>  This paper provides a detailed discussion of the multilingual tokenizer used
for GPT-SW3. It was trained on the Nordic Pile using the SentencePiece library
and the BPE algorithm. We outline the tokenizer's most important features and
share details on its learned vocabulary. In addition, we systematically analyze
the properties and evaluate the performance of the tokenizer with regard to the
different languages present in the data.
</summary>
    <author>
      <name>Felix Stollenwerk</name>
    </author>
    <link href="http://arxiv.org/abs/2304.14780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08173v1</id>
    <updated>2023-05-14T14:46:12Z</updated>
    <published>2023-05-14T14:46:12Z</published>
    <title>Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset
  of Film Reviews</title>
    <summary>  This paper introduces Cro-FiReDa, a sentiment-annotated dataset for Croatian
in the domain of movie reviews. The dataset, which contains over 10,000
sentences, has been annotated at the sentence level. In addition to presenting
the overall annotation process, we also present benchmark results based on the
transformer-based fine-tuning approach
</summary>
    <author>
      <name>Gaurish Thakkar</name>
    </author>
    <author>
      <name>Nives Mikelic Preradovic</name>
    </author>
    <author>
      <name>Marko Tadić</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LTC 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.08173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08187v1</id>
    <updated>2023-05-14T15:53:54Z</updated>
    <published>2023-05-14T15:53:54Z</published>
    <title>CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus</title>
    <summary>  This article presents a sentence-level sentiment dataset for the Croatian
news domain. In addition to the 3K annotated texts already present, our dataset
contains 14.5K annotated sentence occurrences that have been tagged with 5
classes. We provide baseline scores in addition to the annotation process and
inter-annotator agreement.
</summary>
    <author>
      <name>Gaurish Thakkar</name>
    </author>
    <author>
      <name>Nives Mikelic Preradović</name>
    </author>
    <author>
      <name>Marko Tadić</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Slavic NLP 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.08187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09410v1</id>
    <updated>2023-05-16T12:55:43Z</updated>
    <published>2023-05-16T12:55:43Z</published>
    <title>About Evaluation of F1 Score for RECENT Relation Extraction System</title>
    <summary>  This document contains a discussion of the F1 score evaluation used in the
article 'Relation Classification with Entity Type Restriction' by Shengfei Lyu,
Huanhuan Chen published on Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and
claim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on
the TACRED dataset, while after correcting errors and reevaluation the final
result is 65.16
</summary>
    <author>
      <name>Michał Olek</name>
    </author>
    <link href="http://arxiv.org/abs/2305.09410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10848v1</id>
    <updated>2023-05-18T10:07:50Z</updated>
    <published>2023-05-18T10:07:50Z</published>
    <title>Advancing Full-Text Search Lemmatization Techniques with Paradigm
  Retrieval from OpenCorpora</title>
    <summary>  In this paper, we unveil a groundbreaking method to amplify full-text search
lemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm
retrieval algorithm. Our primary aim is to streamline the extraction of a
word's primary form or lemma - a crucial factor in full-text search.
Additionally, we propose a compact dictionary storage strategy, significantly
boosting the speed and precision of lemma retrieval.
</summary>
    <author>
      <name>Dmitriy Kalugin-Balashov</name>
    </author>
    <link href="http://arxiv.org/abs/2305.10848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.02549v1</id>
    <updated>2023-06-05T02:52:54Z</updated>
    <published>2023-06-05T02:52:54Z</published>
    <title>Evaluation of AI Chatbots for Patient-Specific EHR Questions</title>
    <summary>  This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions.
</summary>
    <author>
      <name>Alaleh Hamidi</name>
    </author>
    <author>
      <name>Kirk Roberts</name>
    </author>
    <link href="http://arxiv.org/abs/2306.02549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.02549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08116v1</id>
    <updated>2023-06-13T20:18:24Z</updated>
    <published>2023-06-13T20:18:24Z</published>
    <title>CipherSniffer: Classifying Cipher Types</title>
    <summary>  Ciphers are a powerful tool for encrypting communication. There are many
different cipher types, which makes it computationally expensive to solve a
cipher using brute force. In this paper, we frame the decryption task as a
classification problem. We first create a dataset of transpositions,
substitutions, text reversals, word reversals, sentence shifts, and unencrypted
text. Then, we evaluate the performance of various tokenizer-model combinations
on this task.
</summary>
    <author>
      <name>Brendan Artley</name>
    </author>
    <author>
      <name>Greg Mehdiyev</name>
    </author>
    <link href="http://arxiv.org/abs/2306.08116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12719v1</id>
    <updated>2023-06-22T07:52:34Z</updated>
    <published>2023-06-22T07:52:34Z</published>
    <title>Natural Language Generation for Advertising: A Survey</title>
    <summary>  Natural language generation methods have emerged as effective tools to help
advertisers increase the number of online advertisements they produce. This
survey entails a review of the research trends on this topic over the past
decade, from template-based to extractive and abstractive approaches using
neural networks. Additionally, key challenges and directions revealed through
the survey, including metric optimization, faithfulness, diversity,
multimodality, and the development of benchmark datasets, are discussed.
</summary>
    <author>
      <name>Soichiro Murakami</name>
    </author>
    <author>
      <name>Sho Hoshino</name>
    </author>
    <author>
      <name>Peinan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2306.12719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.05081v1</id>
    <updated>2023-07-11T07:29:18Z</updated>
    <published>2023-07-11T07:29:18Z</published>
    <title>Argumentative Segmentation Enhancement for Legal Summarization</title>
    <summary>  We use the combination of argumentative zoning [1] and a legal argumentative
scheme to create legal argumentative segments. Based on the argumentative
segmentation, we propose a novel task of classifying argumentative segments of
legal case decisions. GPT-3.5 is used to generate summaries based on
argumentative segments. In terms of automatic evaluation metrics, our method
generates higher quality argumentative summaries while leaving out less
relevant context as compared to GPT-4 and non-GPT models.
</summary>
    <author>
      <name>Huihui Xu</name>
    </author>
    <author>
      <name>Kevin Ashley</name>
    </author>
    <link href="http://arxiv.org/abs/2307.05081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.05081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.10303v1</id>
    <updated>2023-07-18T18:51:06Z</updated>
    <published>2023-07-18T18:51:06Z</published>
    <title>Analyzing sports commentary in order to automatically recognize events
  and extract insights</title>
    <summary>  In this paper, we carefully investigate how we can use multiple different
Natural Language Processing techniques and methods in order to automatically
recognize the main actions in sports events. We aim to extract insights by
analyzing live sport commentaries from different sources and by classifying
these major actions into different categories. We also study if sentiment
analysis could help detect these main actions.
</summary>
    <author>
      <name>Yanis Miraoui</name>
    </author>
    <link href="http://arxiv.org/abs/2307.10303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.10303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11766v1</id>
    <updated>2023-07-15T14:45:33Z</updated>
    <published>2023-07-15T14:45:33Z</published>
    <title>Three-way Decisions with Evaluative Linguistic Expressions</title>
    <summary>  We propose a linguistic interpretation of three-way decisions, where the
regions of acceptance, rejection, and non-commitment are constructed by using
the so-called evaluative linguistic expressions, which are expressions of
natural language such as small, medium, very short, quite roughly strong,
extremely good, etc. Our results highlight new connections between two
different research areas: three-way decisions and the theory of evaluative
linguistic expressions.
</summary>
    <author>
      <name>Stefania Boffa</name>
    </author>
    <author>
      <name>Davide Ciucci</name>
    </author>
    <link href="http://arxiv.org/abs/2307.11766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11779v1</id>
    <updated>2023-07-20T02:12:00Z</updated>
    <published>2023-07-20T02:12:00Z</published>
    <title>The Extractive-Abstractive Axis: Measuring Content "Borrowing" in
  Generative Language Models</title>
    <summary>  Generative language models produce highly abstractive outputs by design, in
contrast to extractive responses in search engines. Given this characteristic
of LLMs and the resulting implications for content Licensing &amp; Attribution, we
propose the the so-called Extractive-Abstractive axis for benchmarking
generative models and highlight the need for developing corresponding metrics,
datasets and annotation guidelines. We limit our discussion to the text
modality.
</summary>
    <author>
      <name>Nedelina Teneva</name>
    </author>
    <link href="http://arxiv.org/abs/2307.11779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15002v5</id>
    <updated>2023-08-08T17:39:57Z</updated>
    <published>2023-07-27T16:57:32Z</published>
    <title>Gzip versus bag-of-words for text classification</title>
    <summary>  The effectiveness of compression in text classification ('gzip') has recently
garnered lots of attention. In this note we show that `bag-of-words' approaches
can achieve similar or better results, and are more efficient.
</summary>
    <author>
      <name>Juri Opitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">improved writing, extended with more results</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.15002v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15002v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02323v1</id>
    <updated>2023-08-04T13:40:54Z</updated>
    <published>2023-08-04T13:40:54Z</published>
    <title>Dataflow Dialogue Generation</title>
    <summary>  We demonstrate task-oriented dialogue generation within the dataflow dialogue
paradigm. We show an example of agenda driven dialogue generation for the
MultiWOZ domain, and an example of generation without an agenda for the
SMCalFlow domain, where we show an improvement in the accuracy of the
translation of user requests to dataflow expressions when the generated
dialogues are used to augment the translation training dataset.
</summary>
    <author>
      <name>Joram Meron</name>
    </author>
    <author>
      <name>Victor Guimarães</name>
    </author>
    <link href="http://arxiv.org/abs/2308.02323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.06764v1</id>
    <updated>2023-10-10T16:40:00Z</updated>
    <published>2023-10-10T16:40:00Z</published>
    <title>OmniLingo: Listening- and speaking-based language learning</title>
    <summary>  In this demo paper we present OmniLingo, an architecture for distributing
data for listening- and speaking-based language learning applications and a
demonstration client built using the architecture. The architecture is based on
the Interplanetary Filesystem (IPFS) and puts at the forefront user sovereignty
over data.
</summary>
    <author>
      <name>Francis M. Tyers</name>
    </author>
    <author>
      <name>Nicholas Howell</name>
    </author>
    <link href="http://arxiv.org/abs/2310.06764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.06764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.10956v1</id>
    <updated>2023-10-17T03:05:42Z</updated>
    <published>2023-10-17T03:05:42Z</published>
    <title>Computing the optimal keyboard through a geometric analysis of the
  English language</title>
    <summary>  In the context of a group project for the course COMSW4995 002 - Geometric
Data Analysis, we bring our attention to the design of fast-typing keyboards.
Leveraging some geometric tools in an optimization framework allowed us to
propose novel keyboard layouts that offer a faster typing.
</summary>
    <author>
      <name>Jules Deschamps</name>
    </author>
    <author>
      <name>Quentin Hubert</name>
    </author>
    <author>
      <name>Lucas Ryckelynck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.10956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.10956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.11655v1</id>
    <updated>2023-10-18T01:56:16Z</updated>
    <published>2023-10-18T01:56:16Z</published>
    <title>Field-testing items using artificial intelligence: Natural language
  processing with transformers</title>
    <summary>  Five thousand variations of the RoBERTa model, an artificially intelligent
"transformer" that can understand text language, completed an English literacy
exam with 29 multiple-choice questions. Data were used to calculate the
psychometric properties of the items, which showed some degree of agreement to
those obtained from human examinee data.
</summary>
    <author>
      <name>Hotaka Maeda</name>
    </author>
    <link href="http://arxiv.org/abs/2310.11655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.11655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13017v1</id>
    <updated>2023-10-18T16:41:47Z</updated>
    <published>2023-10-18T16:41:47Z</published>
    <title>Position Interpolation Improves ALiBi Extrapolation</title>
    <summary>  Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.
</summary>
    <author>
      <name>Faisal Al-Khateeb</name>
    </author>
    <author>
      <name>Nolan Dey</name>
    </author>
    <author>
      <name>Daria Soboleva</name>
    </author>
    <author>
      <name>Joel Hestness</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages content, 1 page references, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.13017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19539v1</id>
    <updated>2023-10-30T13:46:24Z</updated>
    <published>2023-10-30T13:46:24Z</published>
    <title>A Novel Representation to Improve Team Problem Solving in Real-Time</title>
    <summary>  This paper proposes a novel representation to support computing metrics that
help understanding and improving in real-time a team's behavior during problem
solving in real-life. Even though teams are important in modern activities,
there is little computing aid to improve their activity. The representation
captures the different mental images developed, enhanced, and utilized during
solving. A case study illustrates the representation.
</summary>
    <author>
      <name>Alex Doboli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2308.06273</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.19539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.02985v2</id>
    <updated>2023-11-08T15:00:19Z</updated>
    <published>2023-11-06T09:42:44Z</published>
    <title>Towards a Transformer-Based Reverse Dictionary Model for Quality
  Estimation of Definitions</title>
    <summary>  In the last years, several variants of transformers have emerged. In this
paper, we compare different transformer-based models for solving the reverse
dictionary task and explore their use in the context of a serious game called
The Dictionary Game.
</summary>
    <author>
      <name>Julien Guité-Vinet</name>
    </author>
    <author>
      <name>Alexandre Blondin Massé</name>
    </author>
    <author>
      <name>Fatiha Sadat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/aaai.v38i21.30449</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/aaai.v38i21.30449" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AAAI Conference on Artificial Intelligence,
  38(21), 23508-23509,2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.02985v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.02985v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.04139v1</id>
    <updated>2023-11-07T17:12:39Z</updated>
    <published>2023-11-07T17:12:39Z</published>
    <title>Modelling Sentiment Analysis: LLMs and data augmentation techniques</title>
    <summary>  This paper provides different approaches for a binary sentiment
classification on a small training dataset. LLMs that provided state-of-the-art
results in sentiment analysis and similar domains are being used, such as BERT,
RoBERTa and XLNet.
</summary>
    <author>
      <name>Guillem Senabre Prades</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. For more information check the github link in the
  conclusion. Enjoy!</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.04139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.04139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.07217v1</id>
    <updated>2023-11-13T10:24:51Z</updated>
    <published>2023-11-13T10:24:51Z</published>
    <title>Troubles and Failures in Interactional Language. Towards a
  Linguistically Informed Taxonomy</title>
    <summary>  The goal of this talk is to introduce a systematic research agenda which aims
to understand the nature of interaction between humans and artificial
conversational agents (CA) (henceforth humanmachine interaction, HMI).
Specifically, we shall take an explicit linguistic perspective focusing on
linguistically defined variables that are known to influence the flow of
conversations among humans (henceforth human-human interaction, HHI).
</summary>
    <author>
      <name>Martina Wiltschko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, Part of WTF 23 workshop proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.07217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.07217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.13784v1</id>
    <updated>2023-11-23T03:03:54Z</updated>
    <published>2023-11-23T03:03:54Z</published>
    <title>DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for
  Korean NLP</title>
    <summary>  This paper presents the DaG LLM (David and Goliath Large Language Model), a
language model specialized for Korean and fine-tuned through Instruction Tuning
across 41 tasks within 13 distinct categories.
</summary>
    <author>
      <name>Dongjun Jang</name>
    </author>
    <author>
      <name>Sangah Lee</name>
    </author>
    <author>
      <name>Sungjoo Byun</name>
    </author>
    <author>
      <name>Jinwoong Kim</name>
    </author>
    <author>
      <name>Jean Seo</name>
    </author>
    <author>
      <name>Minseok Kim</name>
    </author>
    <author>
      <name>Soyeon Kim</name>
    </author>
    <author>
      <name>Chaeyoung Oh</name>
    </author>
    <author>
      <name>Jaeyoon Kim</name>
    </author>
    <author>
      <name>Hyemi Jo</name>
    </author>
    <author>
      <name>Hyopil Shin</name>
    </author>
    <link href="http://arxiv.org/abs/2311.13784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.13784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03342v1</id>
    <updated>2023-12-06T08:33:51Z</updated>
    <published>2023-12-06T08:33:51Z</published>
    <title>Topic and genre in dialogue</title>
    <summary>  In this paper we argue that topic plays a fundamental role in conversations,
and that the concept is needed in addition to that of genre to define
interactions. In particular, the concepts of genre and topic need to be
separated and orthogonally defined. This would enable modular, reliable and
controllable flexible-domain dialogue systems.
</summary>
    <author>
      <name>Amandine Decker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA, GU, UL</arxiv:affiliation>
    </author>
    <author>
      <name>Ellen Breitholtz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GU</arxiv:affiliation>
    </author>
    <author>
      <name>Christine Howes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GU</arxiv:affiliation>
    </author>
    <author>
      <name>Staffan Larsson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GU</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SEMDIAL 2023, Aug 2023, Maribor, Slovenia. pp.143-145</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2312.03342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.03515v1</id>
    <updated>2024-01-07T15:13:24Z</updated>
    <published>2024-01-07T15:13:24Z</published>
    <title>RoBERTurk: Adjusting RoBERTa for Turkish</title>
    <summary>  We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model
outperforms BERTurk family models on the BOUN dataset for the POS task while
resulting in underperformance on the IMST dataset for the same task and
achieving competitive scores on the Turkish split of the XTREME dataset for the
NER task - all while being pretrained on smaller data than its competitors. We
release our pretrained model and tokenizer.
</summary>
    <author>
      <name>Nuri Tas</name>
    </author>
    <link href="http://arxiv.org/abs/2401.03515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.03515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.09041v1</id>
    <updated>2024-01-17T08:16:05Z</updated>
    <published>2024-01-17T08:16:05Z</published>
    <title>Textual Summarisation of Large Sets: Towards a General Approach</title>
    <summary>  We are developing techniques to generate summary descriptions of sets of
objects. In this paper, we present and evaluate a rule-based NLG technique for
summarising sets of bibliographical references in academic papers. This extends
our previous work on summarising sets of consumer products and shows how our
model generalises across these two very different domains.
</summary>
    <author>
      <name>Kittipitch Kuptavanich</name>
    </author>
    <author>
      <name>Ehud Reiter</name>
    </author>
    <author>
      <name>Kees Van Deemter</name>
    </author>
    <author>
      <name>Advaith Siddharthan</name>
    </author>
    <link href="http://arxiv.org/abs/2401.09041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.09041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.14569v1</id>
    <updated>2024-01-25T23:54:34Z</updated>
    <published>2024-01-25T23:54:34Z</published>
    <title>Detecting Structured Language Alternations in Historical Documents by
  Combining Language Identification with Fourier Analysis</title>
    <summary>  In this study, we present a generalizable workflow to identify documents in a
historic language with a nonstandard language and script combination,
Armeno-Turkish. We introduce the task of detecting distinct patterns of
multilinguality based on the frequency of structured language alternations
within a document.
</summary>
    <author>
      <name>Hale Sirin</name>
    </author>
    <author>
      <name>Sabrina Li</name>
    </author>
    <author>
      <name>Tom Lippincott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to LaTeCH@EACL2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.14569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.14569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.00075v1</id>
    <updated>2024-01-30T22:07:12Z</updated>
    <published>2024-01-30T22:07:12Z</published>
    <title>D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models</title>
    <summary>  D-Nikud, a novel approach to Hebrew diacritization that integrates the
strengths of LSTM networks and BERT-based (transformer) pre-trained model.
Inspired by the methodologies employed in Nakdimon, we integrate it with the
TavBERT pre-trained model, our system incorporates advanced architectural
choices and diverse training data. Our experiments showcase state-of-the-art
results on several benchmark datasets, with a particular emphasis on modern
texts and more specified diacritization like gender.
</summary>
    <author>
      <name>Adi Rosenthal</name>
    </author>
    <author>
      <name>Nadav Shaked</name>
    </author>
    <link href="http://arxiv.org/abs/2402.00075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.00075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.01065v1</id>
    <updated>2024-02-01T23:46:05Z</updated>
    <published>2024-02-01T23:46:05Z</published>
    <title>Evaluation Methodology for Large Language Models for Multilingual
  Document Question and Answer</title>
    <summary>  With the widespread adoption of Large Language Models (LLMs), in this paper
we investigate the multilingual capability of these models. Our preliminary
results show that, translating the native language context, question and answer
into a high resource language produced the best results.
</summary>
    <author>
      <name>Adar Kahana</name>
    </author>
    <author>
      <name>Jaya Susan Mathew</name>
    </author>
    <author>
      <name>Said Bleik</name>
    </author>
    <author>
      <name>Jeremy Reynolds</name>
    </author>
    <author>
      <name>Oren Elisha</name>
    </author>
    <link href="http://arxiv.org/abs/2402.01065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.01065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.08392v1</id>
    <updated>2024-02-13T11:37:30Z</updated>
    <published>2024-02-13T11:37:30Z</published>
    <title>Large Language Models as Minecraft Agents</title>
    <summary>  In this work we examine the use of Large Language Models (LLMs) in the
challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs
in the builder and architect settings, introduce clarification questions and
examining the challenges and opportunities for improvement. In addition, we
present a platform for online interaction with the agents and an evaluation
against previous works.
</summary>
    <author>
      <name>Chris Madge</name>
    </author>
    <author>
      <name>Massimo Poesio</name>
    </author>
    <link href="http://arxiv.org/abs/2402.08392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.08392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.10302v1</id>
    <updated>2024-02-15T20:08:07Z</updated>
    <published>2024-02-15T20:08:07Z</published>
    <title>How to Discern Important Urgent News?</title>
    <summary>  We found that a simple property of clusters in a clustered dataset of news
correlate strongly with importance and urgency of news (IUN) as assessed by
LLM. We verified our finding across different news datasets, dataset sizes,
clustering algorithms and embeddings. The found correlation should allow using
clustering (as an alternative to LLM) for identifying the most important urgent
news, or for filtering out unimportant articles.
</summary>
    <author>
      <name>Oleg Vasilyev</name>
    </author>
    <author>
      <name>John Bohannon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures, 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.10302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.10302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.15931v1</id>
    <updated>2024-02-24T23:23:06Z</updated>
    <published>2024-02-24T23:23:06Z</published>
    <title>Frustratingly Simple Prompting-based Text Denoising</title>
    <summary>  This paper introduces a novel perspective on the automated essay scoring
(AES) task, challenging the conventional view of the ASAP dataset as a static
entity. Employing simple text denoising techniques using prompting, we explore
the dynamic potential within the dataset. While acknowledging the previous
emphasis on building regression systems, our paper underscores how making minor
changes to a dataset through text denoising can enhance the final results.
</summary>
    <author>
      <name>Jungyeul Park</name>
    </author>
    <author>
      <name>Mengyang Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a Tiny Paper at ICLR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.15931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.15931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.16406v2</id>
    <updated>2025-03-01T14:22:20Z</updated>
    <published>2024-02-26T08:59:05Z</published>
    <title>From RAGs to riches: Utilizing large language models to write documents
  for clinical trials</title>
    <summary>  This manuscript has now been published: - Link to article on journal website:
https://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:
https://pubmed.ncbi.nlm.nih.gov/40013826/
</summary>
    <author>
      <name>Nigel Markey</name>
    </author>
    <author>
      <name>Ilyass El-Mansouri</name>
    </author>
    <author>
      <name>Gaetan Rensonnet</name>
    </author>
    <author>
      <name>Casper van Langen</name>
    </author>
    <author>
      <name>Christoph Meier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/17407745251320806</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/17407745251320806" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Clinical Trials: Journal of the Society for Clinical Trials (2025)
  -- online ahead of print</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.16406v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.16406v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.04771v1</id>
    <updated>2024-02-26T05:34:16Z</updated>
    <published>2024-02-26T05:34:16Z</published>
    <title>QASE Enhanced PLMs: Improved Control in Text Generation for MRC</title>
    <summary>  To address the challenges of out-of-control generation in generative models
for machine reading comprehension (MRC), we introduce the Question-Attended
Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained
generative language models (PLMs), QASE enables these PLMs to match SOTA
extractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without
significant increases in computational costs.
</summary>
    <author>
      <name>Lin Ai</name>
    </author>
    <author>
      <name>Zheng Hui</name>
    </author>
    <author>
      <name>Zizhou Liu</name>
    </author>
    <author>
      <name>Julia Hirschberg</name>
    </author>
    <link href="http://arxiv.org/abs/2403.04771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.04771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.09719v1</id>
    <updated>2024-03-12T08:40:44Z</updated>
    <published>2024-03-12T08:40:44Z</published>
    <title>Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew
  Language</title>
    <summary>  In this paper, we introduce summarization MevakerSumm and conclusion
extraction MevakerConc datasets for the Hebrew language based on the State
Comptroller and Ombudsman of Israel reports, along with two auxiliary datasets.
We accompany these datasets with models for conclusion extraction (HeConE,
HeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and
model checkpoints used in this work are publicly available.
</summary>
    <author>
      <name>Vitaly Shalumov</name>
    </author>
    <author>
      <name>Harel Haskey</name>
    </author>
    <author>
      <name>Yuval Solaz</name>
    </author>
    <link href="http://arxiv.org/abs/2403.09719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.09719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15440v1</id>
    <updated>2024-03-16T23:10:42Z</updated>
    <published>2024-03-16T23:10:42Z</published>
    <title>Linguistics from a topological viewpoint</title>
    <summary>  Typological databases in linguistics are usually categorical-valued. As a
result, it is difficult to have a clear visualization of the data. In this
paper, we describe a workflow to analyze the topological shapes of South
American languages by applying multiple correspondence analysis technique and
topological data analysis methods.
</summary>
    <author>
      <name>Rui Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.15440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91F20, 62R40, 55N31, 55U10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.19511v1</id>
    <updated>2024-03-28T15:44:18Z</updated>
    <published>2024-03-28T15:44:18Z</published>
    <title>Improving Clinical NLP Performance through Language Model-Generated
  Synthetic Clinical Data</title>
    <summary>  Generative models have been showing potential for producing data in mass.
This study explores the enhancement of clinical natural language processing
performance by utilizing synthetic data generated from advanced language
models. Promising results show feasible applications in such a high-stakes
domain.
</summary>
    <author>
      <name>Shan Chen</name>
    </author>
    <author>
      <name>Jack Gallifant</name>
    </author>
    <author>
      <name>Marco Guevara</name>
    </author>
    <author>
      <name>Yanjun Gao</name>
    </author>
    <author>
      <name>Majid Afshar</name>
    </author>
    <author>
      <name>Timothy Miller</name>
    </author>
    <author>
      <name>Dmitriy Dligach</name>
    </author>
    <author>
      <name>Danielle S. Bitterman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to review</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.19511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.19511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.20134v1</id>
    <updated>2024-03-29T11:54:13Z</updated>
    <published>2024-03-29T11:54:13Z</published>
    <title>User Modeling Challenges in Interactive AI Assistant Systems</title>
    <summary>  Interactive Artificial Intelligent(AI) assistant systems are designed to
offer timely guidance to help human users to complete a variety tasks. One of
the remaining challenges is to understand user's mental states during the task
for more personalized guidance. In this work, we analyze users' mental states
during task executions and investigate the capabilities and challenges for
large language models to interpret user profiles for more personalized user
guidance.
</summary>
    <author>
      <name>Megan Su</name>
    </author>
    <author>
      <name>Yuwei Bao</name>
    </author>
    <link href="http://arxiv.org/abs/2403.20134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.20134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00458v2</id>
    <updated>2024-08-30T15:59:46Z</updated>
    <published>2024-03-30T19:45:04Z</published>
    <title>Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for
  Embedding Model Selection</title>
    <summary>  This position paper proposes a systematic approach towards developing a
framework to help select the most effective embedding models for natural
language processing (NLP) tasks, addressing the challenge posed by the
proliferation of both proprietary and open-source encoder models.
</summary>
    <author>
      <name>Vivek Khetan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">It was an initial idea - we plan to work on a detailed version</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.00458v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00458v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.01234v1</id>
    <updated>2024-04-01T16:51:50Z</updated>
    <published>2024-04-01T16:51:50Z</published>
    <title>GFLean: An Autoformalisation Framework for Lean via GF</title>
    <summary>  We present an autoformalisation framework for the Lean theorem prover, called
GFLean. GFLean uses a high-level grammar writing tool called Grammatical
Framework (GF) for parsing and linearisation. GFLean is implemented in Haskell.
We explain the functionalities of GFLean, its inner working and discuss its
limitations. We also discuss how we can use neural network based translation
programs and rule based translation programs together complimenting each other
to build robust autoformalisation frameworks.
</summary>
    <author>
      <name>Shashank Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 Pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.01234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.01234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.10696v1</id>
    <updated>2024-04-16T16:15:19Z</updated>
    <published>2024-04-16T16:15:19Z</published>
    <title>Integrating knowledge bases to improve coreference and bridging
  resolution for the chemical domain</title>
    <summary>  Resolving coreference and bridging relations in chemical patents is important
for better understanding the precise chemical process, where chemical domain
knowledge is very critical. We proposed an approach incorporating external
knowledge into a multi-task learning model for both coreference and bridging
resolution in the chemical domain. The results show that integrating external
knowledge can benefit both chemical coreference and bridging resolution.
</summary>
    <author>
      <name>Pengcheng Lu</name>
    </author>
    <author>
      <name>Massimo Poesio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">working in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.10696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.10696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13069v1</id>
    <updated>2024-04-14T14:54:01Z</updated>
    <published>2024-04-14T14:54:01Z</published>
    <title>Subtle Signs of Scribal Intent in the Voynich Manuscript</title>
    <summary>  This study explores the cryptic Voynich Manuscript, by looking for subtle
signs of scribal intent hidden in overlooked features of the "Voynichese"
script. The findings indicate that distributions of tokens within paragraphs
vary significantly based on positions defined not only by elements intrinsic to
the script such as paragraph and line boundaries but also by extrinsic
elements, namely the hand-drawn illustrations of plants.
</summary>
    <author>
      <name>Andrew Steckley</name>
    </author>
    <author>
      <name>Noah Steckley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Histocrypt 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.13069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13547v1</id>
    <updated>2024-04-21T06:03:43Z</updated>
    <published>2024-04-21T06:03:43Z</published>
    <title>E-QGen: Educational Lecture Abstract-based Question Generation System</title>
    <summary>  To optimize the preparation process for educators in academic lectures and
associated question-and-answer sessions, this paper presents E-QGen, a lecture
abstract-based question generation system. Given a lecture abstract, E-QGen
generates potential student inquiries. The questions suggested by our system
are expected to not only facilitate teachers in preparing answers in advance
but also enable them to supply additional resources when necessary.
</summary>
    <author>
      <name>Mao-Siang Chen</name>
    </author>
    <author>
      <name>An-Zi Yen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2024 Demo Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.13547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.01942v1</id>
    <updated>2024-05-03T09:10:40Z</updated>
    <published>2024-05-03T09:10:40Z</published>
    <title>CRCL at SemEval-2024 Task 2: Simple prompt optimizations</title>
    <summary>  We present a baseline for the SemEval 2024 task 2 challenge, whose objective
is to ascertain the inference relationship between pairs of clinical trial
report sections and statements. We apply prompt optimization techniques with
LLM Instruct models provided as a Language Model-as-a-Service (LMaaS). We
observed, in line with recent findings, that synthetic CoT prompts
significantly enhance manually crafted ones.
</summary>
    <author>
      <name>Clément Brutti-Mairesse</name>
    </author>
    <author>
      <name>Loïc Verlingue</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SemEval-2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2405.01942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.01942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.03170v1</id>
    <updated>2024-05-06T05:36:29Z</updated>
    <published>2024-05-06T05:36:29Z</published>
    <title>Oracle-Checker Scheme for Evaluating a Generative Large Language Model</title>
    <summary>  This work presents a novel approach called oracle-checker scheme for
evaluating the answer given by a generative large language model (LLM). Two
types of checkers are presented. The first type of checker follows the idea of
property testing. The second type of checker follows the idea of program
checking. Their applications are demonstrated in two separate contexts, entity
extraction and paraphrase decision, respectively.
</summary>
    <author>
      <name>Yueling Jenny Zeng</name>
    </author>
    <author>
      <name>Li-C. Wang</name>
    </author>
    <author>
      <name>Thomas Ibbetson</name>
    </author>
    <link href="http://arxiv.org/abs/2405.03170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.03170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.07597v1</id>
    <updated>2024-05-13T09:58:59Z</updated>
    <published>2024-05-13T09:58:59Z</published>
    <title>Using Model-Theoretic Approaches to Uncover Linguistic Organization</title>
    <summary>  In this paper, we consider pluractional markers in Kaqchikel, Karuk, and
Yurok. Like Balinese, each of these languages marks one type of pluractionality
via reduplication, and a different type of pluractionality via
non-reduplicative affixation. This paper serves as a proof-of-concept for
applying model-theoretic approaches to language as a lens that can help us to
recognize linguistic organization that is not apparent on the surface.
</summary>
    <author>
      <name>Olivia Griffin</name>
    </author>
    <author>
      <name>Jerry Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2405.07597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.07597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.17116v1</id>
    <updated>2024-05-27T12:33:47Z</updated>
    <published>2024-05-27T12:33:47Z</published>
    <title>Mixtures of Unsupervised Lexicon Classification</title>
    <summary>  This paper presents a mixture version of the method-of-moment unsupervised
lexicon classification by an incorporation of a Dirichlet process.
</summary>
    <author>
      <name>Peratham Wiriyathammabhum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A draft on lexicon classification unsupervised learning. It shows
  that aggregating lexicon scores is equivalent to a finite mixture of
  multinomial Naive Bayes models. A very preliminary work of a few days
  man-hours, like a weekly report/note, but might be useful</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.17116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.17116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04855v2</id>
    <updated>2024-10-28T12:07:49Z</updated>
    <published>2024-06-07T11:38:12Z</published>
    <title>The Russian Legislative Corpus</title>
    <summary>  We present the comprehensive Russian primary and secondary legislation corpus
covering 1991 to 2023. The corpus collects all 281,413 texts (176,523,268
tokens) of non-secret federal regulations and acts, along with their metadata.
The corpus has two versions the original text with minimal preprocessing and a
version prepared for linguistic analysis with morphosyntactic markup.
</summary>
    <author>
      <name>Denis Saveliev</name>
    </author>
    <author>
      <name>Ruslan Kuchakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04855v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04855v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.05274v1</id>
    <updated>2024-06-07T21:59:55Z</updated>
    <published>2024-06-07T21:59:55Z</published>
    <title>Behavior Structformer: Learning Players Representations with Structured
  Tokenization</title>
    <summary>  In this paper, we introduce the Behavior Structformer, a method for modeling
user behavior using structured tokenization within a Transformer-based
architecture. By converting tracking events into dense tokens, this approach
enhances model training efficiency and effectiveness. We demonstrate its
superior performance through ablation studies and benchmarking against
traditional tabular and semi-structured baselines. The results indicate that
structured tokenization with sequential processing significantly improves
behavior modeling.
</summary>
    <author>
      <name>Oleg Smirnov</name>
    </author>
    <author>
      <name>Labinot Polisi</name>
    </author>
    <link href="http://arxiv.org/abs/2406.05274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.05274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09938v1</id>
    <updated>2024-06-14T11:34:36Z</updated>
    <published>2024-06-14T11:34:36Z</published>
    <title>Experiments in News Bias Detection with Pre-Trained Neural Transformers</title>
    <summary>  The World Wide Web provides unrivalled access to information globally,
including factual news reporting and commentary. However, state actors and
commercial players increasingly spread biased (distorted) or fake (non-factual)
information to promote their agendas. We compare several large, pre-trained
language models on the task of sentence-level news bias detection and sub-type
classification, providing quantitative and qualitative results.
</summary>
    <author>
      <name>Tim Menzner</name>
    </author>
    <author>
      <name>Jochen L. Leidner</name>
    </author>
    <link href="http://arxiv.org/abs/2406.09938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11338v1</id>
    <updated>2024-06-17T08:55:48Z</updated>
    <published>2024-06-17T08:55:48Z</published>
    <title>Fine-grained Controllable Text Generation through In-context Learning
  with Feedback</title>
    <summary>  We present a method for rewriting an input sentence to match specific values
of nontrivial linguistic features, such as dependency depth. In contrast to
earlier work, our method uses in-context learning rather than finetuning,
making it applicable in use cases where data is sparse. We show that our model
performs accurate rewrites and matches the state of the art on rewriting
sentences to a specified school grade level.
</summary>
    <author>
      <name>Sarubi Thillainathan</name>
    </author>
    <author>
      <name>Alexander Koller</name>
    </author>
    <link href="http://arxiv.org/abs/2406.11338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.12213v4</id>
    <updated>2024-10-16T00:55:50Z</updated>
    <published>2024-06-18T02:25:33Z</published>
    <title>AI-Oracle Machines for Intelligent Computing</title>
    <summary>  We introduce the concept of AI-oracle machines for intelligent computing and
outline several applications to demonstrate their potential. Following this, we
advocate for the development of a comprehensive platform to streamline the
implementation of AI-oracle machines.
</summary>
    <author>
      <name>Jie Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.12213v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.12213v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.4.1; I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.16925v1</id>
    <updated>2024-06-12T06:43:59Z</updated>
    <published>2024-06-12T06:43:59Z</published>
    <title>Analyzing Multi-Head Attention on Trojan BERT Models</title>
    <summary>  This project investigates the behavior of multi-head attention in Transformer
models, specifically focusing on the differences between benign and trojan
models in the context of sentiment analysis. Trojan attacks cause models to
perform normally on clean inputs but exhibit misclassifications when presented
with inputs containing predefined triggers. We characterize attention head
functions in trojan and benign models, identifying specific 'trojan' heads and
analyzing their behavior.
</summary>
    <author>
      <name>Jingwei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.16925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.16925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.19116v1</id>
    <updated>2024-06-27T11:53:15Z</updated>
    <published>2024-06-27T11:53:15Z</published>
    <title>CHEW: A Dataset of CHanging Events in Wikipedia</title>
    <summary>  We introduce CHEW, a novel dataset of changing events in Wikipedia expressed
in naturally occurring text. We use CHEW for probing LLMs for their timeline
understanding of Wikipedia entities and events in generative and classification
experiments. Our results suggest that LLMs, despite having temporal information
available, struggle to construct accurate timelines. We further show the
usefulness of CHEW-derived embeddings for identifying meaning shift.
</summary>
    <author>
      <name>Hsuvas Borkakoty</name>
    </author>
    <author>
      <name>Luis Espinosa-Anke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.19116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.19116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.02917v1</id>
    <updated>2024-07-03T08:49:18Z</updated>
    <published>2024-07-03T08:49:18Z</published>
    <title>Towards Negotiative Dialogue for the Talkamatic Dialogue Manager</title>
    <summary>  The paper describes a number of dialogue phenomena associated with
negotiative dialogue, as implemented in a development version of the Talkamatic
Dialogue Manager (TDM). This implementation is an initial step towards full
coverage of general features of negotiative dialogue in TDM.
</summary>
    <author>
      <name>Staffan Larsson</name>
    </author>
    <author>
      <name>Alexander Berman</name>
    </author>
    <author>
      <name>David Hjelm</name>
    </author>
    <link href="http://arxiv.org/abs/2407.02917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.02917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18730v1</id>
    <updated>2024-07-26T13:30:24Z</updated>
    <published>2024-07-26T13:30:24Z</published>
    <title>Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of
  Shakespeare and Milton</title>
    <summary>  In this work we present a corpus of poems by William Shakespeare and John
Milton that have been enriched with readings from the public domain. We have
aligned all the lines with their respective audio segments, at the line, word,
syllable and phone level, and we have included their scansion. We make a basic
visualization platform for these poems and we conclude by conjecturing possible
future directions.
</summary>
    <author>
      <name>Manex Agirrezabal</name>
    </author>
    <link href="http://arxiv.org/abs/2407.18730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.19400v1</id>
    <updated>2024-07-28T05:06:58Z</updated>
    <published>2024-07-28T05:06:58Z</published>
    <title>Word Segmentation for Asian Languages: Chinese, Korean, and Japanese</title>
    <summary>  We provide a detailed overview of various approaches to word segmentation of
Asian Languages, specifically Chinese, Korean, and Japanese languages. For each
language, approaches to deal with word segmentation differs. We also include
our analysis about certain advantages and disadvantages to each method. In
addition, there is room for future work in this field.
</summary>
    <author>
      <name>Matthew Rho</name>
    </author>
    <author>
      <name>Yexin Tian</name>
    </author>
    <author>
      <name>Qin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2407.19400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.19400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.20581v1</id>
    <updated>2024-07-30T06:29:01Z</updated>
    <published>2024-07-30T06:29:01Z</published>
    <title>Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings</title>
    <summary>  We present Knesset-DictaBERT, a large Hebrew language model fine-tuned on the
Knesset Corpus, which comprises Israeli parliamentary proceedings. The model is
based on the DictaBERT architecture and demonstrates significant improvements
in understanding parliamentary language according to the MLM task. We provide a
detailed evaluation of the model's performance, showing improvements in
perplexity and accuracy over the baseline DictaBERT model.
</summary>
    <author>
      <name>Gili Goldin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Haifa, Israel</arxiv:affiliation>
    </author>
    <author>
      <name>Shuly Wintner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Haifa, Israel</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.20581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.20581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.11857v1</id>
    <updated>2024-08-15T20:17:33Z</updated>
    <published>2024-08-15T20:17:33Z</published>
    <title>Hermes 3 Technical Report</title>
    <summary>  Instruct (or "chat") tuned models have become the primary way in which most
people interact with large language models. As opposed to "base" or
"foundation" models, instruct-tuned models are optimized to respond to
imperative statements. We present Hermes 3, a neutrally-aligned generalist
instruct and tool use model with strong reasoning and creative abilities. Its
largest version, Hermes 3 405B, achieves state of the art performance among
open weight models on several public benchmarks.
</summary>
    <author>
      <name>Ryan Teknium</name>
    </author>
    <author>
      <name>Jeffrey Quesnelle</name>
    </author>
    <author>
      <name>Chen Guang</name>
    </author>
    <link href="http://arxiv.org/abs/2408.11857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.02393v1</id>
    <updated>2024-09-04T02:41:44Z</updated>
    <published>2024-09-04T02:41:44Z</published>
    <title>Determination of language families using deep learning</title>
    <summary>  We use a c-GAN (convolutional generative adversarial) neural network to
analyze transliterated text fragments of extant, dead comprehensible, and one
dead non-deciphered (Cypro-Minoan) language to establish linguistic affinities.
The paper is agnostic with respect to translation and/or deciphering. However,
there is hope that the proposed approach can be useful for decipherment with
more sophisticated neural network techniques.
</summary>
    <author>
      <name>Peter B. Lerner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First draft. Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.02393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.02393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03707v1</id>
    <updated>2024-09-05T17:13:38Z</updated>
    <published>2024-09-05T17:13:38Z</published>
    <title>A Different Level Text Protection Mechanism With Differential Privacy</title>
    <summary>  The article introduces a method for extracting words of different degrees of
importance based on the BERT pre-training model and proves the effectiveness of
this method. The article also discusses the impact of maintaining the same
perturbation results for words of different importance on the overall text
utility. This method can be applied to long text protection.
</summary>
    <author>
      <name>Qingwen Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19813v1</id>
    <updated>2024-09-29T22:23:52Z</updated>
    <published>2024-09-29T22:23:52Z</published>
    <title>Transforming Hidden States into Binary Semantic Features</title>
    <summary>  Large language models follow a lineage of many NLP applications that were
directly inspired by distributional semantics, but do not seem to be closely
related to it anymore. In this paper, we propose to employ the distributional
theory of meaning once again. Using Independent Component Analysis to overcome
some of its challenging aspects, we show that large language models represent
semantic features in their hidden states.
</summary>
    <author>
      <name>Tomáš Musil</name>
    </author>
    <author>
      <name>David Mareček</name>
    </author>
    <link href="http://arxiv.org/abs/2409.19813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.20466v1</id>
    <updated>2024-09-30T16:26:19Z</updated>
    <published>2024-09-30T16:26:19Z</published>
    <title>Language Resources in Spanish for Automatic Text Simplification across
  Domains</title>
    <summary>  This work describes the language resources and models developed for automatic
simplification of Spanish texts in three domains: Finance, Medicine and History
studies. We created several corpora in each domain, annotation and
simplification guidelines, a lexicon of technical and simplified medical terms,
datasets used in shared tasks for the financial domain, and two simplification
tools. The methodology, resources and companion publications are shared
publicly on the web-site: https://clara-nlp.uned.es/.
</summary>
    <author>
      <name>Antonio Moreno-Sandoval</name>
    </author>
    <author>
      <name>Leonardo Campillos-Llanos</name>
    </author>
    <author>
      <name>Ana García-Serrano</name>
    </author>
    <link href="http://arxiv.org/abs/2409.20466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.20466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00522v1</id>
    <updated>2024-10-01T09:06:52Z</updated>
    <published>2024-10-01T09:06:52Z</published>
    <title>Annotation Guidelines for Corpus Novelties: Part 2 -- Alias Resolution
  Version 1.0</title>
    <summary>  The Novelties corpus is a collection of novels (and parts of novels)
annotated for Alias Resolution, among other tasks. This document describes the
guidelines applied during the annotation process. It contains the instructions
used by the annotators, as well as a number of examples retrieved from the
annotated novels, and illustrating how canonical names should be defined, and
which names should be considered as referring to the same entity.
</summary>
    <author>
      <name>Arthur Amalvy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent Labatut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2410.00522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.12617v1</id>
    <updated>2024-10-16T14:34:30Z</updated>
    <published>2024-10-16T14:34:30Z</published>
    <title>Parsing Akkadian Verbs with Prolog</title>
    <summary>  This paper describes a parsing/generation system for finite verbal forms in
Akkadian, with the possible addition of suffixes, implemented in Prolog. The
work described provides the framework and engine to interpret the D, N, and G
stems along with accusative, dative and ventive endings.
</summary>
    <author>
      <name>Aaron Macks</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3115/1118637.1118638</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3115/1118637.1118638" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures, presented at ACL-02 the Association of
  Computational Linguistics, 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.12617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.12617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02795v1</id>
    <updated>2024-11-05T04:10:05Z</updated>
    <published>2024-11-05T04:10:05Z</published>
    <title>The Evolution of RWKV: Advancements in Efficient Language Modeling</title>
    <summary>  This paper reviews the development of the Receptance Weighted Key Value
(RWKV) architecture, emphasizing its advancements in efficient language
modeling. RWKV combines the training efficiency of Transformers with the
inference efficiency of RNNs through a novel linear attention mechanism. We
examine its core innovations, adaptations across various domains, and
performance advantages over traditional models. The paper also discusses
challenges and future directions for RWKV as a versatile architecture in deep
learning.
</summary>
    <author>
      <name>Akul Datta</name>
    </author>
    <link href="http://arxiv.org/abs/2411.02795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.06194v1</id>
    <updated>2024-11-09T14:30:58Z</updated>
    <published>2024-11-09T14:30:58Z</published>
    <title>WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles</title>
    <summary>  We assess the difficulty of gender resolution in literary-style dialogue
settings and the influence of gender stereotypes. Instances of the test suite
contain spoken dialogue interleaved with external meta-context about the
characters and the manner of speaking. We find that character and manner
stereotypes outside of the dialogue significantly impact the gender agreement
of referents within the dialogue.
</summary>
    <author>
      <name>Hillary Dawkins</name>
    </author>
    <author>
      <name>Isar Nejadgholi</name>
    </author>
    <author>
      <name>Chi-kiu Lo</name>
    </author>
    <link href="http://arxiv.org/abs/2411.06194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.06194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18634v1</id>
    <updated>2024-11-14T18:20:52Z</updated>
    <published>2024-11-14T18:20:52Z</published>
    <title>Semantic, Orthographic, and Morphological Biases in Humans' Wordle
  Gameplay</title>
    <summary>  We show that human players' gameplay in the game of Wordle is influenced by
the semantics, orthography, and morphology of the player's previous guesses. We
demonstrate this influence by comparing actual human players' guesses to
near-optimal guesses, showing that human players' guesses are biased to be
similar to previous guesses semantically, orthographically, and
morphologically.
</summary>
    <author>
      <name>Gary Liang</name>
    </author>
    <author>
      <name>Adam Kabbara</name>
    </author>
    <author>
      <name>Cindy Liu</name>
    </author>
    <author>
      <name>Ronaldo Luo</name>
    </author>
    <author>
      <name>Kina Kim</name>
    </author>
    <author>
      <name>Michael Guerzhoy</name>
    </author>
    <link href="http://arxiv.org/abs/2411.18634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05499v1</id>
    <updated>2024-12-07T02:01:27Z</updated>
    <published>2024-12-07T02:01:27Z</published>
    <title>SplaXBERT: Leveraging Mixed Precision Training and Context Splitting for
  Question Answering</title>
    <summary>  SplaXBERT, built on ALBERT-xlarge with context-splitting and mixed precision
training, achieves high efficiency in question-answering tasks on lengthy
texts. Tested on SQuAD v1.1, it attains an Exact Match of 85.95% and an F1
Score of 92.97%, outperforming traditional BERT-based models in both accuracy
and resource efficiency.
</summary>
    <author>
      <name>Zhu Yufan</name>
    </author>
    <author>
      <name>Hao Zeyu</name>
    </author>
    <author>
      <name>Li Siqi</name>
    </author>
    <author>
      <name>Niu Boqian</name>
    </author>
    <link href="http://arxiv.org/abs/2412.05499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.05499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.10291v1</id>
    <updated>2024-12-13T17:21:29Z</updated>
    <published>2024-12-13T17:21:29Z</published>
    <title>Still "Talking About Large Language Models": Some Clarifications</title>
    <summary>  My paper "Talking About Large Language Models" has more than once been
interpreted as advocating a reductionist stance towards large language models.
But the paper was not intended that way, and I do not endorse such positions.
This short note situates the paper in the context of a larger philosophical
project that is concerned with the (mis)use of words rather than metaphysics,
in the spirit of Wittgenstein's later writing.
</summary>
    <author>
      <name>Murray Shanahan</name>
    </author>
    <link href="http://arxiv.org/abs/2412.10291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.10291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11314v1</id>
    <updated>2024-12-15T21:22:46Z</updated>
    <published>2024-12-15T21:22:46Z</published>
    <title>Reliable, Reproducible, and Really Fast Leaderboards with Evalica</title>
    <summary>  The rapid advancement of natural language processing (NLP) technologies, such
as instruction-tuned large language models (LLMs), urges the development of
modern evaluation protocols with human and machine feedback. We introduce
Evalica, an open-source toolkit that facilitates the creation of reliable and
reproducible model leaderboards. This paper presents its design, evaluates its
performance, and demonstrates its usability through its Web interface,
command-line interface, and Python API.
</summary>
    <author>
      <name>Dmitry Ustalov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at COLING 2025 system demonstration track</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.11314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-04" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11835v1</id>
    <updated>2024-12-16T14:56:31Z</updated>
    <published>2024-12-16T14:56:31Z</published>
    <title>Improved Models for Media Bias Detection and Subcategorization</title>
    <summary>  We present improved models for the granular detection and sub-classification
news media bias in English news articles. We compare the performance of
zero-shot versus fine-tuned large pre-trained neural transformer language
models, explore how the level of detail of the classes affects performance on a
novel taxonomy of 27 news bias-types, and demonstrate how using synthetically
generated example data can be used to improve quality
</summary>
    <author>
      <name>Tim Menzner</name>
    </author>
    <author>
      <name>Jochen L. Leidner</name>
    </author>
    <link href="http://arxiv.org/abs/2412.11835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.05990v1</id>
    <updated>2025-01-10T14:21:03Z</updated>
    <published>2025-01-10T14:21:03Z</published>
    <title>Constraining constructions with WordNet: pros and cons for the semantic
  annotation of fillers in the Italian Constructicon</title>
    <summary>  The paper discusses the role of WordNet-based semantic classification in the
formalization of constructions, and more specifically in the semantic
annotation of schematic fillers, in the Italian Constructicon. We outline how
the Italian Constructicon project uses Open Multilingual WordNet topics to
represent semantic features and constraints of constructions.
</summary>
    <author>
      <name>Flavio Pisciotta</name>
    </author>
    <author>
      <name>Ludovica Pannitto</name>
    </author>
    <author>
      <name>Lucia Busso</name>
    </author>
    <author>
      <name>Beatrice Bernasconi</name>
    </author>
    <author>
      <name>Francesca Masini</name>
    </author>
    <link href="http://arxiv.org/abs/2501.05990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.05990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.12050v1</id>
    <updated>2025-02-17T17:18:39Z</updated>
    <published>2025-02-17T17:18:39Z</published>
    <title>SpeechT: Findings of the First Mentorship in Speech Translation</title>
    <summary>  This work presents the details and findings of the first mentorship in speech
translation (SpeechT), which took place in December 2024 and January 2025. To
fulfil the requirements of the mentorship, the participants engaged in key
activities, including data preparation, modelling, and advanced research.
</summary>
    <author>
      <name>Yasmin Moslem</name>
    </author>
    <author>
      <name>Juan Julián Cea Morán</name>
    </author>
    <author>
      <name>Mariano Gonzalez-Gomez</name>
    </author>
    <author>
      <name>Muhammad Hazim Al Farouq</name>
    </author>
    <author>
      <name>Farah Abdou</name>
    </author>
    <author>
      <name>Satarupa Deb</name>
    </author>
    <link href="http://arxiv.org/abs/2502.12050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16705v1</id>
    <updated>2025-02-23T20:31:22Z</updated>
    <published>2025-02-23T20:31:22Z</published>
    <title>Can ChatGPT Learn to Count Letters?</title>
    <summary>  Large language models (LLMs) struggle on simple tasks such as counting the
number of occurrences of a letter in a word. In this paper, we investigate if
ChatGPT can learn to count letters and propose an efficient solution.
</summary>
    <author>
      <name>Javier Conde</name>
    </author>
    <author>
      <name>Gonzalo Martínez</name>
    </author>
    <author>
      <name>Pedro Reviriego</name>
    </author>
    <author>
      <name>Zhen Gao</name>
    </author>
    <author>
      <name>Shanshan Liu</name>
    </author>
    <author>
      <name>Fabrizio Lombardi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MC.2024.3488313</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MC.2024.3488313" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer (Volume: 58, Issue: 3, March 2025)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2502.16705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16721v1</id>
    <updated>2025-02-23T21:28:55Z</updated>
    <published>2025-02-23T21:28:55Z</published>
    <title>Speed and Conversational Large Language Models: Not All Is About Tokens
  per Second</title>
    <summary>  The speed of open-weights large language models (LLMs) and its dependency on
the task at hand, when run on GPUs, is studied to present a comparative
analysis of the speed of the most popular open LLMs.
</summary>
    <author>
      <name>Javier Conde</name>
    </author>
    <author>
      <name>Miguel González</name>
    </author>
    <author>
      <name>Pedro Reviriego</name>
    </author>
    <author>
      <name>Zhen Gao</name>
    </author>
    <author>
      <name>Shanshan Liu</name>
    </author>
    <author>
      <name>Fabrizio Lombardi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MC.2024.3399384</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MC.2024.3399384" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer (Volume: 57, Issue: 8, August 2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2502.16721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.18886v1</id>
    <updated>2025-02-26T07:04:20Z</updated>
    <published>2025-02-26T07:04:20Z</published>
    <title>On Pruning State-Space LLMs</title>
    <summary>  Recent work proposed state-space models (SSMs) as an efficient alternative to
transformer-based LLMs. Can these models be pruned to further reduce their
computation costs? We adapt several pruning methods to the SSM structure, and
apply them to four SSM-based LLMs across multiple tasks. We find that such
models are quite robust to some pruning methods (e.g. WANDA), while using other
methods lead to fast performance degradation.
</summary>
    <author>
      <name>Tamer Ghattas</name>
    </author>
    <author>
      <name>Michael Hassid</name>
    </author>
    <author>
      <name>Roy Schwartz</name>
    </author>
    <link href="http://arxiv.org/abs/2502.18886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01753v1</id>
    <updated>2025-03-03T17:23:08Z</updated>
    <published>2025-03-03T17:23:08Z</published>
    <title>Boolean-aware Attention for Dense Retrieval</title>
    <summary>  We present Boolean-aware attention, a novel attention mechanism that
dynamically adjusts token focus based on Boolean operators (e.g., and, or,
not). Our model employs specialized Boolean experts, each tailored to amplify
or suppress attention for operator-specific contexts. A predefined gating
mechanism activates the corresponding experts based on the detected Boolean
type. Experiments on Boolean retrieval datasets demonstrate that integrating
BoolAttn with BERT greatly enhances the model's capability to process Boolean
queries.
</summary>
    <author>
      <name>Quan Mai</name>
    </author>
    <author>
      <name>Susan Gauch</name>
    </author>
    <author>
      <name>Douglas Adams</name>
    </author>
    <link href="http://arxiv.org/abs/2503.01753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.12225v1</id>
    <updated>2025-03-15T18:43:13Z</updated>
    <published>2025-03-15T18:43:13Z</published>
    <title>Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents</title>
    <summary>  This article explores the gaps that can manifest when using a large language
model (LLM) to obtain simplified interpretations of data practices from a
complex privacy policy. We exemplify these gaps to showcase issues in accuracy,
completeness, clarity and representation, while advocating for continued
research to realize an LLM's true potential in revolutionizing privacy
management through personal assistants and automated compliance checking.
</summary>
    <author>
      <name>Rinku Dewri</name>
    </author>
    <link href="http://arxiv.org/abs/2503.12225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.12225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0565v1</id>
    <updated>2008-07-03T13:42:00Z</updated>
    <published>2008-07-03T13:42:00Z</published>
    <title>Music, Complexity, Information</title>
    <summary>  These are the preparatory notes for a Science &amp; Music essay, "Playing by
numbers", appeared in Nature 453 (2008) 988-989.
</summary>
    <author>
      <name>Damian H. Zanette</name>
    </author>
    <link href="http://arxiv.org/abs/0807.0565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08982v1</id>
    <updated>2015-12-30T15:46:30Z</updated>
    <published>2015-12-30T15:46:30Z</published>
    <title>Technical Report: a tool for measuring Prosodic Accommodation</title>
    <summary>  This article has been withdrawn by arXiv administrators because the submitter
did not have the legal authority to grant the license applied to the work.
</summary>
    <author>
      <name>Sucheta Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by arXiv administrators</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02831v1</id>
    <updated>2019-03-07T11:06:10Z</updated>
    <published>2019-03-07T11:06:10Z</published>
    <title>Predicting Research Trends From Arxiv</title>
    <summary>  We perform trend detection on two datasets of Arxiv papers, derived from its
machine learning (cs.LG) and natural language processing (cs.CL) categories.
Our approach is bottom-up: we first rank papers by their normalized citation
counts, then group top-ranked papers into different categories based on the
tasks that they pursue and the methods they use. We then analyze these
resulting topics. We find that the dominating paradigm in cs.CL revolves around
natural language generation problems and those in cs.LG revolve around
reinforcement learning and adversarial principles. By extrapolation, we predict
that these topics will remain lead problems/approaches in their fields in the
short- and mid-term.
</summary>
    <author>
      <name>Steffen Eger</name>
    </author>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Florian Netzer</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Refresh workshop paper (December 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.02831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809020v1</id>
    <updated>1998-09-15T23:49:32Z</updated>
    <published>1998-09-15T23:49:32Z</published>
    <title>Linear Segmentation and Segment Significance</title>
    <summary>  We present a new method for discovering a segmental discourse structure of a
document while categorizing segment function. We demonstrate how retrieval of
noun phrases and pronominal forms, along with a zero-sum weighting scheme,
determines topicalized segmentation. Futhermore, we use term distribution to
aid in identifying the role that the segment performs in the document. Finally,
we present results of evaluation in terms of precision and recall which surpass
earlier approaches.
</summary>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <author>
      <name>Judith L. Klavans</name>
    </author>
    <author>
      <name>Kathleen R. McKeown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, US Letter, 4 figures. Software License can be found at
  http://www.cs.columbia.edu/nlp/licenses/segmenterLicenseDownload.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 6th International Workshop of Very Large Corpora
  (WVLC-6), Montreal, Quebec, Canada: Aug. 1998. pp. 197-205</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809028v1</id>
    <updated>1998-09-18T04:44:02Z</updated>
    <published>1998-09-18T04:44:02Z</published>
    <title>Separating Dependency from Constituency in a Tree Rewriting System</title>
    <summary>  In this paper we present a new tree-rewriting formalism called Link-Sharing
Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using
LSTAG we define an approach towards coordination where linguistic dependency is
distinguished from the notion of constituency. Such an approach towards
coordination that explicitly distinguishes dependencies from constituency gives
a better formal understanding of its representation when compared to previous
approaches that use tree-rewriting systems which conflate the two issues.
</summary>
    <author>
      <name>Anoop Sarkar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pennsylvania</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 Postscript figures, uses fullname.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Fifth Meeting on Mathematics of Language,
  Saarbruecken, August 1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; D.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809029v1</id>
    <updated>1998-09-18T05:03:48Z</updated>
    <published>1998-09-18T05:03:48Z</published>
    <title>Incremental Parser Generation for Tree Adjoining Grammars</title>
    <summary>  This paper describes the incremental generation of parse tables for the
LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented
handles modifications to the input grammar by updating the parser generated so
far. In this paper, a lazy generation of LR-type parsers for TALs is defined in
which parse tables are created by need while parsing. We then describe an
incremental parser generator for TALs which responds to modification of the
input grammar by updating parse tables built so far.
</summary>
    <author>
      <name>Anoop Sarkar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pennsylvania</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.1594535</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.1594535" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 Postscript figures, uses fullname.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Longer version of paper in Proceedings of the 34th Meeting of the
  ACL, Student Session. Santa Cruz, June 1996</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; D.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809050v1</id>
    <updated>1998-09-23T12:59:39Z</updated>
    <published>1998-09-23T12:59:39Z</published>
    <title>A Freely Available Morphological Analyzer, Disambiguator and Context
  Sensitive Lemmatizer for German</title>
    <summary>  In this paper we present Morphy, an integrated tool for German morphology,
part-of-speech tagging and context-sensitive lemmatization. Its large lexicon
of more than 320,000 word forms plus its ability to process German compound
nouns guarantee a wide morphological coverage. Syntactic ambiguities can be
resolved with a standard statistical part-of-speech tagger. By using the output
of the tagger, the lemmatizer can determine the correct root even for ambiguous
word forms. The complete package is freely available and can be downloaded from
the World Wide Web.
</summary>
    <author>
      <name>Wolfgang Lezius</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Paderborn</arxiv:affiliation>
    </author>
    <author>
      <name>Reinhard Rapp</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Mainz</arxiv:affiliation>
    </author>
    <author>
      <name>Manfred Wettler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Paderborn</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Postscript only</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the COLING-ACL 1998, pp. 743-748</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809112v1</id>
    <updated>1998-09-28T07:49:11Z</updated>
    <published>1998-09-28T07:49:11Z</published>
    <title>On the Evaluation and Comparison of Taggers: The Effect of Noise in
  Testing Corpora</title>
    <summary>  This paper addresses the issue of {\sc pos} tagger evaluation. Such
evaluation is usually performed by comparing the tagger output with a reference
test corpus, which is assumed to be error-free. Currently used corpora contain
noise which causes the obtained performance to be a distortion of the real
value. We analyze to what extent this distortion may invalidate the comparison
between taggers or the measure of the improvement given by a new system. The
main conclusion is that a more rigorous testing experimentation
setting/designing is needed to reliably evaluate and compare tagger accuracies.
</summary>
    <author>
      <name>L. Padro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>L. Marquez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in proceedings of joint COLING-ACL 1998, Montreal, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809113v1</id>
    <updated>1998-09-28T07:50:55Z</updated>
    <published>1998-09-28T07:50:55Z</published>
    <title>Improving Tagging Performance by Using Voting Taggers</title>
    <summary>  We present a bootstrapping method to develop an annotated corpus, which is
specially useful for languages with few available resources. The method is
being applied to develop a corpus of Spanish of over 5Mw. The method consists
on taking advantage of the collaboration of two different POS taggers. The
cases in which both taggers agree present a higher accuracy and are used to
retrain the taggers.
</summary>
    <author>
      <name>L. Marquez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>L. Padro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>H. Rodriguez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in proceedings of NLP+IA/TAL+AI'98. Moncton, New Brunswick,
  Canada, 1998</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810015v1</id>
    <updated>1998-10-13T21:17:13Z</updated>
    <published>1998-10-13T21:17:13Z</published>
    <title>Restrictions on Tree Adjoining Languages</title>
    <summary>  Several methods are known for parsing languages generated by Tree Adjoining
Grammars (TAGs) in O(n^6) worst case running time. In this paper we investigate
which restrictions on TAGs and TAG derivations are needed in order to lower
this O(n^6) time complexity, without introducing large runtime constants, and
without losing any of the generative power needed to capture the syntactic
constructions in natural language that can be handled by unrestricted TAGs. In
particular, we describe an algorithm for parsing a strict subclass of TAG in
O(n^5), and attempt to show that this subclass retains enough generative power
to make it useful in the general case.
</summary>
    <author>
      <name>Giorgio Satta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universita di Padova</arxiv:affiliation>
    </author>
    <author>
      <name>William Schuler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pennsylvania</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages LaTeX + 5 eps figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING-ACL'98</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9810015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811006v1</id>
    <updated>1998-11-02T18:57:23Z</updated>
    <published>1998-11-02T18:57:23Z</published>
    <title>Machine Learning of Generic and User-Focused Summarization</title>
    <summary>  A key problem in text summarization is finding a salience function which
determines what information in the source should be included in the summary.
This paper describes the use of machine learning on a training corpus of
documents and their abstracts to discover salience functions which describe
what combination of features is optimal for a given summarization task. The
method addresses both "generic" and user-focused summaries.
</summary>
    <author>
      <name>Inderjeet Mani</name>
    </author>
    <author>
      <name>Eric Bloedorn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Fifteenth National Conference on AI (AAAI-98),
  p. 821-826</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9811006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811008v1</id>
    <updated>1998-11-02T21:29:41Z</updated>
    <published>1998-11-02T21:29:41Z</published>
    <title>Translating near-synonyms: Possibilities and preferences in the
  interlingua</title>
    <summary>  This paper argues that an interlingual representation must explicitly
represent some parts of the meaning of a situation as possibilities (or
preferences), not as necessary or definite components of meaning (or
constraints). Possibilities enable the analysis and generation of nuance,
something required for faithful translation. Furthermore, the representation of
the meaning of words, especially of near-synonyms, is crucial, because it
specifies which nuances words can convey in which contexts.
</summary>
    <author>
      <name>Philip Edmonds</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, LaTeX2e, 1 eps figure, uses colacl.sty, epsfig.sty, avm.sty,
  times.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AMTA/SIG-IL Second Workshop on Interlinguas,
  October 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811009v1</id>
    <updated>1998-11-02T23:06:19Z</updated>
    <published>1998-11-02T23:06:19Z</published>
    <title>Choosing the Word Most Typical in Context Using a Lexical Co-occurrence
  Network</title>
    <summary>  This paper presents a partial solution to a component of the problem of
lexical choice: choosing the synonym most typical, or expected, in context. We
apply a new statistical approach to representing the context of a word through
lexical co-occurrence networks. The implementation was trained and evaluated on
a large corpus, and results show that the inclusion of second-order
co-occurrence relations improves the performance of our implemented lexical
choice program.
</summary>
    <author>
      <name>Philip Edmonds</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, LaTeX2e, 1 ps figure, uses mathptm.sty, colacl.sty,
  psfig.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACL-EACL '97, student session</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811022v2</id>
    <updated>2000-01-25T15:37:36Z</updated>
    <published>1998-11-12T17:31:17Z</published>
    <title>Expoiting Syntactic Structure for Language Modeling</title>
    <summary>  The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words--binary-parse-structure with headword annotation and
operates in a left-to-right manner --- therefore usable for automatic speech
recognition. The model, its probabilistic parameterization, and a set of
experiments meant to evaluate its predictive power are presented; an
improvement over standard trigram modeling is achieved.
</summary>
    <author>
      <name>Ciprian Chelba</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP The Johns Hopkins University</arxiv:affiliation>
    </author>
    <author>
      <name>Frederick Jelinek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP The Johns Hopkins University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">changed ACM-class membership and buggy author names</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACL'98, Montreal, Canada</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811022v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811022v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3, I.2.7, I.5.1, I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811025v2</id>
    <updated>2000-01-25T15:46:48Z</updated>
    <published>1998-11-13T16:53:15Z</published>
    <title>A Structured Language Model</title>
    <summary>  The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words - binary-parse-structure with headword annotation. The
model, its probabilistic parametrization, and a set of experiments meant to
evaluate its predictive power are presented.
</summary>
    <author>
      <name>Ciprian Chelba</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP, The Johns Hopkins University, USA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">changed ACM-class membership, Proceedings of ACL-EACL'97, Student
  Section, Madrid, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9811025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3, I.2.7, I.5.1, I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812005v1</id>
    <updated>1998-12-04T16:16:35Z</updated>
    <published>1998-12-04T16:16:35Z</published>
    <title>Optimal Multi-Paragraph Text Segmentation by Dynamic Programming</title>
    <summary>  There exist several methods of calculating a similarity curve, or a sequence
of similarity values, representing the lexical cohesion of successive text
constituents, e.g., paragraphs. Methods for deciding the locations of fragment
boundaries are, however, scarce. We propose a fragmentation method based on
dynamic programming. The method is theoretically sound and guaranteed to
provide an optimal splitting on the basis of a similarity curve, a preferred
fragment length, and a cost function defined. The method is especially useful
when control on fragment size is of importance.
</summary>
    <author>
      <name>Oskari Heinonen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Helsinki</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 eps figures, LaTeX2e; includes errata; uses colacl, epsf,
  times</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING-ACL '98, pp. 1484-1486, Montreal, Canada</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9812005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9904004v1</id>
    <updated>1999-04-12T11:37:49Z</updated>
    <published>1999-04-12T11:37:49Z</published>
    <title>Mixing Metaphors</title>
    <summary>  Mixed metaphors have been neglected in recent metaphor research. This paper
suggests that such neglect is short-sighted. Though mixing is a more complex
phenomenon than straight metaphors, the same kinds of reasoning and knowledge
structures are required. This paper provides an analysis of both parallel and
serial mixed metaphors within the framework of an AI system which is already
capable of reasoning about straight metaphorical manifestations and argues that
the processes underlying mixing are central to metaphorical meaning. Therefore,
any theory of metaphors must be able to account for mixing.
</summary>
    <author>
      <name>Mark Lee</name>
    </author>
    <author>
      <name>John Barnden</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AISB'99 Symposium on Metaphor, Artificial
  Intelligence, and Cognition, pages 11-16, Edinburgh</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9904004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9904004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9904008v1</id>
    <updated>1999-04-15T14:00:41Z</updated>
    <published>1999-04-15T14:00:41Z</published>
    <title>Transducers from Rewrite Rules with Backreferences</title>
    <summary>  Context sensitive rewrite rules have been widely used in several areas of
natural language processing, including syntax, morphology, phonology and speech
processing. Kaplan and Kay, Karttunen, and Mohri &amp; Sproat have given various
algorithms to compile such rewrite rules into finite-state transducers. The
present paper extends this work by allowing a limited form of backreferencing
in such rules. The explicit use of backreferencing leads to more elegant and
general solutions.
</summary>
    <author>
      <name>Dale Gerdemann</name>
    </author>
    <author>
      <name>Gertjan van Noord</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, EACL 1999 Bergen</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9904008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9904008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.4.3; I.2.1; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9904018v1</id>
    <updated>1999-04-24T23:45:26Z</updated>
    <published>1999-04-24T23:45:26Z</published>
    <title>A Computational Memory and Processing Model for Processing for Prosody</title>
    <summary>  This paper links prosody to the information in a text and how it is processed
by the speaker. It describes the operation and output of LOQ, a text-to-speech
implementation that includes a model of limited attention and working memory.
Attentional limitations are key. Varying the attentional parameter in the
simulations varies in turn what counts as given and new in a text, and
therefore, the intonational contours with which it is uttered. Currently, the
system produces prosody in three different styles: child-like, adult
expressive, and knowledgeable. This prosody also exhibits differences within
each style -- no two simulations are alike. The limited resource approach
captures some of the stylistic and individual variety found in natural prosody.
</summary>
    <author>
      <name>Janet E. Cahn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9904018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9904018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7, I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9905008v1</id>
    <updated>1999-05-19T14:52:33Z</updated>
    <published>1999-05-19T14:52:33Z</published>
    <title>Inducing a Semantically Annotated Lexicon via EM-Based Clustering</title>
    <summary>  We present a technique for automatic induction of slot annotations for
subcategorization frames, based on induction of hidden classes in the EM
framework of statistical estimation. The models are empirically evalutated by a
general decision test. Induction of slot labeling for subcategorization frames
is accomplished by a further application of EM, and applied experimentally on
frame observations derived from parsing large corpora. We outline an
interpretation of the learned representations as theoretical-linguistic
decompositional lexical entries.
</summary>
    <author>
      <name>Mats Rooth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Riezler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Detlef Prescher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Glenn Carroll</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Franz Beil</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, uses colacl.sty. Proceedings of the 37th Annual Meeting of
  the ACL, 1999</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9905008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9905008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9905009v1</id>
    <updated>1999-05-19T14:47:21Z</updated>
    <published>1999-05-19T14:47:21Z</published>
    <title>Inside-Outside Estimation of a Lexicalized PCFG for German</title>
    <summary>  The paper describes an extensive experiment in inside-outside estimation of a
lexicalized probabilistic context free grammar for German verb-final clauses.
Grammar and formalism features which make the experiment feasible are
described. Successive models are evaluated on precision and recall of phrase
markup.
</summary>
    <author>
      <name>Franz Beil</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Glenn Carroll</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Detlef Prescher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Riezler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <author>
      <name>Mats Rooth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS, University of Stuttgart</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, uses colacl.sty. Proceedings of the 37th Annual Meeting of
  the ACL, 1999</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9905009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9905009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906004v1</id>
    <updated>1999-06-02T13:41:51Z</updated>
    <published>1999-06-02T13:41:51Z</published>
    <title>Cascaded Grammatical Relation Assignment</title>
    <summary>  In this paper we discuss cascaded Memory-Based grammatical relations
assignment. In the first stages of the cascade, we find chunks of several types
(NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local,
temporal). In the last stage, we assign grammatical relations to pairs of
chunks. We studied the effect of adding several levels to this cascaded
classifier and we found that even the less performing chunkers enhanced the
performance of the relation finder.
</summary>
    <author>
      <name>Sabine Buchholz</name>
    </author>
    <author>
      <name>Jorn Veenstra</name>
    </author>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, to appear in: proceedings of EMNLP/VLC-99, University of
  Maryland, USA, June 21-22, 1999</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9906004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.2;I.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906005v1</id>
    <updated>1999-06-02T13:48:48Z</updated>
    <published>1999-06-02T13:48:48Z</published>
    <title>Memory-Based Shallow Parsing</title>
    <summary>  We present a memory-based learning (MBL) approach to shallow parsing in which
POS tagging, chunking, and identification of syntactic relations are formulated
as memory-based modules. The experiments reported in this paper show
competitive results, the F-value for the Wall Street Journal (WSJ) treebank is:
93.8% for NP chunking, 94.7% for VP chunking, 77.1% for subject detection and
79.0% for object detection.
</summary>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <author>
      <name>Sabine Buchholz</name>
    </author>
    <author>
      <name>Jorn Veenstra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, to appear in: Proceedings of the EACL'99 workshop on
  Computational Natural Language Learning (CoNLL-99), Bergen, Norway, June 1999</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9906005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.2;I.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906015v1</id>
    <updated>1999-06-14T22:06:24Z</updated>
    <published>1999-06-14T22:06:24Z</published>
    <title>Learning Transformation Rules to Find Grammatical Relations</title>
    <summary>  Grammatical relationships are an important level of natural language
processing. We present a trainable approach to find these relationships through
transformation sequences and error-driven learning. Our approach finds
grammatical relationships between core syntax groups and bypasses much of the
parsing phase. On our training and test set, our procedure achieves 63.6%
recall and 77.3% precision (f-score = 69.8).
</summary>
    <author>
      <name>Lisa Ferro</name>
    </author>
    <author>
      <name>Marc Vilain</name>
    </author>
    <author>
      <name>Alexander Yeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. Uses latex-acl.sty and named.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Natural Language Learning (CoNLL-99), pages 43-52,
  June, 1999. Bergen, Norway</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9906015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906025v1</id>
    <updated>1999-06-24T16:56:45Z</updated>
    <published>1999-06-24T16:56:45Z</published>
    <title>Mapping Multilingual Hierarchies Using Relaxation Labeling</title>
    <summary>  This paper explores the automatic construction of a multilingual Lexical
Knowledge Base from pre-existing lexical resources. We present a new and robust
approach for linking already existing lexical/semantic hierarchies. We used a
constraint satisfaction algorithm (relaxation labeling) to select --among all
the candidate translations proposed by a bilingual dictionary-- the right
English WordNet synset for each sense in a taxonomy automatically derived from
a Spanish monolingual dictionary. Although on average, there are 15 possible
WordNet connections for each sense in the taxonomy, the method achieves an
accuracy over 80%. Finally, we also propose several ways in which this
technique could be applied to enrich and improve existing lexical databases.
</summary>
    <author>
      <name>J. Daude</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TALP Research Center. LSI Dept. Universitat Politecnica de Catalunya. Barcelona</arxiv:affiliation>
    </author>
    <author>
      <name>L. Padro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TALP Research Center. LSI Dept. Universitat Politecnica de Catalunya. Barcelona</arxiv:affiliation>
    </author>
    <author>
      <name>G. Rigau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TALP Research Center. LSI Dept. Universitat Politecnica de Catalunya. Barcelona</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. 1 eps figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9906025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906026v1</id>
    <updated>1999-06-25T08:16:23Z</updated>
    <published>1999-06-25T08:16:23Z</published>
    <title>Robust Grammatical Analysis for Spoken Dialogue Systems</title>
    <summary>  We argue that grammatical analysis is a viable alternative to concept
spotting for processing spoken input in a practical spoken dialogue system. We
discuss the structure of the grammar, and a model for robust parsing which
combines linguistic sources of information and statistical sources of
information. We discuss test results suggesting that grammatical processing
allows fast and accurate processing of spoken input.
</summary>
    <author>
      <name>Gertjan van Noord</name>
    </author>
    <author>
      <name>Gosse Bouma</name>
    </author>
    <author>
      <name>Rob Koeling</name>
    </author>
    <author>
      <name>Mark-Jan Nederhof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for JNLE</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9906026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.0;H.5.1;H.5.2;I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907006v1</id>
    <updated>1999-07-06T12:44:20Z</updated>
    <published>1999-07-06T12:44:20Z</published>
    <title>Representing Text Chunks</title>
    <summary>  Dividing sentences in chunks of words is a useful preprocessing step for
parsing, information extraction and information retrieval. (Ramshaw and Marcus,
1995) have introduced a "convenient" data representation for chunking by
converting it to a tagging task. In this paper we will examine seven different
data representations for the problem of recognizing noun phrase chunks. We will
show that the the data representation choice has a minor influence on chunking
performance. However, equipped with the most suitable data representation, our
memory-based learning chunker was able to improve the best published chunking
results for a standard data set.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <author>
      <name>Jorn Veenstra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EACL'99, Bergen</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9907006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907008v1</id>
    <updated>1999-07-06T18:35:41Z</updated>
    <published>1999-07-06T18:35:41Z</published>
    <title>Explanation-based Learning for Machine Translation</title>
    <summary>  In this paper we present an application of explanation-based learning (EBL)
in the parsing module of a real-time English-Spanish machine translation system
designed to translate closed captions. We discuss the efficiency/coverage
trade-offs available in EBL and introduce the techniques we use to increase
coverage while maintaining a high level of space and time efficiency. Our
performance results indicate that this approach is effective.
</summary>
    <author>
      <name>Janine Toole</name>
    </author>
    <author>
      <name>Fred Popowich</name>
    </author>
    <author>
      <name>Devlan Nicholson</name>
    </author>
    <author>
      <name>Davide Turcato</name>
    </author>
    <author>
      <name>Paul McFetridge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, To appear in Proceedings of the 8th
  International Conference on Theoretical and Methodological Issues in Machine
  Translation</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9907008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907010v1</id>
    <updated>1999-07-07T09:28:40Z</updated>
    <published>1999-07-07T09:28:40Z</published>
    <title>Language Identification With Confidence Limits</title>
    <summary>  A statistical classification algorithm and its application to language
identification from noisy input are described. The main innovation is to
compute confidence limits on the classification, so that the algorithm
terminates when enough evidence to make a clear decision has been made, and so
avoiding problems with categories that have similar characteristics. A second
application, to genre identification, is briefly examined. The results show
that some of the problems of other language identification techniques can be
avoided, and illustrate a more important point: that a statistical language
process can be used to provide feedback about its own success rate.
</summary>
    <author>
      <name>David Elworthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; needs colacl.sty. Appeared in Proceedings of the Sixth
  Workshop on Very Large Corpora (COLING-ACL 98)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9907010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907012v1</id>
    <updated>1999-07-08T09:46:37Z</updated>
    <published>1999-07-08T09:46:37Z</published>
    <title>Selective Magic HPSG Parsing</title>
    <summary>  We propose a parser for constraint-logic grammars implementing HPSG that
combines the advantages of dynamic bottom-up and advanced top-down control. The
parser allows the user to apply magic compilation to specific constraints in a
grammar which as a result can be processed dynamically in a bottom-up and
goal-directed fashion. State of the art top-down processing techniques are used
to deal with the remaining constraints. We discuss various aspects concerning
the implementation of the parser as part of a grammar development system.
</summary>
    <author>
      <name>Guido Minnen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Sussex</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, LaTeX with 4 postscript figures (uses avm.sty, eaclap.sty
  and psfig-scale.sty)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of EACL99, Bergen, Norway, June 8-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9907012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907013v1</id>
    <updated>1999-07-08T10:08:59Z</updated>
    <published>1999-07-08T10:08:59Z</published>
    <title>Corpus Annotation for Parser Evaluation</title>
    <summary>  We describe a recently developed corpus annotation scheme for evaluating
parsers that avoids shortcomings of current methods. The scheme encodes
grammatical relations between heads and dependents, and has been used to mark
up a new public-domain corpus of naturally occurring English text. We show how
the corpus can be used to evaluate the accuracy of a robust parser, and relate
the corpus to extant resources.
</summary>
    <author>
      <name>John Carroll</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Sussex</arxiv:affiliation>
    </author>
    <author>
      <name>Guido Minnen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Sussex</arxiv:affiliation>
    </author>
    <author>
      <name>Ted Briscoe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cambridge University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, LaTeX (uses eaclap.sty)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the EACL99 workshop on Linguistically Interpreted
  Corpora (LINC), Bergen, Norway, June 12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9907013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9908001v1</id>
    <updated>1999-08-01T14:02:57Z</updated>
    <published>1999-08-01T14:02:57Z</published>
    <title>Detecting Sub-Topic Correspondence through Bipartite Term Clustering</title>
    <summary>  This paper addresses a novel task of detecting sub-topic correspondence in a
pair of text fragments, enhancing common notions of text similarity. This task
is addressed by coupling corresponding term subsets through bipartite
clustering. The paper presents a cost-based clustering scheme and compares it
with a bipartite version of the single-link method, providing illustrating
results.
</summary>
    <author>
      <name>Zvika Marx</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bar-Ilan University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Hebrew University of Jerusalem</arxiv:affiliation>
    </author>
    <author>
      <name>Ido Dagan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bar-Ilan University</arxiv:affiliation>
    </author>
    <author>
      <name>Eli Shamir</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Hebrew University of Jerusalem</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">html with 3 gif figures; generated from 7 pages MS-Word file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACL'99 Workshop on Unsupervised Learning in Natural
  Language Processing, 1999, pp 45-51</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9908001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9908001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6, I.2.7, H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9909002v1</id>
    <updated>1999-09-02T15:53:07Z</updated>
    <published>1999-09-02T15:53:07Z</published>
    <title>Semantic robust parsing for noun extraction from natural language
  queries</title>
    <summary>  This paper describes how robust parsing techniques can be fruitful applied
for building a query generation module which is part of a pipelined NLP
architecture aimed at process natural language queries in a restricted domain.
We want to show that semantic robustness represents a key issue in those NLP
systems where it is more likely to have partial and ill-formed utterances due
to various factors (e.g. noisy environments, low quality of speech recognition
modules, etc...) and where it is necessary to succeed, even if partially, in
extracting some meaningful information.
</summary>
    <author>
      <name>Afzal Ballim</name>
    </author>
    <author>
      <name>Vincenzo Pallotta</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of WPDI'99 (Workshop on Procedures in Discourse
  Interpretation),1999, Iasi - Romania</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9909002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9909002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910022v1</id>
    <updated>1999-10-25T15:00:52Z</updated>
    <published>1999-10-25T15:00:52Z</published>
    <title>Practical experiments with regular approximation of context-free
  languages</title>
    <summary>  Several methods are discussed that construct a finite automaton given a
context-free grammar, including both methods that lead to subsets and those
that lead to supersets of the original context-free language. Some of these
methods of regular approximation are new, and some others are presented here in
a more refined form with respect to existing literature. Practical experiments
with the different methods of regular approximation are performed for
spoken-language input: hypotheses from a speech recognizer are filtered through
a finite automaton.
</summary>
    <author>
      <name>Mark-Jan Nederhof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages. To appear in Computational Linguistics 26(1), March 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9910022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.3; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912006v1</id>
    <updated>1999-12-13T05:19:46Z</updated>
    <published>1999-12-13T05:19:46Z</published>
    <title>Resolution of Verb Ellipsis in Japanese Sentence using Surface
  Expressions and Examples</title>
    <summary>  Verbs are sometimes omitted in Japanese sentences. It is necessary to recover
omitted verbs for purposes of language understanding, machine translation, and
conversational processing. This paper describes a practical way to recover
omitted verbs by using surface expressions and examples. We experimented the
resolution of verb ellipses by using this information, and obtained a recall
rate of 73% and a precision rate of 66% on test sentences.
</summary>
    <author>
      <name>M. Murata</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kyoto University</arxiv:affiliation>
    </author>
    <author>
      <name>M. Nagao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kyoto University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 0 figures. Computation and Language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Language Processing Pacific Rim Symposium 1997 (NLPRS'97),
  Cape Panwa Hotel, Phuket, Thailand, December 2-4, 1997 p75-80</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9912006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912009v1</id>
    <updated>1999-12-15T11:02:22Z</updated>
    <published>1999-12-15T11:02:22Z</published>
    <title>Deduction over Mixed-Level Logic Representations for Text Passage
  Retrieval</title>
    <summary>  A system is described that uses a mixed-level representation of (part of)
meaning of natural language documents (based on standard Horn Clause Logic) and
a variable-depth search strategy that distinguishes between the different
levels of abstraction in the knowledge representation to locate specific
passages in the documents. Mixed-level representations as well as
variable-depth search strategies are applicable in fields outside that of NLP.
</summary>
    <author>
      <name>Michael Hess</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TAI.1996.560480</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TAI.1996.560480" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Proceedings of the Eighth International Conference on Tools
  with Artificial Intelligence (TAI'96), Los Alamitos CA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computer Society Press, 1996. 383-390</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9912009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912016v1</id>
    <updated>1999-12-23T01:07:33Z</updated>
    <published>1999-12-23T01:07:33Z</published>
    <title>HMM Specialization with Selective Lexicalization</title>
    <summary>  We present a technique which complements Hidden Markov Models by
incorporating some lexicalized states representing syntactically uncommon
words. Our approach examines the distribution of transitions, selects the
uncommon words, and makes lexicalized states for the words. We performed a
part-of-speech tagging experiment on the Brown corpus to evaluate the resultant
language model and discovered that this technique improved the tagging accuracy
by 0.21% at the 95% level of confidence.
</summary>
    <author>
      <name>Jin-Dong Kim</name>
    </author>
    <author>
      <name>Sang-Zoo Lee</name>
    </author>
    <author>
      <name>Hae-Chang Rim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1999 Joint SIGDAT Conference on Empirical
  Methods in Natural Language Processing and Very Large Corpora, pp.121-127,
  1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9912016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912017v1</id>
    <updated>1999-12-23T15:48:26Z</updated>
    <published>1999-12-23T15:48:26Z</published>
    <title>Mixed-Level Knowledge Representation and Variable-Depth Inference in
  Natural Language Processing</title>
    <summary>  A system is described that uses a mixed-level knowledge representation based
on standard Horn Clause Logic to represent (part of) the meaning of natural
language documents. A variable-depth search strategy is outlined that
distinguishes between the different levels of abstraction in the knowledge
representation to locate specific passages in the documents. A detailed
description of the linguistic aspects of the system is given. Mixed-level
representations as well as variable-depth search strategies are applicable in
fields outside that of NLP.
</summary>
    <author>
      <name>Michael Hess</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Artificial Intelligence Tools (IJAIT),
  vol 6, no 4, 1997. 481-509</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9912017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001021v1</id>
    <updated>2000-01-24T20:55:17Z</updated>
    <published>2000-01-24T20:55:17Z</published>
    <title>Refinement of a Structured Language Model</title>
    <summary>  A new language model for speech recognition inspired by linguistic analysis
is presented. The model develops hidden hierarchical structure incrementally
and uses it to extract meaningful information from the word history - thus
enabling the use of extended distance dependencies - in an attempt to
complement the locality of currently used n-gram Markov models. The model, its
probabilistic parametrization, a reestimation algorithm for the model
parameters and a set of experiments meant to evaluate its potential for speech
recognition are presented.
</summary>
    <author>
      <name>Ciprian Chelba</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP The Johns Hopkins University</arxiv:affiliation>
    </author>
    <author>
      <name>Frederick Jelinek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP The Johns Hopkins University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Advances in Pattern
  Recognition, 1998, pp. 275-284, Plymouth, UK</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0001021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3, I.2.7, I.5.1, I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001022v1</id>
    <updated>2000-01-24T21:18:37Z</updated>
    <published>2000-01-24T21:18:37Z</published>
    <title>Recognition Performance of a Structured Language Model</title>
    <summary>  A new language model for speech recognition inspired by linguistic analysis
is presented. The model develops hidden hierarchical structure incrementally
and uses it to extract meaningful information from the word history - thus
enabling the use of extended distance dependencies - in an attempt to
complement the locality of currently used trigram models. The structured
language model, its probabilistic parameterization and performance in a
two-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus
show an improvement in both perplexity and word error rate over conventional
trigram models.
</summary>
    <author>
      <name>Ciprian Chelba</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP The Johns Hopkins University</arxiv:affiliation>
    </author>
    <author>
      <name>Frederick Jelinek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP The Johns Hopkins University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech, 1999, pp. 1567-1570, Budapest, Hungary</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0001022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3, I.2.7, I.5.1, I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001023v1</id>
    <updated>2000-01-25T19:35:01Z</updated>
    <published>2000-01-25T19:35:01Z</published>
    <title>Structured Language Modeling for Speech Recognition</title>
    <summary>  A new language model for speech recognition is presented. The model develops
hidden hierarchical syntactic-like structure incrementally and uses it to
extract meaningful information from the word history, thus complementing the
locality of currently used trigram models. The structured language model (SLM)
and its performance in a two-pass speech recognizer --- lattice decoding ---
are presented. Experiments on the WSJ corpus show an improvement in both
perplexity (PPL) and word error rate (WER) over conventional trigram models.
</summary>
    <author>
      <name>Ciprian Chelba</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP, The Johns Hopkins University</arxiv:affiliation>
    </author>
    <author>
      <name>Frederick Jelinek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP, The Johns Hopkins University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages + 2 pages of ERRATA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NLDB'99, Klagenfurt, Austria</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0001023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3, I.2.7, I.5.1, I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003055v1</id>
    <updated>2000-03-13T09:55:08Z</updated>
    <published>2000-03-13T09:55:08Z</published>
    <title>TnT - A Statistical Part-of-Speech Tagger</title>
    <summary>  Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.
Contrary to claims found elsewhere in the literature, we argue that a tagger
based on Markov models performs at least as well as other current approaches,
including the Maximum Entropy framework. A recent comparison has even shown
that TnT performs significantly better for the tested corpora. We describe the
basic model of TnT, the techniques used for smoothing and for handling unknown
words. Furthermore, we present evaluations on two corpora.
</summary>
    <author>
      <name>Thorsten Brants</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland University, Germany</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ANLP-2000, Seattle, WA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0003055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003060v1</id>
    <updated>2000-03-14T13:09:28Z</updated>
    <published>2000-03-14T13:09:28Z</published>
    <title>Message Classification in the Call Center</title>
    <summary>  Customer care in technical domains is increasingly based on e-mail
communication, allowing for the reproduction of approved solutions. Identifying
the customer's problem is often time-consuming, as the problem space changes if
new products are launched. This paper describes a new approach to the
classification of e-mail requests based on shallow text processing and machine
learning techniques. It is implemented within an assistance system for call
center agents that is used in a commercial setting.
</summary>
    <author>
      <name>Stephan Busemann</name>
    </author>
    <author>
      <name>Sven Schmeier</name>
    </author>
    <author>
      <name>Roman G. Arens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages with 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ANLP-2000, Seattle, WA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0003060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; I.7.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003074v1</id>
    <updated>2000-03-23T11:29:15Z</updated>
    <published>2000-03-23T11:29:15Z</published>
    <title>A Finite State and Data-Oriented Method for Grapheme to Phoneme
  Conversion</title>
    <summary>  A finite-state method, based on leftmost longest-match replacement, is
presented for segmenting words into graphemes, and for converting graphemes
into phonemes. A small set of hand-crafted conversion rules for Dutch achieves
a phoneme accuracy of over 93%. The accuracy of the system is further improved
by using transformation-based learning. The phoneme accuracy of the best system
(using a large set of rule templates and a `lazy' variant of Brill's algoritm),
trained on only 40K words, reaches 99% accuracy.
</summary>
    <author>
      <name>Gosse Bouma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NAACL-2000, Seattle, WA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0003074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003081v1</id>
    <updated>2000-03-29T16:35:58Z</updated>
    <published>2000-03-29T16:35:58Z</published>
    <title>Variable Word Rate N-grams</title>
    <summary>  The rate of occurrence of words is not uniform but varies from document to
document. Despite this observation, parameters for conventional n-gram language
models are usually derived using the assumption of a constant word rate. In
this paper we investigate the use of variable word rate assumption, modelled by
a Poisson distribution or a continuous mixture of Poissons. We present an
approach to estimating the relative frequencies of words or n-grams taking
prior information of their occurrences into account. Discounting and smoothing
schemes are also considered. Using the Broadcast News task, the approach
demonstrates a reduction of perplexity up to 10%.
</summary>
    <author>
      <name>Yoshihiko Gotoh</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, ICASSP-2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005006v1</id>
    <updated>2000-05-07T00:15:59Z</updated>
    <published>2000-05-07T00:15:59Z</published>
    <title>A Simple Approach to Building Ensembles of Naive Bayesian Classifiers
  for Word Sense Disambiguation</title>
    <summary>  This paper presents a corpus-based approach to word sense disambiguation that
builds an ensemble of Naive Bayesian classifiers, each of which is based on
lexical features that represent co--occurring words in varying sized windows of
context. Despite the simplicity of this approach, empirical results
disambiguating the widely studied nouns line and interest show that such an
ensemble achieves accuracy rivaling the best previously published results.
</summary>
    <author>
      <name>Ted Pedersen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Minnesota Duluth</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Latex, uses colnaacl.sty. Appears in Proceedings of NAACL,
  pages 63-69, May 2000, Seattle, WA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0005006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005015v1</id>
    <updated>2000-05-10T11:58:12Z</updated>
    <published>2000-05-10T11:58:12Z</published>
    <title>Noun Phrase Recognition by System Combination</title>
    <summary>  The performance of machine learning algorithms can be improved by combining
the output of different systems. In this paper we apply this idea to the
recognition of noun phrases.We generate different classifiers by using
different representations of the data. By combining the results with voting
techniques described in (Van Halteren et.al. 1998) we manage to improve the
best reported performances on standard data sets for base noun phrases and
arbitrary noun phrases.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NAACL 2000, Seattle, WA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005016v1</id>
    <updated>2000-05-10T14:51:14Z</updated>
    <published>2000-05-10T14:51:14Z</published>
    <title>Improving Testsuites via Instrumentation</title>
    <summary>  This paper explores the usefulness of a technique from software engineering,
namely code instrumentation, for the development of large-scale natural
language grammars. Information about the usage of grammar rules in test
sentences is used to detect untested rules, redundant test sentences, and
likely causes of overgeneration. Results show that less than half of a
large-coverage grammar for German is actually tested by two large testsuites,
and that 10-30% of testing time is redundant. The methodology applied can be
seen as a re-use of grammar writing knowledge for testsuite compilation.
</summary>
    <author>
      <name>Norbert Broeker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, LaTeX2e</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ANLP--NAACL, Seattle/WA, Apr29--May4 2000, pp.325-330</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005020v1</id>
    <updated>2000-05-12T17:24:06Z</updated>
    <published>2000-05-12T17:24:06Z</published>
    <title>Centroid-based summarization of multiple documents: sentence extraction,
  utility-based evaluation, and user studies</title>
    <summary>  We present a multi-document summarizer, called MEAD, which generates
summaries using cluster centroids produced by a topic detection and tracking
system. We also describe two new techniques, based on sentence utility and
subsumption, which we have applied to the evaluation of both single and
multiple document summaries. Finally, we describe two user studies that test
our models of multi-document summarization.
</summary>
    <author>
      <name>Dragomir R. Radev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Michigan</arxiv:affiliation>
    </author>
    <author>
      <name>Hongyan Jing</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>Malgorzata Budzikowska</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM TJ Watson Research Center</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages Corpus availability at http://perun.si.umich.edu/~radev/mds</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL/ANLP Workshop on Automatic Summarization, Seattle, WA, April
  30, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.4; H.3.7; H.5.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005025v1</id>
    <updated>2000-05-22T14:07:28Z</updated>
    <published>2000-05-22T14:07:28Z</published>
    <title>Finite-State Reduplication in One-Level Prosodic Morphology</title>
    <summary>  Reduplication, a central instance of prosodic morphology, is particularly
challenging for state-of-the-art computational morphology, since it involves
copying of some part of a phonological string. In this paper I advocate a
finite-state method that combines enriched lexical representations via
intersection to implement the copying. The proposal includes a
resource-conscious variant of automata and can benefit from the existence of
lazy algorithms. Finally, the implementation of a complex case from Koasati is
presented.
</summary>
    <author>
      <name>Markus Walther</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Marburg</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. NAACL-2000, Seattle/WA, pp.296-302</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005029v1</id>
    <updated>2000-05-30T17:10:33Z</updated>
    <published>2000-05-30T17:10:33Z</published>
    <title>Ranking suspected answers to natural language questions using predictive
  annotation</title>
    <summary>  In this paper, we describe a system to rank suspected answers to natural
language questions. We process both corpus and query using a new technique,
predictive annotation, which augments phrases in texts with labels anticipating
their being targets of certain kinds of questions. Given a natural language
question, an IR system returns a set of matching passages, which are then
analyzed and ranked according to various criteria described in this paper. We
provide an evaluation of the techniques based on results from the TREC Q&amp;A
evaluation in which our system participated.
</summary>
    <author>
      <name>Dragomir R. Radev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Michigan</arxiv:affiliation>
    </author>
    <author>
      <name>John Prager</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM TJ Watson Research Center</arxiv:affiliation>
    </author>
    <author>
      <name>Valerie Samn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Teachers College, Columbia University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ANLP'00, Seattle, WA, May 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;H.3.3;H.3.4;H.5.2;I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006003v1</id>
    <updated>2000-06-01T18:42:24Z</updated>
    <published>2000-06-01T18:42:24Z</published>
    <title>Exploiting Diversity in Natural Language Processing: Combining Parsers</title>
    <summary>  Three state-of-the-art statistical parsers are combined to produce more
accurate parses, as well as new bounds on achievable Treebank parsing accuracy.
Two general approaches are presented and two combination techniques are
described for each approach. Both parametric and non-parametric models are
explored. The resulting parsers surpass the best previously published
performance results for the Penn Treebank.
</summary>
    <author>
      <name>John C. Henderson</name>
    </author>
    <author>
      <name>Eric Brill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Fourth Conference on Empirical Methods in
  Natural Language Processing (EMNLP-99), pages 187-194. College Park,
  Maryland, USA. June, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006011v1</id>
    <updated>2000-06-05T18:04:51Z</updated>
    <published>2000-06-05T18:04:51Z</published>
    <title>Bagging and Boosting a Treebank Parser</title>
    <summary>  Bagging and boosting, two effective machine learning techniques, are applied
to natural language parsing. Experiments using these techniques with a
trainable statistical parser are described. The best resulting system provides
roughly as large of a gain in F-measure as doubling the corpus size. Error
analysis of the result of the boosting technique reveals some inconsistent
annotations in the Penn Treebank, suggesting a semi-automatic method for
finding inconsistent treebank annotations.
</summary>
    <author>
      <name>John C. Henderson</name>
    </author>
    <author>
      <name>Eric Brill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1st Meeting of the North American Chapter of
  the Association for Computational Linguistics (NAACL-2000), pages 34-41</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006019v1</id>
    <updated>2000-06-09T21:41:54Z</updated>
    <published>2000-06-09T21:41:54Z</published>
    <title>A Compact Architecture for Dialogue Management Based on Scripts and
  Meta-Outputs</title>
    <summary>  We describe an architecture for spoken dialogue interfaces to semi-autonomous
systems that transforms speech signals through successive representations of
linguistic, dialogue, and domain knowledge. Each step produces an output, and a
meta-output describing the transformation, with an executable program in a
simple scripting language as the final result. The output/meta-output
distinction permits perspicuous treatment of diverse tasks such as resolving
pronouns, correcting user misconceptions, and optimizing scripts.
</summary>
    <author>
      <name>Manny Rayner</name>
    </author>
    <author>
      <name>Beth Ann Hockey</name>
    </author>
    <author>
      <name>Frankie James</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Language Technology Joint Conference ANLP-NAACL 2000. 29 April - 4
  May 2000, Seattle, WA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006021v1</id>
    <updated>2000-06-09T22:03:10Z</updated>
    <published>2000-06-09T22:03:10Z</published>
    <title>Compiling Language Models from a Linguistically Motivated Unification
  Grammar</title>
    <summary>  Systems now exist which are able to compile unification grammars into
language models that can be included in a speech recognizer, but it is so far
unclear whether non-trivial linguistically principled grammars can be used for
this purpose. We describe a series of experiments which investigate the
question empirically, by incrementally constructing a grammar and discovering
what problems emerge when successively larger versions are compiled into finite
state graph representations and used as language models for a medium-vocabulary
recognition task.
</summary>
    <author>
      <name>Manny Rayner</name>
    </author>
    <author>
      <name>Beth Ann Hockey</name>
    </author>
    <author>
      <name>Frankie James</name>
    </author>
    <author>
      <name>Elizabeth O. Bratt</name>
    </author>
    <author>
      <name>Sharon Goldwater</name>
    </author>
    <author>
      <name>Mark Gawron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in COLING 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006038v1</id>
    <updated>2000-06-28T10:06:02Z</updated>
    <published>2000-06-28T10:06:02Z</published>
    <title>Approximation and Exactness in Finite State Optimality Theory</title>
    <summary>  Previous work (Frank and Satta 1998; Karttunen, 1998) has shown that
Optimality Theory with gradient constraints generally is not finite state. A
new finite-state treatment of gradient constraints is presented which improves
upon the approximation of Karttunen (1998). The method turns out to be exact,
and very compact, for the syllabification analysis of Prince and Smolensky
(1993).
</summary>
    <author>
      <name>Dale Gerdemann</name>
    </author>
    <author>
      <name>Gertjan van Noord</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, Finite-State Phonology : SIGPHON 2000, Fifth
  Meeting of the ACL Special Interest Group in Computational Phonology, COLING
  2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006042v1</id>
    <updated>2000-06-29T09:17:45Z</updated>
    <published>2000-06-29T09:17:45Z</published>
    <title>Semantic Parsing based on Verbal Subcategorization</title>
    <summary>  The aim of this work is to explore new methodologies on Semantic Parsing for
unrestricted texts. Our approach follows the current trends in Information
Extraction (IE) and is based on the application of a verbal subcategorization
lexicon (LEXPIR) by means of complex pattern recognition techniques. LEXPIR is
framed on the theoretical model of the verbal subcategorization developed in
the Pirapides project.
</summary>
    <author>
      <name>Jordi Atserias</name>
    </author>
    <author>
      <name>Irene Castellon</name>
    </author>
    <author>
      <name>Montse Civit</name>
    </author>
    <author>
      <name>German Rigau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, extended version of the paper. Spanish version of the paper
  also available from authors home page</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Intelligence text Processing and Computational
  Linguistics, CICLing 2000. pg 330-340</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7;I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006044v1</id>
    <updated>2000-06-30T13:22:33Z</updated>
    <published>2000-06-30T13:22:33Z</published>
    <title>Finite-State Non-Concatenative Morphotactics</title>
    <summary>  Finite-state morphology in the general tradition of the Two-Level and Xerox
implementations has proved very successful in the production of robust
morphological analyzer-generators, including many large-scale commercial
systems. However, it has long been recognized that these implementations have
serious limitations in handling non-concatenative phenomena. We describe a new
technique for constructing finite-state transducers that involves reapplying
the regular-expression compiler to its own output. Implemented in an algorithm
called compile-replace, this technique has proved useful for handling
non-concatenative phenomena; and we demonstrate it on Malay full-stem
reduplication and Arabic stem interdigitation.
</summary>
    <author>
      <name>Kenneth R. Beesley</name>
    </author>
    <author>
      <name>Lauri Karttunen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGPHON-2000, Proceedings of the Fifth Workshop of the ACL Special
  Interest Group in Computational Phonology, p. 1-12. Aug. 6, 2000. Luxembourg</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A0;F1.1;J5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007009v1</id>
    <updated>2000-07-06T14:15:26Z</updated>
    <published>2000-07-06T14:15:26Z</published>
    <title>Incremental construction of minimal acyclic finite-state automata</title>
    <summary>  In this paper, we describe a new method for constructing minimal,
deterministic, acyclic finite-state automata from a set of strings. Traditional
methods consist of two phases: the first to construct a trie, the second one to
minimize it. Our approach is to construct a minimal automaton in a single phase
by adding new strings one by one and minimizing the resulting automaton
on-the-fly. We present a general algorithm as well as a specialization that
relies upon the lexicographical ordering of the input strings.
</summary>
    <author>
      <name>Jan Daciuk</name>
    </author>
    <author>
      <name>Stoyan Mihov</name>
    </author>
    <author>
      <name>Bruce Watson</name>
    </author>
    <author>
      <name>Richard Watson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Linguistics, Vol. 26, Number 1, March 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0007009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007018v1</id>
    <updated>2000-07-13T12:46:00Z</updated>
    <published>2000-07-13T12:46:00Z</published>
    <title>Bootstrapping a Tagged Corpus through Combination of Existing
  Heterogeneous Taggers</title>
    <summary>  This paper describes a new method, Combi-bootstrap, to exploit existing
taggers and lexical resources for the annotation of corpora with new tagsets.
Combi-bootstrap uses existing resources as features for a second level machine
learning module, that is trained to make the mapping to the new tagset on a
very small sample of annotated corpus material. Experiments show that
Combi-bootstrap: i) can integrate a wide variety of existing resources, and ii)
achieves much higher accuracy (up to 44.7 % error reduction) than both the best
single tagger and an ensemble tagger constructed out of the same small training
sample.
</summary>
    <author>
      <name>Jakub Zavrel</name>
    </author>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2nd International Conference on Language
  Resources and Evaluation (LREC 2000), pp. 17--20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0007018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007031v2</id>
    <updated>2000-07-24T04:17:49Z</updated>
    <published>2000-07-21T06:07:31Z</published>
    <title>Parameter-free Model of Rank Polysemantic Distribution</title>
    <summary>  A model of rank polysemantic distribution with a minimal number of fitting
parameters is offered. In an ideal case a parameter-free description of the
dependence on the basis of one or several immediate features of the
distribution is possible.
</summary>
    <author>
      <name>Victor Kromer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, no figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 4th conference of the International
  Quantitative Linguistics Association (QUALICO 2000). Prague, August 24-26,
  2000. P. 21-22.The full version (in Russian) is available in Web Journal
  FCCL. See URL http://fccl.ksu.ru/fcclpap.htm</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0007031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007035v1</id>
    <updated>2000-07-25T17:20:47Z</updated>
    <published>2000-07-25T17:20:47Z</published>
    <title>Mapping WordNets Using Structural Information</title>
    <summary>  We present a robust approach for linking already existing lexical/semantic
hierarchies. We used a constraint satisfaction algorithm (relaxation labeling)
to select --among a set of candidates-- the node in a target taxonomy that
bests matches each node in a source taxonomy. In particular, we use it to map
the nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision
and a very low remaining ambiguity.
</summary>
    <author>
      <name>J. Daude</name>
    </author>
    <author>
      <name>L. Padro</name>
    </author>
    <author>
      <name>G. Rigau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, uses epsfig. To appear in ACL'2000 proceedings</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">38th Anual Meeting of the Association for Computational
  Linguistics (ACL'2000). Hong Kong, October 2000.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0007035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008004v1</id>
    <updated>2000-08-08T22:42:51Z</updated>
    <published>2000-08-08T22:42:51Z</published>
    <title>Comparing two trainable grammatical relations finders</title>
    <summary>  Grammatical relationships (GRs) form an important level of natural language
processing, but different sets of GRs are useful for different purposes.
Therefore, one may often only have time to obtain a small training corpus with
the desired GR annotations. On such a small training corpus, we compare two
systems. They use different learning techniques, but we find that this
difference by itself only has a minor effect. A larger factor is that in
English, a different GR length measure appears better suited for finding simple
argument GRs than for finding modifier GRs. We also find that partitioning the
data may help memory-based learning.
</summary>
    <author>
      <name>Alexander Yeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, uses colacl.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">18th International Conference on Computational Linguistics (COLING
  2000), pages 1146-1150, Saarbruecken, Germany, July, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008005v1</id>
    <updated>2000-08-08T23:52:02Z</updated>
    <published>2000-08-08T23:52:02Z</published>
    <title>More accurate tests for the statistical significance of result
  differences</title>
    <summary>  Statistical significance testing of differences in values of metrics like
recall, precision and balanced F-score is a necessary part of empirical natural
language processing. Unfortunately, we find in a set of experiments that many
commonly used tests often underestimate the significance and so are less likely
to detect differences that exist between different techniques. This
underestimation comes from an independence assumption that is often violated.
We point out some useful tests that do not make this assumption, including
computationally-intensive randomization tests.
</summary>
    <author>
      <name>Alexander Yeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, uses colacl.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">18th International Conference on Computational Linguistics (COLING
  2000), pages 947-953, Saarbruecken, Germany, July, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008012v1</id>
    <updated>2000-08-17T13:13:42Z</updated>
    <published>2000-08-17T13:13:42Z</published>
    <title>Applying System Combination to Base Noun Phrase Identification</title>
    <summary>  We use seven machine learning algorithms for one task: identifying base noun
phrases. The results have been processed by different system combination
methods and all of these outperformed the best individual result. We have
applied the seven learners with the best combinator, a majority vote of the top
five systems, to a standard data set and managed to improve the best published
result for this data set.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <author>
      <name>Herve Dejean</name>
    </author>
    <author>
      <name>Rob Koeling</name>
    </author>
    <author>
      <name>Yuval Krymolowski</name>
    </author>
    <author>
      <name>Vasin Punyakanok</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING 2000, Saarbruecken, Germany</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008017v1</id>
    <updated>2000-08-21T19:27:18Z</updated>
    <published>2000-08-21T19:27:18Z</published>
    <title>Efficient probabilistic top-down and left-corner parsing</title>
    <summary>  This paper examines efficient predictive broad-coverage parsing without
dynamic programming. In contrast to bottom-up methods, depth-first top-down
parsing produces partial parses that are fully connected trees spanning the
entire left context, from which any kind of non-local dependency or partial
semantic interpretation can in principle be read. We contrast two predictive
parsing approaches, top-down and left-corner parsing, and find both to be
viable. In addition, we find that enhancement with non-local information not
only improves parser accuracy, but also substantially improves the search
efficiency.
</summary>
    <author>
      <name>Brian Roark</name>
    </author>
    <author>
      <name>Mark Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 tables, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 37th Annual Meeting of the Association for
  Computational Linguistics, 1999, pages 421-428</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008021v1</id>
    <updated>2000-08-22T15:16:22Z</updated>
    <published>2000-08-22T15:16:22Z</published>
    <title>Compact non-left-recursive grammars using the selective left-corner
  transform and factoring</title>
    <summary>  The left-corner transform removes left-recursion from (probabilistic)
context-free grammars and unification grammars, permitting simple top-down
parsing techniques to be used. Unfortunately the grammars produced by the
standard left-corner transform are usually much larger than the original. The
selective left-corner transform described in this paper produces a transformed
grammar which simulates left-corner recognition of a user-specified set of the
original productions, and top-down recognition of the others. Combined with two
factorizations, it produces non-left-recursive grammars that are not much
larger than the original.
</summary>
    <author>
      <name>Mark Johnson</name>
    </author>
    <author>
      <name>Brian Roark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 tables, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Computational
  Linguistics (COLING), 2000, pages 355-361</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008028v1</id>
    <updated>2000-08-25T17:23:07Z</updated>
    <published>2000-08-25T17:23:07Z</published>
    <title>Estimators for Stochastic ``Unification-Based'' Grammars</title>
    <summary>  Log-linear models provide a statistically sound framework for Stochastic
``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds
of grammars. We describe two computationally-tractable ways of estimating the
parameters of such grammars from a training corpus of syntactic analyses, and
apply these to estimate a stochastic version of Lexical-Functional Grammar.
</summary>
    <author>
      <name>Mark Johnson</name>
    </author>
    <author>
      <name>Stuart Geman</name>
    </author>
    <author>
      <name>Stephen Canon</name>
    </author>
    <author>
      <name>Zhiyi Chi</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc 37th Annual Conference of the Association for Computational
  Linguistics, 1999, pages 535-541</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008029v1</id>
    <updated>2000-08-25T17:31:53Z</updated>
    <published>2000-08-25T17:31:53Z</published>
    <title>Exploiting auxiliary distributions in stochastic unification-based
  grammars</title>
    <summary>  This paper describes a method for estimating conditional probability
distributions over the parses of ``unification-based'' grammars which can
utilize auxiliary distributions that are estimated by other means. We show how
this can be used to incorporate information about lexical selectional
preferences gathered from other sources into Stochastic ``Unification-based''
Grammars (SUBGs). While we apply this estimator to a Stochastic
Lexical-Functional Grammar, the method is general, and should be applicable to
stochastic versions of HPSGs, categorial grammars and transformational
grammars.
</summary>
    <author>
      <name>Mark Johnson</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc 1st NAACL, 2000, pages 154-161</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008030v1</id>
    <updated>2000-08-28T08:10:14Z</updated>
    <published>2000-08-28T08:10:14Z</published>
    <title>Metonymy Interpretation Using X NO Y Examples</title>
    <summary>  We developed on example-based method of metonymy interpretation. One
advantages of this method is that a hand-built database of metonymy is not
necessary because it instead uses examples in the form ``Noun X no Noun Y (Noun
Y of Noun X).'' Another advantage is that we will be able to interpret
newly-coined metonymic sentences by using a new corpus. We experimented with
metonymy interpretation and obtained a precision rate of 66% when using this
method.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Qing Ma</name>
    </author>
    <author>
      <name>Atsumu Yamamoto</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Computation and Language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SNLP2000, Chiang Mai, Thailand, May 10, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008031v1</id>
    <updated>2000-08-28T08:17:18Z</updated>
    <published>2000-08-28T08:17:18Z</published>
    <title>Bunsetsu Identification Using Category-Exclusive Rules</title>
    <summary>  This paper describes two new bunsetsu identification methods using supervised
learning. Since Japanese syntactic analysis is usually done after bunsetsu
identification, bunsetsu identification is important for analyzing Japanese
sentences. In experiments comparing the four previously available
machine-learning methods (decision tree, maximum-entropy method, example-based
approach and decision list) and two new methods using category-exclusive rules,
the new method using the category-exclusive rules with the highest similarity
performed best.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Kiyotaka Uchimoto</name>
    </author>
    <author>
      <name>Qing Ma</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. Computation and Language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">COLING'2000, Saarbrucken, Germany, August, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008033v1</id>
    <updated>2000-08-28T19:51:32Z</updated>
    <published>2000-08-28T19:51:32Z</published>
    <title>Temporal Expressions in Japanese-to-English Machine Translation</title>
    <summary>  This paper describes in outline a method for translating Japanese temporal
expressions into English. We argue that temporal expressions form a special
subset of language that is best handled as a special module in machine
translation. The paper deals with problems of lexical idiosyncrasy as well as
the choice of articles and prepositions within temporal expressions. In
addition temporal expressions are considered as parts of larger structures, and
the question of whether to translate them as noun phrases or adverbials is
addressed.
</summary>
    <author>
      <name>Francis Bond</name>
    </author>
    <author>
      <name>Kentaro Ogura</name>
    </author>
    <author>
      <name>Hajime Uchino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, slightly reformatted to avoid obscure style file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Seventh International Conference on Theoretical and Methodological
  Issues in Machine Translation: TMI-97, Santa Fe, July 1997, pp 55--62</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008034v1</id>
    <updated>2000-08-30T13:14:58Z</updated>
    <published>2000-08-30T13:14:58Z</published>
    <title>Lexicalized Stochastic Modeling of Constraint-Based Grammars using
  Log-Linear Measures and EM Training</title>
    <summary>  We present a new approach to stochastic modeling of constraint-based grammars
that is based on log-linear models and uses EM for estimation from unannotated
data. The techniques are applied to an LFG grammar for German. Evaluation on an
exact match task yields 86% precision for an ambiguity rate of 5.4, and 90%
precision on a subcat frame match for an ambiguity rate of 25. Experimental
comparison to training from a parsebank shows a 10% gain from EM training.
Also, a new class-based grammar lexicalization is presented, showing a 10% gain
over unlexicalized models.
</summary>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <author>
      <name>Detlef Prescher</name>
    </author>
    <author>
      <name>Jonas Kuhn</name>
    </author>
    <author>
      <name>Mark Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, uses acl2000.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 38th Annual Meeting of the ACL, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008035v1</id>
    <updated>2000-08-30T13:24:06Z</updated>
    <published>2000-08-30T13:24:06Z</published>
    <title>Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity
  Resolution</title>
    <summary>  This paper presents the use of probabilistic class-based lexica for
disambiguation in target-word selection. Our method employs minimal but precise
contextual information for disambiguation. That is, only information provided
by the target-verb, enriched by the condensed information of a probabilistic
class-based lexicon, is used. Induction of classes and fine-tuning to verbal
arguments is done in an unsupervised manner by EM-based clustering techniques.
The method shows promising results in an evaluation on real-world translations.
</summary>
    <author>
      <name>Detlef Prescher</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <author>
      <name>Mats Rooth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, uses colacl.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th COLING, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6, I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009003v1</id>
    <updated>2000-09-08T15:48:53Z</updated>
    <published>2000-09-08T15:48:53Z</published>
    <title>Automatic Extraction of Subcategorization Frames for Czech</title>
    <summary>  We present some novel machine learning techniques for the identification of
subcategorization information for verbs in Czech. We compare three different
statistical techniques applied to this problem. We show how the learning
algorithm can be used to discover previously unknown subcategorization frames
from the Czech Prague Dependency Treebank. The algorithm can then be used to
label dependents of a verb in the Czech treebank as either arguments or
adjuncts. Using our techniques, we ar able to achieve 88% precision on unseen
parsed text.
</summary>
    <author>
      <name>Anoop Sarkar</name>
    </author>
    <author>
      <name>Daniel Zeman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. Another version under the name "Learning Verb
  Subcategorization from Corpora: Counting Frame Subsets", authors: Zeman,
  Sarkar, in proceedings of LREC 2000, Athens, Greece</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Computational
  Linguistics (Coling 2000), Universit</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7, G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009011v1</id>
    <updated>2000-09-19T00:44:47Z</updated>
    <published>2000-09-19T00:44:47Z</published>
    <title>Anaphora Resolution in Japanese Sentences Using Surface Expressions and
  Examples</title>
    <summary>  Anaphora resolution is one of the major problems in natural language
processing. It is also one of the important tasks in machine translation and
man/machine dialogue. We solve the problem by using surface expressions and
examples. Surface expressions are the words in sentences which provide clues
for anaphora resolution. Examples are linguistic data which are actually used
in conversations and texts. The method using surface expressions and examples
is a practical method. This thesis handles almost all kinds of anaphora: i. The
referential property and number of a noun phrase ii. Noun phrase direct
anaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase
ellipsis
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">156 pages. Doctoral thesis in Kyoto University, December 1996,
  supervised by M. Nagao</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0009011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009015v1</id>
    <updated>2000-09-20T13:23:17Z</updated>
    <published>2000-09-20T13:23:17Z</published>
    <title>A Tableaux Calculus for Ambiguous Quantification</title>
    <summary>  Coping with ambiguity has recently received a lot of attention in natural
language processing. Most work focuses on the semantic representation of
ambiguous expressions. In this paper we complement this work in two ways.
First, we provide an entailment relation for a language with ambiguous
expressions. Second, we give a sound and complete tableaux calculus for
reasoning with statements involving ambiguous quantification. The calculus
interleaves partial disambiguation steps with steps in a traditional deductive
process, so as to minimize and postpone branching in the proof process, and
thereby increases its efficiency.
</summary>
    <author>
      <name>Christof Monz</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In: H. de Swart (editor). Automated Reasoning with Analytic Tableaux
  and Related Methods, Tableaux'98 LNAI 1397, Springer, 1998, pp. 232-246</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0009015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1 I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009017v1</id>
    <updated>2000-09-21T14:49:19Z</updated>
    <published>2000-09-21T14:49:19Z</published>
    <title>A Tableau Calculus for Pronoun Resolution</title>
    <summary>  We present a tableau calculus for reasoning in fragments of natural language.
We focus on the problem of pronoun resolution and the way in which it
complicates automated theorem proving for natural language processing. A method
for explicitly manipulating contextual information during deduction is
proposed, where pronouns are resolved against this context during deduction. As
a result, pronoun resolution and deduction can be interleaved in such a way
that pronouns are only resolved if this is licensed by a deduction rule; this
helps us to avoid the combinatorial complexity of total pronoun disambiguation.
</summary>
    <author>
      <name>Christof Monz</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: N.V. Murray (ed.) Automated Reasoning with Analytic Tableaux
  and Related Methods. Lecture Notes in Artificial Intelligence 1617, Springer,
  1999, pages 247-262</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009018v1</id>
    <updated>2000-09-21T15:21:01Z</updated>
    <published>2000-09-21T15:21:01Z</published>
    <title>A Resolution Calculus for Dynamic Semantics</title>
    <summary>  This paper applies resolution theorem proving to natural language semantics.
The aim is to circumvent the computational complexity triggered by natural
language ambiguities like pronoun binding, by interleaving pronoun binding with
resolution deduction. Therefore disambiguation is only applied to expression
that actually occur during derivations.
</summary>
    <author>
      <name>Christof Monz</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: J. Dix, L. Farinas del Cerro, and U. Furbach (eds.) Logics in
  Artificial Intelligence (JELIA'98). Lecture Notes in Artificial Intelligence
  1489, Springer, 1998, pp. 184-198</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009022v1</id>
    <updated>2000-09-22T15:02:26Z</updated>
    <published>2000-09-22T15:02:26Z</published>
    <title>A Comparison between Supervised Learning Algorithms for Word Sense
  Disambiguation</title>
    <summary>  This paper describes a set of comparative experiments, including cross-corpus
evaluation, between five alternative algorithms for supervised Word Sense
Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW,
Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The
LazyBoosting algorithm outperforms the other four state-of-the-art algorithms
in terms of accuracy and ability to tune to new domains; 2) The domain
dependence of WSD systems seems very strong and suggests that some kind of
adaptation or tuning is required for cross-corpus application.
</summary>
    <author>
      <name>Gerard Escudero</name>
    </author>
    <author>
      <name>Lluis Marquez</name>
    </author>
    <author>
      <name>German Rigau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 4th Conference on Computational Natural
  Language Learning, CoNLL'2000, pp. 31-36</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7;I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009025v1</id>
    <updated>2000-09-27T13:22:54Z</updated>
    <published>2000-09-27T13:22:54Z</published>
    <title>Parsing with the Shortest Derivation</title>
    <summary>  Common wisdom has it that the bias of stochastic grammars in favor of shorter
derivations of a sentence is harmful and should be redressed. We show that the
common wisdom is wrong for stochastic grammars that use elementary trees
instead of context-free rules, such as Stochastic Tree-Substitution Grammars
used by Data-Oriented Parsing models. For such grammars a non-probabilistic
metric based on the shortest derivation outperforms a probabilistic metric on
the ATIS and OVIS corpora, while it obtains very competitive results on the
Wall Street Journal corpus. This paper also contains the first published
experiments with DOP on the Wall Street Journal.
</summary>
    <author>
      <name>Rens Bod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings COLING'2000, with a minor correction</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0009026v1</id>
    <updated>2000-09-27T13:38:31Z</updated>
    <published>2000-09-27T13:38:31Z</published>
    <title>An improved parser for data-oriented lexical-functional analysis</title>
    <summary>  We present an LFG-DOP parser which uses fragments from LFG-annotated
sentences to parse new sentences. Experiments with the Verbmobil and Homecentre
corpora show that (1) Viterbi n best search performs about 100 times faster
than Monte Carlo search while both achieve the same accuracy; (2) the DOP
hypothesis which states that parse accuracy increases with increasing fragment
size is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator
performs worse than a discounted frequency estimator; and (4) LFG-DOP
significantly outperforms Tree-DOP is evaluated on tree structures only.
</summary>
    <author>
      <name>Rens Bod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings ACL'2000, Hong Kong</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0009026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0009026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010014v1</id>
    <updated>2000-10-10T17:33:02Z</updated>
    <published>2000-10-10T17:33:02Z</published>
    <title>On a cepstrum-based speech detector robust to white noise</title>
    <summary>  We study effects of additive white noise on the cepstral representation of
speech signals. Distribution of each individual cepstrum coefficient of speech
is shown to depend strongly on noise and to overlap significantly with the
cepstrum distribution of noise. Based on these studies, we suggest a scalar
quantity, V, equal to the sum of weighted cepstral coefficients, which is able
to classify frames containing speech against noise-like frames. The
distributions of V for speech and noise frames are reasonably well separated
above SNR = 5 dB, demonstrating the feasibility of robust speech detector based
on V.
</summary>
    <author>
      <name>Sergei Skorik</name>
    </author>
    <author>
      <name>Frederic Berthommier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages pdf format, requires Acrobat Reader v 4.0 or later</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0010014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.1; I.2.10; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010020v1</id>
    <updated>2000-10-11T22:30:09Z</updated>
    <published>2000-10-11T22:30:09Z</published>
    <title>Using existing systems to supplement small amounts of annotated
  grammatical relations training data</title>
    <summary>  Grammatical relationships (GRs) form an important level of natural language
processing, but different sets of GRs are useful for different purposes.
Therefore, one may often only have time to obtain a small training corpus with
the desired GR annotations. To boost the performance from using such a small
training corpus on a transformation rule learner, we use existing systems that
find related types of annotations.
</summary>
    <author>
      <name>Alexander Yeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, uses acl2000.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">38th Annual Meeting of the Association for Computational
  Linguistics (ACL-2000), pages 126-132, Hong Kong, October, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0010020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010030v1</id>
    <updated>2000-10-23T15:14:02Z</updated>
    <published>2000-10-23T15:14:02Z</published>
    <title>Reduction of Intermediate Alphabets in Finite-State Transducer Cascades</title>
    <summary>  This article describes an algorithm for reducing the intermediate alphabets
in cascades of finite-state transducers (FSTs). Although the method modifies
the component FSTs, there is no change in the overall relation described by the
whole cascade. No additional information or special algorithm, that could
decelerate the processing of input, is required at runtime. Two examples from
Natural Language Processing are used to illustrate the effect of the algorithm
on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs
and symbols shrank considerably.
</summary>
    <author>
      <name>Andre Kempe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, LaTeX (+ eps)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. TALN 2000, pp. 207-215, Lausanne, Switzerland. October 16-18</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0010030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011001v1</id>
    <updated>2000-11-02T09:02:12Z</updated>
    <published>2000-11-02T09:02:12Z</published>
    <title>Utilizing the World Wide Web as an Encyclopedia: Extracting Term
  Descriptions from Semi-Structured Texts</title>
    <summary>  In this paper, we propose a method to extract descriptions of technical terms
from Web pages in order to utilize the World Wide Web as an encyclopedia. We
use linguistic patterns and HTML text structures to extract text fragments
containing term descriptions. We also use a language model to discard
extraneous descriptions, and a clustering method to summarize resultant
descriptions. We show the effectiveness of our method by way of experiments.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 Postscript figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 38th Annual Meeting of the Association for
  Computational Linguistics (ACL-2000), pp.488-495, Oct. 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0011001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011002v1</id>
    <updated>2000-11-02T10:00:27Z</updated>
    <published>2000-11-02T10:00:27Z</published>
    <title>A Novelty-based Evaluation Method for Information Retrieval</title>
    <summary>  In information retrieval research, precision and recall have long been used
to evaluate IR systems. However, given that a number of retrieval systems
resembling one another are already available to the public, it is valuable to
retrieve novel relevant documents, i.e., documents that cannot be retrieved by
those existing systems. In view of this problem, we propose an evaluation
method that favors systems retrieving as many novel documents as possible. We
also used our method to evaluate systems that participated in the IREX
workshop.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2nd International Conference on Language
  Resources and Evaluation (LREC-2000), pp.1637-1641, Jun. 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0011002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011007v1</id>
    <updated>2000-11-06T13:56:42Z</updated>
    <published>2000-11-06T13:56:42Z</published>
    <title>Tree-gram Parsing: Lexical Dependencies and Structural Relations</title>
    <summary>  This paper explores the kinds of probabilistic relations that are important
in syntactic disambiguation. It proposes that two widely used kinds of
relations, lexical dependencies and structural relations, have complementary
disambiguation capabilities. It presents a new model based on structural
relations, the Tree-gram model, and reports experiments showing that structural
relations should benefit from enrichment by lexical dependencies.
</summary>
    <author>
      <name>Khalil Sima'an</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Appeared in Proceedings of the 38th Annual Meeting of the
  Association for Computational Linguistics (ACL'00), Hong Kong, China</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0011007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; K.3.2; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011020v2</id>
    <updated>2000-11-30T15:07:33Z</updated>
    <published>2000-11-16T17:40:10Z</published>
    <title>The Use of Instrumentation in Grammar Engineering</title>
    <summary>  This paper explores the usefulness of a technique from software engineering,
code instrumentation, for the development of large-scale natural language
grammars. Information about the usage of grammar rules in test and corpus
sentences is used to improve grammar and testsuite, as well as adapting a
grammar to a specific genre. Results show that less than half of a
large-coverage grammar for German is actually tested by two large testsuites,
and that 10--30% of testing time is redundant. This methodology applied can be
seen as a re-use of grammar writing knowledge for testsuite compilation.
</summary>
    <author>
      <name>Norbert Broeker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, LaTeX2e, correction includes bibliography</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">adapted from COLING2000, Saarbruecken/FRG, July31--Aug4 2000,
  pp.118-124</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0011020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011034v1</id>
    <updated>2000-11-22T15:35:46Z</updated>
    <published>2000-11-22T15:35:46Z</published>
    <title>Semantic interpretation of temporal information by abductive inference</title>
    <summary>  Besides temporal information explicitly available in verbs and adjuncts, the
temporal interpretation of a text also depends on general world knowledge and
default assumptions. We will present a theory for describing the relation
between, on the one hand, verbs, their tenses and adjuncts and, on the other,
the eventualities and periods of time they represent and their relative
temporal locations.
  The theory is formulated in logic and is a practical implementation of the
concepts described in Ness Schelkens et al. We will show how an abductive
resolution procedure can be used on this representation to extract temporal
information from texts.
</summary>
    <author>
      <name>Sven Verdoolaege</name>
    </author>
    <author>
      <name>Marc Denecker</name>
    </author>
    <author>
      <name>Ness Schelkens</name>
    </author>
    <author>
      <name>Danny De Schreye</name>
    </author>
    <author>
      <name>Frank Van Eynde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0011034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0102022v2</id>
    <updated>2001-02-23T03:27:18Z</updated>
    <published>2001-02-22T14:10:20Z</published>
    <title>Finite-State Phonology: Proceedings of the 5th Workshop of the ACL
  Special Interest Group in Computational Phonology (SIGPHON)</title>
    <summary>  Home page of the workshop proceedings, with pointers to the individually
archived papers. Includes front matter from the printed version of the
proceedings.
</summary>
    <author>
      <name>Jason Eisner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rochester</arxiv:affiliation>
    </author>
    <author>
      <name>Lauri Karttunen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Xerox Research Centre Europe</arxiv:affiliation>
    </author>
    <author>
      <name>Alain Theriault</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite de Montreal</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HTML page, Conference programme, short abstracts, links to papers,
  preface</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Jason Eisner, Lauri Karttunen and Alain Theriault (eds.),
  Finite-State Phonology: Proceedings of the 5th Workshop of the ACL Special
  Interest Group in Computational Phonology (SIGPHON). Luxembourg, August 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0102022v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0102022v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0102026v1</id>
    <updated>2001-02-24T06:56:43Z</updated>
    <published>2001-02-24T06:56:43Z</published>
    <title>Mathematical Model of Word Length on the Basis of the Cebanov-Fucks
  Distribution with Uniform Parameter Distribution</title>
    <summary>  The data on 13 typologically different languages have been processed using a
two-parameter word length model, based on 1-displaced uniform Poisson
distribution. Statistical dependencies of the 2nd parameter on the 1st one are
revealed for the German texts and genre of letters.
</summary>
    <author>
      <name>Victor Kromer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 table, 2 figures.Submitted to Conference on Informatics
  and Telecommunications, to be held in Sibirian State University of
  Telecommunications (Novosibirsk, Russia) in April, 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Kromer V.W. Matematiceskaja model' dliny slova na osnove
  raspredelenija Cebanova-Fuksa s ravnomernym raspredeleniem parametra //
  Informatika i problemy telekommunikacij: Mezdunarodnaja naucno-techniceskaja
  konferencija (SibGUTI, 26-27 aprelja 2001 g.) Materialy konferencii. -
  Novosibirsk: Isd-vo SibGUTI, 2001. - S. 74-75.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0102026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0102026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103002v1</id>
    <updated>2001-03-02T00:20:01Z</updated>
    <published>2001-03-02T00:20:01Z</published>
    <title>Quantitative Neural Network Model of the Tip-of-the-Tongue Phenomenon
  Based on Synthesized Memory-Psycholinguistic-Metacognitive Approach</title>
    <summary>  A new three-stage computer artificial neural network model of the
tip-of-the-tongue phenomenon is proposed. Each word's node is build from some
interconnected learned auto-associative two-layer neural networks each of which
represents separate word's semantic, lexical, or phonological components. The
model synthesizes memory, psycholinguistic, and metamemory approaches, bridges
speech errors and naming chronometry research traditions, and can explain
quantitatively many tip-of-the-tongue effects.
</summary>
    <author>
      <name>Petro M. Gopych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of The Second International Conference
  Internet-Education-Science-2000 (IES-2000): New Informational and Computer
  Technologies in Education and Science, held on October 10-12, 2000 in
  Vinnytsia, Ukraine, page 273</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103026v1</id>
    <updated>2001-03-29T23:08:33Z</updated>
    <published>2001-03-29T23:08:33Z</published>
    <title>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense</title>
    <summary>  This paper presents a corpus-based approach to word sense disambiguation
where a decision tree assigns a sense to an ambiguous word based on the bigrams
that occur nearby. This approach is evaluated using the sense-tagged corpora
from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate
than the average results reported for 30 of 36 words, and is more accurate than
the best results for 19 of 36 words.
</summary>
    <author>
      <name>Ted Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Second Meeting of the North American Chapter of
  the Association for Computational Linguistics (NAACL-01), June 2-7, 2001,
  Pittsburgh, PA; 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104010v1</id>
    <updated>2001-04-03T23:22:17Z</updated>
    <published>2001-04-03T23:22:17Z</published>
    <title>Type Arithmetics: Computation based on the theory of types</title>
    <summary>  The present paper shows meta-programming turn programming, which is rich
enough to express arbitrary arithmetic computations. We demonstrate a type
system that implements Peano arithmetics, slightly generalized to negative
numbers. Certain types in this system denote numerals. Arithmetic operations on
such types-numerals - addition, subtraction, and even division - are expressed
as type reduction rules executed by a compiler. A remarkable trait is that
division by zero becomes a type error - and reported as such by a compiler.
</summary>
    <author>
      <name>Oleg Kiselyov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 HTML page, 1 C++ source code file</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0104010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.3.3; F.4.2; D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104019v1</id>
    <updated>2001-04-27T22:50:31Z</updated>
    <published>2001-04-27T22:50:31Z</published>
    <title>Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based
  Adaptation</title>
    <summary>  This paper presents a novel method of generating and applying hierarchical,
dynamic topic-based language models. It proposes and evaluates new cluster
generation, hierarchical smoothing and adaptive topic-probability estimation
techniques. These combined models help capture long-distance lexical
dependencies. Experiments on the Broadcast News corpus show significant
improvement in perplexity (10.5% overall and 33.5% on target vocabulary).
</summary>
    <author>
      <name>Radu Florian</name>
    </author>
    <author>
      <name>David Yarowsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 29 figures, presented at ACL99, College Park, Maryland</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 37th Annual Meeting of the ACL, pages 167-174,
  College Park, Maryland</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0104019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105001v1</id>
    <updated>2001-05-02T05:29:27Z</updated>
    <published>2001-05-02T05:29:27Z</published>
    <title>Correction of Errors in a Modality Corpus Used for Machine Translation
  by Using Machine-learning Method</title>
    <summary>  We performed corpus correction on a modality corpus for machine translation
by using such machine-learning methods as the maximum-entropy method. We thus
constructed a high-quality modality corpus based on corpus correction. We
compared several kinds of methods for corpus correction in our experiments and
developed a good method for corpus correction.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Masao Utiyama</name>
    </author>
    <author>
      <name>Kiyotaka Uchimoto</name>
    </author>
    <author>
      <name>Qing Ma</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. Computation and Language. This paper is the English
  translation of our Japanese papar</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0105001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105002v1</id>
    <updated>2001-05-02T07:49:14Z</updated>
    <published>2001-05-02T07:49:14Z</published>
    <title>Man [and Woman] vs. Machine: A Case Study in Base Noun Phrase Learning</title>
    <summary>  A great deal of work has been done demonstrating the ability of machine
learning algorithms to automatically extract linguistic knowledge from
annotated corpora. Very little work has gone into quantifying the difference in
ability at this task between a person and a machine. This paper is a first step
in that direction.
</summary>
    <author>
      <name>Eric Brill</name>
    </author>
    <author>
      <name>Grace Ngai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, presented at ACL 1999</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 37th Annual Meeting of the Association of
  Computational Linguistics, pages 65-72, College Park, MD, USA (1999)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0105002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105003v1</id>
    <updated>2001-05-02T08:39:32Z</updated>
    <published>2001-05-02T08:39:32Z</published>
    <title>Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun
  Phrase Chunking</title>
    <summary>  This paper presents a comprehensive empirical comparison between two
approaches for developing a base noun phrase chunker: human rule writing and
active learning using interactive real-time human annotation. Several novel
variations on active learning are investigated, and underlying cost models for
cross-modal machine learning comparison are presented and explored. Results
show that it is more efficient and more successful by several measures to train
a system using active learning annotation rather than hand-crafted rule writing
at a comparable level of human labor investment.
</summary>
    <author>
      <name>Grace Ngai</name>
    </author>
    <author>
      <name>David Yarowsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, appeared in ACL2000</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 38th Annual Meeting of the Association for
  Computational Linguistics, pages 117-125, Hong Kong (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0105003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105005v1</id>
    <updated>2001-05-04T08:55:02Z</updated>
    <published>2001-05-04T08:55:02Z</published>
    <title>A Complete WordNet1.5 to WordNet1.6 Mapping</title>
    <summary>  We describe a robust approach for linking already existing lexical/semantic
hierarchies. We use a constraint satisfaction algorithm (relaxation labelling)
to select --among a set of candidates-- the node in a target taxonomy that
bests matches each node in a source taxonomy. In this paper we present the
complete mapping of the nominal, verbal, adjectival and adverbial parts of
WordNet 1.5 onto WordNet 1.6.
</summary>
    <author>
      <name>J. Daudé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TALP Research Center, Universitat Politècnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>L. Padró</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TALP Research Center, Universitat Politècnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>G. Rigau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TALP Research Center, Universitat Politècnica de Catalunya</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures. To appear in proceedings of NAACL'01 Workshop on
  WordNet and Other Lexical Resources</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0105005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105012v1</id>
    <updated>2001-05-07T14:25:22Z</updated>
    <published>2001-05-07T14:25:22Z</published>
    <title>Joint and conditional estimation of tagging and parsing models</title>
    <summary>  This paper compares two different ways of estimating statistical language
models. Many statistical NLP tagging and parsing models are estimated by
maximizing the (joint) likelihood of the fully-observed training data. However,
since these applications only require the conditional probability
distributions, these distributions can in principle be learnt by maximizing the
conditional likelihood of the training data. Perhaps somewhat surprisingly,
models estimated by maximizing the joint were superior to models estimated by
maximizing the conditional, even though some of the latter models intuitively
had access to ``more information''.
</summary>
    <author>
      <name>Mark Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Proceedings of the ACL 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0105012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105023v1</id>
    <updated>2001-05-14T09:05:45Z</updated>
    <published>2001-05-14T09:05:45Z</published>
    <title>Generating a 3D Simulation of a Car Accident from a Written Description
  in Natural Language: the CarSim System</title>
    <summary>  This paper describes a prototype system to visualize and animate 3D scenes
from car accident reports, written in French. The problem of generating such a
3D simulation can be divided into two subtasks: the linguistic analysis and the
virtual scene generation. As a means of communication between these two
modules, we first designed a template formalism to represent a written accident
report. The CarSim system first processes written reports, gathers relevant
information, and converts it into a formal description. Then, it creates the
corresponding 3D scene and animates the vehicles.
</summary>
    <author>
      <name>Sylvain Dupuy</name>
    </author>
    <author>
      <name>Arjan Egges</name>
    </author>
    <author>
      <name>Vincent Legendre</name>
    </author>
    <author>
      <name>Pierre Nugues</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ACL 2001, Workshop on Temporal and Spatial Information
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0105023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0105037v1</id>
    <updated>2001-05-31T18:08:57Z</updated>
    <published>2001-05-31T18:08:57Z</published>
    <title>Integrating Prosodic and Lexical Cues for Automatic Topic Segmentation</title>
    <summary>  We present a probabilistic model that uses both prosodic and lexical cues for
the automatic segmentation of speech into topically coherent units. We propose
two methods for combining lexical and prosodic information using hidden Markov
models and decision trees. Lexical information is obtained from a speech
recognizer, and prosodic features are extracted automatically from speech
waveforms. We evaluate our approach on the Broadcast News corpus, using the
DARPA-TDT evaluation metrics. Results show that the prosodic model alone is
competitive with word-based segmentation methods. Furthermore, we achieve a
significant reduction in error by combining the prosodic and word-based
knowledge sources.
</summary>
    <author>
      <name>G. Tur</name>
    </author>
    <author>
      <name>D. Hakkani-Tur</name>
    </author>
    <author>
      <name>A. Stolcke</name>
    </author>
    <author>
      <name>E. Shriberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/089120101300346796</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/089120101300346796" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computation Linguistics 27(1), 31-57, March 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0105037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0105037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106011v1</id>
    <updated>2001-06-07T15:47:31Z</updated>
    <published>2001-06-07T15:47:31Z</published>
    <title>Computational properties of environment-based disambiguation</title>
    <summary>  The standard pipeline approach to semantic processing, in which sentences are
morphologically and syntactically resolved to a single tree before they are
interpreted, is a poor fit for applications such as natural language
interfaces. This is because the environment information, in the form of the
objects and events in the application's run-time environment, cannot be used to
inform parsing decisions unless the input sentence is semantically analyzed,
but this does not occur until after parsing in the single-tree semantic
architecture. This paper describes the computational properties of an
alternative architecture, in which semantic analysis is performed on all
possible interpretations during parsing, in polynomial time.
</summary>
    <author>
      <name>William Schuler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, published in Proceedings of the 39th Annual Meeting of the
  ACL 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106015v1</id>
    <updated>2001-06-10T06:14:09Z</updated>
    <published>2001-06-10T06:14:09Z</published>
    <title>Organizing Encyclopedic Knowledge based on the Web and its Application
  to Question Answering</title>
    <summary>  We propose a method to generate large-scale encyclopedic knowledge, which is
valuable for much NLP research, based on the Web. We first search the Web for
pages containing a term in question. Then we use linguistic patterns and HTML
structures to extract text fragments describing the term. Finally, we organize
extracted term descriptions based on word senses and domains. In addition, we
apply an automatically generated encyclopedia to a question answering system
targeting the Japanese Information-Technology Engineers Examination.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Proceedings of the 39th Annual Meeting of the Association
  for Computational Linguistics (To appear)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 39th Annual Meeting of the Association for
  Computational Linguistics (ACL-EACL 2001), pp.196-203, July. 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0106015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3; H.3.4; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106016v1</id>
    <updated>2001-06-10T14:56:51Z</updated>
    <published>2001-06-10T14:56:51Z</published>
    <title>File mapping Rule-based DBMS and Natural Language Processing</title>
    <summary>  This paper describes the system of storage, extract and processing of
information structured similarly to the natural language. For recursive
inference the system uses the rules having the same representation, as the
data. The environment of storage of information is provided with the File
Mapping (SHM) mechanism of operating system. In the paper the main principles
of construction of dynamic data structure and language for record of the
inference rules are stated; the features of available implementation are
considered and the description of the application realizing semantic
information retrieval on the natural language is given.
</summary>
    <author>
      <name>Vjacheslav M. Novikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106040v1</id>
    <updated>2001-06-19T14:56:02Z</updated>
    <published>2001-06-19T14:56:02Z</published>
    <title>Stacking classifiers for anti-spam filtering of e-mail</title>
    <summary>  We evaluate empirically a scheme for combining classifiers, known as stacked
generalization, in the context of anti-spam filtering, a novel cost-sensitive
application of text categorization. Unsolicited commercial e-mail, or "spam",
floods mailboxes, causing frustration, wasting bandwidth, and exposing minors
to unsuitable content. Using a public corpus, we show that stacking can improve
the efficiency of automatically induced anti-spam filters, and that such
filters can be used in real-life applications.
</summary>
    <author>
      <name>G. Sakkis</name>
    </author>
    <author>
      <name>I. Androutsopoulos</name>
    </author>
    <author>
      <name>G. Paliouras</name>
    </author>
    <author>
      <name>V. Karkaletsis</name>
    </author>
    <author>
      <name>C. D. Spyropoulos</name>
    </author>
    <author>
      <name>P. Stamatopoulos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of "Empirical Methods in Natural Language Processing"
  (EMNLP 2001), L. Lee and D. Harman (Eds.), pp. 44-50, Carnegie Mellon
  University, Pittsburgh, PA, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0106040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.3; I.2.6; I.2.7; I.5.4; K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106043v1</id>
    <updated>2001-06-20T14:16:17Z</updated>
    <published>2001-06-20T14:16:17Z</published>
    <title>Using the Distribution of Performance for Studying Statistical NLP
  Systems and Corpora</title>
    <summary>  Statistical NLP systems are frequently evaluated and compared on the basis of
their performances on a single split of training and test data. Results
obtained using a single split are, however, subject to sampling noise. In this
paper we argue in favour of reporting a distribution of performance figures,
obtained by resampling the training data, rather than a single number. The
additional information from distributions can be used to make statistically
quantified statements about differences across parameter settings, systems, and
corpora.
</summary>
    <author>
      <name>Yuval Krymolowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented in ACL/EACL Workshop on Evaluation for Language and
  Dialogue Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106047v1</id>
    <updated>2001-06-21T20:37:43Z</updated>
    <published>2001-06-21T20:37:43Z</published>
    <title>Modeling informational novelty in a conversational system with a hybrid
  statistical and grammar-based approach to natural language generation</title>
    <summary>  We present a hybrid statistical and grammar-based system for surface natural
language generation (NLG) that uses grammar rules, conditions on using those
grammar rules, and corpus statistics to determine the word order. We also
describe how this surface NLG module is implemented in a prototype
conversational system, and how it attempts to model informational novelty by
varying the word order. Using a combination of rules and statistical
information, the conversational system expresses the novel information
differently than the given information, based on the run-time dialog state. We
also discuss our plans for evaluating the generation strategy.
</summary>
    <author>
      <name>Adwait Ratnaparkhi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the NAACL Workshop on Adaptation in Dialogue
  Systems, June 4, 2001, Pittsburgh, PA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0106047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107005v1</id>
    <updated>2001-07-03T10:27:44Z</updated>
    <published>2001-07-03T10:27:44Z</published>
    <title>The Role of Conceptual Relations in Word Sense Disambiguation</title>
    <summary>  We explore many ways of using conceptual distance measures in Word Sense
Disambiguation, starting with the Agirre-Rigau conceptual density measure. We
use a generalized form of this measure, introducing many (parameterized)
refinements and performing an exhaustive evaluation of all meaningful
combinations. We finally obtain a 42% improvement over the original algorithm,
and show that measures of conceptual distance are not worse indicators for
sense disambiguation than measures based on word-coocurrence (exemplified by
the Lesk algorithm). Our results, however, reinforce the idea that only a
combination of different sources of knowledge might eventually lead to accurate
word sense disambiguation.
</summary>
    <author>
      <name>David Fernandez-Amoros</name>
    </author>
    <author>
      <name>Julio Gonzalo</name>
    </author>
    <author>
      <name>Felisa Verdejo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, published in the proceedings of NLDB'01</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0107005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107019v2</id>
    <updated>2001-07-16T23:21:34Z</updated>
    <published>2001-07-16T22:59:15Z</published>
    <title>Applying Natural Language Generation to Indicative Summarization</title>
    <summary>  The task of creating indicative summaries that help a searcher decide whether
to read a particular document is a difficult task. This paper examines the
indicative summarization task from a generation perspective, by first analyzing
its required content via published guidelines and corpus analysis. We show how
these summaries can be factored into a set of document features, and how an
implemented content planner uses the topicality document feature to create
indicative multidocument query-based summaries.
</summary>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <author>
      <name>Kathleen R. McKeown</name>
    </author>
    <author>
      <name>Judith L. Klavans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, published in Proc. of 8th European Workshop on NLG</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0107019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109010v2</id>
    <updated>2002-09-13T18:13:28Z</updated>
    <published>2001-09-09T16:41:59Z</published>
    <title>Anaphora and Discourse Structure</title>
    <summary>  We argue in this paper that many common adverbial phrases generally taken to
signal a discourse relation between syntactically connected units within
discourse structure, instead work anaphorically to contribute relational
meaning, with only indirect dependence on discourse structure. This allows a
simpler discourse structure to provide scaffolding for compositional semantics,
and reveals multiple ways in which the relational meaning conveyed by adverbial
connectives can interact with that associated with discourse structure. We
conclude by sketching out a lexicalised grammar for discourse that facilitates
discourse interpretation as a product of compositional rules, anaphor
resolution and inference.
</summary>
    <author>
      <name>Bonnie Webber</name>
    </author>
    <author>
      <name>Matthew Stone</name>
    </author>
    <author>
      <name>Aravind Joshi</name>
    </author>
    <author>
      <name>Alistair Knott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 17 figures. Revised resubmission to Computational
  Linguistics</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109013v1</id>
    <updated>2001-09-11T12:17:04Z</updated>
    <published>2001-09-11T12:17:04Z</published>
    <title>Conceptual Analysis of Lexical Taxonomies: The Case of WordNet Top-Level</title>
    <summary>  In this paper we propose an analysis and an upgrade of WordNet's top-level
synset taxonomy. We briefly review WordNet and identify its main semantic
limitations. Some principles from a forthcoming OntoClean methodology are
applied to the ontological analysis of WordNet. A revised top-level taxonomy is
proposed, which is meant to be more conceptually rigorous, cognitively
transparent, and efficiently exploitable in several applications.
</summary>
    <author>
      <name>Aldo Gangemi</name>
    </author>
    <author>
      <name>Nicola Guarino</name>
    </author>
    <author>
      <name>Alessandro Oltramari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 2 tables, submitted to FOIS 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109015v1</id>
    <updated>2001-09-13T14:52:41Z</updated>
    <published>2001-09-13T14:52:41Z</published>
    <title>Boosting Trees for Anti-Spam Email Filtering</title>
    <summary>  This paper describes a set of comparative experiments for the problem of
automatically filtering unwanted electronic mail messages. Several variants of
the AdaBoost algorithm with confidence-rated predictions [Schapire &amp; Singer,
99] have been applied, which differ in the complexity of the base learners
considered. Two main conclusions can be drawn from our experiments: a) The
boosting-based methods clearly outperform the baseline learning algorithms
(Naive Bayes and Induction of Decision Trees) on the PU1 corpus, achieving very
high levels of the F1 measure; b) Increasing the complexity of the base
learners allows to obtain better ``high-precision'' classifiers, which is a
very important issue when misclassification costs are considered.
</summary>
    <author>
      <name>Xavier Carreras</name>
    </author>
    <author>
      <name>Lluis Marquez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of RANLP-2001, pp. 58-64, Bulgaria, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0109015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7;I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109023v1</id>
    <updated>2001-09-17T14:41:14Z</updated>
    <published>2001-09-17T14:41:14Z</published>
    <title>Integrating Multiple Knowledge Sources for Robust Semantic Parsing</title>
    <summary>  This work explores a new robust approach for Semantic Parsing of unrestricted
texts. Our approach considers Semantic Parsing as a Consistent Labelling
Problem (CLP), allowing the integration of several knowledge types (syntactic
and semantic) obtained from different sources (linguistic and statistic). The
current implementation obtains 95% accuracy in model identification and 72% in
case-role filling.
</summary>
    <author>
      <name>Jordi Atserias</name>
    </author>
    <author>
      <name>Lluis Padro</name>
    </author>
    <author>
      <name>German Rigau</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Euroconference on Recent Advances in Natural
  Language Processing (RANLP'01), p.8-14. Tzigov Chark, Bulgaria. Sept. 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0109023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109029v1</id>
    <updated>2001-09-18T14:00:27Z</updated>
    <published>2001-09-18T14:00:27Z</published>
    <title>Learning class-to-class selectional preferences</title>
    <summary>  Selectional preference learning methods have usually focused on word-to-class
relations, e.g., a verb selects as its subject a given nominal class. This
papers extends previous statistical models to class-to-class preferences, and
presents a model that learns selectional preferences for classes of verbs. The
motivation is twofold: different senses of a verb may have different
preferences, and some classes of verbs can share preferences. The model is
tested on a word sense disambiguation task which uses subject-verb and
object-verb relationships extracted from a small sense-disambiguated corpus.
</summary>
    <author>
      <name>E. Agirre</name>
    </author>
    <author>
      <name>D. Martinez</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop "Computational Natural Language
  Learning" (CoNLL-2001). In conjunction with ACL'2001/EACL'2001. Toulouse,
  France. 6-7th July 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0109029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109031v2</id>
    <updated>2001-09-19T10:41:58Z</updated>
    <published>2001-09-18T14:18:58Z</published>
    <title>Enriching WordNet concepts with topic signatures</title>
    <summary>  This paper explores the possibility of enriching the content of existing
ontologies. The overall goal is to overcome the lack of topical links among
concepts in WordNet. Each concept is to be associated to a topic signature,
i.e., a set of related words with associated weights. The signatures can be
automatically constructed from the WWW or from sense-tagged corpora. Both
approaches are compared and evaluated on a word sense disambiguation task. The
results show that it is possible to construct clean signatures from the WWW
using some filtering techniques.
</summary>
    <author>
      <name>Eneko Agirre</name>
    </author>
    <author>
      <name>Olatz Ansa</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <author>
      <name>David Martinez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Author list corrected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the NAACL workshop on WordNet and Other lexical
  Resources: Applications, Extensions and Customizations. Pittsburg, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0109031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109039v1</id>
    <updated>2001-09-20T06:42:11Z</updated>
    <published>2001-09-20T06:42:11Z</published>
    <title>Testing for Mathematical Lineation in Jim Crace's "Quarantine" and T. S.
  Eliot's "Four Quartets"</title>
    <summary>  The mathematical distinction between prose and verse may be detected in
writings that are not apparently lineated, for example in T. S. Eliot's "Burnt
Norton", and Jim Crace's "Quarantine". In this paper we offer comments on
appropriate statistical methods for such work, and also on the nature of formal
innovation in these two texts. Additional remarks are made on the roots of
lineation as a metrical form, and on the prose-verse continuum.
</summary>
    <author>
      <name>John Constable</name>
    </author>
    <author>
      <name>Hideaki Aoyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures in LaTeX2e and EPS formats</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;I.2.7;I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110015v1</id>
    <updated>2001-10-03T18:34:36Z</updated>
    <published>2001-10-03T18:34:36Z</published>
    <title>Richer Syntactic Dependencies for Structured Language Modeling</title>
    <summary>  The paper investigates the use of richer syntactic dependencies in the
structured language model (SLM). We present two simple methods of enriching the
dependencies in the syntactic parse trees used for intializing the SLM. We
evaluate the impact of both methods on the perplexity (PPL) and
word-error-rate(WER, N-best rescoring) performance of the SLM. We show that the
new model achieves an improvement in PPL and WER over the baseline results
reported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)
corpora, respectively.
</summary>
    <author>
      <name>Ciprian Chelba</name>
    </author>
    <author>
      <name>Peng Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ASRU 2001, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110050v1</id>
    <updated>2001-10-24T11:01:08Z</updated>
    <published>2001-10-24T11:01:08Z</published>
    <title>What is the minimal set of fragments that achieves maximal parse
  accuracy?</title>
    <summary>  We aim at finding the minimal set of fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street
Journal treebank show that counts of almost arbitrary fragments within parse
trees are important, leading to improved parse accuracy over previous models
tested on this treebank (a precision of 90.8% and a recall of 90.6%). We
isolate some dependency relations which previous models neglect but which
contribute to higher parse accuracy.
</summary>
    <author>
      <name>Rens Bod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings ACL'2001, Toulouse, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0110050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110051v1</id>
    <updated>2001-10-24T11:30:50Z</updated>
    <published>2001-10-24T11:30:50Z</published>
    <title>Combining semantic and syntactic structure for language modeling</title>
    <summary>  Structured language models for speech recognition have been shown to remedy
the weaknesses of n-gram models. All current structured language models are,
however, limited in that they do not take into account dependencies between
non-headwords. We show that non-headword dependencies contribute to
significantly improved word error rate, and that a data-oriented parsing model
trained on semantically and syntactically annotated data can exploit these
dependencies. This paper also contains the first DOP model trained by means of
a maximum likelihood reestimation procedure, which solves some of the
theoretical shortcomings of previous DOP models.
</summary>
    <author>
      <name>Rens Bod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings ICSLP'2000, Beijing, China</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0110051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112003v1</id>
    <updated>2001-12-05T05:35:07Z</updated>
    <published>2001-12-05T05:35:07Z</published>
    <title>Using a Support-Vector Machine for Japanese-to-English Translation of
  Tense, Aspect, and Modality</title>
    <summary>  This paper describes experiments carried out using a variety of
machine-learning methods, including the k-nearest neighborhood method that was
used in a previous study, for the translation of tense, aspect, and modality.
It was found that the support-vector machine method was the most precise of all
the methods tested.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Kiyotaka Uchimoto</name>
    </author>
    <author>
      <name>Qing Ma</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Computation and Language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL Workshop, the Data-Driven Machine Translation, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0112003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204005v1</id>
    <updated>2002-04-03T17:32:53Z</updated>
    <published>2002-04-03T17:32:53Z</published>
    <title>Creating Annotation Tools with the Annotation Graph Toolkit</title>
    <summary>  The Annotation Graph Toolkit is a collection of software supporting the
development of annotation tools based on the annotation graph model. The
toolkit includes application programming interfaces for manipulating annotation
graph data and for importing data from other formats. There are interfaces for
the scripting languages Tcl and Python, a database interface, specialized
graphical user interfaces for a variety of annotation tasks, and several sample
applications. This paper describes all the toolkit components for the benefit
of would-be application developers.
</summary>
    <author>
      <name>Kazuaki Maeda</name>
    </author>
    <author>
      <name>Steven Bird</name>
    </author>
    <author>
      <name>Xiaoyi Ma</name>
    </author>
    <author>
      <name>Haejoong Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Third International Conference on Language
  Resources and Evaluation, Paris: European Language Resources Association,
  2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.13; H.5.5; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204007v1</id>
    <updated>2002-04-03T18:55:01Z</updated>
    <published>2002-04-03T18:55:01Z</published>
    <title>An Integrated Framework for Treebanks and Multilayer Annotations</title>
    <summary>  Treebank formats and associated software tools are proliferating rapidly,
with little consideration for interoperability. We survey a wide variety of
treebank structures and operations, and show how they can be mapped onto the
annotation graph model, and leading to an integrated framework encompassing
tree and non-tree annotations alike. This development opens up new
possibilities for managing and exploiting multilayer annotations.
</summary>
    <author>
      <name>Scott Cotton</name>
    </author>
    <author>
      <name>Steven Bird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Third International Conference on Language
  Resources and Evaluation, Paris: European Language Resources Association,
  2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204022v1</id>
    <updated>2002-04-10T15:37:26Z</updated>
    <published>2002-04-10T15:37:26Z</published>
    <title>Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure
  for Interdisciplinary Education, Research and Development</title>
    <summary>  Annotation graphs and annotation servers offer infrastructure to support the
analysis of human language resources in the form of time-series data such as
text, audio and video. This paper outlines areas of common need among empirical
linguists and computational linguists. After reviewing examples of data and
tools used or under development for each of several areas, it proposes a common
framework for future tool development, data annotation and resource sharing
based upon annotation graphs and servers.
</summary>
    <author>
      <name>Christopher Cieri</name>
    </author>
    <author>
      <name>Steven Bird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACL Workshop on Sharing Tools and Resources for
  Research and Education, Toulouse, July 2001, pp 23-30</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.5.3; H.5.5; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204023v1</id>
    <updated>2002-04-10T15:49:24Z</updated>
    <published>2002-04-10T15:49:24Z</published>
    <title>Computational Phonology</title>
    <summary>  Phonology, as it is practiced, is deeply computational. Phonological analysis
is data-intensive and the resulting models are nothing other than specialized
data structures and algorithms. In the past, phonological computation -
managing data and developing analyses - was done manually with pencil and
paper. Increasingly, with the proliferation of affordable computers, IPA fonts
and drawing software, phonologists are seeking to move their computation work
online. Computational Phonology provides the theoretical and technological
framework for this migration, building on methodologies and tools from
computational linguistics. This piece consists of an apology for computational
phonology, a history, and an overview of current research.
</summary>
    <author>
      <name>Steven Bird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Oxford International Encyclopedia of Linguistics, 2nd Edition,
  2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204049v1</id>
    <updated>2002-04-24T14:48:31Z</updated>
    <published>2002-04-24T14:48:31Z</published>
    <title>Memory-Based Shallow Parsing</title>
    <summary>  We present memory-based learning approaches to shallow parsing and apply
these to five tasks: base noun phrase identification, arbitrary base phrase
recognition, clause detection, noun phrase parsing and full parsing. We use
feature selection techniques and system combination methods for improving the
performance of the memory-based learner. Our approach is evaluated on standard
data sets and the results are compared with that of other systems. This reveals
that our approach works well for base phrase identification while its
application towards recognizing embedded structures leaves some room for
improvement.
</summary>
    <author>
      <name>Erik F. Tjong Kim Sang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research, volume 2 (March), 2002, pp.
  559-594</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205028v1</id>
    <updated>2002-05-17T12:51:00Z</updated>
    <published>2002-05-17T12:51:00Z</published>
    <title>NLTK: The Natural Language Toolkit</title>
    <summary>  NLTK, the Natural Language Toolkit, is a suite of open source program
modules, tutorials and problem sets, providing ready-to-use computational
linguistics courseware. NLTK covers symbolic and statistical natural language
processing, and is interfaced to annotated corpora. Students augment and
replace existing components, learn structured programming by example, and
manipulate sophisticated models from the outset.
</summary>
    <author>
      <name>Edward Loper</name>
    </author>
    <author>
      <name>Steven Bird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, Proceedings of the ACL Workshop on Effective Tools
  and Methodologies for Teaching Natural Language Processing and Computational
  Linguistics, Philadelphia, July 2002, Association for Computational
  Linguistics</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.6; I.2.7; J.5; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205057v1</id>
    <updated>2002-05-21T14:37:22Z</updated>
    <published>2002-05-21T14:37:22Z</published>
    <title>Unsupervised Discovery of Morphemes</title>
    <summary>  We present two methods for unsupervised segmentation of words into
morpheme-like units. The model utilized is especially suited for languages with
a rich morphology, such as Finnish. The first method is based on the Minimum
Description Length (MDL) principle and works online. In the second method,
Maximum Likelihood (ML) optimization is used. The quality of the segmentations
is measured using an evaluation method that compares the segmentations produced
to an existing morphological analysis. Experiments on both Finnish and English
corpora show that the presented methods perform well compared to a current
state-of-the-art system.
</summary>
    <author>
      <name>Mathias Creutz</name>
    </author>
    <author>
      <name>Krista Lagus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, to appear in Proceedings of Morphological and Phonological
  Learning Workshop of ACL'02</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205069v1</id>
    <updated>2002-05-27T18:57:11Z</updated>
    <published>2002-05-27T18:57:11Z</published>
    <title>Machine Learning with Lexical Features: The Duluth Approach to
  Senseval-2</title>
    <summary>  This paper describes the sixteen Duluth entries in the Senseval-2 comparative
exercise among word sense disambiguation systems. There were eight pairs of
Duluth systems entered in the Spanish and English lexical sample tasks. These
are all based on standard machine learning algorithms that induce classifiers
from sense-tagged training text where the context in which ambiguous words
occur are represented by simple lexical features. These are highly portable,
robust methods that can serve as a foundation for more tailored approaches.
</summary>
    <author>
      <name>Ted Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the Proceedings of SENSEVAL-2: Second International
  Workshop on Evaluating Word Sense Disambiguation Systems July 5-6, 2001,
  Toulouse, France</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205070v1</id>
    <updated>2002-05-28T02:01:55Z</updated>
    <published>2002-05-28T02:01:55Z</published>
    <title>Thumbs up? Sentiment Classification using Machine Learning Techniques</title>
    <summary>  We consider the problem of classifying documents not by topic, but by overall
sentiment, e.g., determining whether a review is positive or negative. Using
movie reviews as data, we find that standard machine learning techniques
definitively outperform human-produced baselines. However, the three machine
learning methods we employed (Naive Bayes, maximum entropy classification, and
support vector machines) do not perform as well on sentiment classification as
on traditional topic-based categorization. We conclude by examining factors
that make the sentiment classification problem more challenging.
</summary>
    <author>
      <name>Bo Pang</name>
    </author>
    <author>
      <name>Lillian Lee</name>
    </author>
    <author>
      <name>Shivakumar Vaithyanathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP-2002</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205072v1</id>
    <updated>2002-05-29T17:48:48Z</updated>
    <published>2002-05-29T17:48:48Z</published>
    <title>Unsupervised Learning of Morphology without Morphemes</title>
    <summary>  The first morphological learner based upon the theory of Whole Word
Morphology Ford et al. (1997) is outlined, and preliminary evaluation results
are presented. The program, Whole Word Morphologizer, takes a POS-tagged
lexicon as input, induces morphological relationships without attempting to
discover or identify morphemes, and is then able to generate new words beyond
the learning sample. The accuracy (precision) of the generated new words is as
high as 80% using the pure Whole Word theory, and 92% after a post-hoc
adjustment is added to the routine.
</summary>
    <author>
      <name>Sylvain Neuvel</name>
    </author>
    <author>
      <name>Sean A. Fulop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, to appear in Proceedings of the Workshop on Morphological
  and Phonological Learning 2002, ACL Publications</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206026v1</id>
    <updated>2002-06-18T15:11:52Z</updated>
    <published>2002-06-18T15:11:52Z</published>
    <title>Interleaved semantic interpretation in environment-based parsing</title>
    <summary>  This paper extends a polynomial-time parsing algorithm that resolves
structural ambiguity in input to a speech-based user interface by calculating
and comparing the denotations of rival constituents, given some model of the
interfaced application environment (Schuler 2001). The algorithm is extended to
incorporate a full set of logical operators, including quantifiers and
conjunctions, into this calculation without increasing the complexity of the
overall algorithm beyond polynomial time, both in terms of the length of the
input and the number of entities in the environment model.
</summary>
    <author>
      <name>William Schuler</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 19th International Conference on Computational
  Linguistics (COLING 2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0206026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206034v1</id>
    <updated>2002-06-24T07:46:06Z</updated>
    <published>2002-06-24T07:46:06Z</published>
    <title>Applying a Hybrid Query Translation Method to Japanese/English
  Cross-Language Patent Retrieval</title>
    <summary>  This paper applies an existing query translation method to cross-language
patent retrieval. In our method, multiple dictionaries are used to derive all
possible translations for an input query, and collocational statistics are used
to resolve translation ambiguity. We used Japanese/English parallel patent
abstracts to perform comparative experiments, where our method outperformed a
simple dictionary-based query translation method, and achieved 76% of
monolingual retrieval in terms of average precision.
</summary>
    <author>
      <name>Masatoshi Fukui</name>
    </author>
    <author>
      <name>Shigeto Higuchi</name>
    </author>
    <author>
      <name>Youichi Nakatani</name>
    </author>
    <author>
      <name>Masao Tanaka</name>
    </author>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGIR 2000 Workshop on Patent Retrieval, July, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0206034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206035v1</id>
    <updated>2002-06-24T08:00:45Z</updated>
    <published>2002-06-24T08:00:45Z</published>
    <title>PRIME: A System for Multi-lingual Patent Retrieval</title>
    <summary>  Given the growing number of patents filed in multiple countries, users are
interested in retrieving patents across languages. We propose a multi-lingual
patent retrieval system, which translates a user query into the target
language, searches a multilingual database for patents relevant to the query,
and improves the browsing efficiency by way of machine translation and
clustering. Our system also extracts new translations from patent families
consisting of comparable patents, to enhance the translation dictionary.
</summary>
    <author>
      <name>Shigeto Higuchi</name>
    </author>
    <author>
      <name>Masatoshi Fukui</name>
    </author>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of MT Summit VIII, pp.163-167, Sep. 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0206035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206036v1</id>
    <updated>2002-06-24T08:38:36Z</updated>
    <published>2002-06-24T08:38:36Z</published>
    <title>Language Modeling for Multi-Domain Speech-Driven Text Retrieval</title>
    <summary>  We report experimental results associated with speech-driven text retrieval,
which facilitates retrieving information in multiple domains with spoken
queries. Since users speak contents related to a target collection, we produce
language models used for speech recognition based on the target collection, so
as to improve both the recognition and retrieval accuracy. Experiments using
existing test collections combined with dictated queries showed the
effectiveness of our method.
</summary>
    <author>
      <name>Katunobu Itou</name>
    </author>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASRU.2001.1034653</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASRU.2001.1034653" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Automatic Speech Recognition and Understanding Workshop, Dec.
  2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0206036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206037v1</id>
    <updated>2002-06-24T10:28:02Z</updated>
    <published>2002-06-24T10:28:02Z</published>
    <title>Speech-Driven Text Retrieval: Using Target IR Collections for
  Statistical Language Model Adaptation in Speech Recognition</title>
    <summary>  Speech recognition has of late become a practical technology for real world
applications. Aiming at speech-driven text retrieval, which facilitates
retrieving information with spoken queries, we propose a method to integrate
speech recognition and retrieval methods. Since users speak contents related to
a target collection, we adapt statistical language models used for speech
recognition based on the target collection, so as to improve both the
recognition and retrieval accuracy. Experiments using existing test collections
combined with dictated queries showed the effectiveness of our method.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Katunobu Itou</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Anni R. Coden and Eric W. Brown and Savitha Srinivasan (Eds.),
  Information Retrieval Techniques for Speech Applications (LNCS 2273),
  pp.94-104, Springer, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0206037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208020v1</id>
    <updated>2002-08-13T03:39:20Z</updated>
    <published>2002-08-13T03:39:20Z</published>
    <title>Using the DIFF Command for Natural Language Processing</title>
    <summary>  Diff is a software program that detects differences between two data sets and
is useful in natural language processing. This paper shows several examples of
the application of diff. They include the detection of differences between two
different datasets, extraction of rewriting rules, merging of two different
datasets, and the optimal matching of two different data sets. Since diff comes
with any standard UNIX system, it is readily available and very easy to use.
Our studies showed that diff is a practical tool for research into natural
language processing.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. Computation and Language. This paper is the rough English
  translation of our Japanese papar</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0208020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208035v1</id>
    <updated>2002-08-21T14:09:48Z</updated>
    <published>2002-08-21T14:09:48Z</published>
    <title>Evaluation of Coreference Rules on Complex Narrative Texts</title>
    <summary>  This article studies the problem of assessing relevance to each of the rules
of a reference resolution system. The reference solver described here stems
from a formal model of reference and is integrated in a reference processing
workbench. Evaluation of the reference resolution is essential, as it enables
differential evaluation of individual rules. Numerical values of these measures
are given, and discussed, for simple selection rules and other processing
rules; such measures are then studied for numerical parameters.
</summary>
    <author>
      <name>Andrei Popescu-Belis</name>
    </author>
    <author>
      <name>Isabelle Robba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of DAARC2 (Discourse Anaphora and Anaphor Resolution
  Colloquium), Lancaster, UK, 1998, p.178-185</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208036v1</id>
    <updated>2002-08-21T14:28:51Z</updated>
    <published>2002-08-21T14:28:51Z</published>
    <title>Three New Methods for Evaluating Reference Resolution</title>
    <summary>  Reference resolution on extended texts (several thousand references) cannot
be evaluated manually. An evaluation algorithm has been proposed for the MUC
tests, using equivalence classes for the coreference relation. However, we show
here that this algorithm is too indulgent, yielding good scores even for poor
resolution strategies. We elaborate on the same formalism to propose two new
evaluation algorithms, comparing them first with the MUC algorithm and giving
then results on a variety of examples. A third algorithm using only
distributional comparison of equivalence classes is finally described; it
assesses the relative importance of the recall vs. precision errors.
</summary>
    <author>
      <name>Andrei Popescu-Belis</name>
    </author>
    <author>
      <name>Isabelle Robba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the LREC'98 Workshop on Linguistic Coreference,
  Madrid, Spain, 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208038v1</id>
    <updated>2002-08-21T14:43:18Z</updated>
    <published>2002-08-21T14:43:18Z</published>
    <title>Reference Resolution Beyond Coreference: a Conceptual Frame and its
  Application</title>
    <summary>  A model for reference use in communication is proposed, from a
representationist point of view. Both the sender and the receiver of a message
handle representations of their common environment, including mental
representations of objects. Reference resolution by a computer is viewed as the
construction of object representations using referring expressions from the
discourse, whereas often only coreference links between such expressions are
looked for. Differences between these two approaches are discussed. The model
has been implemented with elementary rules, and tested on complex narrative
texts (hundreds to thousands of referring expressions). The results support the
mental representations paradigm.
</summary>
    <author>
      <name>Andrei Popescu-Belis</name>
    </author>
    <author>
      <name>Isabelle Robba</name>
    </author>
    <author>
      <name>Gerard Sabah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING-ACL'98, Montreal, Canada, 1998, p.1046-1052</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211017v1</id>
    <updated>2002-11-14T16:16:44Z</updated>
    <published>2002-11-14T16:16:44Z</published>
    <title>Probabilistic Parsing Strategies</title>
    <summary>  We present new results on the relation between purely symbolic context-free
parsing strategies and their probabilistic counter-parts. Such parsing
strategies are seen as constructions of push-down devices from grammars. We
show that preservation of probability distribution is possible under two
conditions, viz. the correct-prefix property and the property of strong
predictiveness. These results generalize existing results in the literature
that were obtained by considering parsing strategies in isolation. From our
general results we also derive negative results on so-called generalized LR
parsing.
</summary>
    <author>
      <name>Mark-Jan Nederhof</name>
    </author>
    <author>
      <name>Giorgio Satta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304006v1</id>
    <updated>2003-04-02T23:02:44Z</updated>
    <published>2003-04-02T23:02:44Z</published>
    <title>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence
  Alignment</title>
    <summary>  We address the text-to-text generation problem of sentence-level paraphrasing
-- a phenomenon distinct from and more difficult than word- or phrase-level
paraphrasing. Our approach applies multiple-sequence alignment to sentences
gathered from unannotated comparable corpora: it learns a set of paraphrasing
patterns represented by word lattice pairs and automatically determines how to
apply these patterns to rewrite new sentences. The results of our evaluation
experiments show that the system derives accurate paraphrases, outperforming
baseline systems.
</summary>
    <author>
      <name>Regina Barzilay</name>
    </author>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of HLT-NAACL 2003 (Human Language Technology Conference)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0304006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304027v1</id>
    <updated>2003-04-21T22:10:21Z</updated>
    <published>2003-04-21T22:10:21Z</published>
    <title>"I'm sorry Dave, I'm afraid I can't do that": Linguistics, Statistics,
  and Natural Language Processing circa 2001</title>
    <summary>  A brief, general-audience overview of the history of natural language
processing, focusing on data-driven approaches.Topics include "Ambiguity and
language analysis", "Firth things first", "A 'C' change", and "The empiricists
strike back".
</summary>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, National Research Council study on the Fundamentals of
  Computer Science. 7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In "Computer Science: Reflections on the Field, Reflections from
  the Field" (report of the National Academies' Study on the Fundamentals of
  Computer Science), pp. 111--118, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0304027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304029v1</id>
    <updated>2003-04-22T13:45:37Z</updated>
    <published>2003-04-22T13:45:37Z</published>
    <title>An XML based Document Suite</title>
    <summary>  We report about the current state of development of a document suite and its
applications. This collection of tools for the flexible and robust processing
of documents in German is based on the use of XML as unifying formalism for
encoding input and output data as well as process information. It is organized
in modules with limited responsibilities that can easily be combined into
pipelines to solve complex tasks. Strong emphasis is laid on a number of
techniques to deal with lexical and conceptual gaps that are typical when
starting a new application.
</summary>
    <author>
      <name>Dietmar Roesner</name>
    </author>
    <author>
      <name>Manuela Kunze</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING 2002; p. 1278-1282</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0304029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304035v1</id>
    <updated>2003-04-23T08:02:53Z</updated>
    <published>2003-04-23T08:02:53Z</published>
    <title>Exploiting Sublanguage and Domain Characteristics in a Bootstrapping
  Approach to Lexicon and Ontology Creation</title>
    <summary>  It is very costly to build up lexical resources and domain ontologies.
Especially when confronted with a new application domain lexical gaps and a
poor coverage of domain concepts are a problem for the successful exploitation
of natural language document analysis systems that need and exploit such
knowledge sources. In this paper we report about ongoing experiments with
`bootstrapping techniques' for lexicon and ontology creation.
</summary>
    <author>
      <name>Dietmar Roesner</name>
    </author>
    <author>
      <name>Manuela Kunze</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop-Proceedings of the OntoLex 2002 - Ontologies and Lexical
  Knowledge Bases at the LREC 2002, p. 68-73</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0304035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306022v1</id>
    <updated>2003-06-04T23:08:03Z</updated>
    <published>2003-06-04T23:08:03Z</published>
    <title>Techniques for effective vocabulary selection</title>
    <summary>  The vocabulary of a continuous speech recognition (CSR) system is a
significant factor in determining its performance. In this paper, we present
three principled approaches to select the target vocabulary for a particular
domain by trading off between the target out-of-vocabulary (OOV) rate and
vocabulary size. We evaluate these approaches against an ad-hoc baseline
strategy. Results are presented in the form of OOV rate graphs plotted against
increasing vocabulary size for each technique.
</summary>
    <author>
      <name>Anand Venkataraman</name>
    </author>
    <author>
      <name>Wen Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. To appear Proc. Eurospeech 2003, Geneva</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306039v1</id>
    <updated>2003-06-10T04:40:45Z</updated>
    <published>2003-06-10T04:40:45Z</published>
    <title>Bayesian Information Extraction Network</title>
    <summary>  Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various
aspects of language in one model. Many existing algorithms developed for
learning and inference in DBNs are applicable to probabilistic language
modeling. To demonstrate the potential of DBNs for natural language processing,
we employ a DBN in an information extraction task. We show how to assemble
wealth of emerging linguistic instruments for shallow parsing, syntactic and
semantic tagging, morphological decomposition, named entity recognition etc. in
order to incrementally build a robust information extraction system. Our method
outperforms previously published results on an established benchmark domain.
</summary>
    <author>
      <name>Leonid Peshkin</name>
    </author>
    <author>
      <name>Avi Pfeffer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Intl. Joint Conference on Artificial Intelligence, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0306039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.5.1; I.7.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306062v1</id>
    <updated>2003-06-13T09:05:10Z</updated>
    <published>2003-06-13T09:05:10Z</published>
    <title>Learning to Order Facts for Discourse Planning in Natural Language
  Generation</title>
    <summary>  This paper presents a machine learning approach to discourse planning in
natural language generation. More specifically, we address the problem of
learning the most natural ordering of facts in discourse plans for a specific
domain. We discuss our methodology and how it was instantiated using two
different machine learning algorithms. A quantitative evaluation performed in
the domain of museum exhibit descriptions indicates that our approach performs
significantly better than manually constructed ordering rules. Being
retrainable, the resulting planners can be ported easily to other similar
domains, without requiring language technology expertise.
</summary>
    <author>
      <name>Aggeliki Dimitromanolaki</name>
    </author>
    <author>
      <name>Ion Androutsopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of EACL 2003 Workshop on Natural Language Generation</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0306062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307028v1</id>
    <updated>2003-07-11T14:43:51Z</updated>
    <published>2003-07-11T14:43:51Z</published>
    <title>Issues in Communication Game</title>
    <summary>  As interaction between autonomous agents, communication can be analyzed in
game-theoretic terms. Meaning game is proposed to formalize the core of
intended communication in which the sender sends a message and the receiver
attempts to infer its meaning intended by the sender. Basic issues involved in
the game of natural language communication are discussed, such as salience,
grammaticality, common sense, and common belief, together with some
demonstration of the feasibility of game-theoretic account of language.
</summary>
    <author>
      <name>Koiti Hasida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, Proceedings of the 16th International Conference
  on Computational Linguistics, pp.531-536</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309019v1</id>
    <updated>2003-09-12T12:43:00Z</updated>
    <published>2003-09-12T12:43:00Z</published>
    <title>Building a Test Collection for Speech-Driven Web Retrieval</title>
    <summary>  This paper describes a test collection (benchmark data) for retrieval systems
driven by spoken queries. This collection was produced in the subtask of the
NTCIR-3 Web retrieval task, which was performed in a TREC-style evaluation
workshop. The search topics and document collection for the Web retrieval task
were used to produce spoken queries and language models for speech recognition,
respectively. We used this collection to evaluate the performance of our
retrieval system. Experimental results showed that (a) the use of target
documents for language modeling and (b) enhancement of the vocabulary size in
speech recognition were effective in improving the system performance.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Katunobu Itou</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 8th European Conference on Speech Communication
  and Technology (Eurospeech 2003), pp.1153-1156, Sep. 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0309019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310041v1</id>
    <updated>2003-10-21T18:19:52Z</updated>
    <published>2003-10-21T18:19:52Z</published>
    <title>A Dynamic Programming Algorithm for the Segmentation of Greek Texts</title>
    <summary>  In this paper we introduce a dynamic programming algorithm to perform linear
text segmentation by global minimization of a segmentation cost function which
consists of: (a) within-segment word similarity and (b) prior information about
segment length. The evaluation of the segmentation accuracy of the algorithm on
a text collection consisting of Greek texts showed that the algorithm achieves
high segmentation accuracy and appears to be very innovating and promissing.
</summary>
    <author>
      <name>Pavlina Fragkou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will appear in the Proceedings of the CONSOLE XII
  Conference (Patras, Greece, 2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.3; H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310058v1</id>
    <updated>2003-10-29T20:13:30Z</updated>
    <published>2003-10-29T20:13:30Z</published>
    <title>Application Architecture for Spoken Language Resources in Organisational
  Settings</title>
    <summary>  Special technologies need to be used to take advantage of, and overcome, the
challenges associated with acquiring, transforming, storing, processing, and
distributing spoken language resources in organisations. This paper introduces
an application architecture consisting of tools and supporting utilities for
indexing and transcription, and describes how these tools, together with
downstream processing and distribution systems, can be integrated into a
workflow. Two sample applications for this architecture are outlined- the
analysis of decision-making processes in organisations and the deployment of
systems development methods by designers in the field.
</summary>
    <author>
      <name>Rodney J. Clarke</name>
    </author>
    <author>
      <name>Dali Dong</name>
    </author>
    <author>
      <name>Philip C. Windridge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312050v1</id>
    <updated>2003-12-22T16:23:34Z</updated>
    <published>2003-12-22T16:23:34Z</published>
    <title>A Flexible Pragmatics-driven Language Generator for Animated Agents</title>
    <summary>  This paper describes the NECA MNLG; a fully implemented Multimodal Natural
Language Generation module. The MNLG is deployed as part of the NECA system
which generates dialogues between animated agents. The generation module
supports the seamless integration of full grammar rules, templates and canned
text. The generator takes input which allows for the specification of
syntactic, semantic and pragmatic constraints on the output.
</summary>
    <author>
      <name>Paul Piwek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Research Note Sessions of the 10th Conference
  of the European Chapter of the Association for Computational Linguistics
  (EACL'03), 2003, pp. 151-154</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312052v1</id>
    <updated>2003-12-22T17:07:31Z</updated>
    <published>2003-12-22T17:07:31Z</published>
    <title>Dialogue as Discourse: Controlling Global Properties of Scripted
  Dialogue</title>
    <summary>  This paper explains why scripted dialogue shares some crucial properties with
discourse. In particular, when scripted dialogues are generated by a Natural
Language Generation system, the generator can apply revision strategies that
cannot normally be used when the dialogue results from an interaction between
autonomous agents (i.e., when the dialogue is not scripted). The paper explains
that the relevant revision operators are best applied at the level of a
dialogue plan and discusses how the generator may decide when to apply a given
revision operator.
</summary>
    <author>
      <name>Paul Piwek</name>
    </author>
    <author>
      <name>Kees van Deemter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of AAAI Spring Symposium on Natural Language
  Generation in Spoken and Written Dialogue, Stanford, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312058v1</id>
    <updated>2003-12-25T16:45:20Z</updated>
    <published>2003-12-25T16:45:20Z</published>
    <title>Acquiring Lexical Paraphrases from a Single Corpus</title>
    <summary>  This paper studies the potential of identifying lexical paraphrases within a
single corpus, focusing on the extraction of verb paraphrases. Most previous
approaches detect individual paraphrase instances within a pair (or set) of
comparable corpora, each of them containing roughly the same information, and
rely on the substantial level of correspondence of such corpora. We present a
novel method that successfully detects isolated paraphrase instances within a
single corpus without relying on any a-priori structure and information. A
comparison suggests that an instance-based approach may be combined with a
vector based approach in order to assess better the paraphrase likelihood for
many verb pairs.
</summary>
    <author>
      <name>Oren Glickman</name>
    </author>
    <author>
      <name>Ido Dagan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0312058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312060v1</id>
    <updated>2003-12-27T21:21:48Z</updated>
    <published>2003-12-27T21:21:48Z</published>
    <title>Part-of-Speech Tagging with Minimal Lexicalization</title>
    <summary>  We use a Dynamic Bayesian Network to represent compactly a variety of
sublexical and contextual features relevant to Part-of-Speech (PoS) tagging.
The outcome is a flexible tagger (LegoTag) with state-of-the-art performance
(3.6% error on a benchmark corpus). We explore the effect of eliminating
redundancy and radically reducing the size of feature vocabularies. We find
that a small but linguistically motivated set of suffixes results in improved
cross-corpora generalization. We also show that a minimal lexicon limited to
function words is sufficient to ensure reasonable performance.
</summary>
    <author>
      <name>Virginia Savova</name>
    </author>
    <author>
      <name>Leonid Peshkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages text; 1 figure. To appear in "Current Issues in Linguistic
  Theory: Recent Advances in Natural Language Processing";John Benjamins
  Publishers, Amsterdam</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402055v1</id>
    <updated>2004-02-24T09:34:16Z</updated>
    <published>2004-02-24T09:34:16Z</published>
    <title>Lexical Base as a Compressed Language Model of the World (on the
  material of the Ukrainian language)</title>
    <summary>  In the article the fact is verified that the list of words selected by formal
statistical methods (frequency and functional genre unrestrictedness) is not a
conglomerate of non-related words. It creates a system of interrelated items
and it can be named "lexical base of language". This selected list of words
covers all the spheres of human activities. To verify this statement the
invariant synoptical scheme common for ideographic dictionaries of different
language was determined.
</summary>
    <author>
      <name>Solomiya Buk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/v10057-009-0008-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/v10057-009-0008-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Psychology of Language and Communication. 2009, vol. 13, no. 2,
  pp. 35-44</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0402055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403039v1</id>
    <updated>2004-03-23T21:06:11Z</updated>
    <published>2004-03-23T21:06:11Z</published>
    <title>A Flexible Rule Compiler for Speech Synthesis</title>
    <summary>  We present a flexible rule compiler developed for a text-to-speech (TTS)
system. The compiler converts a set of rules into a finite-state transducer
(FST). The input and output of the FST are subject to parameterization, so that
the system can be applied to strings and sequences of feature-structures. The
resulting transducer is guaranteed to realize a function (as opposed to a
relation), and therefore can be implemented as a deterministic device (either a
deterministic FST or a bimachine).
</summary>
    <author>
      <name>Wojciech Skut</name>
    </author>
    <author>
      <name>Stefan Ulrich</name>
    </author>
    <author>
      <name>Kathrine Hammervold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Klopotek, Mieczyslaw A.; Wierzchon, Slawomir T.; Trojanowski,
  Krzysztof (Eds.): "Intelligent Information Processing and Web Mining -
  Proceedings of the International IIS:IIPWM?04 Conference"; Springer Verlag,
  2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0403039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404007v1</id>
    <updated>2004-04-05T02:14:50Z</updated>
    <published>2004-04-05T02:14:50Z</published>
    <title>Polarity sensitivity and evaluation order in type-logical grammar</title>
    <summary>  We present a novel, type-logical analysis of_polarity sensitivity_: how
negative polarity items (like "any" and "ever") or positive ones (like "some")
are licensed or prohibited. It takes not just scopal relations but also linear
order into account, using the programming-language notions of delimited
continuations and evaluation order, respectively. It thus achieves greater
empirical coverage than previous proposals.
</summary>
    <author>
      <name>Chung-chieh Shan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2004 Human Language Technology Conference of
  the North American Chapter of the Association for Computational Linguistics</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5; D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404025v1</id>
    <updated>2004-04-10T08:43:31Z</updated>
    <published>2004-04-10T08:43:31Z</published>
    <title>Test Collections for Patent-to-Patent Retrieval and Patent Map
  Generation in NTCIR-4 Workshop</title>
    <summary>  This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop,
and the test collections produced in this task. We perform the invalidity
search task, in which each participant group searches a patent collection for
the patents that can invalidate the demand in an existing claim. We also
perform the automatic patent map generation task, in which the patents
associated with a specific topic are organized in a multi-dimensional matrix.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Makoto Iwayama</name>
    </author>
    <author>
      <name>Noriko Kando</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Proceedings of the 4th International Conference on Language
  Resources and Evaluation (to appear)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 4th International Conference on Language
  Resources and Evaluation (LREC-2004), pp.1643-1646, May. 2004.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404041v2</id>
    <updated>2006-02-06T15:05:13Z</updated>
    <published>2004-04-21T06:30:28Z</published>
    <title>NLOMJ--Natural Language Object Model in Java</title>
    <summary>  In this paper we present NLOMJ--a natural language object model in Java with
English as the experiment language. This modal describes the grammar elements
of any permissible expression in a natural language and their complicated
relations with each other with the concept "Object" in OOP(Object Oriented
Programming). Directly mapped to the syntax and semantics of the natural
language, it can be used in information retrieval as a linguistic method.
Around the UML diagram of the NLOMJ the important classes(Sentence, Clause and
Phrase) and their sub classes are introduced and their syntactic and semantic
meanings are explained.
</summary>
    <author>
      <name>Jiyou Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure. Submitted to ICICP04</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; D.1.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405039v1</id>
    <updated>2004-05-12T14:14:52Z</updated>
    <published>2004-05-12T14:14:52Z</published>
    <title>Catching the Drift: Probabilistic Content Models, with Applications to
  Generation and Summarization</title>
    <summary>  We consider the problem of modeling the content structure of texts within a
specific domain, in terms of the topics the texts address and the order in
which these topics appear. We first present an effective knowledge-lean method
for learning content models from un-annotated documents, utilizing a novel
adaptation of algorithms for Hidden Markov Models. We then apply our method to
two complementary tasks: information ordering and extractive summarization. Our
experiments show that incorporating content models in these applications yields
substantial improvement over previously-proposed methods.
</summary>
    <author>
      <name>Regina Barzilay</name>
    </author>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Best paper award</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HLT-NAACL 2004: Proceedings of the Main Conference, pp. 113--120</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407002v1</id>
    <updated>2004-07-01T16:18:52Z</updated>
    <published>2004-07-01T16:18:52Z</published>
    <title>Annotating Predicate-Argument Structure for a Parallel Treebank</title>
    <summary>  We report on a recently initiated project which aims at building a
multi-layered parallel treebank of English and German. Particular attention is
devoted to a dedicated predicate-argument layer which is used for aligning
translationally equivalent sentences of the two languages. We describe both our
conceptual decisions and aspects of their technical realisation. We discuss
some selected problems and conclude with a few remarks on how this project
relates to similar projects in the field.
</summary>
    <author>
      <name>Lea Cyrus</name>
    </author>
    <author>
      <name>Hendrik Feddes</name>
    </author>
    <author>
      <name>Frank Schumacher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the LREC 2004 Workshop on Building Lexical
  Resources from Semantically Annotated Corpora, Lisbon, May 30, 2004, pp.
  39-46</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0407002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407026v1</id>
    <updated>2004-07-10T11:18:42Z</updated>
    <published>2004-07-10T11:18:42Z</published>
    <title>Summarizing Encyclopedic Term Descriptions on the Web</title>
    <summary>  We are developing an automatic method to compile an encyclopedic corpus from
the Web. In our previous work, paragraph-style descriptions for a term are
extracted from Web pages and organized based on domains. However, these
descriptions are independent and do not comprise a condensed text as in
hand-crafted encyclopedias. To resolve this problem, we propose a summarization
method, which produces a single text from multiple descriptions. The resultant
summary concisely describes a term from different viewpoints. We also show the
effectiveness of our method by means of experiments.
</summary>
    <author>
      <name>Atsushi Fujii</name>
    </author>
    <author>
      <name>Tetsuya Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Proceedings of the 20th International Conference on
  Computational Linguistics (to appear)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 20th International Conference on Computational
  Linguistics (COLING 2004), pp.645-651, Aug. 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0407026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408026v1</id>
    <updated>2004-08-10T11:09:48Z</updated>
    <published>2004-08-10T11:09:48Z</published>
    <title>Incremental Construction of Minimal Acyclic Sequential Transducers from
  Unsorted Data</title>
    <summary>  This paper presents an efficient algorithm for the incremental construction
of a minimal acyclic sequential transducer (ST) for a dictionary consisting of
a list of input and output strings. The algorithm generalises a known method of
constructing minimal finite-state automata (Daciuk et al. 2000). Unlike the
algorithm published by Mihov and Maurel (2001), it does not require the input
strings to be sorted. The new method is illustrated by an application to
pronunciation dictionaries.
</summary>
    <author>
      <name>Wojciech Skut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING 2004 (to appear), 7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0408026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408052v1</id>
    <updated>2004-08-22T19:32:48Z</updated>
    <published>2004-08-22T19:32:48Z</published>
    <title>Application of the Double Metaphone Algorithm to Amharic Orthography</title>
    <summary>  The Metaphone algorithm applies the phonetic encoding of orthographic
sequences to simplify words prior to comparison. While Metaphone has been
highly successful for the English language, for which it was designed, it may
not be applied directly to Ethiopian languages. The paper details how the
principles of Metaphone can be applied to Ethiopic script and uses Amharic as a
case study. Match results improve as specific considerations are made for
Amharic writing practices. Results are shown to improve further when common
errors from Amharic input methods are considered.
</summary>
    <author>
      <name>Daniel Yacob</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Ethiopian Studies XV, 13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0408052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408059v1</id>
    <updated>2004-08-26T10:50:45Z</updated>
    <published>2004-08-26T10:50:45Z</published>
    <title>Proofing Tools Technology at Neurosoft S.A.</title>
    <summary>  The aim of this paper is to present the R&amp;D activities carried out at
Neurosoft S.A. regarding the development of proofing tools for Modern Greek.
Firstly, we focus on infrastructure issues that we faced during our initial
steps. Subsequently, we describe the most important insights of three proofing
tools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the
thesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss
some improvement ideas and give our future directions.
</summary>
    <author>
      <name>Ch. Tsalidis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neurosoft S.A</arxiv:affiliation>
    </author>
    <author>
      <name>G. Orphanos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neurosoft S.A</arxiv:affiliation>
    </author>
    <author>
      <name>A. Iordanidou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Patra's University</arxiv:affiliation>
    </author>
    <author>
      <name>A. Vagelatos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RACTI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on International Proofing Tools and Language Technologies
  July 1-2, 2004, Patras, Greece</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0408059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408060v1</id>
    <updated>2004-08-26T12:44:15Z</updated>
    <published>2004-08-26T12:44:15Z</published>
    <title>Verbal chunk extraction in French using limited resources</title>
    <summary>  A way of extracting French verbal chunks, inflected and infinitive, is
explored and tested on effective corpus. Declarative morphological and local
grammar rules specifying chunks and some simple contextual structures are used,
relying on limited lexical information and some simple heuristic/statistic
properties obtained from restricted corpora. The specific goals, the
architecture and the formalism of the system, the linguistic information on
which it relies and the obtained results on effective corpus are presented.
</summary>
    <author>
      <name>Gabriel G. Bes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIL</arxiv:affiliation>
    </author>
    <author>
      <name>Lionel Lamadon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIL</arxiv:affiliation>
    </author>
    <author>
      <name>Francois Trouilleux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIL</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0408060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409058v1</id>
    <updated>2004-09-29T20:34:04Z</updated>
    <published>2004-09-29T20:34:04Z</published>
    <title>A Sentimental Education: Sentiment Analysis Using Subjectivity
  Summarization Based on Minimum Cuts</title>
    <summary>  Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;
an example application is classifying a movie review as "thumbs up" or "thumbs
down". To determine this sentiment polarity, we propose a novel
machine-learning method that applies text-categorization techniques to just the
subjective portions of the document. Extracting these portions can be
implemented using efficient techniques for finding minimum cuts in graphs; this
greatly facilitates incorporation of cross-sentence contextual constraints.
</summary>
    <author>
      <name>Bo Pang</name>
    </author>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Data available at
  http://www.cs.cornell.edu/people/pabo/movie-review-data/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 42nd ACL, pp. 271--278, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0409058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410058v1</id>
    <updated>2004-10-22T23:41:52Z</updated>
    <published>2004-10-22T23:41:52Z</published>
    <title>Robust Dialogue Understanding in HERALD</title>
    <summary>  We tackle the problem of robust dialogue processing from the perspective of
language engineering. We propose an agent-oriented architecture that allows us
a flexible way of composing robust processors. Our approach is based on
Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP
agent model can be enriched with special features and components that allow us
to deal with classical problems of dialogue understanding.
</summary>
    <author>
      <name>Vincenzo Pallotta</name>
    </author>
    <author>
      <name>Afzal Ballim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of RANLP 2001 - EuroConference on Recent Advances in
  Natural Language Processing, September 5-7, 2001, Tzigov-Chark, Bulgaria</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0410058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2, I.2.7, I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412015v2</id>
    <updated>2005-03-11T14:05:04Z</updated>
    <published>2004-12-03T17:10:17Z</published>
    <title>A Tutorial on the Expectation-Maximization Algorithm Including
  Maximum-Likelihood Estimation and EM Training of Probabilistic Context-Free
  Grammars</title>
    <summary>  The paper gives a brief review of the expectation-maximization algorithm
(Dempster 1977) in the comprehensible framework of discrete mathematics. In
Section 2, two prominent estimation methods, the relative-frequency estimation
and the maximum-likelihood estimation are presented. Section 3 is dedicated to
the expectation-maximization algorithm and a simpler variant, the generalized
expectation-maximization algorithm. In Section 4, two loaded dice are rolled. A
more interesting example is presented in Section 5: The estimation of
probabilistic context-free grammars.
</summary>
    <author>
      <name>Detlef Prescher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 15th European Summer School in Logic, Language and
  Information (ESSLLI 2003). Example 5 extended (and partially corrected)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412016v1</id>
    <updated>2004-12-03T18:10:17Z</updated>
    <published>2004-12-03T18:10:17Z</published>
    <title>Inside-Outside Estimation Meets Dynamic EM</title>
    <summary>  We briefly review the inside-outside and EM algorithm for probabilistic
context-free grammars. As a result, we formally prove that inside-outside
estimation is a dynamic-programming variant of EM. This is interesting in its
own right, but even more when considered in a theoretical context since the
well-known convergence behavior of inside-outside estimation has been confirmed
by many experiments but apparently has never been formally proved. However,
being a version of EM, inside-outside estimation also inherits the good
convergence behavior of EM. Therefore, the as yet imperfect line of
argumentation can be transformed into a coherent proof.
</summary>
    <author>
      <name>Detlef Prescher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, some typos corrected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IWPT 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0412016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0509092v1</id>
    <updated>2005-09-28T16:15:27Z</updated>
    <published>2005-09-28T16:15:27Z</published>
    <title>Automatic extraction of paraphrastic phrases from medium size corpora</title>
    <summary>  This paper presents a versatile system intended to acquire paraphrastic
phrases from a representative corpus. In order to decrease the time spent on
the elaboration of resources for NLP system (for example Information
Extraction, IE hereafter), we suggest to use a machine learning system that
helps defining new templates and associated resources. This knowledge is
automatically derived from the text collection, in interaction with a large
semantic network.
</summary>
    <author>
      <name>Thierry Poibeau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Actes de la conf\'{e}rence Computational Linguisitcs (COLING 2004)
  (2004) 638-644</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0509092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511079v1</id>
    <updated>2005-11-22T07:06:43Z</updated>
    <published>2005-11-22T07:06:43Z</published>
    <title>An elitist approach for extracting automatically well-realized speech
  sounds with high confidence</title>
    <summary>  This paper presents an "elitist approach" for extracting automatically
well-realized speech sounds with high confidence. The elitist approach uses a
speech recognition system based on Hidden Markov Models (HMM). The HMM are
trained on speech sounds which are systematically well-detected in an iterative
procedure. The results show that, by using the HMM models defined in the
training phase, the speech recognizer detects reliably specific speech sounds
with a small rate of errors.
</summary>
    <author>
      <name>Jean-Baptiste Maj</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Bonneau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Dominique Fohr</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Yves Laprie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0511079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512102v1</id>
    <updated>2005-12-28T13:45:54Z</updated>
    <published>2005-12-28T13:45:54Z</published>
    <title>Statistical Parameters of the Novel "Perekhresni stezhky" ("The
  Cross-Paths") by Ivan Franko</title>
    <summary>  In the paper, a complex statistical characteristics of a Ukrainian novel is
given for the first time. The distribution of word-forms with respect to their
size is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath
are analyzed.
</summary>
    <author>
      <name>Solomija Buk</name>
    </author>
    <author>
      <name>Andrij Rovenchak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1515/9783110894219.39</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1515/9783110894219.39" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quantitative Linguistics 62: Exact methods in the study of
  language and text: dedicated to Professor Gabriel Altmann on the occasion of
  his 75th birthday / Ed. by P. Grzybek and R. Kohler (Berlin; New York: de
  Gruyter), 39-48 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0512102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604027v1</id>
    <updated>2006-04-07T13:10:30Z</updated>
    <published>2006-04-07T13:10:30Z</published>
    <title>Unification of multi-lingual scientific terminological resources using
  the ISO 16642 standard. The TermSciences initiative</title>
    <summary>  This paper presents the TermSciences portal, which deals with the
implementation of a conceptual model that uses the recent ISO 16642 standard
(Terminological Markup Framework). This standard turns out to be suitable for
concept modeling since it allowed for organizing the original resources by
concepts and to associate the various terms for a given concept. Additional
structuring is produced by sharing conceptual relationships, that is,
cross-linking of resource results through the introduction of semantic
relations which may have initially be missing.
</summary>
    <author>
      <name>Majid Khayari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INIST</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Schneider</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INIST</arxiv:affiliation>
    </author>
    <author>
      <name>Isabelle Kramer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>the termsciences Collaboration</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6p</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606118v1</id>
    <updated>2006-06-28T14:43:42Z</updated>
    <published>2006-06-28T14:43:42Z</published>
    <title>Adapting a general parser to a sublanguage</title>
    <summary>  In this paper, we propose a method to adapt a general parser (Link Parser) to
sublanguages, focusing on the parsing of texts in biology. Our main proposal is
the use of terminology (identication and analysis of terms) in order to reduce
the complexity of the text to be parsed. Several other strategies are explored
and finally combined among which text normalization, lexicon and
morpho-guessing module extensions and grammar rules adaptation. We compare the
parsing results before and after these adaptations.
</summary>
    <author>
      <name>Sophie Aubin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Adeline Nazarenko</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Claire Nédellec</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIG</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Recent Advances in
  Natural Language Processing (RANLP'05) (2005) 89-93</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0606118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609044v1</id>
    <updated>2006-09-08T14:11:50Z</updated>
    <published>2006-09-08T14:11:50Z</published>
    <title>The role of time in considering collections</title>
    <summary>  The paper concerns the understanding of plurals in the framework of
Artificial Intelligence and emphasizes the role of time. The construction of
collection(s) and their evolution across time is often crucial and has to be
accounted for. The paper contrasts a "de dicto" collection where the collection
can be considered as persisting over these situations even if its members
change with a "de re" collection whose composition does not vary through time.
It expresses different criteria of choice between the two interpretations (de
re and de dicto) depending on the context of enunciation.
</summary>
    <author>
      <name>Françoise Gayral</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Kayser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>François Lévy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journ\'{e}es de S\'{e}mantique et Mod\'{e}lisation, France (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0609044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610004v1</id>
    <updated>2006-10-01T17:39:44Z</updated>
    <published>2006-10-01T17:39:44Z</published>
    <title>Rapport technique du projet OGRE</title>
    <summary>  This repport concerns automatic understanding of (french) iterative
sentences, i.e. sentences where one single verb has to be interpreted by a more
or less regular plurality of events. A linguistic analysis is proposed along an
extension of Reichenbach's theory, several formal representations are
considered and a corpus of 18000 newspaper extracts is described.
</summary>
    <author>
      <name>Gérard Bécher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GREYC</arxiv:affiliation>
    </author>
    <author>
      <name>Patrice Enjalbert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GREYC</arxiv:affiliation>
    </author>
    <author>
      <name>Estelle Fievé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMSI</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Gosselin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DS</arxiv:affiliation>
    </author>
    <author>
      <name>François Lévy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Gérard Ligozat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMSI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">92 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611069v1</id>
    <updated>2006-11-15T12:35:45Z</updated>
    <published>2006-11-15T12:35:45Z</published>
    <title>Scaling Construction Grammar up to Production Systems: the SCIM</title>
    <summary>  While a great effort has concerned the development of fully integrated
modular understanding systems, few researches have focused on the problem of
unifying existing linguistic formalisms with cognitive processing models. The
Situated Constructional Interpretation Model is one of these attempts. In this
model, the notion of "construction" has been adapted in order to be able to
mimic the behavior of Production Systems. The Construction Grammar approach
establishes a model of the relations between linguistic forms and meaning, by
the mean of constructions. The latter can be considered as pairings from a
topologically structured space to an unstructured space, in some way a special
kind of production rules.
</summary>
    <author>
      <name>Guillaume Pitel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Scalable Natural Language Understanding 2006 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701181v1</id>
    <updated>2007-01-27T19:09:53Z</updated>
    <published>2007-01-27T19:09:53Z</published>
    <title>A Note on Local Ultrametricity in Text</title>
    <summary>  High dimensional, sparsely populated data spaces have been characterized in
terms of ultrametric topology. This implies that there are natural, not
necessarily unique, tree or hierarchy structures defined by the ultrametric
topology. In this note we study the extent of local ultrametric topology in
texts, with the aim of finding unique ``fingerprints'' for a text or corpus,
discriminating between texts from different domains, and opening up the
possibility of exploiting hierarchical structures in the data. We use coherent
and meaningful collections of over 1000 texts, comprising over 1.3 million
words.
</summary>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pp</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3; I.7.2; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701194v1</id>
    <updated>2007-01-30T16:58:07Z</updated>
    <published>2007-01-30T16:58:07Z</published>
    <title>Menzerath-Altmann Law for Syntactic Structures in Ukrainian</title>
    <summary>  In the paper, the definition of clause suitable for an automated processing
of a Ukrainian text is proposed. The Menzerath-Altmann law is verified on the
sentence level and the parameters for the dependences of the clause length
counted in words and syllables on the sentence length counted in clauses are
calculated for "Perekhresni Stezhky" ("The Cross-Paths"), a novel by Ivan
Franko.
</summary>
    <author>
      <name>Solomija Buk</name>
    </author>
    <author>
      <name>Andrij Rovenchak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1515/glot-2008-0002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1515/glot-2008-0002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; submitted to the Proceedings of the International scientific
  conference on Modern Methods in Linguistics held in honour of the anniversary
  of Prof. Gabriel L. Altmann (October 23rd and 24th, 2006, Budmerice Castle,
  Slovakia)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Glottotheory. Vol. 1, No. 1, pp 10-17 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702081v1</id>
    <updated>2007-02-14T06:05:20Z</updated>
    <published>2007-02-14T06:05:20Z</published>
    <title>Random Sentences from a Generalized Phrase-Structure Grammar Interpreter</title>
    <summary>  In numerous domains in cognitive science it is often useful to have a source
for randomly generated corpora. These corpora may serve as a foundation for
artificial stimuli in a learning experiment (e.g., Ellefson &amp; Christiansen,
2000), or as input into computational models (e.g., Christiansen &amp; Dale, 2001).
The following compact and general C program interprets a phrase-structure
grammar specified in a text file. It follows parameters set at a Unix or
Unix-based command-line and generates a corpus of random sentences from that
grammar.
</summary>
    <author>
      <name>Rick Dale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Brief paper with source code and examples</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703027v2</id>
    <updated>2008-05-31T16:10:52Z</updated>
    <published>2007-03-06T19:50:37Z</published>
    <title>Interroger un corpus par le sens</title>
    <summary>  In textual knowledge management, statistical methods prevail. Nonetheless,
some difficulties cannot be overcome by these methodologies. I propose a
symbolic approach using a complete textual analysis to identify which analysis
level can improve the the answers provided by a system. The approach identifies
word senses and relation between words and generates as many rephrasings as
possible. Using synonyms and derivative, the system provides new utterances
without changing the original meaning of the sentences. Such a way, an
information can be retrieved whatever the question or answer's wording may be.
</summary>
    <author>
      <name>Bernard Jacquemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pp</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans "Mots, termes et contextes", Actes des septi\`emes Journ\'ees
  scientifiques du r\'eseau de chercheurs Lexicologie, Terminologie, Traduction
  - Bruxelles : Belgique (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703027v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703027v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703135v1</id>
    <updated>2007-03-27T22:12:25Z</updated>
    <published>2007-03-27T22:12:25Z</published>
    <title>Dependency Parsing with Dynamic Bayesian Network</title>
    <summary>  Exact parsing with finite state automata is deemed inappropriate because of
the unbounded non-locality languages overwhelmingly exhibit. We propose a way
to structure the parsing task in order to make it amenable to local
classification methods. This allows us to build a Dynamic Bayesian Network
which uncovers the syntactic dependency structure of English sentences.
Experiments with the Wall Street Journal demonstrate that the model
successfully learns from labeled data.
</summary>
    <author>
      <name>Virginia Savova</name>
    </author>
    <author>
      <name>Leonid Peshkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of American Association for Artificial Intelligence
  AAAI 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.1; G.3; H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.2083v1</id>
    <updated>2007-04-17T01:04:01Z</updated>
    <published>2007-04-17T01:04:01Z</published>
    <title>Introduction to Arabic Speech Recognition Using CMUSphinx System</title>
    <summary>  In this paper Arabic was investigated from the speech recognition problem
point of view. We propose a novel approach to build an Arabic Automated Speech
Recognition System (ASR). This system is based on the open source CMU Sphinx-4,
from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;
speaker-independent, continuous speech recognition system based on discrete
Hidden Markov Models (HMMs). We build a model using utilities from the
OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this
system to Arabic voice recognition.
</summary>
    <author>
      <name>H. Satori</name>
    </author>
    <author>
      <name>M. Harti</name>
    </author>
    <author>
      <name>N. Chenfour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures and 2 tables, was in Information and Communication
  Technologies International Symposium proceeding ICTIS07 Fes (2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.2083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.2201v1</id>
    <updated>2007-04-17T17:04:26Z</updated>
    <published>2007-04-17T17:04:26Z</published>
    <title>Arabic Speech Recognition System using CMU-Sphinx4</title>
    <summary>  In this paper we present the creation of an Arabic version of Automated
Speech Recognition System (ASR). This system is based on the open source
Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition
system based on discrete hidden Markov models (HMMs). We investigate the
changes that must be made to the model to adapt Arabic voice recognition.
  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,
CMUSphinx-4, Artificial intelligence.
</summary>
    <author>
      <name>H. Satori</name>
    </author>
    <author>
      <name>M. Harti</name>
    </author>
    <author>
      <name>N. Chenfour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures and 2 tables, in French</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.2201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3665v1</id>
    <updated>2007-04-27T05:58:32Z</updated>
    <published>2007-04-27T05:58:32Z</published>
    <title>On the Development of Text Input Method - Lessons Learned</title>
    <summary>  Intelligent Input Methods (IM) are essential for making text entries in many
East Asian scripts, but their application to other languages has not been fully
explored. This paper discusses how such tools can contribute to the development
of computer processing of other oriental languages. We propose a design
philosophy that regards IM as a text service platform, and treats the study of
IM as a cross disciplinary subject from the perspectives of software
engineering, human-computer interaction (HCI), and natural language processing
(NLP). We discuss these three perspectives and indicate a number of possible
future research directions.
</summary>
    <author>
      <name>Mike Tian-Jian Jiang</name>
    </author>
    <author>
      <name>Deng Liu</name>
    </author>
    <author>
      <name>Meng-Juei Hsieh</name>
    </author>
    <author>
      <name>Wen-Lien Hsu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.3665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3270v1</id>
    <updated>2007-07-22T15:25:27Z</updated>
    <published>2007-07-22T15:25:27Z</published>
    <title>A Formal Model of Dictionary Structure and Content</title>
    <summary>  We show that a general model of lexical information conforms to an abstract
model that reflects the hierarchy of information found in a typical dictionary
entry. We show that this model can be mapped into a well-formed XML document,
and how the XSL transformation language can be used to implement a semantics
defined over the abstract model to enable extraction and manipulation of the
information in any format.
</summary>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Nancy Ide</name>
    </author>
    <author>
      <name>Adam Kilgarriff</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Euralex 2000 Euralex 2000, Stuttgart : Allemagne (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.3270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3559v1</id>
    <updated>2007-07-24T14:30:27Z</updated>
    <published>2007-07-24T14:30:27Z</published>
    <title>Practical Approach to Knowledge-based Question Answering with Natural
  Language Understanding and Advanced Reasoning</title>
    <summary>  This research hypothesized that a practical approach in the form of a
solution framework known as Natural Language Understanding and Reasoning for
Intelligence (NaLURI), which combines full-discourse natural language
understanding, powerful representation formalism capable of exploiting
ontological information and reasoning approach with advanced features, will
solve the following problems without compromising practicality factors: 1)
restriction on the nature of question and response, and 2) limitation to scale
across domains and to real-life natural language text.
</summary>
    <author>
      <name>Wilson Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master of Science thesis, National Technical University College of
  Malaysia, 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.3559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.5.2; H.3.4; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3972v1</id>
    <updated>2007-07-26T17:02:40Z</updated>
    <published>2007-07-26T17:02:40Z</published>
    <title>Learning Probabilistic Models of Word Sense Disambiguation</title>
    <summary>  This dissertation presents several new methods of supervised and unsupervised
learning of word sense disambiguation models. The supervised methods focus on
performing model searches through a space of probabilistic models, and the
unsupervised methods rely on the use of Gibbs Sampling and the Expectation
Maximization (EM) algorithm. In both the supervised and unsupervised case, the
Naive Bayesian model is found to perform well. An explanation for this success
is presented in terms of learning rates and bias-variance decompositions.
</summary>
    <author>
      <name>Ted Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">195 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PhD dissertation, May 1998, Department of Computer Science and
  Engineering, Southern Methodist University</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.3972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1564v1</id>
    <updated>2007-08-11T13:09:27Z</updated>
    <published>2007-08-11T13:09:27Z</published>
    <title>Learning Phonotactics Using ILP</title>
    <summary>  This paper describes experiments on learning Dutch phonotactic rules using
Inductive Logic Programming, a machine learning discipline based on inductive
logical operators. Two different ways of approaching the problem are
experimented with, and compared against each other as well as with related work
on the task. The results show a direct correspondence between the quality and
informedness of the background knowledge and the constructed theory,
demonstrating the ability of ILP to take good advantage of the prior domain
knowledge available. Further research is outlined.
</summary>
    <author>
      <name>Stasinos Konstantopoulos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Special Issue of the WEB-SLS Journal: The Language Sections of the
  ESSLLI-01 Student Session. 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.2401v1</id>
    <updated>2007-09-15T01:37:21Z</updated>
    <published>2007-09-15T01:37:21Z</published>
    <title>Bootstrapping Deep Lexical Resources: Resources for Courses</title>
    <summary>  We propose a range of deep lexical acquisition methods which make use of
morphological, syntactic and ontological language resources to model word
similarity and bootstrap from a seed lexicon. The different methods are
deployed in learning lexical items for a precision grammar, and shown to each
have strengths and weaknesses over different word classes. A particular focus
of this paper is the relative accessibility of different language resource
types, and predicted ``bang for the buck'' associated with each in deep lexical
acquisition applications.
</summary>
    <author>
      <name>Timothy Baldwin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACL-SIGLEX 2005 Workshop on Deep Lexical
  Acquisition, Ann Arbor, USA, pp. 67-76</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.2401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.2401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0105v2</id>
    <updated>2007-10-03T03:36:45Z</updated>
    <published>2007-09-30T03:21:54Z</published>
    <title>Zipf's Law and Avoidance of Excessive Synonymy</title>
    <summary>  Zipf's law states that if words of language are ranked in the order of
decreasing frequency in texts, the frequency of a word is inversely
proportional to its rank. It is very robust as an experimental observation, but
to date it escaped satisfactory theoretical explanation. We suggest that Zipf's
law may arise from the evolution of word semantics dominated by expansion of
meanings and competition of synonyms.
</summary>
    <author>
      <name>Dmitrii Manin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/03640210802020003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/03640210802020003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages; fixed reference list missing in v.1</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Main text in Cognitive Science, 32 (7) 2008, pp. 1075 - 1098;
  Appendix A TBP separately in J. Quant. Ling.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0105v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0105v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0225v1</id>
    <updated>2007-10-01T08:23:24Z</updated>
    <published>2007-10-01T08:23:24Z</published>
    <title>On the role of autocorrelations in texts</title>
    <summary>  The task of finding a criterion allowing to distinguish a text from an
arbitrary set of words is rather relevant in itself, for instance, in the
aspect of development of means for internet-content indexing or separating
signals and noise in communication channels. The Zipf law is currently
considered to be the most reliable criterion of this kind [3]. At any rate,
conventional stochastic word sets do not meet this law. The present paper deals
with one of possible criteria based on the determination of the degree of data
compression.
</summary>
    <author>
      <name>D. V. Lande</name>
    </author>
    <author>
      <name>A. A. Snarskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, 5 references</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.0225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.1481v1</id>
    <updated>2007-10-08T08:36:32Z</updated>
    <published>2007-10-08T08:36:32Z</published>
    <title>What's in a Name?</title>
    <summary>  This paper describes experiments on identifying the language of a single name
in isolation or in a document written in a different language. A new corpus has
been compiled and made available, matching names against languages. This corpus
is used in a series of experiments measuring the performance of general
language models and names-only language models on the language identification
task. Conclusions are drawn from the comparison between using general language
models and names-only language models and between identifying the language of
isolated names and the language of very short document fragments. Future
research directions are outlined.
</summary>
    <author>
      <name>Stasinos Konstantopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Computational Phonology Workshop, 6th Intl. Conf.
  Recent Advances in NLP, Borovets, Bulgaria, September 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.1481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.1481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2674v1</id>
    <updated>2007-10-14T16:09:53Z</updated>
    <published>2007-10-14T16:09:53Z</published>
    <title>Linguistic Information Energy</title>
    <summary>  In this treatment a text is considered to be a series of word impulses which
are read at a constant rate. The brain then assembles these units of
information into higher units of meaning. A classical systems approach is used
to model an initial part of this assembly process. The concepts of linguistic
system response, information energy, and ordering energy are defined and
analyzed. Finally, as a demonstration, information energy is used to estimate
the publication dates of a series of texts and the similarity of a set of
texts.
</summary>
    <author>
      <name>James Ford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 graphs</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2852v1</id>
    <updated>2007-10-15T15:45:13Z</updated>
    <published>2007-10-15T15:45:13Z</published>
    <title>Generating models for temporal representations</title>
    <summary>  We discuss the use of model building for temporal representations. We chose
Polish to illustrate our discussion because it has an interesting aspectual
system, but the points we wish to make are not language specific. Rather, our
goal is to develop theoretical and computational tools for temporal model
building tasks in computational semantics. To this end, we present a
first-order theory of time and events which is rich enough to capture
interesting semantic distinctions, and an algorithm which takes minimal models
for first-order theories and systematically attempts to ``perturb'' their
temporal component to provide non-minimal, but semantically significant,
models.
</summary>
    <author>
      <name>Patrick Blackburn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Hinderer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Recent Advances in Natural Language Processing (2007) 69-75</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.2852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2988v1</id>
    <updated>2007-10-16T09:16:24Z</updated>
    <published>2007-10-16T09:16:24Z</published>
    <title>Using Description Logics for Recognising Textual Entailment</title>
    <summary>  The aim of this paper is to show how we can handle the Recognising Textual
Entailment (RTE) task by using Description Logics (DLs). To do this, we propose
a representation of natural language semantics in DLs inspired by existing
representations in first-order logic. But our most significant contribution is
the definition of two novel inference tasks: A-Box saturation and subgraph
detection which are crucial for our approach to RTE.
</summary>
    <author>
      <name>Paul Bedaride</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans 19th European Summer School in Logic, Language and
  Information (2007) 11-21</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.2988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2270v1</id>
    <updated>2007-11-14T18:32:09Z</updated>
    <published>2007-11-14T18:32:09Z</published>
    <title>Can a Computer Laugh ?</title>
    <summary>  A computer model of "a sense of humour" suggested previously
[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific
malfunction in information processing, is given in somewhat different
exposition. Psychological aspects of humour are elaborated more thoroughly. The
mechanism of laughter is formulated on the more general level. Detailed
discussion is presented for the higher levels of information processing, which
are responsible for a perception of complex samples of humour. Development of a
sense of humour in the process of evolution is discussed.
</summary>
    <author>
      <name>I. M. Suslov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">P.L.Kapitza Institute for Physical Problems, Moscow, Russia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">English translation of the paper in Russian; 18 pages, 6 figures
  included</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Chronicle (Moscow), 1994, issue 1, p.1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.2270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3449v1</id>
    <updated>2007-11-21T20:34:08Z</updated>
    <published>2007-11-21T20:34:08Z</published>
    <title>Lexicon management and standard formats</title>
    <summary>  International standards for lexicon formats are in preparation. To a certain
extent, the proposed formats converge with prior results of standardization
projects. However, their adequacy for (i) lexicon management and (ii)
lexicon-driven applications have been little debated in the past, nor are they
as a part of the present standardization effort. We examine these issues. IGM
has developed XML formats compatible with the emerging international standards,
and we report experimental results on large-coverage lexica.
</summary>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Archives of Control Sciences 15, 3 (2005) 329-340</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.3449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3453v1</id>
    <updated>2007-11-21T20:41:59Z</updated>
    <published>2007-11-21T20:41:59Z</published>
    <title>A resource-based Korean morphological annotation system</title>
    <summary>  We describe a resource-based method of morphological annotation of written
Korean text. Korean is an agglutinative language. The output of our system is a
graph of morphemes annotated with accurate linguistic information. The language
resources used by the system can be easily updated, which allows us-ers to
control the evolution of the per-formances of the system. We show that
morphological annotation of Korean text can be performed directly with a
lexicon of words and without morpho-logical rules.
</summary>
    <author>
      <name>Hyun-Gue Huh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Proceedings of the International Joint Conference on Natural
  Language Processing (IJCNLP) - A resource-based Korean morphological
  annotation system, Jeju : Cor\'ee, R\'epublique de (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.3453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3454v1</id>
    <updated>2007-11-21T20:44:04Z</updated>
    <published>2007-11-21T20:44:04Z</published>
    <title>Graphes paramétrés et outils de lexicalisation</title>
    <summary>  Shifting to a lexicalized grammar reduces the number of parsing errors and
improves application results. However, such an operation affects a syntactic
parser in all its aspects. One of our research objectives is to design a
realistic model for grammar lexicalization. We carried out experiments for
which we used a grammar with a very simple content and formalism, and a very
informative syntactic lexicon, the lexicon-grammar of French elaborated by the
LADL. Lexicalization was performed by applying the parameterized-graph
approach. Our results tend to show that most information in the lexicon-grammar
can be transferred into a grammar and exploited successfully for the syntactic
parsing of sentences.
</summary>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Paumier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Verbum ex machina. Proceedings of TALN - Graphes
  param\'etr\'es et outils de lexicalisation, Louvain : Belgique (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.3454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3457v1</id>
    <updated>2007-11-21T20:49:21Z</updated>
    <published>2007-11-21T20:49:21Z</published>
    <title>Evaluation of a Grammar of French Determiners</title>
    <summary>  Existing syntactic grammars of natural languages, even with a far from
complete coverage, are complex objects. Assessments of the quality of parts of
such grammars are useful for the validation of their construction. We evaluated
the quality of a grammar of French determiners that takes the form of a
recursive transition network. The result of the application of this local
grammar gives deeper syntactic information than chunking or information
available in treebanks. We performed the evaluation by comparison with a corpus
independently annotated with information on determiners. We obtained 86%
precision and 92% recall on text not tagged for parts of speech.
</summary>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Annals of the 27th Congress of the Brazilian Society of
  Computation - Evaluation of a Grammar of French Determiners, Rio de Janeiro :
  Br\'esil (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.3457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3239v1</id>
    <updated>2008-01-21T17:41:57Z</updated>
    <published>2008-01-21T17:41:57Z</published>
    <title>Online-concordance "Perekhresni stezhky" ("The Cross-Paths"), a novel by
  Ivan Franko</title>
    <summary>  In the article, theoretical principles and practical realization for the
compilation of the concordance to "Perekhresni stezhky" ("The Cross-Paths"), a
novel by Ivan Franko, are described. Two forms for the context presentation are
proposed. The electronic version of this lexicographic work is available
online.
</summary>
    <author>
      <name>Solomiya Buk</name>
    </author>
    <author>
      <name>Andrij Rovenchak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Ukrainian</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ivan Franko: Spirit, Science, Thought, Will (Proceedings of the
  International Scientific Congress dedicated to the 150th anniversary (Lviv,
  27 September -- 1 October 2006, Lviv University Press, Vol. 2, pp. 203-211,
  2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4746v5</id>
    <updated>2008-02-10T08:26:02Z</updated>
    <published>2008-01-30T19:40:45Z</published>
    <title>Concerning Olga, the Beautiful Little Street Dancer (Adjectives as
  Higher-Order Polymorphic Functions)</title>
    <summary>  In this paper we suggest a typed compositional seman-tics for nominal
compounds of the form [Adj Noun] that models adjectives as higher-order
polymorphic functions, and where types are assumed to represent concepts in an
ontology that reflects our commonsense view of the world and the way we talk
about it in or-dinary language. In addition to [Adj Noun] compounds our
proposal seems also to suggest a plausible explana-tion for well known
adjective ordering restrictions.
</summary>
    <author>
      <name>Walid S. Saba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.4746v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4746v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3269v1</id>
    <updated>2008-04-21T15:38:45Z</updated>
    <published>2008-04-21T15:38:45Z</published>
    <title>Phoneme recognition in TIMIT with BLSTM-CTC</title>
    <summary>  We compare the performance of a recurrent neural network with the best
results published so far on phoneme recognition in the TIMIT database. These
published results have been obtained with a combination of classifiers.
However, in this paper we apply a single recurrent neural network to the same
task. Our recurrent neural network attains an error rate of 24.6%. This result
is not significantly different from that obtained by the other best methods,
but they rely on a combination of classifiers for achieving comparable
performance.
</summary>
    <author>
      <name>Santiago Fernández</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4584v1</id>
    <updated>2008-04-29T11:39:18Z</updated>
    <published>2008-04-29T11:39:18Z</published>
    <title>Feature Unification in TAG Derivation Trees</title>
    <summary>  The derivation trees of a tree adjoining grammar provide a first insight into
the sentence semantics, and are thus prime targets for generation systems. We
define a formalism, feature-based regular tree grammars, and a translation from
feature based tree adjoining grammars into this new formalism. The translation
preserves the derivation structures of the original grammar, and accounts for
feature unification.
</summary>
    <author>
      <name>Sylvain Schmitz</name>
    </author>
    <author>
      <name>Joseph Le Roux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures In TAG+9, Ninth International Workshop on Tree
  Adjoining Grammars and Related Formalisms, 2008</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In TAG+9, Ninth International Workshop on Tree Adjoining Grammars
  and Related Formalisms, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.4584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2537v1</id>
    <updated>2008-05-16T13:58:44Z</updated>
    <published>2008-05-16T13:58:44Z</published>
    <title>A toolkit for a generative lexicon</title>
    <summary>  In this paper we describe the conception of a software toolkit designed for
the construction, maintenance and collaborative use of a Generative Lexicon. In
order to ease its portability and spreading use, this tool was built with free
and open source products. We eventually tested the toolkit and showed it
filters the adequate form of anaphoric reference to the modifier in endocentric
compounds.
</summary>
    <author>
      <name>Patrick Henry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Bassac</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">poster - 6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans A toolkit for a Generative Lexicon - Fourth International
  Workshop on Generative Approaches to the Lexicon, PARIS : France (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.2537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3366v1</id>
    <updated>2008-05-21T23:44:06Z</updated>
    <published>2008-05-21T23:44:06Z</published>
    <title>Computational Representation of Linguistic Structures using
  Domain-Specific Languages</title>
    <summary>  We describe a modular system for generating sentences from formal definitions
of underlying linguistic structures using domain-specific languages. The system
uses Java in general, Prolog for lexical entries and custom domain-specific
languages based on Functional Grammar and Functional Discourse Grammar
notation, implemented using the ANTLR parser generator. We show how linguistic
and technological parts can be brought together in a natural language
processing system and how domain-specific languages can be used as a tool for
consistent formal notation in linguistic description.
</summary>
    <author>
      <name>Fabian Steeg</name>
    </author>
    <author>
      <name>Christoph Benden</name>
    </author>
    <author>
      <name>Paul O. Samuelsdorff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures; based on work presented at the 12th
  International Conference on Functional Grammar</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.3366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3410v1</id>
    <updated>2008-05-22T08:48:28Z</updated>
    <published>2008-05-22T08:48:28Z</published>
    <title>Exploring a type-theoretic approach to accessibility constraint
  modelling</title>
    <summary>  The type-theoretic modelling of DRT that [degroote06] proposed features
continuations for the management of the context in which a clause has to be
interpreted. This approach, while keeping the standard definitions of
quantifier scope, translates the rules of the accessibility constraints of
discourse referents inside the semantic recipes. In this paper, we deal with
additional rules for these accessibility constraints. In particular in the case
of discourse referents introduced by proper nouns, that negation does not
block, and in the case of rhetorical relations that structure discourses. We
show how this continuation-based approach applies to those accessibility
constraints and how we can consider the parallel management of various
principles.
</summary>
    <author>
      <name>Sylvain Pogodalla</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Journ\'ees S\'emantiques et Mod\'elisation (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.3410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.4521v1</id>
    <updated>2008-05-29T11:53:39Z</updated>
    <published>2008-05-29T11:53:39Z</published>
    <title>Textual Entailment Recognizing by Theorem Proving Approach</title>
    <summary>  In this paper we present two original methods for recognizing textual
inference. First one is a modified resolution method such that some linguistic
considerations are introduced in the unification of two atoms. The approach is
possible due to the recent methods of transforming texts in logic formulas.
Second one is based on semantic relations in text, as presented in WordNet.
Some similarities between these two methods are remarked.
</summary>
    <author>
      <name>Doina Tatar</name>
    </author>
    <author>
      <name>Militon Frentiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studia Univ.Babes-Bolyai, Informatica, Vol.LI, Number 2, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.4521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.4521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.2581v1</id>
    <updated>2008-06-16T13:48:55Z</updated>
    <published>2008-06-16T13:48:55Z</published>
    <title>A chain dictionary method for Word Sense Disambiguation and applications</title>
    <summary>  A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)
is that of dictionary-based methods. Various algorithms have as the root Lesk's
algorithm, which exploits the sense definitions in the dictionary directly. Our
approach uses the lexical base WordNet for a new algorithm originated in
Lesk's, namely "chain algorithm for disambiguation of all words", CHAD. We show
how translation from a language into another one and also text entailment
verification could be accomplished by this disambiguation.
</summary>
    <author>
      <name>Doina Tatar</name>
    </author>
    <author>
      <name>Gabriela Serban</name>
    </author>
    <author>
      <name>Andreea Mihis</name>
    </author>
    <author>
      <name>Mihaiela Lupea</name>
    </author>
    <author>
      <name>Dana Lupsa</name>
    </author>
    <author>
      <name>Militon Frentiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studia Universitatis Babes-Bolyai, Special Issue, KEPT 2007,
  Knowledge Engineering: Principles and Technologies, Cluj-Napoca, June 6-8,
  2007, pp 33-40,</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.2581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.2581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0311v1</id>
    <updated>2008-07-02T09:49:14Z</updated>
    <published>2008-07-02T09:49:14Z</published>
    <title>About the creation of a parallel bilingual corpora of web-publications</title>
    <summary>  The algorithm of the creation texts parallel corpora was presented. The
algorithm is based on the use of "key words" in text documents, and on the
means of their automated translation. Key words were singled out by means of
using Russian and Ukrainian morphological dictionaries, as well as dictionaries
of the translation of nouns for the Russian and Ukrainianlanguages. Besides, to
calculate the weights of the terms in the documents, empiric-statistic rules
were used. The algorithm under consideration was realized in the form of a
program complex, integrated into the content-monitoring InfoStream system. As a
result, a parallel bilingual corpora of web-publications containing about 30
thousand documents, was created
</summary>
    <author>
      <name>D. V. Lande</name>
    </author>
    <author>
      <name>V. V. Zhygalo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.0311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.3563v1</id>
    <updated>2008-08-26T18:17:44Z</updated>
    <published>2008-08-26T18:17:44Z</published>
    <title>What It Feels Like To Hear Voices: Fond Memories of Julian Jaynes</title>
    <summary>  Julian Jaynes's profound humanitarian convictions not only prevented him from
going to war, but would have prevented him from ever kicking a dog. Yet
according to his theory, not only are language-less dogs unconscious, but so
too were the speaking/hearing Greeks in the Bicameral Era, when they heard
gods' voices telling them what to do rather than thinking for themselves. I
argue that to be conscious is to be able to feel, and that all mammals (and
probably lower vertebrates and invertebrates too) feel, hence are conscious.
Julian Jaynes's brilliant analysis of our concepts of consciousness
nevertheless keeps inspiring ever more inquiry and insights into the age-old
mind/body problem and its relation to cognition and language.
</summary>
    <author>
      <name>Stevan Harnad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.3563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.3563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.3616v3</id>
    <updated>2009-03-30T11:53:07Z</updated>
    <published>2008-08-27T02:02:40Z</published>
    <title>Constructing word similarities in Meroitic as an aid to decipherment</title>
    <summary>  Meroitic is the still undeciphered language of the ancient civilization of
Kush. Over the years, various techniques for decipherment such as finding a
bilingual text or cognates from modern or other ancient languages in the Sudan
and surrounding areas has not been successful. Using techniques borrowed from
information theory and natural language statistics, similar words are paired
and attempts are made to use currently defined words to extract at least
partial meaning from unknown words.
</summary>
    <author>
      <name>Reginald D. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; 2 figures; to appear in British Museum studies in Ancient
  Egypt and Sudan</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">British Museum Studies in Ancient Egypt and Sudan, 12, 1-10 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0808.3616v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.3616v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.0103v1</id>
    <updated>2008-08-31T06:08:15Z</updated>
    <published>2008-08-31T06:08:15Z</published>
    <title>On the nature of long-range letter correlations in texts</title>
    <summary>  The origin of long-range letter correlations in natural texts is studied
using random walk analysis and Jensen-Shannon divergence. It is concluded that
they result from slow variations in letter frequency distribution, which are a
consequence of slow variations in lexical composition within the text. These
correlations are preserved by random letter shuffling within a moving window.
As such, they do reflect structural properties of the text, but in a very
indirect manner.
</summary>
    <author>
      <name>Dmitrii Y. Manin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures, unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.0103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.0103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.3250v1</id>
    <updated>2008-09-18T20:48:13Z</updated>
    <published>2008-09-18T20:48:13Z</published>
    <title>Using descriptive mark-up to formalize translation quality assessment</title>
    <summary>  The paper deals with using descriptive mark-up to emphasize translation
mistakes. The author postulates the necessity to develop a standard and formal
XML-based way of describing translation mistakes. It is considered to be
important for achieving impersonal translation quality assessment. Marked-up
translations can be used in corpus translation studies; moreover, automatic
translation assessment based on marked-up mistakes is possible. The paper
concludes with setting up guidelines for further activity within the described
field.
</summary>
    <author>
      <name>Andrey Kutuzov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Russian in 'Translation industry and information
  supply in international business activities: materials of international
  conference' - Perm, 2008, pp. 90-101</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.3250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.3250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.0453v1</id>
    <updated>2008-11-04T09:08:32Z</updated>
    <published>2008-11-04T09:08:32Z</published>
    <title>CoZo+ - A Content Zoning Engine for textual documents</title>
    <summary>  Content zoning can be understood as a segmentation of textual documents into
zones. This is inspired by [6] who initially proposed an approach for the
argumentative zoning of textual documents. With the prototypical CoZo+ engine,
we focus on content zoning towards an automatic processing of textual streams
while considering only the actors as the zones. We gain information that can be
used to realize an automatic recognition of content for pre-defined actors. We
understand CoZo+ as a necessary pre-step towards an automatic generation of
summaries and to make intellectual ownership of documents detectable.
</summary>
    <author>
      <name>Cynthia Wagner</name>
    </author>
    <author>
      <name>Christoph Schommer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.0453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.0453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3070v1</id>
    <updated>2008-12-16T14:24:23Z</updated>
    <published>2008-12-16T14:24:23Z</published>
    <title>A Computational Model to Disentangle Semantic Information Embedded in
  Word Association Norms</title>
    <summary>  Two well-known databases of semantic relationships between pairs of words
used in psycholinguistics, feature-based and association-based, are studied as
complex networks. We propose an algorithm to disentangle feature based
relationships from free association semantic networks. The algorithm uses the
rich topology of the free association semantic network to produce a new set of
relationships between words similar to those observed in feature production
norms.
</summary>
    <author>
      <name>J. Borge</name>
    </author>
    <author>
      <name>A. Arenas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.3070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.4180v2</id>
    <updated>2015-01-28T20:10:34Z</updated>
    <published>2009-01-27T06:29:10Z</published>
    <title>Google distance between words</title>
    <summary>  Cilibrasi and Vitanyi have demonstrated that it is possible to extract the
meaning of words from the world-wide web. To achieve this, they rely on the
number of webpages that are found through a Google search containing a given
word and they associate the page count to the probability that the word appears
on a webpage. Thus, conditional probabilities allow them to correlate one word
with another word's meaning. Furthermore, they have developed a similarity
distance function that gauges how closely related a pair of words is. We
present a specific counterexample to the triangle inequality for this
similarity distance function.
</summary>
    <author>
      <name>Bjørn Kjos-Hanssen</name>
    </author>
    <author>
      <name>Alberto J. Evangelista</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Frontiers in Undergraduate Research, University of
  Connecticut, 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.4180v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.4180v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.2230v1</id>
    <updated>2009-02-12T23:02:06Z</updated>
    <published>2009-02-12T23:02:06Z</published>
    <title>BagPack: A general framework to represent semantic relations</title>
    <summary>  We introduce a way to represent word pairs instantiating arbitrary semantic
relations that keeps track of the contexts in which the words in the pair occur
both together and independently. The resulting features are of sufficient
generality to allow us, with the help of a standard supervised machine learning
algorithm, to tackle a variety of unrelated semantic tasks with good results
and almost no task-specific tailoring.
</summary>
    <author>
      <name>Amaç Herdağdelen</name>
    </author>
    <author>
      <name>Marco Baroni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long paper presented at GEMS - Geometric Models of Natural Language
  Semantics, workshop held in conjunction with the 12th Conference of the
  European Chapter of the Association for Computational Linguistics (EACL-09),
  Athens, Greece</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.2230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.3072v1</id>
    <updated>2009-02-18T08:51:28Z</updated>
    <published>2009-02-18T08:51:28Z</published>
    <title>Syntactic variation of support verb constructions</title>
    <summary>  We report experiments about the syntactic variations of support verb
constructions, a special type of multiword expressions (MWEs) containing
predicative nouns. In these expressions, the noun can occur with or without the
verb, with no clear-cut semantic difference. We extracted from a large French
corpus a set of examples of the two situations and derived statistical results
from these data. The extraction involved large-coverage language resources and
finite-state techniques. The results show that, most frequently, predicative
nouns occur without a support verb. This fact has consequences on methods of
extracting or recognising MWEs.
</summary>
    <author>
      <name>Eric Laporte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <author>
      <name>Elisabete Ranchhod</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ONSET-CEL</arxiv:affiliation>
    </author>
    <author>
      <name>Anastasia Yannacopoulou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lingvisticae Investigationes 31, 2 (2008) 173-185</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.3072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.3072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.0740v1</id>
    <updated>2009-05-06T04:29:51Z</updated>
    <published>2009-05-06T04:29:51Z</published>
    <title>A FORTRAN coded regular expression Compiler for IBM 1130 Computing
  System</title>
    <summary>  REC (Regular Expression Compiler) is a concise programming language which
allows students to write programs without knowledge of the complicated syntax
of languages like FORTRAN and ALGOL. The language is recursive and contains
only four elements for control. This paper describes an interpreter of REC
written in FORTRAN.
</summary>
    <author>
      <name>Gerardo Cisneros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version of REC is archaeological reconstruction of REC/A
  language on IBM1130 Simulator (SIMH IBM 1130 Emulator and Disk Monitor System
  R2V12) from Computer History Simulation Project (www.ibm1130.org), also see
  REC language is a live for Ignacio Vega-Paez</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Mexicana de Ciencia y Tecnologia Vol. IV No. 1, page 30-86,
  1970</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.0740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.0740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.1609v1</id>
    <updated>2009-05-11T12:17:36Z</updated>
    <published>2009-05-11T12:17:36Z</published>
    <title>Acquisition of morphological families and derivational series from a
  machine readable dictionary</title>
    <summary>  The paper presents a linguistic and computational model aiming at making the
morphological structure of the lexicon emerge from the formal and semantic
regularities of the words it contains. The model is word-based. The proposed
morphological structure consists of (1) binary relations that connect each
headword with words that are morphologically related, and especially with the
members of its morphological family and its derivational series, and of (2) the
analogies that hold between the words. The model has been tested on the lexicon
of French using the TLFi machine readable dictionary.
</summary>
    <author>
      <name>Nabil Hathout</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLLE</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">proceedings of the 6th D\'ecembrettes</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.1609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.1609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0785v1</id>
    <updated>2009-07-04T18:43:16Z</updated>
    <published>2009-07-04T18:43:16Z</published>
    <title>A Bayesian Model for Discovering Typological Implications</title>
    <summary>  A standard form of analysis for linguistic typology is the universal
implication. These implications state facts about the range of extant
languages, such as ``if objects come after verbs, then adjectives come after
nouns.'' Such implications are typically discovered by painstaking hand
analysis over a small sample of languages. We propose a computational model for
assisting at this process. Our model is able to discover both well-known
implications as well as some novel implications that deserve further study.
Moreover, through a careful application of hierarchical analysis, we are able
to cope with the well-known sampling problem: languages are not independent.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <author>
      <name>Lyle Campbell</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.0785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0806v1</id>
    <updated>2009-07-04T22:26:47Z</updated>
    <published>2009-07-04T22:26:47Z</published>
    <title>A Noisy-Channel Model for Document Compression</title>
    <summary>  We present a document compression system that uses a hierarchical
noisy-channel model of text production. Our compression system first
automatically derives the syntactic structure of each sentence and the overall
discourse structure of the text given as input. The system then uses a
statistical hierarchical model of text production in order to drop
non-important syntactic and discourse constituents so as to generate coherent,
grammatical document compressions of arbitrary length. The system outperforms
both a baseline and a sentence-based compression system that operates by
simplifying sequentially all sentences in a text. Our results support the claim
that discourse knowledge plays an important role in document summarization.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <author>
      <name>Daniel Marcu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.0806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1814v1</id>
    <updated>2009-07-10T13:24:55Z</updated>
    <published>2009-07-10T13:24:55Z</published>
    <title>Bayesian Query-Focused Summarization</title>
    <summary>  We present BayeSum (for ``Bayesian summarization''), a model for sentence
extraction in query-focused summarization. BayeSum leverages the common case in
which multiple documents are relevant to a single query. Using these documents
as reinforcement for query terms, BayeSum is not afflicted by the paucity of
information in short queries. We show that approximate inference in BayeSum is
possible on large data sets and results in a state-of-the-art summarization
system. Furthermore, we show how BayeSum can be understood as a justified query
expansion technique in the language modeling for IR framework.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.1814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.2452v1</id>
    <updated>2009-07-14T21:02:14Z</updated>
    <published>2009-07-14T21:02:14Z</published>
    <title>Pattern Based Term Extraction Using ACABIT System</title>
    <summary>  In this paper, we propose a pattern-based term extraction approach for
Japanese, applying ACABIT system originally developed for French. The proposed
approach evaluates termhood using morphological patterns of basic terms and
term variants. After extracting term candidates, ACABIT system filters out
non-terms from the candidates based on log-likelihood. This approach is
suitable for Japanese term extraction because most of Japanese terms are
compound nouns or simple phrasal patterns.
</summary>
    <author>
      <name>Koichi Takeuchi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NII</arxiv:affiliation>
    </author>
    <author>
      <name>Kyo Kageura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NII</arxiv:affiliation>
    </author>
    <author>
      <name>Teruo Koyama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NII</arxiv:affiliation>
    </author>
    <author>
      <name>Béatrice Daille</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEIC Technical Report 103, 280 (2003) 31-36</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.2452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.2452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3781v1</id>
    <updated>2009-07-22T06:25:59Z</updated>
    <published>2009-07-22T06:25:59Z</published>
    <title>Un système modulaire d'acquisition automatique de traductions à
  partir du Web</title>
    <summary>  We present a method of automatic translation (French/English) of Complex
Lexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular
system is based on linguistic properties (compositionality, polysemy, etc.).
Different aspects of the multilingual Web are used to validate candidate
translations and collect new terms. We first build a French corpus of Web pages
to collect CLU. Three adapted processing stages are applied for each linguistic
property : compositional and non polysemous translations, compositional
polysemous translations and non compositional translations. Our evaluation on a
sample of CLU shows that our technique based on the Web can reach a very high
precision.
</summary>
    <author>
      <name>Stéphanie Léon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">TALN'09 (Traitement Automatique des Langues Naturelles), France
  (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.3781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2715v1</id>
    <updated>2009-09-15T06:02:56Z</updated>
    <published>2009-09-15T06:02:56Z</published>
    <title>Marking-up multiple views of a Text: Discourse and Reference</title>
    <summary>  We describe an encoding scheme for discourse structure and reference, based
on the TEI Guidelines and the recommendations of the Corpus Encoding
Specification (CES). A central feature of the scheme is a CES-based data
architecture enabling the encoding of and access to multiple views of a
marked-up document. We describe a tool architecture that supports the encoding
scheme, and then show how we have used the encoding scheme and the tools to
perform a discourse analytic task in support of a model of global discourse
cohesion called Veins Theory (Cristea &amp; Ide, 1998).
</summary>
    <author>
      <name>Dan Cristea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Nancy Ide</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">First International Language Resources and Evaluation Conference,
  Grenada, Espagne : France (1998)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.2715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2719v1</id>
    <updated>2009-09-15T06:10:45Z</updated>
    <published>2009-09-15T06:10:45Z</published>
    <title>Standards for Language Resources</title>
    <summary>  This paper presents an abstract data model for linguistic annotations and its
implementation using XML, RDF and related standards; and to outline the work of
a newly formed committee of the International Standards Organization (ISO),
ISO/TC 37/SC 4 Language Resource Management, which will use this work as its
starting point. The primary motive for presenting the latter is to solicit the
participation of members of the research community to contribute to the work of
the committee.
</summary>
    <author>
      <name>Nancy Ide</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">COMPUTER Science Department</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Colloque avec actes et comit\'e de lecture. internationale</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Third International Conference on Language Resources and
  Evaluation - LREC 2002, Las Palmas, Spain : France (2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.2719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3027v1</id>
    <updated>2009-09-16T14:38:19Z</updated>
    <published>2009-09-16T14:38:19Z</published>
    <title>Language Models for Handwritten Short Message Services</title>
    <summary>  Handwriting is an alternative method for entering texts composing Short
Message Services. However, a whole new language features the texts which are
produced. They include for instance abbreviations and other consonantal writing
which sprung up for time saving and fashion. We have collected and processed a
significant number of such handwriting SMS, and used various strategies to
tackle this challenging area of handwriting recognition. We proposed to study
more specifically three different phenomena: consonant skeleton, rebus, and
phonetic writing. For each of them, we compare the rough results produced by a
standard recognition system with those obtained when using a specific language
model.
</summary>
    <author>
      <name>Emmanuel Ep Prochasson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Viard-Gaudin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Morin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Document Analysis and Recognition,
  Brazil (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.3027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3028v1</id>
    <updated>2009-09-16T14:39:05Z</updated>
    <published>2009-09-16T14:39:05Z</published>
    <title>Vers la reconnaissance de mini-messages manuscrits</title>
    <summary>  Handwriting is an alternative method for entering texts which composed Short
Message Services. However, a whole new language features the texts which are
produced. They include for instance abbreviations and other consonantal writing
which sprung up for time saving and fashion. We have collected and processed a
significant number of such handwritten SMS, and used various strategies to
tackle this challenging area of handwriting recognition. We proposed to study
more specifically three different phenomena: consonant skeleton, rebus, and
phonetic writing. For each of them, we compare the rough results produced by a
standard recognition system with those obtained when using a specific language
model to take care of them.
</summary>
    <author>
      <name>Emmanuel Prochasson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Morin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Viard-Gaudin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Colloque International sur le Lexique et la Grammaire, Bonifacio :
  France (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.3028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3591v1</id>
    <updated>2009-09-19T15:38:55Z</updated>
    <published>2009-09-19T15:38:55Z</published>
    <title>Mathematics, Recursion, and Universals in Human Languages</title>
    <summary>  There are many scientific problems generated by the multiple and conflicting
alternative definitions of linguistic recursion and human recursive processing
that exist in the literature. The purpose of this article is to make available
to the linguistic community the standard mathematical definition of recursion
and to apply it to discuss linguistic recursion. As a byproduct, we obtain an
insight into certain "soft universals" of human languages, which are related to
cognitive constructs necessary to implement mathematical reasoning, i.e.
mathematical model theory.
</summary>
    <author>
      <name>P. Gilkey</name>
    </author>
    <author>
      <name>S. Lopez Ornat</name>
    </author>
    <author>
      <name>A. Karousou</name>
    </author>
    <link href="http://arxiv.org/abs/0909.3591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91F20, 03B65, 68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0537v1</id>
    <updated>2009-10-03T12:26:47Z</updated>
    <published>2009-10-03T12:26:47Z</published>
    <title>A Note On Higher Order Grammar</title>
    <summary>  Both syntax-phonology and syntax-semantics interfaces in Higher Order Grammar
(HOG) are expressed as axiomatic theories in higher-order logic (HOL), i.e. a
language is defined entirely in terms of provability in the single logical
system. An important implication of this elegant architecture is that the
meaning of a valid expression turns out to be represented not by a single, nor
even by a few "discrete" terms (in case of ambiguity), but by a "continuous"
set of logically equivalent terms. The note is devoted to precise formulation
and proof of this observation.
</summary>
    <author>
      <name>Victor Gluzberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages in single-spaced pdf format</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.0537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.1484v1</id>
    <updated>2009-10-08T12:21:22Z</updated>
    <published>2009-10-08T12:21:22Z</published>
    <title>Ludics and its Applications to natural Language Semantics</title>
    <summary>  Proofs, in Ludics, have an interpretation provided by their counter-proofs,
that is the objects they interact with. We follow the same idea by proposing
that sentence meanings are given by the counter-meanings they are opposed to in
a dialectical interaction. The conception is at the intersection of a
proof-theoretic and a game-theoretic accounts of semantics, but it enlarges
them by allowing to deal with possibly infinite processes.
</summary>
    <author>
      <name>Alain Lecomte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, SFLTAMP</arxiv:affiliation>
    </author>
    <author>
      <name>Myriam Quatrini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IML</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Artificial Intelligence LNAI, 5514 (2009) pp
  242--255</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.1484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.1484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.1842v1</id>
    <updated>2009-11-10T07:12:03Z</updated>
    <published>2009-11-10T07:12:03Z</published>
    <title>Standards for Language Resources</title>
    <summary>  The goal of this paper is two-fold: to present an abstract data model for
linguistic annotations and its implementation using XML, RDF and related
standards; and to outline the work of a newly formed committee of the
International Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource
Management, which will use this work as its starting point.
</summary>
    <author>
      <name>Nancy Ide</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Colloque avec actes et comit\'e de lecture. internationale</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IRCS Workshop on Linguistic Databases, Philadelphia : United
  States (2001)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.1842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.1842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.1965v1</id>
    <updated>2009-11-10T19:52:01Z</updated>
    <published>2009-11-10T19:52:01Z</published>
    <title>Active Learning for Mention Detection: A Comparison of Sentence
  Selection Strategies</title>
    <summary>  We propose and compare various sentence selection strategies for active
learning for the task of detecting mentions of entities. The best strategy
employs the sum of confidences of two statistical classifiers trained on
different views of the data. Our experimental results show that, compared to
the random selection strategy, this strategy reduces the amount of required
labeled training data by over 50% while achieving the same performance. The
effect is even more significant when only named mentions are considered: the
system achieves the same performance by using only 42% of the training data
required by the random selection strategy.
</summary>
    <author>
      <name>Nitin Madnani</name>
    </author>
    <author>
      <name>Hongyan Jing</name>
    </author>
    <author>
      <name>Nanda Kambhatla</name>
    </author>
    <author>
      <name>Salim Roukos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.1965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.1965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3917v1</id>
    <updated>2009-12-19T18:29:43Z</updated>
    <published>2009-12-19T18:29:43Z</published>
    <title>Speech Recognition Oriented Vowel Classification Using Temporal Radial
  Basis Functions</title>
    <summary>  The recent resurgence of interest in spatio-temporal neural network as speech
recognition tool motivates the present investigation. In this paper an approach
was developed based on temporal radial basis function "TRBF" looking to many
advantages: few parameters, speed convergence and time invariance. This
application aims to identify vowels taken from natural speech samples from the
Timit corpus of American speech. We report a recognition accuracy of 98.06
percent in training and 90.13 in test on a subset of 6 vowel phonemes, with the
possibility to expend the vowel sets in future.
</summary>
    <author>
      <name>Mustapha Guezouri</name>
    </author>
    <author>
      <name>Larbi Mesbahi</name>
    </author>
    <author>
      <name>Abdelkader Benyettou</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 1, Issue 1, pp 162-167, December 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.3917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0485v1</id>
    <updated>2010-02-02T13:35:02Z</updated>
    <published>2010-02-02T13:35:02Z</published>
    <title>Morphological study of Albanian words, and processing with NooJ</title>
    <summary>  We are developing electronic dictionaries and transducers for the automatic
processing of the Albanian Language. We will analyze the words inside a linear
segment of text. We will also study the relationship between units of sense and
units of form. The composition of words takes different forms in Albanian. We
have found that morphemes are frequently concatenated or simply juxtaposed or
contracted. The inflected grammar of NooJ allows constructing the dictionaries
of flexed forms (declensions or conjugations). The diversity of word structures
requires tools to identify words created by simple concatenation, or to treat
contractions. The morphological tools of NooJ allow us to create grammatical
tools to represent and treat these phenomena. But certain problems exceed the
morphological analysis and must be represented by syntactical grammars.
</summary>
    <author>
      <name>Odile Piton</name>
    </author>
    <author>
      <name>Klara Lagji</name>
    </author>
    <link href="http://arxiv.org/abs/1002.0485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1095v1</id>
    <updated>2010-02-04T22:48:31Z</updated>
    <published>2010-02-04T22:48:31Z</published>
    <title>Towards a Heuristic Categorization of Prepositional Phrases in English
  with WordNet</title>
    <summary>  This document discusses an approach and its rudimentary realization towards
automatic classification of PPs; the topic, that has not received as much
attention in NLP as NPs and VPs. The approach is a rule-based heuristics
outlined in several levels of our research. There are 7 semantic categories of
PPs considered in this document that we are able to classify from an annotated
corpus.
</summary>
    <author>
      <name>Frank Rudzicz</name>
    </author>
    <author>
      <name>Serguei A. Mokhov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; 4 tables; 1 figure; a year 2003 report</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.1095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0337v1</id>
    <updated>2010-03-01T18:04:39Z</updated>
    <published>2010-03-01T18:04:39Z</published>
    <title>Change of word types to word tokens ratio in the course of translation
  (based on Russian translations of K. Vonnegut novels)</title>
    <summary>  The article provides lexical statistical analysis of K. Vonnegut's two novels
and their Russian translations. It is found out that there happen some changes
between the speed of word types and word tokens ratio change in the source and
target texts. The author hypothesizes that these changes are typical for
English-Russian translations, and moreover, they represent an example of
Baker's translation feature of levelling out.
</summary>
    <author>
      <name>Andrey Kutuzov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, to be reported at International Computational
  Linguistic Conference "Dialog-21"-2010 (http://dialog-21.ru)</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0628v1</id>
    <updated>2010-03-02T16:52:32Z</updated>
    <published>2010-03-02T16:52:32Z</published>
    <title>Linguistic Geometries for Unsupervised Dimensionality Reduction</title>
    <summary>  Text documents are complex high dimensional objects. To effectively visualize
such data it is important to reduce its dimensionality and visualize the low
dimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore
dimensionality reduction methods that draw upon domain knowledge in order to
achieve a better low dimensional embedding and visualization of documents. We
consider the use of geometries specified manually by an expert, geometries
derived automatically from corpus statistics, and geometries computed from
linguistic resources.
</summary>
    <author>
      <name>Yi Mao</name>
    </author>
    <author>
      <name>Krishnakumar Balasubramanian</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4149v1</id>
    <updated>2010-03-22T13:09:57Z</updated>
    <published>2010-03-22T13:09:57Z</published>
    <title>Les Entités Nommées : usage et degrés de précision et de
  désambiguïsation</title>
    <summary>  The recognition and classification of Named Entities (NER) are regarded as an
important component for many Natural Language Processing (NLP) applications.
The classification is usually made by taking into account the immediate context
in which the NE appears. In some cases, this immediate context does not allow
getting the right classification. We show in this paper that the use of an
extended syntactic context and large-scale resources could be very useful in
the NER task.
</summary>
    <author>
      <name>Claude Martineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <author>
      <name>Elsa Tolone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <author>
      <name>Stavroula Voyatzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IGM-LabInfo</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">26\`eme Colloque international sur le Lexique et la Grammaire
  (LGC'07), Bonifacio : France (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5466v1</id>
    <updated>2010-05-29T16:37:02Z</updated>
    <published>2010-05-29T16:37:02Z</published>
    <title>Quantitative parametrization of texts written by Ivan Franko: An attempt
  of the project</title>
    <summary>  In the article, the project of quantitative parametrization of all texts by
Ivan Franko is manifested. It can be made only by using modern computer
techniques after the frequency dictionaries for all Franko's works are
compiled. The paper describes the application spheres, methodology, stages,
principles and peculiarities in the compilation of the frequency dictionary of
the second half of the 19th century - the beginning of the 20th century. The
relation between the Ivan Franko frequency dictionary, explanatory dictionary
of writer's language and text corpus is discussed.
</summary>
    <author>
      <name>Solomiya Buk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, in Ukrainian</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.5466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0153v1</id>
    <updated>2010-06-01T15:20:59Z</updated>
    <published>2010-06-01T15:20:59Z</published>
    <title>Ivan Franko's novel Dlja domashnjoho ohnyshcha (For the Hearth) in the
  light of the frequency dictionary</title>
    <summary>  In the article, the methodology and the principles of the compilation of the
Frequency dictionary for Ivan Franko's novel Dlja domashnjoho ohnyshcha (For
the Hearth) are described. The following statistical parameters of the novel
vocabulary are obtained: variety, exclusiveness, concentration indexes,
correlation between word rank and text coverage, etc. The main quantitative
characteristics of Franko's novels Perekhresni stezhky (The Cross-Paths) and
Dlja domashnjoho ohnyshcha are compared on the basis of their frequency
dictionaries.
</summary>
    <author>
      <name>Solomiya Buk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, in Ukrainian</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.0153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1343v1</id>
    <updated>2010-06-07T19:36:18Z</updated>
    <published>2010-06-07T19:36:18Z</published>
    <title>Segmentation and Nodal Points in Narrative: Study of Multiple Variations
  of a Ballad</title>
    <summary>  The Lady Maisry ballads afford us a framework within which to segment a
storyline into its major components. Segments and as a consequence nodal points
are discussed for nine different variants of the Lady Maisry story of a (young)
woman being burnt to death by her family, on account of her becoming pregnant
by a foreign personage. We motivate the importance of nodal points in textual
and literary analysis. We show too how the openings of the nine variants can be
analyzed comparatively, and also the conclusions of the ballads.
</summary>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <author>
      <name>Adam Ganz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pp., 13 figures. Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.1343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2835v1</id>
    <updated>2010-06-14T20:07:32Z</updated>
    <published>2010-06-14T20:07:32Z</published>
    <title>Fuzzy Modeling and Natural Language Processing for Panini's Sanskrit
  Grammar</title>
    <summary>  Indian languages have long history in World Natural languages. Panini was the
first to define Grammar for Sanskrit language with about 4000 rules in fifth
century. These rules contain uncertainty information. It is not possible to
Computer processing of Sanskrit language with uncertain information. In this
paper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate
uncertain information for reasoning with Sanskrit grammar. The Sanskrit
language processing is also discussed in this paper.
</summary>
    <author>
      <name>P. Venkata Subba Reddy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Journal of Computer Science and Engineering, see
  http://sites.google.com/site/jcseuk/volume-1-issue-1-may-2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer Science and Engineering, Volume 1, Issue 1,
  p99-101, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.2835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5880v1</id>
    <updated>2010-06-30T15:10:33Z</updated>
    <published>2010-06-30T15:10:33Z</published>
    <title>Testing SDRT's Right Frontier</title>
    <summary>  The Right Frontier Constraint (RFC), as a constraint on the attachment of new
constituents to an existing discourse structure, has important implications for
the interpretation of anaphoric elements in discourse and for Machine Learning
(ML) approaches to learning discourse structures. In this paper we provide
strong empirical support for SDRT's version of RFC. The analysis of about 100
doubly annotated documents by five different naive annotators shows that SDRT's
RFC is respected about 95% of the time. The qualitative analysis of presumed
violations that we have performed shows that they are either click-errors or
structural misconceptions.
</summary>
    <author>
      <name>Stergos Afantenos</name>
    </author>
    <author>
      <name>Nicholas Asher</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.5880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1986v1</id>
    <updated>2010-08-11T20:01:59Z</updated>
    <published>2010-08-11T20:01:59Z</published>
    <title>For the sake of simplicity: Unsupervised extraction of lexical
  simplifications from Wikipedia</title>
    <summary>  We report on work in progress on extracting lexical simplifications (e.g.,
"collaborate" -&gt; "work together"), focusing on utilizing edit histories in
Simple English Wikipedia for this task. We consider two main approaches: (1)
deriving simplification probabilities via an edit model that accounts for a
mixture of different operations, and (2) using metadata to focus on edits that
are more likely to be simplification operations. We find our methods to
outperform a reasonable baseline and yield many high-quality lexical
simplifications not included in an independently-created manually prepared
list.
</summary>
    <author>
      <name>Mark Yatskar</name>
    </author>
    <author>
      <name>Bo Pang</name>
    </author>
    <author>
      <name>Cristian Danescu-Niculescu-Mizil</name>
    </author>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pp; data available at
  http://www.cs.cornell.edu/home/llee/data/simple/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the NAACL, pp. 365-368, 2010. Short paper</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.1986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1117v2</id>
    <updated>2010-09-24T06:37:16Z</updated>
    <published>2010-09-06T18:35:08Z</published>
    <title>Constructions définitoires des tables du Lexique-Grammaire</title>
    <summary>  Lexicon-Grammar tables are a very rich syntactic lexicon for the French
language. This linguistic database is nevertheless not directly suitable for
use by computer programs, as it is incomplete and lacks consistency. Tables are
defined on the basis of features which are not explicitly recorded in the
lexicon. These features are only described in literature. Our aim is to define
for each tables these essential properties to make them usable in various
Natural Language Processing (NLP) applications, such as parsing.
</summary>
    <author>
      <name>Elsa Tolone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Stavroula Voyatzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Leclère</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">29\`eme Colloque international sur le Lexique et la Grammaire
  (LGC'10), Belgrade : Serbie (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.1117v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1117v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.2384v1</id>
    <updated>2010-10-12T13:20:30Z</updated>
    <published>2010-10-12T13:20:30Z</published>
    <title>Learning Taxonomy for Text Segmentation by Formal Concept Analysis</title>
    <summary>  In this paper the problems of deriving a taxonomy from a text and
concept-oriented text segmentation are approached. Formal Concept Analysis
(FCA) method is applied to solve both of these linguistic problems. The
proposed segmentation method offers a conceptual view for text segmentation,
using a context-driven clustering of sentences. The Concept-oriented Clustering
Segmentation algorithm (COCS) is based on k-means linear clustering of the
sentences. Experimental results obtained using COCS algorithm are presented.
</summary>
    <author>
      <name>Mihaiela Lupea</name>
    </author>
    <author>
      <name>Doina Tatar</name>
    </author>
    <author>
      <name>Zsuzsana Marian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Synasc 2010, Timisoara, Romania</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.2384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.2384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50, 03H65" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.0835v1</id>
    <updated>2010-11-03T10:05:15Z</updated>
    <published>2010-11-03T10:05:15Z</published>
    <title>A PDTB-Styled End-to-End Discourse Parser</title>
    <summary>  We have developed a full discourse parser in the Penn Discourse Treebank
(PDTB) style. Our trained parser first identifies all discourse and
non-discourse relations, locates and labels their arguments, and then
classifies their relation types. When appropriate, the attribution spans to
these relations are also determined. We present a comprehensive evaluation from
both component-wise and error-cascading perspectives.
</summary>
    <author>
      <name>Ziheng Lin</name>
    </author>
    <author>
      <name>Hwee Tou Ng</name>
    </author>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Language Engineering 20 (02), 151 - 184, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1011.0835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.0835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2922v1</id>
    <updated>2010-11-12T14:43:04Z</updated>
    <published>2010-11-12T14:43:04Z</published>
    <title>Emoticonsciousness</title>
    <summary>  A temporal analysis of emoticon use in Swedish, Italian, German and English
asynchronous electronic communication is reported. Emoticons are classified as
positive, negative and neutral. Postings to newsgroups over a 66 week period
are considered. The aggregate analysis of emoticon use in newsgroups for
science and politics tend on the whole to be consistent over the entire time
period. Where possible, events that coincide with divergences from trends in
language-subject pairs are noted. Political discourse in Italian over the
period shows marked use of negative emoticons, and in Swedish, positive
emoticons.
</summary>
    <author>
      <name>Carl Vogel</name>
    </author>
    <author>
      <name>Jerom Janssen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-12397-9_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-12397-9_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COST Action 2102 and euCognition International School Vietri sul
  Mare, Italy, April 21-26, 2008 Revised Selected and Invited Papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91C99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4155v1</id>
    <updated>2010-11-18T08:59:55Z</updated>
    <published>2010-11-18T08:59:55Z</published>
    <title>Motifs de graphe pour le calcul de dépendances syntaxiques complètes</title>
    <summary>  This article describes a method to build syntactical dependencies starting
from the phrase structure parsing process. The goal is to obtain all the
information needed for a detailled semantical analysis. Interaction Grammars
are used for parsing; the saturation of polarities which is the core of this
formalism can be mapped to dependency relation. Formally, graph patterns are
used to express the set of constraints which control dependency creations.
</summary>
    <author>
      <name>Jonathan Marchand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Guillaume</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Guy Perrier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence sur le Traitement Automatique des Langues Naturelles
  - TALN'10, Montr\'eal : Canada (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1011.4155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5209v2</id>
    <updated>2011-01-29T10:31:58Z</updated>
    <published>2010-11-23T20:11:35Z</published>
    <title>The semantic mapping of words and co-words in contexts</title>
    <summary>  Meaning can be generated when information is related at a systemic level.
Such a system can be an observer, but also a discourse, for example,
operationalized as a set of documents. The measurement of semantics as
similarity in patterns (correlations) and latent variables (factor analysis)
has been enhanced by computer techniques and the use of statistics; for
example, in "Latent Semantic Analysis". This communication provides an
introduction, an example, pointers to relevant software, and summarizes the
choices that can be made by the analyst. Visualization ("semantic mapping") is
thus made more accessible.
</summary>
    <author>
      <name>Loet Leydesdorff</name>
    </author>
    <author>
      <name>Kasper Welbers</name>
    </author>
    <link href="http://arxiv.org/abs/1011.5209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5494v1</id>
    <updated>2011-01-28T09:58:39Z</updated>
    <published>2011-01-28T09:58:39Z</published>
    <title>Developing a New Approach for Arabic Morphological Analysis and
  Generation</title>
    <summary>  Arabic morphological analysis is one of the essential stages in Arabic
Natural Language Processing. In this paper we present an approach for Arabic
morphological analysis. This approach is based on Arabic morphological
automaton (AMAUT). The proposed technique uses a morphological database
realized using XMODEL language. Arabic morphology represents a special type of
morphological systems because it is based on the concept of scheme to represent
Arabic words. We use this concept to develop the Arabic morphological automata.
The proposed approach has development standardization aspect. It can be
exploited by NLP applications such as syntactic and semantic analysis,
information retrieval, machine translation and orthographical correction. The
proposed approach is compared with Xerox Arabic Analyzer and Smrz Arabic
Analyzer.
</summary>
    <author>
      <name>Mourad Gridach</name>
    </author>
    <author>
      <name>Noureddine Chenfour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 15 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5757v1</id>
    <updated>2011-01-30T10:41:46Z</updated>
    <published>2011-01-30T10:41:46Z</published>
    <title>Polarized Montagovian Semantics for the Lambek-Grishin calculus</title>
    <summary>  Grishin proposed enriching the Lambek calculus with multiplicative
disjunction (par) and coresiduals. Applications to linguistics were discussed
by Moortgat, who spoke of the Lambek-Grishin calculus (LG). In this paper, we
adapt Girard's polarity-sensitive double negation embedding for classical logic
to extract a compositional Montagovian semantics from a display calculus for
focused proof search in LG. We seize the opportunity to illustrate our approach
alongside an analysis of extraction, providing linguistic motivation for linear
distributivity of tensor over par, thus answering a question of
Kurtonina&amp;Moortgat. We conclude by comparing our proposal to the continuation
semantics of Bernardi&amp;Moortgat, corresponding to call-by- name and
call-by-value evaluation strategies.
</summary>
    <author>
      <name>Arno Bastenhof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of the 15th conference on Formal
  Grammar, Copenhagen, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2034v1</id>
    <updated>2011-04-11T19:50:50Z</updated>
    <published>2011-04-11T19:50:50Z</published>
    <title>Materials to the Russian-Bulgarian Comparative Dictionary "EAD"</title>
    <summary>  This article presents a fragment of a new comparative dictionary "A
comparative dictionary of names of expansive action in Russian and Bulgarian
languages". Main features of the new web-based comparative dictionary are
placed, the principles of its formation are shown, primary links between the
word-matches are classified. The principal difference between translation
dictionaries and the model of double comparison is also shown. The
classification scheme of the pages is proposed. New concepts and keywords have
been introduced. The real prototype of the dictionary with a few key pages is
published. The broad debate about the possibility of this prototype to become a
version of Russian-Bulgarian comparative dictionary of a new generation is
available.
</summary>
    <author>
      <name>Yavor Angelov Parvanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Bulgarian Rusistics; Vol. 1 (2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.2034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2086v1</id>
    <updated>2011-04-11T23:06:54Z</updated>
    <published>2011-04-11T23:06:54Z</published>
    <title>A Universal Part-of-Speech Tagset</title>
    <summary>  To facilitate future research in unsupervised induction of syntactic
structure and to standardize best-practices, we propose a tagset that consists
of twelve universal part-of-speech categories. In addition to the tagset, we
develop a mapping from 25 different treebank tagsets to this universal set. As
a result, when combined with the original treebank data, this universal tagset
and mapping produce a dataset consisting of common parts-of-speech for 22
different languages. We highlight the use of this resource via two experiments,
including one that reports competitive accuracies for unsupervised grammar
induction without gold standard part-of-speech tags.
</summary>
    <author>
      <name>Slav Petrov</name>
    </author>
    <author>
      <name>Dipanjan Das</name>
    </author>
    <author>
      <name>Ryan McDonald</name>
    </author>
    <link href="http://arxiv.org/abs/1104.2086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1072v1</id>
    <updated>2011-05-05T13:51:46Z</updated>
    <published>2011-05-05T13:51:46Z</published>
    <title>English-Lithuanian-English Machine Translation lexicon and engine:
  current state and future work</title>
    <summary>  This article overviews the current state of the English-Lithuanian-English
machine translation system. The first part of the article describes the
problems that system poses today and what actions will be taken to solve them
in the future. The second part of the article tackles the main issue of the
translation process. Article briefly overviews the word sense disambiguation
for MT technique using Google.
</summary>
    <author>
      <name>G. Barisevičius</name>
    </author>
    <author>
      <name>B. Tamulynas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Informacin\.{e}s technologijos 2006 : conference proceedings /
  Kaunas University of Technology. T. 1. Kaunas : Technologija, 2006. p.
  109-112</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.1072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4687v2</id>
    <updated>2011-10-07T09:50:12Z</updated>
    <published>2011-07-23T12:56:02Z</published>
    <title>Fence - An Efficient Parser with Ambiguity Support for Model-Driven
  Language Specification</title>
    <summary>  Model-based language specification has applications in the implementation of
language processors, the design of domain-specific languages, model-driven
software development, data integration, text mining, natural language
processing, and corpus-based induction of models. Model-based language
specification decouples language design from language processing and, unlike
traditional grammar-driven approaches, which constrain language designers to
specific kinds of grammars, it needs general parser generators able to deal
with ambiguities. In this paper, we propose Fence, an efficient bottom-up
parsing algorithm with lexical and syntactic ambiguity support that enables the
use of model-based language specification in practice.
</summary>
    <author>
      <name>Luis Quesada</name>
    </author>
    <author>
      <name>Fernando Berzal</name>
    </author>
    <author>
      <name>Francisco J. Cortijo</name>
    </author>
    <link href="http://arxiv.org/abs/1107.4687v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4687v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5752v2</id>
    <updated>2011-09-12T19:29:29Z</updated>
    <published>2011-07-28T15:59:21Z</published>
    <title>An Effective Approach to Biomedical Information Extraction with Limited
  Training Data</title>
    <summary>  Overall, the two main contributions of this work include the application of
sentence simplification to association extraction as described above, and the
use of distributional semantics for concept extraction. The proposed work on
concept extraction amalgamates for the first time two diverse research areas
-distributional semantics and information extraction. This approach renders all
the advantages offered in other semi-supervised machine learning systems, and,
unlike other proposed semi-supervised approaches, it can be used on top of
different basic frameworks and algorithms.
http://gradworks.umi.com/34/49/3449837.html
</summary>
    <author>
      <name>Siddhartha Jonnalagadda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Jonnalagadda S. An effective approach to biomedical information
  extraction with limited training data (PhD Dissertation, Arizona State
  University). 2011;</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.5752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.3850v1</id>
    <updated>2011-08-18T20:17:58Z</updated>
    <published>2011-08-18T20:17:58Z</published>
    <title>Solving puzzles described in English by automated translation to answer
  set programming and learning how to do that translation</title>
    <summary>  We present a system capable of automatically solving combinatorial logic
puzzles given in (simplified) English. It involves translating the English
descriptions of the puzzles into answer set programming(ASP) and using ASP
solvers to provide solutions of the puzzles. To translate the descriptions, we
use a lambda-calculus based approach using Probabilistic Combinatorial
Categorial Grammars (PCCG) where the meanings of words are associated with
parameters to be able to distinguish between multiple meanings of the same
word. Meaning of many words and the parameters are learned. The puzzles are
represented in ASP using an ontology which is applicable to a large set of
logic puzzles.
</summary>
    <author>
      <name>Chitta Baral</name>
    </author>
    <author>
      <name>Juraj Dzifcak</name>
    </author>
    <link href="http://arxiv.org/abs/1108.3850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.3850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4052v1</id>
    <updated>2011-08-19T21:41:29Z</updated>
    <published>2011-08-19T21:41:29Z</published>
    <title>Query Expansion: Term Selection using the EWC Semantic Relatedness
  Measure</title>
    <summary>  This paper investigates the efficiency of the EWC semantic relatedness
measure in an ad-hoc retrieval task. This measure combines the Wikipedia-based
Explicit Semantic Analysis measure, the WordNet path measure and the mixed
collocation index. In the experiments, the open source search engine Terrier
was utilised as a tool to index and retrieve data. The proposed technique was
tested on the NTCIR data collection. The experiments demonstrated promising
results.
</summary>
    <author>
      <name>Vitaly Klyuev</name>
    </author>
    <author>
      <name>Yannis Haralambous</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, accepted at ASIR'11
  &lt;http://fedcsis.org/?q=node/62&gt;</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 1st International Workshop on Advances in Semantic
  Information Retrieval (ASIR'11), Szczecin, Poland, September 18-21, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.4052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4297v1</id>
    <updated>2011-08-22T12:55:20Z</updated>
    <published>2011-08-22T12:55:20Z</published>
    <title>Why is language well-designed for communication? (Commentary on
  Christiansen and Chater: 'Language as shaped by the brain')</title>
    <summary>  Selection through iterated learning explains no more than other
non-functional accounts, such as universal grammar, why language is so
well-designed for communicative efficiency. It does not predict several
distinctive features of language like central embedding, large lexicons or the
lack of iconicity, that seem to serve communication purposes at the expense of
learnability.
</summary>
    <author>
      <name>Jean-Louis Dessalles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INFRES, LTCI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">jld-08041101</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Behavioral and Brain Sciences 31, 5 (2008) 518-519</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.4297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5017v1</id>
    <updated>2011-08-25T06:08:16Z</updated>
    <published>2011-08-25T06:08:16Z</published>
    <title>Event in Compositional Dynamic Semantics</title>
    <summary>  We present a framework which constructs an event-style dis- course semantics.
The discourse dynamics are encoded in continuation semantics and various
rhetorical relations are embedded in the resulting interpretation of the
framework. We assume discourse and sentence are distinct semantic objects, that
play different roles in meaning evalua- tion. Moreover, two sets of composition
functions, for handling different discourse relations, are introduced. The
paper first gives the necessary background and motivation for event and dynamic
semantics, then the framework with detailed examples will be introduced.
</summary>
    <author>
      <name>Sai Qian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Maxime Amblard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages; Logical Aspect of Computational Linguistic, Montpellier :
  France (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5027v1</id>
    <updated>2011-08-25T07:22:09Z</updated>
    <published>2011-08-25T07:22:09Z</published>
    <title>Encoding Phases using Commutativity and Non-commutativity in a Logical
  Framework</title>
    <summary>  This article presents an extension of Minimalist Categorial Gram- mars (MCG)
to encode Chomsky's phases. These grammars are based on Par- tially Commutative
Logic (PCL) and encode properties of Minimalist Grammars (MG) of Stabler. The
first implementation of MCG were using both non- commutative properties (to
respect the linear word order in an utterance) and commutative ones (to model
features of different constituents). Here, we pro- pose to adding Chomsky's
phases with the non-commutative tensor product of the logic. Then we could give
account of the PIC just by using logical prop- erties of the framework.
</summary>
    <author>
      <name>Maxime Amblard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Logical Aspect of Computational Linguistic, Montpellier : France
  (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5096v1</id>
    <updated>2011-08-25T14:15:46Z</updated>
    <published>2011-08-25T14:15:46Z</published>
    <title>Minimalist Grammars and Minimalist Categorial Grammars, definitions
  toward inclusion of generated languages</title>
    <summary>  Stabler proposes an implementation of the Chomskyan Minimalist Program,
Chomsky 95 with Minimalist Grammars - MG, Stabler 97. This framework inherits a
long linguistic tradition. But the semantic calculus is more easily added if
one uses the Curry-Howard isomorphism. Minimalist Categorial Grammars - MCG,
based on an extension of the Lambek calculus, the mixed logic, were introduced
to provide a theoretically-motivated syntax-semantics interface, Amblard 07. In
this article, we give full definitions of MG with algebraic tree descriptions
and of MCG, and take the first steps towards giving a proof of inclusion of
their generated languages.
</summary>
    <author>
      <name>Maxime Amblard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Logic and Grammar (2011) 1-20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.5096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5974v1</id>
    <updated>2011-08-30T14:45:41Z</updated>
    <published>2011-08-30T14:45:41Z</published>
    <title>Emotional Analysis of Blogs and Forums Data</title>
    <summary>  We perform a statistical analysis of emotionally annotated comments in two
large online datasets, examining chains of consecutive posts in the
discussions. Using comparisons with randomised data we show that there is a
high level of correlation for the emotional content of messages.
</summary>
    <author>
      <name>Paweł Weroński</name>
    </author>
    <author>
      <name>Julian Sienkiewicz</name>
    </author>
    <author>
      <name>Georgios Paltoglou</name>
    </author>
    <author>
      <name>Kevan Buckley</name>
    </author>
    <author>
      <name>Mike Thelwall</name>
    </author>
    <author>
      <name>Janusz A. Hołyst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.12693/APhysPolA.121.B-128</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.12693/APhysPolA.121.B-128" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">REVTEX format, 5 pages, 6 figures, 2 tables, accepted to Acta Physica
  Polonica A</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Physica Polonica A 121, B-128 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.5974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0069v2</id>
    <updated>2014-04-20T16:56:40Z</updated>
    <published>2011-09-01T02:20:12Z</published>
    <title>Inter-rater Agreement on Sentence Formality</title>
    <summary>  Formality is one of the most important dimensions of writing style variation.
In this study we conducted an inter-rater reliability experiment for assessing
sentence formality on a five-point Likert scale, and obtained good agreement
results as well as different rating distributions for different sentence
categories. We also performed a difficulty analysis to identify the bottlenecks
of our rating procedure. Our main objective is to design an automatic scoring
mechanism for sentence-level formality, and this study is important for that
purpose.
</summary>
    <author>
      <name>Shibamouli Lahiri</name>
    </author>
    <author>
      <name>Xiaofei Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.0069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0624v1</id>
    <updated>2011-09-03T14:30:44Z</updated>
    <published>2011-09-03T14:30:44Z</published>
    <title>Building Ontologies to Understand Spoken Tunisian Dialect</title>
    <summary>  This paper presents a method to understand spoken Tunisian dialect based on
lexical semantic. This method takes into account the specificity of the
Tunisian dialect which has no linguistic processing tools. This method is
ontology-based which allows exploiting the ontological concepts for semantic
annotation and ontological relations for speech interpretation. This
combination increases the rate of comprehension and limits the dependence on
linguistic resources. This paper also details the process of building the
ontology used for annotation and interpretation of Tunisian dialect in the
context of speech understanding in dialogue systems for restricted domain.
</summary>
    <author>
      <name>Marwa Graja</name>
    </author>
    <author>
      <name>Maher Jaoua</name>
    </author>
    <author>
      <name>Lamia Hadrich Belguith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.1, No.4, August 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5798v1</id>
    <updated>2011-09-27T08:00:46Z</updated>
    <published>2011-09-27T08:00:46Z</published>
    <title>Object-oriented semantics of English in natural language understanding
  system</title>
    <summary>  A new approach to the problem of natural language understanding is proposed.
The knowledge domain under consideration is the social behavior of people.
English sentences are translated into set of predicates of a semantic database,
which describe persons, occupations, organizations, projects, actions, events,
messages, machines, things, animals, location and time of actions, relations
between objects, thoughts, cause-and-effect relations, abstract objects. There
is a knowledge base containing the description of semantics of objects
(functions and structure), actions (motives and causes), and operations.
</summary>
    <author>
      <name>Yuriy Ostapov</name>
    </author>
    <link href="http://arxiv.org/abs/1109.5798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1758v2</id>
    <updated>2012-03-04T19:00:21Z</updated>
    <published>2011-10-08T19:15:12Z</published>
    <title>Data formats for phonological corpora</title>
    <summary>  The goal of the present chapter is to explore the possibility of providing
the research (but also the industrial) community that commonly uses spoken
corpora with a stable portfolio of well-documented standardised formats that
allow a high re-use rate of annotated spoken resources and, as a consequence,
better interoperability across tools used to produce or exploit such resources.
</summary>
    <author>
      <name>Laurent Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDSL, INRIA Saclay - Ile de France</arxiv:affiliation>
    </author>
    <author>
      <name>Andreas Witt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Handbook of Corpus Phonology Oxford University Press (Ed.) (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.1758v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1758v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4248v1</id>
    <updated>2011-10-19T11:45:16Z</updated>
    <published>2011-10-19T11:45:16Z</published>
    <title>Ideogram Based Chinese Sentiment Word Orientation Computation</title>
    <summary>  This paper presents a novel algorithm to compute sentiment orientation of
Chinese sentiment word. The algorithm uses ideograms which are a distinguishing
feature of Chinese language. The proposed algorithm can be applied to any
sentiment classification scheme. To compute a word's sentiment orientation
using the proposed algorithm, only the word itself and a precomputed character
ontology is required, rather than a corpus. The influence of three parameters
over the algorithm performance is analyzed and verified by experiment.
Experiment also shows that proposed algorithm achieves an F Measure of 85.02%
outperforming existing ideogram based algorithm.
</summary>
    <author>
      <name>Luojie Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, accepted by CET 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.4248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1673v1</id>
    <updated>2011-11-07T18:46:54Z</updated>
    <published>2011-11-07T18:46:54Z</published>
    <title>Algebras over a field and semantics for context based reasoning</title>
    <summary>  This paper introduces context algebras and demonstrates their application to
combining logical and vector-based representations of meaning. Other approaches
to this problem attempt to reproduce aspects of logical semantics within new
frameworks. The approach we present here is different: We show how logical
semantics can be embedded within a vector space framework, and use this to
combine distributional semantics, in which the meanings of words are
represented as vectors, with logical semantics, in which the meaning of a
sentence is represented as a logical form.
</summary>
    <author>
      <name>Daoud Clarke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft chapter for a proposed Oxford University Press volume
  "Compositional methods in Physics and Linguistics"</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.1673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.3152v1</id>
    <updated>2011-11-14T09:34:34Z</updated>
    <published>2011-11-14T09:34:34Z</published>
    <title>Évaluation de lexiques syntaxiques par leur intégartion dans
  l'analyseur syntaxiques FRMG</title>
    <summary>  In this paper, we evaluate various French lexica with the parser FRMG: the
Lefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,
the lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,
obtained by merging with DICOVALENCE and partial manual validation. For this,
all these lexica have been converted to the format of the Lefff, Alexina
format. The evaluation was made on the part of the EASy corpus used in the
first evaluation campaign Passage.
</summary>
    <author>
      <name>Elsa Tolone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM, FaMAF</arxiv:affiliation>
    </author>
    <author>
      <name>Éric De La Clergerie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Sagot Benoit</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30\`eme Colloque international sur le Lexique et la Grammaire
  (LGC'11), Nicosie : Chypre (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.3152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.3153v1</id>
    <updated>2011-11-14T09:34:59Z</updated>
    <published>2011-11-14T09:34:59Z</published>
    <title>Construction du lexique LGLex à partir des tables du Lexique-Grammaire
  des verbes du grec moderne</title>
    <summary>  In this paper, we summerize the work done on the resources of Modern Greek on
the Lexicon-Grammar of verbs. We detail the definitional features of each
table, and all changes made to the names of features to make them consistent.
Through the development of the table of classes, including all the features, we
have considered the conversion of tables in a syntactic lexicon: LGLex. The
lexicon, in plain text format or XML, is generated by the LGExtract tool
(Constant &amp; Tolone, 2010). This format is directly usable in applications of
Natural Language Processing (NLP).
</summary>
    <author>
      <name>Kyriaki Ioannidou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTTL</arxiv:affiliation>
    </author>
    <author>
      <name>Elsa Tolone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM, FaMAF</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30\`eme Colloque international sur le Lexique et la Grammaire
  (LGC'11), Nicosie : Chypre (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.3153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.0168v1</id>
    <updated>2011-12-01T12:52:22Z</updated>
    <published>2011-12-01T12:52:22Z</published>
    <title>Statistical Sign Language Machine Translation: from English written text
  to American Sign Language Gloss</title>
    <summary>  This works aims to design a statistical machine translation from English text
to American Sign Language (ASL). The system is based on Moses tool with some
modifications and the results are synthesized through a 3D avatar for
interpretation. First, we translate the input text to gloss, a written form of
ASL. Second, we pass the output to the WebSign Plug-in to play the sign.
Contributions of this work are the use of a new couple of language English/ASL
and an improvement of statistical machine translation based on string matching
thanks to Jaro-distance.
</summary>
    <author>
      <name>Achraf Othman</name>
    </author>
    <author>
      <name>Mohamed Jemni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 3, 2011, 65-73</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.0168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.6384v1</id>
    <updated>2011-12-29T19:16:20Z</updated>
    <published>2011-12-29T19:16:20Z</published>
    <title>Proof nets for the Lambek-Grishin calculus</title>
    <summary>  Grishin's generalization of Lambek's Syntactic Calculus combines a
non-commutative multiplicative conjunction and its residuals (product, left and
right division) with a dual family: multiplicative disjunction, right and left
difference. Interaction between these two families takes the form of linear
distributivity principles. We study proof nets for the Lambek-Grishin calculus
and the correspondence between these nets and unfocused and focused versions of
its sequent calculus.
</summary>
    <author>
      <name>Michael Moortgat</name>
    </author>
    <author>
      <name>Richard Moot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised version to appear as a chapter in E. Grefenstette, C. Heunen,
  and M. Sadrzadeh (eds.) 'Compositional Methods in Physics and Linguistics',
  Oxford University Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.6384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.6384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4733v1</id>
    <updated>2012-01-20T17:35:29Z</updated>
    <published>2012-01-20T17:35:29Z</published>
    <title>Du TAL au TIL</title>
    <summary>  Historically two types of NLP have been investigated: fully automated
processing of language by machines (NLP) and autonomous processing of natural
language by people, i.e. the human brain (psycholinguistics). We believe that
there is room and need for another kind, INLP: interactive natural language
processing. This intermediate approach starts from peoples' needs, trying to
bridge the gap between their actual knowledge and a given goal. Given the fact
that peoples' knowledge is variable and often incomplete, the aim is to build
bridges linking a given knowledge state to a given goal. We present some
examples, trying to show that this goal is worth pursuing, achievable and at a
reasonable cost.
</summary>
    <author>
      <name>Michael Zock</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <author>
      <name>Guy Lapalme</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DIRO</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TALN, Montr\'eal : Canada (2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.4733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.1054v1</id>
    <updated>2012-02-06T06:33:03Z</updated>
    <published>2012-02-06T06:33:03Z</published>
    <title>Considering a resource-light approach to learning verb valencies</title>
    <summary>  Here we describe work on learning the subcategories of verbs in a
morphologically rich language using only minimal linguistic resources. Our goal
is to learn verb subcategorizations for Quechua, an under-resourced
morphologically rich language, from an unannotated corpus. We compare results
from applying this approach to an unannotated Arabic corpus with those achieved
by processing the same text in treebank form. The original plan was to use only
a morphological analyzer and an unannotated corpus, but experiments suggest
that this approach by itself will not be effective for learning the
combinatorial potential of Arabic verbs in general. The lower bound on
resources for acquiring this information is somewhat higher, apparently
requiring a a part-of-speech tagger and chunker for most languages, and a
morphological disambiguater for Arabic.
</summary>
    <author>
      <name>Alex Rudnick</name>
    </author>
    <link href="http://arxiv.org/abs/1202.1054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.1054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.6266v1</id>
    <updated>2012-02-28T16:04:36Z</updated>
    <published>2012-02-28T16:04:36Z</published>
    <title>Realisation d'un systeme de reconnaissance automatique de la parole
  arabe base sur CMU Sphinx</title>
    <summary>  This paper presents the continuation of the work completed by Satori and all.
[SCH07] by the realization of an automatic speech recognition system (ASR) for
Arabic language based SPHINX 4 system. The previous work was limited to the
recognition of the first ten digits, whereas the present work is a remarkable
projection consisting in continuous Arabic speech recognition with a rate of
recognition of surroundings 96%.
</summary>
    <author>
      <name>Ali Sadiqui</name>
    </author>
    <author>
      <name>Noureddine Chenfour</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 27-40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.6266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.6266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.6583v1</id>
    <updated>2012-02-29T15:59:54Z</updated>
    <published>2012-02-29T15:59:54Z</published>
    <title>A Lexical Analysis Tool with Ambiguity Support</title>
    <summary>  Lexical ambiguities naturally arise in languages. We present Lamb, a lexical
analyzer that produces a lexical analysis graph describing all the possible
sequences of tokens that can be found within the input string. Parsers can
process such lexical analysis graphs and discard any sequence of tokens that
does not produce a valid syntactic sentence, therefore performing, together
with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language
specifications.
</summary>
    <author>
      <name>Luis Quesada</name>
    </author>
    <author>
      <name>Fernando Berzal</name>
    </author>
    <author>
      <name>Francisco J. Cortijo</name>
    </author>
    <link href="http://arxiv.org/abs/1202.6583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.6583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5055v1</id>
    <updated>2012-03-22T17:50:08Z</updated>
    <published>2012-03-22T17:50:08Z</published>
    <title>Using Signals to Improve Automatic Classification of Temporal Relations</title>
    <summary>  Temporal information conveyed by language describes how the world around us
changes through time. Events, durations and times are all temporal elements
that can be viewed as intervals. These intervals are sometimes temporally
related in text. Automatically determining the nature of such relations is a
complex and unsolved problem. Some words can act as "signals" which suggest a
temporal ordering between intervals. In this paper, we use these signal words
to improve the accuracy of a recent approach to classification of temporal
links.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>Robert Gaizauskas</name>
    </author>
    <link href="http://arxiv.org/abs/1203.5055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5062v1</id>
    <updated>2012-03-22T18:05:26Z</updated>
    <published>2012-03-22T18:05:26Z</published>
    <title>An Annotation Scheme for Reichenbach's Verbal Tense Structure</title>
    <summary>  In this paper we present RTMML, a markup language for the tenses of verbs and
temporal relations between verbs. There is a richness to tense in language that
is not fully captured by existing temporal annotation schemata. Following
Reichenbach we present an analysis of tense in terms of abstract time points,
with the aim of supporting automated processing of tense and temporal relations
in language. This allows for precise reasoning about tense in documents, and
the deduction of temporal relations between the times and verbal events in a
discourse. We define the syntax of RTMML, and demonstrate the markup in a range
of situations.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>Robert Gaizauskas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 6th Joint ACL-ISO Workshop on Interoperable Semantic
  Annotation (2011) 10-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.5062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5066v1</id>
    <updated>2012-03-22T18:08:47Z</updated>
    <published>2012-03-22T18:08:47Z</published>
    <title>A Corpus-based Study of Temporal Signals</title>
    <summary>  Automatic temporal ordering of events described in discourse has been of
great interest in recent years. Event orderings are conveyed in text via va
rious linguistic mechanisms including the use of expressions such as "before",
"after" or "during" that explicitly assert a temporal relation -- temporal
signals. In this paper, we investigate the role of temporal signals in temporal
relation extraction and provide a quantitative analysis of these expres sions
in the TimeBank annotated corpus.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>Robert Gaizauskas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Corpus Linguistics (2011)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 6th Conference on Corpus Linguistics (2011),
  No. 197, pp. 1--8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.5066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.6136v1</id>
    <updated>2012-03-28T02:13:39Z</updated>
    <published>2012-03-28T02:13:39Z</published>
    <title>Tree Transducers, Machine Translation, and Cross-Language Divergences</title>
    <summary>  Tree transducers are formal automata that transform trees into other trees.
Many varieties of tree transducers have been explored in the automata theory
literature, and more recently, in the machine translation literature. In this
paper I review T and xT transducers, situate them among related formalisms, and
show how they can be used to implement rules for machine translation systems
that cover all of the cross-language structural divergences described in Bonnie
Dorr's influential article on the topic. I also present an implementation of xT
transduction, suitable and convenient for experimenting with translation rules.
</summary>
    <author>
      <name>Alex Rudnick</name>
    </author>
    <link href="http://arxiv.org/abs/1203.6136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.6136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0255v1</id>
    <updated>2012-04-01T19:15:58Z</updated>
    <published>2012-04-01T19:15:58Z</published>
    <title>Keyphrase Extraction : Enhancing Lists</title>
    <summary>  This paper proposes some modest improvements to Extractor, a state-of-the-art
keyphrase extraction system, by using a terabyte-sized corpus to estimate the
informativeness and semantic similarity of keyphrases. We present two
techniques to improve the organization and remove outliers of lists of
keyphrases. The first is a simple ordering according to their occurrences in
the corpus; the second is clustering according to semantic similarity.
Evaluation issues are discussed. We present a novel technique of comparing
extracted keyphrases to a gold standard which relies on semantic similarity
rather than string matching or an evaluation involving human judges.
</summary>
    <author>
      <name>Mario Jarmasz</name>
    </author>
    <author>
      <name>Caroline Barrière</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; Proceedings of the 2nd Conference on Computational
  Linguistics in the North-East (CLiNE 2004), Montr\'eal, Canada, August</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.0255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0258v1</id>
    <updated>2012-04-01T19:25:29Z</updated>
    <published>2012-04-01T19:25:29Z</published>
    <title>Roget's Thesaurus: a Lexical Resource to Treasure</title>
    <summary>  This paper presents the steps involved in creating an electronic lexical
knowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic
relations are labelled with the help of WordNet. The two resources are compared
in a qualitative and quantitative manner. Differences in the organization of
the lexical material are discussed, as well as the possibility of merging both
resources.
</summary>
    <author>
      <name>Mario Jarmasz</name>
    </author>
    <author>
      <name>Stan Szpakowicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the NAACL WordNet and Other Lexical Resources
  workshop. Pittsburgh, June 2001, 186 - 188</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.0258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2009v1</id>
    <updated>2012-06-10T09:27:51Z</updated>
    <published>2012-06-10T09:27:51Z</published>
    <title>Developing a model for a text database indexed pedagogically for
  teaching the Arabic language</title>
    <summary>  In this memory we made the design of an indexing model for Arabic language
and adapting standards for describing learning resources used (the LOM and
their application profiles) with learning conditions such as levels education
of students, their levels of understanding...the pedagogical context with
taking into account the repre-sentative elements of the text, text's
length,...in particular, we highlight the specificity of the Arabic language
which is a complex language, characterized by its flexion, its voyellation and
its agglutination.
</summary>
    <author>
      <name>Asma Boudhief</name>
    </author>
    <author>
      <name>Mohsen Maraoui</name>
    </author>
    <author>
      <name>Mounir Zrigui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages,27 figures,12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.2009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4522v1</id>
    <updated>2012-06-20T15:06:48Z</updated>
    <published>2012-06-20T15:06:48Z</published>
    <title>BADREX: In situ expansion and coreference of biomedical abbreviations
  using dynamic regular expressions</title>
    <summary>  BADREX uses dynamically generated regular expressions to annotate term
definition-term abbreviation pairs, and corefers unpaired acronyms and
abbreviations back to their initial definition in the text. Against the
Medstract corpus BADREX achieves precision and recall of 98% and 97%, and
against a much larger corpus, 90% and 85%, respectively. BADREX yields improved
performance over previous approaches, requires no training data and allows
runtime customisation of its input parameters. BADREX is freely available from
https://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a
plugin for the General Architecture for Text Engineering (GATE) framework and
is licensed under the GPLv3.
</summary>
    <author>
      <name>Phil Gooch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.1.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5333v2</id>
    <updated>2014-05-25T19:10:12Z</updated>
    <published>2012-06-22T22:30:44Z</published>
    <title>TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations</title>
    <summary>  We describe the TempEval-3 task which is currently in preparation for the
SemEval-2013 evaluation exercise. The aim of TempEval is to advance research on
temporal information processing. TempEval-3 follows on from previous TempEval
events, incorporating: a three-part task structure covering event, temporal
expression and temporal relation extraction; a larger dataset; and single
overall task quality scores.
</summary>
    <author>
      <name>Naushad UzZaman</name>
    </author>
    <author>
      <name>Hector Llorens</name>
    </author>
    <author>
      <name>James Allen</name>
    </author>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>Marc Verhagen</name>
    </author>
    <author>
      <name>James Pustejovsky</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1420v1</id>
    <updated>2012-07-04T16:27:56Z</updated>
    <published>2012-07-04T16:27:56Z</published>
    <title>Learning to Map Sentences to Logical Form: Structured Classification
  with Probabilistic Categorial Grammars</title>
    <summary>  This paper addresses the problem of mapping natural language sentences to
lambda-calculus encodings of their meaning. We describe a learning algorithm
that takes as input a training set of sentences labeled with expressions in the
lambda calculus. The algorithm induces a grammar for the problem, along with a
log-linear model that represents a distribution over syntactic and semantic
analyses conditioned on the input sentence. We apply the method to the task of
learning natural language interfaces to databases and show that the learned
parsers outperform previous methods in two benchmark database domains.
</summary>
    <author>
      <name>Luke S. Zettlemoyer</name>
    </author>
    <author>
      <name>Michael Collins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.1420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2714v1</id>
    <updated>2012-07-11T17:31:11Z</updated>
    <published>2012-07-11T17:31:11Z</published>
    <title>Clustering based approach extracting collocations</title>
    <summary>  The following study presents a collocation extraction approach based on
clustering technique. This study uses a combination of several classical
measures which cover all aspects of a given corpus then it suggests separating
bigrams found in the corpus in several disjoint groups according to the
probability of presence of collocations. This will allow excluding groups where
the presence of collocations is very unlikely and thus reducing in a meaningful
way the search space.
</summary>
    <author>
      <name>Mohamed Achraf Ben Mohamed</name>
    </author>
    <author>
      <name>Mounir Zrigui</name>
    </author>
    <author>
      <name>Mohsen Maraoui</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Conference on Arabic Language Processing. CITALA
  2012. May 2nd -- 3rd 2012, Rabat, Morocco</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.2714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4307v1</id>
    <updated>2012-07-18T09:09:35Z</updated>
    <published>2012-07-18T09:09:35Z</published>
    <title>Frame Interpretation and Validation in a Open Domain Dialogue System</title>
    <summary>  Our goal in this paper is to establish a means for a dialogue platform to be
able to cope with open domains considering the possible interaction between the
embodied agent and humans. To this end we present an algorithm capable of
processing natural language utterances and validate them against knowledge
structures of an intelligent agent's mind. Our algorithm leverages dialogue
techniques in order to solve ambiguities and acquire knowledge about unknown
entities.
</summary>
    <author>
      <name>Artur Ventura</name>
    </author>
    <author>
      <name>Nuno Diegues</name>
    </author>
    <author>
      <name>David Martins de Matos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0200v1</id>
    <updated>2012-08-01T13:06:54Z</updated>
    <published>2012-08-01T13:06:54Z</published>
    <title>Adaptation of pedagogical resources description standard (LOM) with the
  specificity of Arabic language</title>
    <summary>  In this article we focus firstly on the principle of pedagogical indexing and
characteristics of Arabic language and secondly on the possibility of adapting
the standard for describing learning resources used (the LOM and its
Application Profiles) with learning conditions such as the educational levels
of students and their levels of understanding,... the educational context with
taking into account the representative elements of text, text length, ... in
particular, we put in relief the specificity of the Arabic language which is a
complex language, characterized by its flexion, its voyellation and
agglutination.
</summary>
    <author>
      <name>Asma Boudhief</name>
    </author>
    <author>
      <name>Mohsen Maraoui</name>
    </author>
    <author>
      <name>Mounir Zrigui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,10 figures. arXiv admin note: substantial text overlap with
  arXiv:1206.2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.0200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1300v1</id>
    <updated>2012-08-19T02:38:12Z</updated>
    <published>2012-08-19T02:38:12Z</published>
    <title>Input Scheme for Hindi Using Phonetic Mapping</title>
    <summary>  Written Communication on Computers requires knowledge of writing text for the
desired language using Computer. Mostly people do not use any other language
besides English. This creates a barrier. To resolve this issue we have
developed a scheme to input text in Hindi using phonetic mapping scheme. Using
this scheme we generate intermediate code strings and match them with
pronunciations of input text. Our system show significant success over other
input systems available.
</summary>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of National Conference on ICT: Theory, Practice and
  Applications. SPSU Press. Organized by Sir Padampat Singhania University,
  Udaipur. Sponsored by CSIR, New Delhi. March, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1301v1</id>
    <updated>2012-08-19T02:31:29Z</updated>
    <published>2012-08-19T02:31:29Z</published>
    <title>Evaluation of Computational Grammar Formalisms for Indian Languages</title>
    <summary>  Natural Language Parsing has been the most prominent research area since the
genesis of Natural Language Processing. Probabilistic Parsers are being
developed to make the process of parser development much easier, accurate and
fast. In Indian context, identification of which Computational Grammar
Formalism is to be used is still a question which needs to be answered. In this
paper we focus on this problem and try to analyze different formalisms for
Indian languages.
</summary>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of International Conference in Computer Engineering and
  Technology, 2012, Organized by Jodhpur Institute of Engineering and
  Technology, Jodhpur. Sponsored by IEEE, USA and Institution of Engineers
  (India), Kolkatta</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2400v1</id>
    <updated>2012-09-11T19:18:26Z</updated>
    <published>2012-09-11T19:18:26Z</published>
    <title>Identification of Fertile Translations in Medical Comparable Corpora: a
  Morpho-Compositional Approach</title>
    <summary>  This paper defines a method for lexicon in the biomedical domain from
comparable corpora. The method is based on compositional translation and
exploits morpheme-level translation equivalences. It can generate translations
for a large variety of morphologically constructed words and can also generate
'fertile' translations. We show that fertile translations increase the overall
quality of the extracted lexicon for English to French translation.
</summary>
    <author>
      <name>Estelle Delpech</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Béatrice Daille</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Morin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Claire Lemaire</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AMTA, San Diego, CA : United States (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0852v1</id>
    <updated>2012-10-02T17:41:58Z</updated>
    <published>2012-10-02T17:41:58Z</published>
    <title>Detecting multiword phrases in mathematical text corpora</title>
    <summary>  We present an approach for detecting multiword phrases in mathematical text
corpora. The method used is based on characteristic features of mathematical
terminology. It makes use of a software tool named Lingo which allows to
identify words by means of previously defined dictionaries for specific word
classes as adjectives, personal names or nouns. The detection of multiword
groups is done algorithmically. Possible advantages of the method for indexing
and information retrieval and conclusions for applying dictionary-based methods
of automatic indexing instead of stemming procedures are discussed.
</summary>
    <author>
      <name>Winfried Gödert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.0852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.7.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5486v2</id>
    <updated>2012-11-11T17:10:35Z</updated>
    <published>2012-10-19T17:49:06Z</published>
    <title>A Lightweight Stemmer for Gujarati</title>
    <summary>  Gujarati is a resource poor language with almost no language processing tools
being available. In this paper we have shown an implementation of a rule based
stemmer of Gujarati. We have shown the creation of rules for stemming and the
richness in morphology that Gujarati possesses. We have also evaluated our
results by verifying it with a human expert.
</summary>
    <author>
      <name>Juhi Ameta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 46th Annual Convention of Computer Society of India</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.5486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5581v1</id>
    <updated>2012-10-20T06:09:11Z</updated>
    <published>2012-10-20T06:09:11Z</published>
    <title>Hidden Trends in 90 Years of Harvard Business Review</title>
    <summary>  In this paper, we demonstrate and discuss results of our mining the abstracts
of the publications in Harvard Business Review between 1922 and 2012.
Techniques for computing n-grams, collocations, basic sentiment analysis, and
named-entity recognition were employed to uncover trends hidden in the
abstracts. We present findings about international relationships, sentiment in
HBR's abstracts, important international companies, influential technological
inventions, renown researchers in management theories, US presidents via
chronological analyses.
</summary>
    <author>
      <name>Chia-Chi Tsai</name>
    </author>
    <author>
      <name>Chao-Lin Liu</name>
    </author>
    <author>
      <name>Wei-Jie Huang</name>
    </author>
    <author>
      <name>Man-Kwan Shan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 14 figures, Proceedings of 2012 International Conference on
  Technologies and Applications of Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.5581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5965v1</id>
    <updated>2012-10-22T16:40:35Z</updated>
    <published>2012-10-22T16:40:35Z</published>
    <title>Classification Analysis Of Authorship Fiction Texts in The Space Of
  Semantic Fields</title>
    <summary>  The use of naive Bayesian classifier (NB) and the classifier by the k nearest
neighbors (kNN) in classification semantic analysis of authors' texts of
English fiction has been analysed. The authors' works are considered in the
vector space the basis of which is formed by the frequency characteristics of
semantic fields of nouns and verbs. Highly precise classification of authors'
texts in the vector space of semantic fields indicates about the presence of
particular spheres of author's idiolect in this space which characterizes the
individual author's style.
</summary>
    <author>
      <name>Bohdan Pavlyshenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.5965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0418v1</id>
    <updated>2012-11-02T11:09:44Z</updated>
    <published>2012-11-02T11:09:44Z</published>
    <title>Verbalizing Ontologies in Controlled Baltic Languages</title>
    <summary>  Controlled natural languages (mostly English-based) recently have emerged as
seemingly informal supplementary means for OWL ontology authoring, if compared
to the formal notations that are used by professional knowledge engineers. In
this paper we present by examples controlled Latvian language that has been
designed to be compliant with the state of the art Attempto Controlled English.
We also discuss relation with controlled Lithuanian language that is being
designed in parallel.
</summary>
    <author>
      <name>Normunds Grūzītis</name>
    </author>
    <author>
      <name>Gunta Nešpore</name>
    </author>
    <author>
      <name>Baiba Saulīte</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/978-1-60750-641-6-187</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/978-1-60750-641-6-187" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Human Language Technologies - The Baltic Perspective, Frontiers in
  Artificial Intelligence and Applications, Vol. 219, IOS Press, 2010, pp.
  187-194</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.0418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0498v1</id>
    <updated>2012-11-02T17:37:06Z</updated>
    <published>2012-11-02T17:37:06Z</published>
    <title>Detecting English Writing Styles For Non-native Speakers</title>
    <summary>  Analyzing writing styles of non-native speakers is a challenging task. In
this paper, we analyze the comments written in the discussion pages of the
English Wikipedia. Using learning algorithms, we are able to detect native
speakers' writing style with an accuracy of 74%. Given the diversity of the
English Wikipedia users and the large number of languages they speak, we
measure the similarities among their native languages by comparing the
influence they have on their English writing style. Our results show that
languages known to have the same origin and development path have similar
footprint on their speakers' English writing style. To enable further studies,
the dataset we extracted from Wikipedia will be made available publicly.
</summary>
    <author>
      <name>Rami Al-Rfou'</name>
    </author>
    <link href="http://arxiv.org/abs/1211.0498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0927v3</id>
    <updated>2013-02-05T16:35:34Z</updated>
    <published>2012-12-05T03:50:46Z</published>
    <title>Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown
  Automaton</title>
    <summary>  We introduce efficient algorithms for finding the $k$ shortest paths of a
weighted pushdown automaton (WPDA), a compact representation of a weighted set
of strings with potential applications in parsing and machine translation. Both
of our algorithms are derived from the same weighted deductive logic
description of the execution of a WPDA using different search strategies.
Experimental results show our Algorithm 2 adds very little overhead vs. the
single shortest path algorithm, even with a large $k$.
</summary>
    <author>
      <name>Ke Wu</name>
    </author>
    <author>
      <name>Philip Resnik</name>
    </author>
    <link href="http://arxiv.org/abs/1212.0927v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0927v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1478v1</id>
    <updated>2012-12-06T21:28:19Z</updated>
    <published>2012-12-06T21:28:19Z</published>
    <title>The Clustering of Author's Texts of English Fiction in the Vector Space
  of Semantic Fields</title>
    <summary>  The clustering of text documents in the vector space of semantic fields and
in the semantic space with orthogonal basis has been analysed. It is shown that
using the vector space model with the basis of semantic fields is effective in
the cluster analysis algorithms of author's texts in English fiction. The
analysis of the author's texts distribution in cluster structure showed the
presence of the areas of semantic space that represent the author's ideolects
of individual authors. SVD factorization of the semantic fields matrix makes it
possible to reduce significantly the dimension of the semantic space in the
cluster analysis of author's texts.
</summary>
    <author>
      <name>Bohdan Pavlyshenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.1478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2006v2</id>
    <updated>2013-12-27T17:28:14Z</updated>
    <published>2012-12-10T09:41:12Z</published>
    <title>A Novel Feature-based Bayesian Model for Query Focused Multi-document
  Summarization</title>
    <summary>  Both supervised learning methods and LDA based topic model have been
successfully applied in the field of query focused multi-document
summarization. In this paper, we propose a novel supervised approach that can
incorporate rich sentence features into Bayesian topic models in a principled
way, thus taking advantages of both topic model and feature based supervised
learning methods. Experiments on TAC2008 and TAC2009 demonstrate the
effectiveness of our approach.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <author>
      <name>Sujian Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in equation</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2006v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2006v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3138v1</id>
    <updated>2012-12-13T11:47:09Z</updated>
    <published>2012-12-13T11:47:09Z</published>
    <title>Identifying Metaphor Hierarchies in a Corpus Analysis of Finance
  Articles</title>
    <summary>  Using a corpus of over 17,000 financial news reports (involving over 10M
words), we perform an analysis of the argument-distributions of the UP- and
DOWN-verbs used to describe movements of indices, stocks, and shares. Using
measures of the overlap in the argument distributions of these verbs and
k-means clustering of their distributions, we advance evidence for the proposal
that the metaphors referred to by these verbs are organised into hierarchical
structures of superordinate and subordinate groups.
</summary>
    <author>
      <name>Aaron Georw</name>
    </author>
    <author>
      <name>Mark Keane</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 33rd Annual Meeting of the Cognitive Science
  Society (CogSci '11), Boston, MA, USA, 20-23 July, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.3138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3493v2</id>
    <updated>2012-12-17T13:10:47Z</updated>
    <published>2012-12-14T14:51:22Z</published>
    <title>Sentence Compression in Spanish driven by Discourse Segmentation and
  Language Models</title>
    <summary>  Previous works demonstrated that Automatic Text Summarization (ATS) by
sentences extraction may be improved using sentence compression. In this work
we present a sentence compressions approach guided by level-sentence discourse
segmentation and probabilistic language models (LM). The results presented here
show that the proposed solution is able to generate coherent summaries with
grammatical compressed sentences. The approach is simple enough to be
transposed into other languages.
</summary>
    <author>
      <name>Alejandro Molina</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Iria da Cunha</name>
    </author>
    <author>
      <name>Eric SanJuan</name>
    </author>
    <author>
      <name>Gerardo Sierra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3493v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3493v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4315v1</id>
    <updated>2012-12-18T11:33:50Z</updated>
    <published>2012-12-18T11:33:50Z</published>
    <title>Assessing Sentiment Strength in Words Prior Polarities</title>
    <summary>  Many approaches to sentiment analysis rely on lexica where words are tagged
with their prior polarity - i.e. if a word out of context evokes something
positive or something negative. In particular, broad-coverage resources like
SentiWordNet provide polarities for (almost) every word. Since words can have
multiple senses, we address the problem of how to compute the prior polarity of
a word starting from the polarity of each sense and returning its polarity
strength as an index between -1 and 1. We compare 14 such formulae that appear
in the literature, and assess which one best approximates the human judgement
of prior polarities, with both regression and classification models.
</summary>
    <author>
      <name>Lorenzo Gatti</name>
    </author>
    <author>
      <name>Marco Guerini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at Coling 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.4315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2466v1</id>
    <updated>2013-01-11T12:02:23Z</updated>
    <published>2013-01-11T12:02:23Z</published>
    <title>Determining token sequence mistakes in responses to questions with open
  text answer</title>
    <summary>  When learning grammar of the new language, a teacher should routinely check
student's exercises for grammatical correctness. The paper describes a method
of automatically detecting and reporting grammar mistakes, regarding an order
of tokens in the response. It could report extra tokens, missing tokens and
misplaced tokens. The method is useful when teaching language, where order of
tokens is important, which includes most formal languages and some natural ones
(like English). The method was implemented in a question type plug-in
CorrectWriting for the widely used learning manage system Moodle.
</summary>
    <author>
      <name>Oleg Sychev</name>
    </author>
    <author>
      <name>Dmitry Mamontov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2811v3</id>
    <updated>2013-04-26T12:33:50Z</updated>
    <published>2013-01-13T19:33:31Z</published>
    <title>Cutting Recursive Autoencoder Trees</title>
    <summary>  Deep Learning models enjoy considerable success in Natural Language
Processing. While deep architectures produce useful representations that lead
to improvements in various tasks, they are often difficult to interpret. This
makes the analysis of learned structures particularly difficult. In this paper,
we rely on empirical tests to see whether a particular structure makes sense.
We present an analysis of the Semi-Supervised Recursive Autoencoder, a
well-known model that produces structural representations of text. We show that
for certain tasks, the structure of the autoencoder can be significantly
reduced without loss of classification accuracy and we evaluate the produced
structures using human judgment.
</summary>
    <author>
      <name>Christian Scheible</name>
    </author>
    <author>
      <name>Hinrich Schuetze</name>
    </author>
    <link href="http://arxiv.org/abs/1301.2811v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2811v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2857v1</id>
    <updated>2013-01-14T04:01:25Z</updated>
    <published>2013-01-14T04:01:25Z</published>
    <title>SpeedRead: A Fast Named Entity Recognition Pipeline</title>
    <summary>  Online content analysis employs algorithmic methods to identify entities in
unstructured text. Both machine learning and knowledge-base approaches lie at
the foundation of contemporary named entities extraction systems. However, the
progress in deploying these approaches on web-scale has been been hampered by
the computational cost of NLP over massive text corpora. We present SpeedRead
(SR), a named entity recognition pipeline that runs at least 10 times faster
than Stanford NLP pipeline. This pipeline consists of a high performance Penn
Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS)
tagger and knowledge-based named entity recognizer.
</summary>
    <author>
      <name>Rami Al-Rfou'</name>
    </author>
    <author>
      <name>Steven Skiena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long paper at COLING 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3214v1</id>
    <updated>2013-01-15T03:45:27Z</updated>
    <published>2013-01-15T03:45:27Z</published>
    <title>The Manifold of Human Emotions</title>
    <summary>  Sentiment analysis predicts the presence of positive or negative emotions in
a text document. In this paper, we consider higher dimensional extensions of
the sentiment concept, which represent a richer set of human emotions. Our
approach goes beyond previous work in that our model contains a continuous
manifold rather than a finite set of human emotions. We investigate the
resulting model, compare it to psychological observations, and explore its
predictive capabilities.
</summary>
    <author>
      <name>Seungyeon Kim</name>
    </author>
    <author>
      <name>Fuxin Li</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <author>
      <name>Irfan Essa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3627v2</id>
    <updated>2013-05-11T12:17:44Z</updated>
    <published>2013-01-16T08:37:39Z</published>
    <title>Two SVDs produce more focal deep learning representations</title>
    <summary>  A key characteristic of work on deep learning and neural networks in general
is that it relies on representations of the input that support generalization,
robust inference, domain adaptation and other desirable functionalities. Much
recent progress in the field has focused on efficient and effective methods for
computing representations. In this paper, we propose an alternative method that
is more efficient than prior work and produces representations that have a
property we call focality -- a property we hypothesize to be important for
neural network representations. The method consists of a simple application of
two consecutive SVDs and is inspired by Anandkumar (2012).
</summary>
    <author>
      <name>Hinrich Schuetze</name>
    </author>
    <author>
      <name>Christian Scheible</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3627v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3627v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3781v3</id>
    <updated>2013-09-07T00:30:40Z</updated>
    <published>2013-01-16T18:24:43Z</published>
    <title>Efficient Estimation of Word Representations in Vector Space</title>
    <summary>  We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.
</summary>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Greg Corrado</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3781v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3781v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2448v1</id>
    <updated>2013-03-11T08:21:17Z</updated>
    <published>2013-03-11T08:21:17Z</published>
    <title>Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon
  Production</title>
    <summary>  In this work we present the results of our experimental work on the
develop-ment of lexical class-based lexica by automatic means. The objective is
to as-sess the use of linguistic lexical-class based information as a feature
selection methodology for the use of classifiers in quick lexical development.
The results show that the approach can help in re-ducing the human effort
required in the development of language resources sig-nificantly.
</summary>
    <author>
      <name>Núria Bel</name>
    </author>
    <author>
      <name>Maria Coll</name>
    </author>
    <author>
      <name>Gabriela Resnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures. Also available in UPF institutional repository
  (http://hdl.handle.net/10230/20325)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 23rd International Conference on Computational
  Linguistics (Coling 2010); 2010 Aug 23-27; Beijing, CN. Stroudsburg: ACL;
  2010. p. 46-52</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.2448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5880v1</id>
    <updated>2013-04-22T09:06:36Z</updated>
    <published>2013-04-22T09:06:36Z</published>
    <title>Dealing with natural language interfaces in a geolocation context</title>
    <summary>  In the geolocation field where high-level programs and low-level devices
coexist, it is often difficult to find a friendly user inter- face to configure
all the parameters. The challenge addressed in this paper is to propose
intuitive and simple, thus natural lan- guage interfaces to interact with
low-level devices. Such inter- faces contain natural language processing and
fuzzy represen- tations of words that facilitate the elicitation of
business-level objectives in our context.
</summary>
    <author>
      <name>M. -A. Abchir</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CHART</arxiv:affiliation>
    </author>
    <author>
      <name>Isis Truck</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CHART</arxiv:affiliation>
    </author>
    <author>
      <name>Anna Pappa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIASD</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1304.5880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7157v1</id>
    <updated>2013-04-26T13:27:19Z</updated>
    <published>2013-04-26T13:27:19Z</published>
    <title>Question Answering Against Very-Large Text Collections</title>
    <summary>  Question answering involves developing methods to extract useful information
from large collections of documents. This is done with specialised search
engines such as Answer Finder. The aim of Answer Finder is to provide an answer
to a question rather than a page listing related documents that may contain the
correct answer. So, a question such as "How tall is the Eiffel Tower" would
simply return "325m" or "1,063ft". Our task was to build on the current version
of Answer Finder by improving information retrieval, and also improving the
pre-processing involved in question series analysis.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <author>
      <name>Richard Shaw</name>
    </author>
    <author>
      <name>Ben Solway</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Master's theses, 2008, University of Sheffield</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7282v1</id>
    <updated>2013-04-25T10:25:41Z</updated>
    <published>2013-04-25T10:25:41Z</published>
    <title>An Improved Approach for Word Ambiguity Removal</title>
    <summary>  Word ambiguity removal is a task of removing ambiguity from a word, i.e.
correct sense of word is identified from ambiguous sentences. This paper
describes a model that uses Part of Speech tagger and three categories for word
sense disambiguation (WSD). Human Computer Interaction is very needful to
improve interactions between users and computers. For this, the Supervised and
Unsupervised methods are combined. The WSD algorithm is used to find the
efficient and accurate sense of a word based on domain information. The
accuracy of this work is evaluated with the aim of finding best suitable domain
of word.
</summary>
    <author>
      <name>Priti Saktel</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages:12 Tables: 07 Figures: 14, International Journal of Human
  Computer Interaction (IJHCI), Volume (3): Issue (3): 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7728v1</id>
    <updated>2013-04-29T18:04:26Z</updated>
    <published>2013-04-29T18:04:26Z</published>
    <title>Machine Translation Systems in India</title>
    <summary>  Machine Translation is the translation of one natural language into another
using automated and computerized means. For a multilingual country like India,
with the huge amount of information exchanged between various regions and in
different languages in digitized format, it has become necessary to find an
automated process from one language to another. In this paper, we take a look
at the various Machine Translation System in India which is specifically built
for the purpose of translation between the Indian languages. We discuss the
various approaches taken for building the machine translation system and then
discuss some of the Machine Translation Systems in India along with their
features.
</summary>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <author>
      <name>Rajdeep Borgohain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0556v2</id>
    <updated>2013-10-11T17:18:51Z</updated>
    <published>2013-05-02T18:49:01Z</published>
    <title>A quantum teleportation inspired algorithm produces sentence meaning
  from word meaning and grammatical structure</title>
    <summary>  We discuss an algorithm which produces the meaning of a sentence given
meanings of its words, and its resemblance to quantum teleportation. In fact,
this protocol was the main source of inspiration for this algorithm which has
many applications in the area of Natural Language Processing.
</summary>
    <author>
      <name>Stephen Clark</name>
    </author>
    <author>
      <name>Bob Coecke</name>
    </author>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <author>
      <name>Stephen Pulman</name>
    </author>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, many pictures</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.0556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1319v1</id>
    <updated>2013-05-06T20:27:55Z</updated>
    <published>2013-05-06T20:27:55Z</published>
    <title>New Alignment Methods for Discriminative Book Summarization</title>
    <summary>  We consider the unsupervised alignment of the full text of a book with a
human-written summary. This presents challenges not seen in other text
alignment problems, including a disparity in length and, consequent to this, a
violation of the expectation that individual words and phrases should align,
since large passages and chapters can be distilled into a single summary
phrase. We present two new methods, based on hidden Markov models, specifically
targeted to this problem, and demonstrate gains on an extractive book
summarization task. While there is still much room for improvement,
unsupervised alignment holds intrinsic value in offering insight into what
features of a book are deemed worthy of summarization.
</summary>
    <author>
      <name>David Bamman</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper reflects work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3310v1</id>
    <updated>2013-07-12T03:05:29Z</updated>
    <published>2013-07-12T03:05:29Z</published>
    <title>Improving the quality of Gujarati-Hindi Machine Translation through
  part-of-speech tagging and stemmer-assisted transliteration</title>
    <summary>  Machine Translation for Indian languages is an emerging research area.
Transliteration is one such module that we design while designing a translation
system. Transliteration means mapping of source language text into the target
language. Simple mapping decreases the efficiency of overall translation
system. We propose the use of stemming and part-of-speech tagging for
transliteration. The effectiveness of translation can be improved if we use
part-of-speech tagging and stemming assisted transliteration.We have shown that
much of the content in Gujarati gets transliterated while being processed for
translation to Hindi language.
</summary>
    <author>
      <name>Juhi Ameta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijnlc.2013.2305</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijnlc.2013.2305" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; June 2013,
  url-http://airccse.org/journal/ijnlc/papers/2313ijnlc05.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3336v1</id>
    <updated>2013-07-12T06:20:36Z</updated>
    <published>2013-07-12T06:20:36Z</published>
    <title>Opinion Mining and Analysis: A survey</title>
    <summary>  The current research is focusing on the area of Opinion Mining also called as
sentiment analysis due to sheer volume of opinion rich web resources such as
discussion forums, review sites and blogs are available in digital form. One
important problem in sentiment analysis of product reviews is to produce
summary of opinions based on product features. We have surveyed and analyzed in
this paper, various techniques that have been developed for the key tasks of
opinion mining. We have provided an overall picture of what is involved in
developing a software system for opinion mining on the basis of our survey and
analysis.
</summary>
    <author>
      <name>Arti Buche</name>
    </author>
    <author>
      <name>Dr. M. B. Chandak</name>
    </author>
    <author>
      <name>Akshay Zadgaonkar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijnlc.2013.2304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijnlc.2013.2304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJNLC Vol. 2, No.3, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3489v1</id>
    <updated>2013-07-11T10:21:01Z</updated>
    <published>2013-07-11T10:21:01Z</published>
    <title>Genetic approach for arabic part of speech tagging</title>
    <summary>  With the growing number of textual resources available, the ability to
understand them becomes critical. An essential first step in understanding
these sources is the ability to identify the part of speech in each sentence.
Arabic is a morphologically rich language, wich presents a challenge for part
of speech tagging. In this paper, our goal is to propose, improve and implement
a part of speech tagger based on a genetic alorithm. The accuracy obtained with
this method is comparable to that of other probabilistic approaches.
</summary>
    <author>
      <name>Bilel Ben Ali</name>
    </author>
    <author>
      <name>Fethi Jarray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Natural Language Computing (IJNLC) Vol.
  2, No.3, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4299v1</id>
    <updated>2013-07-15T15:59:12Z</updated>
    <published>2013-07-15T15:59:12Z</published>
    <title>Part of Speech Tagging of Marathi Text Using Trigram Method</title>
    <summary>  In this paper we present a Marathi part of speech tagger. It is a
morphologically rich language. It is spoken by the native people of
Maharashtra. The general approach used for development of tagger is statistical
using trigram Method. The main concept of trigram is to explore the most likely
POS for a token based on given information of previous two tags by calculating
probabilities to determine which is the best sequence of a tag. In this paper
we show the development of the tagger. Moreover we have also shown the
evaluation done.
</summary>
    <author>
      <name>Jyoti Singh</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijait.2013.3203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijait.2013.3203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Information Technology (IJAIT) Vol.
  3, No.2, April2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6726v1</id>
    <updated>2013-07-25T12:53:54Z</updated>
    <published>2013-07-25T12:53:54Z</published>
    <title>Information content versus word length in natural language: A reply to
  Ferrer-i-Cancho and Moscoso del Prado Martin [arXiv:1209.1751]</title>
    <summary>  Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751]
argued that an observed linear relationship between word length and average
surprisal (Piantadosi, Tily, &amp; Gibson, 2011) is not evidence for communicative
efficiency in human language. We discuss several shortcomings of their approach
and critique: their model critically rests on inaccurate assumptions, is
incapable of explaining key surprisal patterns in language, and is incompatible
with recent behavioral results. More generally, we argue that statistical
models must not critically rely on assumptions that are incompatible with the
real system under study.
</summary>
    <author>
      <name>Steven T. Piantadosi</name>
    </author>
    <author>
      <name>Harry Tily</name>
    </author>
    <author>
      <name>Edward Gibson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.6726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7382v1</id>
    <updated>2013-07-28T16:55:27Z</updated>
    <published>2013-07-28T16:55:27Z</published>
    <title>Learning Frames from Text with an Unsupervised Latent Variable Model</title>
    <summary>  We develop a probabilistic latent-variable model to discover semantic
frames---types of events and their participants---from corpora. We present a
Dirichlet-multinomial model in which frames are latent categories that explain
the linking of verb-subject-object triples, given document-level sparsity. We
analyze what the model learns, and compare it to FrameNet, noting it learns
some novel and interesting frames. This document also contains a discussion of
inference issues, including concentration parameter learning; and a small-scale
error analysis of syntactic parsing accuracy.
</summary>
    <author>
      <name>Brendan O'Connor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages; technical report for Data Analysis Project requirement,
  Machine Learning Department, Carnegie Mellon University</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7973v1</id>
    <updated>2013-07-30T13:37:09Z</updated>
    <published>2013-07-30T13:37:09Z</published>
    <title>Connecting Language and Knowledge Bases with Embedding Models for
  Relation Extraction</title>
    <summary>  This paper proposes a novel approach for relation extraction from free text
which is trained to jointly use information from the text and from existing
knowledge. Our model is based on two scoring functions that operate by learning
low-dimensional embeddings of words and of entities and relationships from a
knowledge base. We empirically show on New York Times articles aligned with
Freebase relations that our approach is able to efficiently use the extra
information provided by a large subset of Freebase data (4M entities, 23k
relationships) to improve over existing methods that rely on text features
alone.
</summary>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Antoine Bordes</name>
    </author>
    <author>
      <name>Oksana Yakhnenko</name>
    </author>
    <author>
      <name>Nicolas Usunier</name>
    </author>
    <link href="http://arxiv.org/abs/1307.7973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1004v3</id>
    <updated>2013-10-02T07:57:15Z</updated>
    <published>2013-08-05T15:14:14Z</published>
    <title>Boundary identification of events in clinical named entity recognition</title>
    <summary>  The problem of named entity recognition in the medical/clinical domain has
gained increasing attention do to its vital role in a wide range of clinical
decision support applications. The identification of complete and correct term
span is vital for further knowledge synthesis (e.g., coding/mapping concepts
thesauruses and classification standards). This paper investigates boundary
adjustment by sequence labeling representations models and post-processing
techniques in the problem of clinical named entity recognition (recognition of
clinical events). Using current state-of-the-art sequence labeling algorithm
(conditional random fields), we show experimentally that sequence labeling
representation and post-processing can be significantly helpful in strict
boundary identification of clinical events.
</summary>
    <author>
      <name>Azad Dehghan</name>
    </author>
    <link href="http://arxiv.org/abs/1308.1004v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1004v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3839v2</id>
    <updated>2013-12-30T05:42:12Z</updated>
    <published>2013-08-18T07:09:03Z</published>
    <title>Consensus Sequence Segmentation</title>
    <summary>  In this paper we introduce a method to detect words or phrases in a given
sequence of alphabets without knowing the lexicon. Our linear time unsupervised
algorithm relies entirely on statistical relationships among alphabets in the
input sequence to detect location of word boundaries. We compare our algorithm
to previous approaches from unsupervised sequence segmentation literature and
provide superior segmentation over number of benchmarks.
</summary>
    <author>
      <name>Tamal Chowdhury</name>
    </author>
    <author>
      <name>Rabindra Rakshit</name>
    </author>
    <author>
      <name>Arko Banerjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors. The paper has been
  withdrawn due to error data input in table no. 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3839v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3839v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5423v1</id>
    <updated>2013-08-25T16:41:06Z</updated>
    <published>2013-08-25T16:41:06Z</published>
    <title>A Literature Review: Stemming Algorithms for Indian Languages</title>
    <summary>  Stemming is the process of extracting root word from the given inflection
word. It also plays significant role in numerous application of Natural
Language Processing (NLP). The stemming problem has addressed in many contexts
and by researchers in many disciplines. This expository paper presents survey
of some of the latest developments on stemming algorithms in data mining and
also presents with some of the solutions for various Indian language stemming
algorithms along with the results.
</summary>
    <author>
      <name>M. Thangarasu</name>
    </author>
    <author>
      <name>R. Manavalan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology, Vol 4, No
  8, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.5423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1125v1</id>
    <updated>2013-09-04T18:10:22Z</updated>
    <published>2013-09-04T18:10:22Z</published>
    <title>Learning to answer questions</title>
    <summary>  We present an open-domain Question-Answering system that learns to answer
questions based on successful past interactions. We follow a pattern-based
approach to Answer-Extraction, where (lexico-syntactic) patterns that relate a
question to its answer are automatically learned and used to answer future
questions. Results show that our approach contributes to the system's best
performance when it is conjugated with typical Answer-Extraction strategies.
Moreover, it allows the system to learn with the answered questions and to
rectify wrong or unsolved past questions.
</summary>
    <author>
      <name>Ana Cristina Mendes</name>
    </author>
    <author>
      <name>Luísa Coheur</name>
    </author>
    <author>
      <name>Sérgio Curto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.1125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1129v1</id>
    <updated>2013-09-04T18:23:30Z</updated>
    <published>2013-09-04T18:23:30Z</published>
    <title>Analysing Quality of English-Hindi Machine Translation Engine Outputs
  Using Bayesian Classification</title>
    <summary>  This paper considers the problem for estimating the quality of machine
translation outputs which are independent of human intervention and are
generally addressed using machine learning techniques.There are various
measures through which a machine learns translations quality. Automatic
Evaluation metrics produce good co-relation at corpus level but cannot produce
the same results at the same segment or sentence level. In this paper 16
features are extracted from the input sentences and their translations and a
quality score is obtained based on Bayesian inference produced from training
data.
</summary>
    <author>
      <name>Rashmi Gupta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2013.4415</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2013.4415" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 4, No. 4, July 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.1129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1649v2</id>
    <updated>2013-09-10T14:11:46Z</updated>
    <published>2013-09-06T14:28:02Z</published>
    <title>Preparing Korean Data for the Shared Task on Parsing Morphologically
  Rich Languages</title>
    <summary>  This document gives a brief description of Korean data prepared for the SPMRL
2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for
the shared task. All constituent trees are collected from the KAIST Treebank
and transformed to the Penn Treebank style. All dependency trees are converted
from the transformed constituent trees using heuristics and labeling rules de-
signed specifically for the KAIST Treebank. In addition to the gold-standard
morphological analysis provided by the KAIST Treebank, two sets of automatic
morphological analysis are provided for the shared task, one is generated by
the HanNanum morphological analyzer, and the other is generated by the Sejong
morphological analyzer.
</summary>
    <author>
      <name>Jinho D. Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.1649v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1649v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2853v1</id>
    <updated>2013-09-11T15:16:26Z</updated>
    <published>2013-09-11T15:16:26Z</published>
    <title>General Purpose Textual Sentiment Analysis and Emotion Detection Tools</title>
    <summary>  Textual sentiment analysis and emotion detection consists in retrieving the
sentiment or emotion carried by a text or document. This task can be useful in
many domains: opinion mining, prediction, feedbacks, etc. However, building a
general purpose tool for doing sentiment analysis and emotion detection raises
a number of issues, theoretical issues like the dependence to the domain or to
the language but also pratical issues like the emotion representation for
interoperability. In this paper we present our sentiment/emotion analysis
tools, the way we propose to circumvent the di culties and the applications
they are used for.
</summary>
    <author>
      <name>Alexandre Denis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel Cruz-Lara</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Nadia Bellalem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Emotion and Computing (2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.2853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.4058v1</id>
    <updated>2013-09-16T18:21:54Z</updated>
    <published>2013-09-16T18:21:54Z</published>
    <title>Why SOV might be initially preferred and then lost or recovered? A
  theoretical framework</title>
    <summary>  Little is known about why SOV order is initially preferred and then discarded
or recovered. Here we present a framework for understanding these and many
related word order phenomena: the diversity of dominant orders, the existence
of free words orders, the need of alternative word orders and word order
reversions and cycles in evolution. Under that framework, word order is
regarded as a multiconstraint satisfaction problem in which at least two
constraints are in conflict: online memory minimization and maximum
predictability.
</summary>
    <author>
      <name>Ramon Ferrer-i-Cancho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/9789814603638_0007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/9789814603638_0007" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Evolang 2014, Cartmill, E. A., Roberts, S., Lyn, H.
  &amp; Cornish, H. (eds.), pp. 66-73 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.4058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.4168v1</id>
    <updated>2013-09-17T03:23:13Z</updated>
    <published>2013-09-17T03:23:13Z</published>
    <title>Exploiting Similarities among Languages for Machine Translation</title>
    <summary>  Dictionaries and phrase tables are the basis of modern statistical machine
translation systems. This paper develops a method that can automate the process
of generating and extending dictionaries and phrase tables. Our method can
translate missing word and phrase entries by learning language structures based
on large monolingual data and mapping between languages from small bilingual
data. It uses distributed representation of words and learns a linear mapping
between vector spaces of languages. Despite its simplicity, our method is
surprisingly effective: we can achieve almost 90% precision@5 for translation
of words between English and Spanish. This method makes little assumption about
the languages, so it can be used to extend and refine dictionaries and
translation tables for any language pairs.
</summary>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <link href="http://arxiv.org/abs/1309.4168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5652v1</id>
    <updated>2013-09-22T21:09:07Z</updated>
    <published>2013-09-22T21:09:07Z</published>
    <title>LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual</title>
    <summary>  The Linguistic Data Consortium (LDC) has developed hundreds of data corpora
for natural language processing (NLP) research. Among these are a number of
annotated treebank corpora for Arabic. Typically, these corpora consist of a
single collection of annotated documents. NLP research, however, usually
requires multiple data sets for the purposes of training models, developing
techniques, and final evaluation. Therefore it becomes necessary to divide the
corpora used into the required data sets (divisions). This document details a
set of rules that have been defined to enable consistent divisions for old and
new Arabic treebanks (ATB) and related corpora.
</summary>
    <author>
      <name>Mona Diab</name>
    </author>
    <author>
      <name>Nizar Habash</name>
    </author>
    <author>
      <name>Owen Rambow</name>
    </author>
    <author>
      <name>Ryan Roth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; one cover</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6176v1</id>
    <updated>2013-09-23T13:51:28Z</updated>
    <published>2013-09-23T13:51:28Z</published>
    <title>Feature Learning with Gaussian Restricted Boltzmann Machine for Robust
  Speech Recognition</title>
    <summary>  In this paper, we first present a new variant of Gaussian restricted
Boltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann
machine (MGRBM), with its definition and learning algorithm. Then we propose
using a learned GRBM or MGRBM to extract better features for robust speech
recognition. Our experiments on Aurora2 show that both GRBM-extracted and
MGRBM-extracted feature performs much better than Mel-frequency cepstral
coefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)
acoustic model, and MGRBM-extracted feature is slightly better.
</summary>
    <author>
      <name>Xin Zheng</name>
    </author>
    <author>
      <name>Zhiyong Wu</name>
    </author>
    <author>
      <name>Helen Meng</name>
    </author>
    <author>
      <name>Weifeng Li</name>
    </author>
    <author>
      <name>Lianhong Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7312v1</id>
    <updated>2013-09-27T17:54:14Z</updated>
    <published>2013-09-27T17:54:14Z</published>
    <title>Development and Transcription of Assamese Speech Corpus</title>
    <summary>  A balanced speech corpus is the basic need for any speech processing task. In
this report we describe our effort on development of Assamese speech corpus. We
mainly focused on some issues and challenges faced during development of the
corpus. Being a less computationally aware language, this is the first effort
to develop speech corpus for Assamese. As corpus development is an ongoing
process, in this paper we report only the initial task.
</summary>
    <author>
      <name>Himangshu Sarma</name>
    </author>
    <author>
      <name>Navanath Saharia</name>
    </author>
    <author>
      <name>Utpal Sharma</name>
    </author>
    <author>
      <name>Smriti Kumar Sinha</name>
    </author>
    <author>
      <name>Mancha Jyoti Malakar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 page,National Conferance</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0581v1</id>
    <updated>2013-10-02T06:15:03Z</updated>
    <published>2013-10-02T06:15:03Z</published>
    <title>Rule Based Stemmer in Urdu</title>
    <summary>  Urdu is a combination of several languages like Arabic, Hindi, English,
Turkish, Sanskrit etc. It has a complex and rich morphology. This is the reason
why not much work has been done in Urdu language processing. Stemming is used
to convert a word into its respective root form. In stemming, we separate the
suffix and prefix from the word. It is useful in search engines, natural
language processing and word processing, spell checkers, word parsing, word
frequency and count studies. This paper presents a rule based stemmer for Urdu.
The stemmer that we have discussed here is used in information retrieval. We
have also evaluated our results by verifying it with a human expert.
</summary>
    <author>
      <name>Vaishali Gupta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 4th International Conference on Computer and
  Communication Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0754v1</id>
    <updated>2013-10-02T16:23:00Z</updated>
    <published>2013-10-02T16:23:00Z</published>
    <title>Stemmers for Tamil Language: Performance Analysis</title>
    <summary>  Stemming is the process of extracting root word from the given inflection
word and also plays significant role in numerous application of Natural
Language Processing (NLP). Tamil Language raises several challenges to NLP,
since it has rich morphological patterns than other languages. The rule based
approach light-stemmer is proposed in this paper, to find stem word for given
inflection Tamil word. The performance of proposed approach is compared to a
rule based suffix removal stemmer based on correctly and incorrectly predicted.
The experimental result clearly show that the proposed approach light stemmer
for Tamil language perform better than suffix removal stemmer and also more
effective in Information Retrieval System (IRS).
</summary>
    <author>
      <name>M. Thangarasu</name>
    </author>
    <author>
      <name>R. Manavalan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Engineering
  Technology, Vol. 4, No. 07, Jul 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.0754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1425v1</id>
    <updated>2013-10-05T00:33:46Z</updated>
    <published>2013-10-05T00:33:46Z</published>
    <title>A State of the Art of Word Sense Induction: A Way Towards Word Sense
  Disambiguation for Under-Resourced Languages</title>
    <summary>  Word Sense Disambiguation (WSD), the process of automatically identifying the
meaning of a polysemous word in a sentence, is a fundamental task in Natural
Language Processing (NLP). Progress in this approach to WSD opens up many
promising developments in the field of NLP and its applications. Indeed,
improvement over current performance levels could allow us to take a first step
towards natural language understanding. Due to the lack of lexical resources it
is sometimes difficult to perform WSD for under-resourced languages. This paper
is an investigation on how to initiate research in WSD for under-resourced
languages by applying Word Sense Induction (WSI) and suggests some interesting
topics to focus on.
</summary>
    <author>
      <name>Mohammad Nasiruddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages TALN/RECITAL 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1975v1</id>
    <updated>2013-10-08T00:30:51Z</updated>
    <published>2013-10-08T00:30:51Z</published>
    <title>ARKref: a rule-based coreference resolution system</title>
    <summary>  ARKref is a tool for noun phrase coreference. It is a deterministic,
rule-based system that uses syntactic information from a constituent parser,
and semantic information from an entity recognition component. Its architecture
is based on the work of Haghighi and Klein (2009). ARKref was originally
written in 2009. At the time of writing, the last released version was in March
2011. This document describes that version, which is open-source and publicly
available at: http://www.ark.cs.cmu.edu/ARKref
</summary>
    <author>
      <name>Brendan O'Connor</name>
    </author>
    <author>
      <name>Michael Heilman</name>
    </author>
    <link href="http://arxiv.org/abs/1310.1975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2527v1</id>
    <updated>2013-10-08T17:19:06Z</updated>
    <published>2013-10-08T17:19:06Z</published>
    <title>Treating clitics with minimalist grammars</title>
    <summary>  We propose an extension of Stabler's version of clitics treatment for a wider
coverage of the French language. For this, we present the lexical entries
needed in the lexicon. Then, we show the recognition of complex syntactic
phenomena as (left and right) dislo- cation, clitic climbing over modal and
extraction from determiner phrase. The aim of this presentation is the
syntax-semantic interface for clitics analyses in which we will stress on
clitic climbing over verb and raising verb.
</summary>
    <author>
      <name>Maxime Amblard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Nancy - Grand Est / LORIA, MSH Lorraine</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 11th conference on Formal Grammar, Malaga : Spain (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.2527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0833v1</id>
    <updated>2013-11-04T20:11:35Z</updated>
    <published>2013-11-04T20:11:35Z</published>
    <title>A Comparative Study on Linguistic Feature Selection in Sentiment
  Polarity Classification</title>
    <summary>  Sentiment polarity classification is perhaps the most widely studied topic.
It classifies an opinionated document as expressing a positive or negative
opinion. In this paper, using movie review dataset, we perform a comparative
study with different single kind linguistic features and the combinations of
these features. We find that the classic topic-based classifier(Naive Bayes and
Support Vector Machine) do not perform as well on sentiment polarity
classification. And we find that with some combination of different linguistic
features, the classification accuracy can be boosted a lot. We give some
reasonable explanations about these boosting outcomes.
</summary>
    <author>
      <name>Zitao Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:cs/0205070 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2978v1</id>
    <updated>2013-11-12T23:11:40Z</updated>
    <published>2013-11-12T23:11:40Z</published>
    <title>Authorship Attribution Using Word Network Features</title>
    <summary>  In this paper, we explore a set of novel features for authorship attribution
of documents. These features are derived from a word network representation of
natural language text. As has been noted in previous studies, natural language
tends to show complex network structure at word level, with low degrees of
separation and scale-free (power law) degree distribution. There has also been
work on authorship attribution that incorporates ideas from complex networks.
The goal of our paper is to explore properties of these complex networks that
are suitable as features for machine-learning-based authorship attribution of
documents. We performed experiments on three different datasets, and obtained
promising results.
</summary>
    <author>
      <name>Shibamouli Lahiri</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <link href="http://arxiv.org/abs/1311.2978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3961v1</id>
    <updated>2013-11-15T19:45:25Z</updated>
    <published>2013-11-15T19:45:25Z</published>
    <title>HEVAL: Yet Another Human Evaluation Metric</title>
    <summary>  Machine translation evaluation is a very important activity in machine
translation development. Automatic evaluation metrics proposed in literature
are inadequate as they require one or more human reference translations to
compare them with output produced by machine translation. This does not always
give accurate results as a text can have several different translations. Human
evaluation metrics, on the other hand, lacks inter-annotator agreement and
repeatability. In this paper we have proposed a new human evaluation metric
which addresses these issues. Moreover this metric also provides solid grounds
for making sound assumptions on the quality of the text produced by a machine
translation.
</summary>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <author>
      <name>Hemant Darbari</name>
    </author>
    <author>
      <name>Ajai Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijnlc.2013.2502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijnlc.2013.2502" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Natural Language Computing Vol. 2, No.5,
  November 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.3961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5836v1</id>
    <updated>2013-11-22T18:13:06Z</updated>
    <published>2013-11-22T18:13:06Z</published>
    <title>Automatic Ranking of MT Outputs using Approximations</title>
    <summary>  Since long, research on machine translation has been ongoing. Still, we do
not get good translations from MT engines so developed. Manual ranking of these
outputs tends to be very time consuming and expensive. Identifying which one is
better or worse than the others is a very taxing task. In this paper, we show
an approach which can provide automatic ranks to MT outputs (translations)
taken from different MT Engines and which is based on N-gram approximations. We
provide a solution where no human intervention is required for ranking systems.
Further we also show the evaluations of our results which show equivalent
results as that of human ranking.
</summary>
    <author>
      <name>Pooja Gupta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/14217-2463</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/14217-2463" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 81(17):27-31,
  November 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.5836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6045v1</id>
    <updated>2013-11-23T20:10:24Z</updated>
    <published>2013-11-23T20:10:24Z</published>
    <title>Build Electronic Arabic Lexicon</title>
    <summary>  There are many known Arabic lexicons organized on different ways, each of
them has a different number of Arabic words according to its organization way.
This paper has used mathematical relations to count a number of Arabic words,
which proofs the number of Arabic words presented by Al Farahidy. The paper
also presents new way to build an electronic Arabic lexicon by using a hash
function that converts each word (as input) to correspond a unique integer
number (as output), these integer numbers will be used as an index to a lexicon
entry.
</summary>
    <author>
      <name>Nidhal El-Abbadi</name>
    </author>
    <author>
      <name>Ahmed Nidhal Khdhair</name>
    </author>
    <author>
      <name>Adel Al-Nasrawi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Arab Journal of Information Technology, Vol. 8,
  No. 2, April 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.6045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2087v1</id>
    <updated>2013-12-07T11:19:20Z</updated>
    <published>2013-12-07T11:19:20Z</published>
    <title>Towards Structural Natural Language Formalization: Mapping Discourse to
  Controlled Natural Language</title>
    <summary>  The author describes a conceptual study towards mapping grounded natural
language discourse representation structures to instances of controlled
language statements. This can be achieved via a pipeline of preexisting state
of the art technologies, namely natural language syntax to semantic discourse
mapping, and a reduction of the latter to controlled language discourse, given
a set of previously learnt reduction rules. Concludingly a description on
evaluation, potential and limitations for ontology-based reasoning is
presented.
</summary>
    <author>
      <name>Nicholas H. Kirk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 17th Workshop on the Semantics and Pragmatics of Dialogue,
  Amsterdam, 16-18 December 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.2087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5129v2</id>
    <updated>2013-12-19T11:01:02Z</updated>
    <published>2013-12-18T13:34:16Z</published>
    <title>Deep Learning Embeddings for Discontinuous Linguistic Units</title>
    <summary>  Deep learning embeddings have been successfully used for many natural
language processing problems. Embeddings are mostly computed for word forms
although a number of recent papers have extended this to other linguistic units
like morphemes and phrases. In this paper, we argue that learning embeddings
for discontinuous linguistic units should also be considered. In an
experimental evaluation on coreference resolution, we show that such embeddings
perform better than word form embeddings.
</summary>
    <author>
      <name>Wenpeng Yin</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <link href="http://arxiv.org/abs/1312.5129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5559v3</id>
    <updated>2014-02-18T14:17:46Z</updated>
    <published>2013-12-19T14:18:14Z</published>
    <title>Distributional Models and Deep Learning Embeddings: Combining the Best
  of Both Worlds</title>
    <summary>  There are two main approaches to the distributed representation of words:
low-dimensional deep learning embeddings and high-dimensional distributional
models, in which each dimension corresponds to a context word. In this paper,
we combine these two approaches by learning embeddings based on
distributional-model vectors - as opposed to one-hot vectors as is standardly
done in deep learning. We show that the combined approach has better
performance on a word relatedness judgment task.
</summary>
    <author>
      <name>Irina Sergienya</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 table, ICLR Workshop; main experimental table was extended
  with more experimental results; related word added</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5559v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5559v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6948v1</id>
    <updated>2013-12-25T09:23:49Z</updated>
    <published>2013-12-25T09:23:49Z</published>
    <title>Description Logics based Formalization of Wh-Queries</title>
    <summary>  The problem of Natural Language Query Formalization (NLQF) is to translate a
given user query in natural language (NL) into a formal language so that the
semantic interpretation has equivalence with the NL interpretation.
Formalization of NL queries enables logic based reasoning during information
retrieval, database query, question-answering, etc. Formalization also helps in
Web query normalization and indexing, query intent analysis, etc. In this paper
we are proposing a Description Logics based formal methodology for wh-query
intent (also called desire) identification and corresponding formal
translation. We evaluated the scalability of our proposed formalism using
Microsoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.
</summary>
    <author>
      <name>Sourish Dasgupta</name>
    </author>
    <author>
      <name>Rupali KaPatel</name>
    </author>
    <author>
      <name>Ankur Padia</name>
    </author>
    <author>
      <name>Kushal Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Language Query Processing, Representation</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.7077v2</id>
    <updated>2014-10-03T08:28:03Z</updated>
    <published>2013-12-26T09:45:02Z</published>
    <title>Language Modeling with Power Low Rank Ensembles</title>
    <summary>  We present power low rank ensembles (PLRE), a flexible framework for n-gram
language modeling where ensembles of low rank matrices and tensors are used to
obtain smoothed probability estimates of words in context. Our method can be
understood as a generalization of n-gram modeling to non-integer n, and
includes standard techniques such as absolute discounting and Kneser-Ney
smoothing as special cases. PLRE training is efficient and our approach
outperforms state-of-the-art modified Kneser Ney baselines in terms of
perplexity on large corpora as well as on BLEU score in a downstream machine
translation task.
</summary>
    <author>
      <name>Ankur P. Parikh</name>
    </author>
    <author>
      <name>Avneesh Saluja</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1312.7077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.7077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.7223v1</id>
    <updated>2013-12-27T09:39:52Z</updated>
    <published>2013-12-27T09:39:52Z</published>
    <title>Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier</title>
    <summary>  In this paper we present an approach for estimating the quality of machine
translation system. There are various methods for estimating the quality of
output sentences, but in this paper we focus on Na\"ive Bayes classifier to
build model using features which are extracted from the input sentences. These
features are used for finding the likelihood of each of the sentences of the
training data which are then further used for determining the scores of the
test data. On the basis of these scores we determine the class labels of the
test data.
</summary>
    <author>
      <name>Rashmi Gupta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 2013 International Conference on Advances in
  Computing, Communications and Informatics</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.7223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.7223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3080v1</id>
    <updated>2014-02-13T10:28:43Z</updated>
    <published>2014-02-13T10:28:43Z</published>
    <title>Software Requirement Specification Using Reverse Speech Technology</title>
    <summary>  Speech analysis had been taken to a new level with the discovery of Reverse
Speech (RS). RS is the discovery of hidden messages, referred as reversals, in
normal speech. Works are in progress for exploiting the relevance of RS in
different real world applications such as investigation, medical field etc. In
this paper we represent an innovative method for preparing a reliable Software
Requirement Specification (SRS) document with the help of reverse speech. As
SRS act as the backbone for the successful completion of any project, a
reliable method is needed to overcome the inconsistencies. Using RS such a
reliable method for SRS documentation was developed.
</summary>
    <author>
      <name>Santhy Viswam</name>
    </author>
    <author>
      <name>Sajeer Karattil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, International Journal of Computer Trends and Technology
  (IJCTT) vol.5 no.4</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.3080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91F20(primary) 03B65, 68T50(secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3405v1</id>
    <updated>2014-02-14T09:25:59Z</updated>
    <published>2014-02-14T09:25:59Z</published>
    <title>Authorship Analysis based on Data Compression</title>
    <summary>  This paper proposes to perform authorship analysis using the Fast Compression
Distance (FCD), a similarity measure based on compression with dictionaries
directly extracted from the written texts. The FCD computes a similarity
between two documents through an effective binary search on the intersection
set between the two related dictionaries. In the reported experiments the
proposed method is applied to documents which are heterogeneous in style,
written in five different languages and coming from different historical
periods. Results are comparable to the state of the art and outperform
traditional compression-based methods.
</summary>
    <author>
      <name>Daniele Cerra</name>
    </author>
    <author>
      <name>Mihai Datcu</name>
    </author>
    <author>
      <name>Peter Reinartz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2014.01.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2014.01.019" rel="related"/>
    <link href="http://arxiv.org/abs/1402.3405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5123v1</id>
    <updated>2014-02-20T20:15:18Z</updated>
    <published>2014-02-20T20:15:18Z</published>
    <title>Detecting Opinions in Tweets</title>
    <summary>  Given the incessant growth of documents describing the opinions of different
people circulating on the web, including Web 2.0 has made it possible to give
an opinion on any product in the net. In this paper, we examine the various
opinions expressed in the tweets and classify them positive, negative or
neutral by using the emoticons for the Bayesian method and adjectives and
adverbs for the Turney's method
</summary>
    <author>
      <name>Abdelmalek Amine</name>
    </author>
    <author>
      <name>Reda Mohamed Hamou</name>
    </author>
    <author>
      <name>Michel Simonet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5958/j.2249-3220.3.1.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5958/j.2249-3220.3.1.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal Of Data Mining And Emerging Technologies,
  Year : 2013, Volume : 3, Issue : 1 First page : ( 23) Last page : ( 32) Print
  ISSN : 2249-3212. Online ISSN : 2249-3220</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.5123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6880v1</id>
    <updated>2014-02-27T12:12:11Z</updated>
    <published>2014-02-27T12:12:11Z</published>
    <title>It's distributions all the way down!: Second order changes in
  statistical distributions also occur</title>
    <summary>  The textual, big-data literature misses Bentley, OBrien, &amp; Brocks (Bentley et
als) message on distributions; it largely examines the first-order effects of
how a single, signature distribution can predict population behaviour,
neglecting second-order effects involving distributional shifts, either between
signature distributions or within a given signature distribution. Indeed,
Bentley et al. themselves under-emphasise the potential richness of the latter,
within-distribution effects.
</summary>
    <author>
      <name>M. T. Keane</name>
    </author>
    <author>
      <name>A. Gerow</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S0140525X13001763</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S0140525X13001763" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Behavioral &amp; Brain Sciences, 2014, 37(1), 87</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.6880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1847v1</id>
    <updated>2014-04-07T16:45:42Z</updated>
    <published>2014-04-07T16:45:42Z</published>
    <title>Evaluation and Ranking of Machine Translated Output in Hindi Language
  using Precision and Recall Oriented Metrics</title>
    <summary>  Evaluation plays a crucial role in development of Machine translation
systems. In order to judge the quality of an existing MT system i.e. if the
translated output is of human translation quality or not, various automatic
metrics exist. We here present the implementation results of different metrics
when used on Hindi language along with their comparisons, illustrating how
effective are these metrics on languages like Hindi (free word order language).
</summary>
    <author>
      <name>Aditi Kalyani</name>
    </author>
    <author>
      <name>Hemant Kumud</name>
    </author>
    <author>
      <name>Shashi Pal Singh</name>
    </author>
    <author>
      <name>Ajai Kumar</name>
    </author>
    <author>
      <name>Hemant Darbari</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Research, Volume-4
  Number-1 Issue-14 March 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1890v1</id>
    <updated>2014-04-07T19:23:29Z</updated>
    <published>2014-04-07T19:23:29Z</published>
    <title>Polish and English wordnets -- statistical analysis of interconnected
  networks</title>
    <summary>  Wordnets are semantic networks containing nouns, verbs, adjectives, and
adverbs organized according to linguistic principles, by means of semantic
relations. In this work, we adopt a complex network perspective to perform a
comparative analysis of the English and Polish wordnets. We determine their
similarities and show that the networks exhibit some of the typical
characteristics observed in other real-world networks. We analyse interlingual
relations between both wordnets and deliberate over the problem of mapping the
Polish lexicon onto the English one.
</summary>
    <author>
      <name>Maksymilian Bujok</name>
    </author>
    <author>
      <name>Piotr Fronczak</name>
    </author>
    <author>
      <name>Agata Fronczak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures, Presented at Summer Solstice 2013 Conference on
  Discrete Models of Complex Systems, Warsaw, Poland</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Phys. Pol. B Proc. Suppl. Vol. 7 (2014) 245-256</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3925v2</id>
    <updated>2014-12-30T03:02:40Z</updated>
    <published>2014-04-13T14:19:27Z</published>
    <title>Complexity of Grammar Induction for Quantum Types</title>
    <summary>  Most categorical models of meaning use a functor from the syntactic category
to the semantic category. When semantic information is available, the problem
of grammar induction can therefore be defined as finding preimages of the
semantic types under this forgetful functor, lifting the information flow from
the semantic level to a valid reduction at the syntactic level. We study the
complexity of grammar induction, and show that for a variety of type systems,
including pivotal and compact closed categories, the grammar induction problem
is NP-complete. Our approach could be extended to linguistic type systems such
as autonomous or bi-closed categories.
</summary>
    <author>
      <name>Antonin Delpeuch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">École Normale Supérieure, Paris</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.172.16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.172.16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QPL 2014, arXiv:1412.8102</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 172, 2014, pp. 236-248</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.3925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3992v1</id>
    <updated>2014-04-15T17:13:26Z</updated>
    <published>2014-04-15T17:13:26Z</published>
    <title>Assessing the Quality of MT Systems for Hindi to English Translation</title>
    <summary>  Evaluation plays a vital role in checking the quality of MT output. It is
done either manually or automatically. Manual evaluation is very time consuming
and subjective, hence use of automatic metrics is done most of the times. This
paper evaluates the translation quality of different MT Engines for
Hindi-English (Hindi data is provided as input and English is obtained as
output) using various automatic metrics like BLEU, METEOR etc. Further the
comparison automatic evaluation results with Human ranking have also been
given.
</summary>
    <author>
      <name>Aditi Kalyani</name>
    </author>
    <author>
      <name>Hemant Kumud</name>
    </author>
    <author>
      <name>Shashi Pal Singh</name>
    </author>
    <author>
      <name>Ajai Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/15711-4629</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/15711-4629" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, Volume 89, No 15,
  March 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.3992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4572v1</id>
    <updated>2014-04-17T16:22:40Z</updated>
    <published>2014-04-17T16:22:40Z</published>
    <title>The First Parallel Multilingual Corpus of Persian: Toward a Persian
  BLARK</title>
    <summary>  In this article, we have introduced the first parallel corpus of Persian with
more than 10 other European languages. This article describes primary steps
toward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,
we have proposed morphosyntactic specification of Persian based on
EAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article
introduces Persian Language, with emphasis on its orthography and
morphosyntactic features, then a new Part-of-Speech categorization and
orthography for Persian in digital environments is proposed. Finally, the
corpus and related statistic will be analyzed.
</summary>
    <author>
      <name>Behrang Qasemizadeh</name>
    </author>
    <author>
      <name>Saeed Rahimi</name>
    </author>
    <author>
      <name>Behrooz Mahmoodi Bakhtiari</name>
    </author>
    <link href="http://arxiv.org/abs/1404.4572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4740v1</id>
    <updated>2014-04-18T10:30:47Z</updated>
    <published>2014-04-18T10:30:47Z</published>
    <title>Challenges in Persian Electronic Text Analysis</title>
    <summary>  Farsi, also known as Persian, is the official language of Iran and Tajikistan
and one of the two main languages spoken in Afghanistan. Farsi enjoys a unified
Arabic script as its writing system. In this paper we briefly introduce the
writing standards of Farsi and highlight problems one would face when analyzing
Farsi electronic texts, especially during development of Farsi corpora
regarding to transcription and encoding of Farsi e-texts. The pointes mentioned
may sounds easy but they are crucial when developing and processing written
corpora of Farsi.
</summary>
    <author>
      <name>Behrang QasemiZadeh</name>
    </author>
    <author>
      <name>Saeed Rahimi</name>
    </author>
    <author>
      <name>Mehdi Safaee Ghalati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in a Local conference 2006, available for the first time</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5278v1</id>
    <updated>2014-04-21T19:31:48Z</updated>
    <published>2014-04-21T19:31:48Z</published>
    <title>The Frobenius anatomy of word meanings I: subject and object relative
  pronouns</title>
    <summary>  This paper develops a compositional vector-based semantics of subject and
object relative pronouns within a categorical framework. Frobenius algebras are
used to formalise the operations required to model the semantics of relative
pronouns, including passing information between the relative clause and the
modified noun phrase, as well as copying, combining, and discarding parts of
the relative clause. We develop two instantiations of the abstract semantics,
one based on a truth-theoretic approach and one based on corpus statistics.
</summary>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
    </author>
    <author>
      <name>Stephen Clark</name>
    </author>
    <author>
      <name>Bob Coecke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/logcom/ext044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/logcom/ext044" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Logic and Computation, Special Issue: The Incomputable,
  an Isaac Newton Institute Workshop, 23(6), pp.1293-1317, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5357v1</id>
    <updated>2014-04-22T00:17:26Z</updated>
    <published>2014-04-22T00:17:26Z</published>
    <title>Morphological Analysis of the Bishnupriya Manipuri Language using Finite
  State Transducers</title>
    <summary>  In this work we present a morphological analysis of Bishnupriya Manipuri
language, an Indo-Aryan language spoken in the north eastern India. As of now,
there is no computational work available for the language. Finite state
morphology is one of the successful approaches applied in a wide variety of
languages over the year. Therefore we adapted the finite state approach to
analyse morphology of the Bishnupriya Manipuri language.
</summary>
    <author>
      <name>Nayan Jyoti Kalita</name>
    </author>
    <author>
      <name>Navanath Saharia</name>
    </author>
    <author>
      <name>Smriti Kumar Sinha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-54906-9_16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-54906-9_16" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Linguistics and Intelligent Text Processing,
  vol.8403, series: Lecture Notes in Computer Science, pp 206-213, (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5585v1</id>
    <updated>2014-04-22T18:26:09Z</updated>
    <published>2014-04-22T18:26:09Z</published>
    <title>A Structural Query System for Han Characters</title>
    <summary>  The IDSgrep structural query system for Han character dictionaries is
presented. This system includes a data model and syntax for describing the
spatial structure of Han characters using Extended Ideographic Description
Sequences (EIDSes) based on the Unicode IDS syntax; a language for querying
EIDS databases, designed to suit the needs of font developers and foreign
language learners; a bit vector index inspired by Bloom filters for faster
query operations; a freely available implementation; and format translation
from popular third-party IDS and XML character databases. Experimental results
are included, with a comparison to other software used for similar
applications.
</summary>
    <author>
      <name>Matthew Skala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures, for submission to ACM Transactions on Asian
  Language Information Processing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Asian Language Processing 23(2) (2015)
  127-159</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6491v1</id>
    <updated>2014-04-23T19:28:55Z</updated>
    <published>2014-04-23T19:28:55Z</published>
    <title>An Account of Opinion Implicatures</title>
    <summary>  While previous sentiment analysis research has concentrated on the
interpretation of explicitly stated opinions and attitudes, this work initiates
the computational study of a type of opinion implicature (i.e.,
opinion-oriented inference) in text. This paper described a rule-based
framework for representing and analyzing opinion implicatures which we hope
will contribute to deeper automatic interpretation of subjective language. In
the course of understanding implicatures, the system recognizes implicit
sentiments (and beliefs) toward various events and entities in the sentence,
often attributed to different sources (holders) and of mixed polarities; thus,
it produces a richer interpretation than is typical in opinion analysis.
</summary>
    <author>
      <name>Janyce Wiebe</name>
    </author>
    <author>
      <name>Lingjia Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 Pages. Submitted to the journal, Language Resources and Evaluation</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0603v1</id>
    <updated>2014-05-03T16:07:19Z</updated>
    <published>2014-05-03T16:07:19Z</published>
    <title>Extracting Family Relationship Networks from Novels</title>
    <summary>  We present an approach to the extraction of family relations from literary
narrative, which incorporates a technique for utterance attribution proposed
recently by Elson and McKeown (2010). In our work this technique is used in
combination with the detection of vocatives - the explicit forms of address
used by the characters in a novel. We take advantage of the fact that certain
vocatives indicate family relations between speakers. The extracted relations
are then propagated using a set of rules. We report the results of the
application of our method to Jane Austen's Pride and Prejudice.
</summary>
    <author>
      <name>Aibek Makazhanov</name>
    </author>
    <author>
      <name>Denilson Barbosa</name>
    </author>
    <author>
      <name>Grzegorz Kondrak</name>
    </author>
    <link href="http://arxiv.org/abs/1405.0603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0616v1</id>
    <updated>2014-05-03T19:02:44Z</updated>
    <published>2014-05-03T19:02:44Z</published>
    <title>Automated Attribution and Intertextual Analysis</title>
    <summary>  In this work, we employ quantitative methods from the realm of statistics and
machine learning to develop novel methodologies for author attribution and
textual analysis. In particular, we develop techniques and software suitable
for applications to Classical study, and we illustrate the efficacy of our
approach in several interesting open questions in the field. We apply our
numerical analysis techniques to questions of authorship attribution in the
case of the Greek tragedian Euripides, to instances of intertextuality and
influence in the poetry of the Roman statesman Seneca the Younger, and to cases
of "interpolated" text with respect to the histories of Livy.
</summary>
    <author>
      <name>James Brofos</name>
    </author>
    <author>
      <name>Ajay Kannan</name>
    </author>
    <author>
      <name>Rui Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 tables, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0701v1</id>
    <updated>2014-05-04T14:23:53Z</updated>
    <published>2014-05-04T14:23:53Z</published>
    <title>"Translation can't change a name": Using Multilingual Data for Named
  Entity Recognition</title>
    <summary>  Named Entities (NEs) are often written with no orthographic changes across
different languages that share a common alphabet. We show that this can be
leveraged so as to improve named entity recognition (NER) by using unsupervised
word clusters from secondary languages as features in state-of-the-art
discriminative NER systems. We observe significant increases in performance,
finding that person and location identification is particularly improved, and
that phylogenetically close languages provide more valuable features than more
distant languages.
</summary>
    <author>
      <name>Manaal Faruqui</name>
    </author>
    <link href="http://arxiv.org/abs/1405.0701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3515v1</id>
    <updated>2014-05-14T14:47:22Z</updated>
    <published>2014-05-14T14:47:22Z</published>
    <title>Temporal Analysis of Language through Neural Language Models</title>
    <summary>  We provide a method for automatically detecting change in language across
time through a chronologically trained neural language model. We train the
model on the Google Books Ngram corpus to obtain word vector representations
specific to each year, and identify words that have changed significantly from
1900 to 2009. The model identifies words such as "cell" and "gay" as having
changed during that time period. The model simultaneously identifies the
specific years during which such words underwent change.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Yi-I Chiu</name>
    </author>
    <author>
      <name>Kentaro Hanaki</name>
    </author>
    <author>
      <name>Darshan Hegde</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ACL 2014 Workshop on Language Technologies and
  Computational Social Science. June, 2014. 61--65</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.3515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3518v2</id>
    <updated>2014-06-28T17:18:54Z</updated>
    <published>2014-05-14T14:50:59Z</published>
    <title>Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme
  for Sentiment Analysis and Text Classification</title>
    <summary>  We provide a simple but novel supervised weighting scheme for adjusting term
frequency in tf-idf for sentiment analysis and text classification. We compare
our method to baseline weighting schemes and find that it outperforms them on
multiple benchmarks. The method is robust and works well on both snippets and
longer documents.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Owen Zhang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 5th Workshop on Computational Approaches to
  Subjectivity, Sentiment and Social Media Analysis. June, 2014. 79--83</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.3518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7397v1</id>
    <updated>2014-05-28T21:05:00Z</updated>
    <published>2014-05-28T21:05:00Z</published>
    <title>An HMM Based Named Entity Recognition System for Indian Languages: The
  JU System at ICON 2013</title>
    <summary>  This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named
Entity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,
Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model
has been used to implement our system. The system has been trained and tested
on the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of
0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,
English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.
</summary>
    <author>
      <name>Vivekananda Gayen</name>
    </author>
    <author>
      <name>Kamal Sarkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The ICON 2013 tools contest on Named Entity Recognition in Indian
  languages (IL) co-located with the 10th International Conference on Natural
  Language Processing(ICON), CDAC Noida, India,18-20 December, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1203v1</id>
    <updated>2014-06-04T20:22:30Z</updated>
    <published>2014-06-04T20:22:30Z</published>
    <title>A Semantic Approach to Summarization</title>
    <summary>  Sentence extraction based summarization methods has some limitations as it
doesn't go into the semantics of the document. Also, it lacks the capability of
sentence generation which is intuitive to humans. Here we present a novel
method to summarize text documents taking the process to semantic levels with
the use of WordNet and other resources, and using a technique for sentence
generation. We involve semantic role labeling to get the semantic
representation of text and use of segmentation to form clusters of the related
pieces of text. Picking out the centroids and sentence generation completes the
task. We evaluate our system against human composed summaries and also present
an evaluation done by humans to measure the quality attributes of our
summaries.
</summary>
    <author>
      <name>Divyanshu Bhartiya</name>
    </author>
    <author>
      <name>Ashudeep Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1406.1203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1234v1</id>
    <updated>2014-06-04T23:28:51Z</updated>
    <published>2014-06-04T23:28:51Z</published>
    <title>A Geometric Method to Obtain the Generation Probability of a Sentence</title>
    <summary>  "How to generate a sentence" is the most critical and difficult problem in
all the natural language processing technologies. In this paper, we present a
new approach to explain the generation process of a sentence from the
perspective of mathematics. Our method is based on the premise that in our
brain a sentence is a part of a word network which is formed by many word
nodes. Experiments show that the probability of the entire sentence can be
obtained by the probabilities of single words and the probabilities of the
co-occurrence of word pairs, which indicate that human use the synthesis method
to generate a sentence.
</summary>
    <author>
      <name>Chen Lijiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.1234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1241v1</id>
    <updated>2014-06-04T23:56:08Z</updated>
    <published>2014-06-04T23:56:08Z</published>
    <title>The Best Templates Match Technique For Example Based Machine Translation</title>
    <summary>  It has been proved that large scale realistic Knowledge Based Machine
Translation applications require acquisition of huge knowledge about language
and about the world. This knowledge is encoded in computational grammars,
lexicons and domain models. Another approach which avoids the need for
collecting and analyzing massive knowledge, is the Example Based approach,
which is the topic of this paper. We show through the paper that using Example
Based in its native form is not suitable for translating into Arabic. Therefore
a modification to the basic approach is presented to improve the accuracy of
the translation process. The basic idea of the new approach is to improve the
technique by which template-based approaches select the appropriate templates.
</summary>
    <author>
      <name>T. El-Shishtawy</name>
    </author>
    <author>
      <name>A. El-Sammak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eleventh International Conference on Artificial Intelligence
  Applications, 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.1241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1953v2</id>
    <updated>2014-08-22T12:25:47Z</updated>
    <published>2014-06-08T07:41:07Z</published>
    <title>Automatic Extraction of Protein Interaction in Literature</title>
    <summary>  Protein-protein interaction extraction is the key precondition of the
construction of protein knowledge network, and it is very important for the
research in the biomedicine. This paper extracted directional protein-protein
interaction from the biological text, using the SVM-based method. Experiments
were evaluated on the LLL05 corpus with good results. The results show that
dependency features are import for the protein-protein interaction extraction
and features related to the interaction word are effective for the interaction
direction judgment. At last, we analyzed the effects of different features and
planed for the next step.
</summary>
    <author>
      <name>Peilei Liu</name>
    </author>
    <author>
      <name>Ting Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to its lack of
  academic value</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.1953v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1953v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2035v2</id>
    <updated>2014-11-06T14:26:21Z</updated>
    <published>2014-06-08T22:35:09Z</published>
    <title>Learning Word Representations with Hierarchical Sparse Coding</title>
    <summary>  We propose a new method for learning word representations using hierarchical
regularization in sparse coding inspired by the linguistic study of word
meanings. We show an efficient learning algorithm based on stochastic proximal
methods that is significantly faster than previous approaches, making it
possible to perform hierarchical sparse coding on a corpus of billions of word
tokens. Experiments on various benchmark tasks---word similarity ranking,
analogies, sentence completion, and sentiment analysis---demonstrate that the
method outperforms or is competitive with state-of-the-art methods. Our word
representations are available at
\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.
</summary>
    <author>
      <name>Dani Yogatama</name>
    </author>
    <author>
      <name>Manaal Faruqui</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1406.2035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2400v1</id>
    <updated>2014-06-10T01:01:48Z</updated>
    <published>2014-06-10T01:01:48Z</published>
    <title>Controlled Natural Language Generation from a Multilingual
  FrameNet-based Grammar</title>
    <summary>  This paper presents a currently bilingual but potentially multilingual
FrameNet-based grammar library implemented in Grammatical Framework. The
contribution of this paper is two-fold. First, it offers a methodological
approach to automatically generate the grammar based on semantico-syntactic
valence patterns extracted from FrameNet-annotated corpora. Second, it provides
a proof of concept for two use cases illustrating how the acquired multilingual
grammar can be exploited in different CNL applications in the domains of arts
and tourism.
</summary>
    <author>
      <name>Dana Dannélls</name>
    </author>
    <author>
      <name>Normunds Grūzītis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-10223-8_15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-10223-8_15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Controlled Natural Language, Lecture Notes in Computer Science,
  Vol. 8625, Springer, 2014, pp. 155-166</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.2400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3460v1</id>
    <updated>2014-06-13T09:23:53Z</updated>
    <published>2014-06-13T09:23:53Z</published>
    <title>Are Style Guides Controlled Languages? The Case of Koenig &amp; Bauer AG</title>
    <summary>  Controlled natural languages for industrial application are often regarded as
a response to the challenges of translation and multilingual communication.
This paper presents a quite different approach taken by Koenig &amp; Bauer AG,
where the main goal was the improvement of the authoring process for technical
documentation. Most importantly, this paper explores the notion of a controlled
language and demonstrates how style guides can emerge from non-linguistic
considerations. Moreover, it shows the transition from loose language
recommendations into precise and prescriptive rules and investigates whether
such rules can be regarded as a full-fledged controlled language.
</summary>
    <author>
      <name>Karolina Suchowolec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth Workshop on Controlled Natural Language (CNL 2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3676v3</id>
    <updated>2014-09-04T00:25:35Z</updated>
    <published>2014-06-14T03:00:23Z</published>
    <title>Question Answering with Subgraph Embeddings</title>
    <summary>  This paper presents a system which learns to answer questions on a broad
range of topics from a knowledge base using few hand-crafted features. Our
model learns low-dimensional embeddings of words and knowledge base
constituents; these representations are used to score natural language
questions against candidate answers. Training our system using pairs of
questions and structured representations of their answers, and pairs of
question paraphrases, yields competitive results on a competitive benchmark of
the literature.
</summary>
    <author>
      <name>Antoine Bordes</name>
    </author>
    <author>
      <name>Sumit Chopra</name>
    </author>
    <author>
      <name>Jason Weston</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3676v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3676v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3987v1</id>
    <updated>2014-06-16T12:03:49Z</updated>
    <published>2014-06-16T12:03:49Z</published>
    <title>Towards an Error Correction Memory to Enhance Technical Texts Authoring
  in LELIE</title>
    <summary>  In this paper, we investigate and experiment the notion of error correction
memory applied to error correction in technical texts. The main purpose is to
induce relatively generic correction patterns associated with more contextual
correction recommendations, based on previously memorized and analyzed
corrections. The notion of error correction memory is developed within the
framework of the LELIE project and illustrated on the case of fuzzy lexical
items, which is a major problem in technical texts.
</summary>
    <author>
      <name>Juyeon Kang</name>
    </author>
    <author>
      <name>Patrick Saint Dizier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4057v1</id>
    <updated>2014-06-16T16:11:32Z</updated>
    <published>2014-06-16T16:11:32Z</published>
    <title>Embedded Controlled Languages</title>
    <summary>  Inspired by embedded programming languages, an embedded CNL (controlled
natural language) is a proper fragment of an entire natural language (its host
language), but it has a parser that recognizes the entire host language. This
makes it possible to process out-of-CNL input and give useful feedback to
users, instead of just reporting syntax errors. This extended abstract explains
the main concepts of embedded CNL implementation in GF (Grammatical Framework),
with examples from machine translation and some other ongoing work.
</summary>
    <author>
      <name>Aarne Ranta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, extended abstract, preprint for CNL 2014 in Galway</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.4057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4211v1</id>
    <updated>2014-06-17T01:34:22Z</updated>
    <published>2014-06-17T01:34:22Z</published>
    <title>Mapping the Economic Crisis: Some Preliminary Investigations</title>
    <summary>  In this paper we describe our contribution to the PoliInformatics 2014
Challenge on the 2007-2008 financial crisis. We propose a state of the art
technique to extract information from texts and provide different
representations, giving first a static overview of the domain and then a
dynamic representation of its main evolutions. We show that this strategy
provides a practical solution to some recent theories in social sciences that
are facing a lack of methods and tools to automatically extract information
from natural language texts.
</summary>
    <author>
      <name>Pierre Bourreau</name>
    </author>
    <author>
      <name>Thierry Poibeau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical paper describing the Lattice submission to the 2014
  PoliInformatics Unshared task</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.4211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5691v1</id>
    <updated>2014-06-22T09:41:24Z</updated>
    <published>2014-06-22T09:41:24Z</published>
    <title>A CNL for Contract-Oriented Diagrams</title>
    <summary>  We present a first step towards a framework for defining and manipulating
normative documents or contracts described as Contract-Oriented (C-O) Diagrams.
These diagrams provide a visual representation for such texts, giving the
possibility to express a signatory's obligations, permissions and prohibitions,
with or without timing constraints, as well as the penalties resulting from the
non-fulfilment of a contract. This work presents a CNL for verbalising C-O
Diagrams, a web-based tool allowing editing in this CNL, and another for
visualising and manipulating the diagrams interactively. We then show how these
proof-of-concept tools can be used by applying them to a small example.
</summary>
    <author>
      <name>John J. Camilleri</name>
    </author>
    <author>
      <name>Gabriele Paganelli</name>
    </author>
    <author>
      <name>Gerardo Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6101v1</id>
    <updated>2014-06-23T22:21:17Z</updated>
    <published>2014-06-23T22:21:17Z</published>
    <title>Improved Frame Level Features and SVM Supervectors Approach for the
  Recogniton of Emotional States from Speech: Application to categorical and
  dimensional states</title>
    <summary>  The purpose of speech emotion recognition system is to classify speakers
utterances into different emotional states such as disgust, boredom, sadness,
neutral and happiness. Speech features that are commonly used in speech emotion
recognition rely on global utterance level prosodic features. In our work, we
evaluate the impact of frame level feature extraction. The speech samples are
from Berlin emotional database and the features extracted from these utterances
are energy, different variant of mel frequency cepstrum coefficients, velocity
and acceleration features.
</summary>
    <author>
      <name>Imen Trabelsi</name>
    </author>
    <author>
      <name>Dorra Ben Ayed</name>
    </author>
    <author>
      <name>Noureddine Ellouze</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5815/ijigsp.2013.09.02</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5815/ijigsp.2013.09.02" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">I.J. Image, Graphics and Signal Processing, 2013, 9, 8-13</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.6101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7314v1</id>
    <updated>2014-06-27T20:56:00Z</updated>
    <published>2014-06-27T20:56:00Z</published>
    <title>On the Use of Different Feature Extraction Methods for Linear and Non
  Linear kernels</title>
    <summary>  The speech feature extraction has been a key focus in robust speech
recognition research; it significantly affects the recognition performance. In
this paper, we first study a set of different features extraction methods such
as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)
and perceptual linear prediction (PLP) with several features normalization
techniques like rasta filtering and cepstral mean subtraction (CMS). Based on
this, a comparative evaluation of these features is performed on the task of
text independent speaker identification using a combination between gaussian
mixture models (GMM) and linear and non-linear kernels based on support vector
machine (SVM).
</summary>
    <author>
      <name>Imen Trabelsi</name>
    </author>
    <author>
      <name>Dorra Ben Ayed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.7314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1605v1</id>
    <updated>2014-07-07T07:08:07Z</updated>
    <published>2014-07-07T07:08:07Z</published>
    <title>Les noms propres se traduisent-ils ? Étude d'un corpus multilingue</title>
    <summary>  In this paper, we tackle the problem of the translation of proper names. We
introduce our hypothesis according to which proper names can be translated more
often than most people seem to think. Then, we describe the construction of a
parallel multilingual corpus used to illustrate our point. We eventually
evaluate both the advantages and limits of this corpus in our study.
</summary>
    <author>
      <name>Émeline Lecuit</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LLL</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Maurel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LI</arxiv:affiliation>
    </author>
    <author>
      <name>Dusko Vitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Corpus 10 (2011) 201-218</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.1605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1976v1</id>
    <updated>2014-07-08T07:35:16Z</updated>
    <published>2014-07-08T07:35:16Z</published>
    <title>Inter-Rater Agreement Study on Readability Assessment in Bengali</title>
    <summary>  An inter-rater agreement study is performed for readability assessment in
Bengali. A 1-7 rating scale was used to indicate different levels of
readability. We obtained moderate to fair agreement among seven independent
annotators on 30 text passages written by four eminent Bengali authors. As a by
product of our study, we obtained a readability-annotated ground truth dataset
in Bengali. .
</summary>
    <author>
      <name>Shanta Phani</name>
    </author>
    <author>
      <name>Shibamouli Lahiri</name>
    </author>
    <author>
      <name>Arindam Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 tables, Accepted in ICCONAC, 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Natural Language Computing (IJNLC), 3(3),
  2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.1976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2694v1</id>
    <updated>2014-07-10T05:26:14Z</updated>
    <published>2014-07-10T05:26:14Z</published>
    <title>Quality Estimation Of Machine Translation Outputs Through Stemming</title>
    <summary>  Machine Translation is the challenging problem for Indian languages. Every
day we can see some machine translators being developed, but getting a high
quality automatic translation is still a very distant dream . The correct
translated sentence for Hindi language is rarely found. In this paper, we are
emphasizing on English-Hindi language pair, so in order to preserve the correct
MT output we present a ranking system, which employs some machine learning
techniques and morphological features. In ranking no human intervention is
required. We have also validated our results by comparing it with human
ranking.
</summary>
    <author>
      <name>Pooja Gupta</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.4, No.3, June 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3636v1</id>
    <updated>2014-07-14T13:22:36Z</updated>
    <published>2014-07-14T13:22:36Z</published>
    <title>Toward Network-based Keyword Extraction from Multitopic Web Documents</title>
    <summary>  In this paper we analyse the selectivity measure calculated from the complex
network in the task of the automatic keyword extraction. Texts, collected from
different web sources (portals, forums), are represented as directed and
weighted co-occurrence complex networks of words. Words are nodes and links are
established between two nodes if they are directly co-occurring within the
sentence. We test different centrality measures for ranking nodes - keyword
candidates. The promising results are achieved using the selectivity measure.
Then we propose an approach which enables extracting word pairs according to
the values of the in/out selectivity and weight measures combined with
filtering.
</summary>
    <author>
      <name>Sabina Šišović</name>
    </author>
    <author>
      <name>Sanda Martinčić-Ipšić</name>
    </author>
    <author>
      <name>Ana Meštrović</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6099v1</id>
    <updated>2014-07-23T03:29:44Z</updated>
    <published>2014-07-23T03:29:44Z</published>
    <title>Autonomous requirements specification processing using natural language
  processing</title>
    <summary>  We describe our ongoing research that centres on the application of natural
language processing (NLP) to software engineering and systems development
activities. In particular, this paper addresses the use of NLP in the
requirements analysis and systems design processes. We have developed a
prototype toolset that can assist the systems analyst or software engineer to
select and verify terms relevant to a project. In this paper we describe the
processes employed by the system to extract and classify objects of interest
from requirements documents. These processes are illustrated using a small
example.
</summary>
    <author>
      <name>S. G. Macdonell</name>
    </author>
    <author>
      <name>K. Min</name>
    </author>
    <author>
      <name>A. M. Connor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ISCA 14th International Conferenceon Intelligent
  and Adaptive Systems and Software Engineering (IASSE 2005)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6853v1</id>
    <updated>2014-07-25T11:17:28Z</updated>
    <published>2014-07-25T11:17:28Z</published>
    <title>Substitute Based SCODE Word Embeddings in Supervised NLP Tasks</title>
    <summary>  We analyze a word embedding method in supervised tasks. It maps words on a
sphere such that words co-occurring in similar contexts lie closely. The
similarity of contexts is measured by the distribution of substitutes that can
fill them. We compared word embeddings, including more recent representations,
in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine
our framework in multilingual dependency parsing as well. The results show that
the proposed method achieves as good as or better results compared to the other
word embeddings in the tasks we investigate. It achieves state-of-the-art
results in multilingual dependency parsing. Word embeddings in 7 languages are
available for public use.
</summary>
    <author>
      <name>Volkan Cirik</name>
    </author>
    <author>
      <name>Deniz Yuret</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0016v1</id>
    <updated>2014-06-27T01:00:59Z</updated>
    <published>2014-06-27T01:00:59Z</published>
    <title>Architecture of a Web-based Predictive Editor for Controlled Natural
  Language Processing</title>
    <summary>  In this paper, we describe the architecture of a web-based predictive text
editor being developed for the controlled natural language PENG$^{ASP)$. This
controlled language can be used to write non-monotonic specifications that have
the same expressive power as Answer Set Programs. In order to support the
writing process of these specifications, the predictive text editor
communicates asynchronously with the controlled natural language processor that
generates lookahead categories and additional auxiliary information for the
author of a specification text. The text editor can display multiple sets of
lookahead categories simultaneously for different possible sentence
completions, anaphoric expressions, and supports the addition of new content
words to the lexicon.
</summary>
    <author>
      <name>Stephen Guy</name>
    </author>
    <author>
      <name>Rolf Schwitter</name>
    </author>
    <link href="http://arxiv.org/abs/1408.0016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2359v2</id>
    <updated>2014-08-22T07:29:44Z</updated>
    <published>2014-08-11T09:32:39Z</published>
    <title>Gap-weighted subsequences for automatic cognate identification and
  phylogenetic inference</title>
    <summary>  In this paper, we describe the problem of cognate identification and its
relation to phylogenetic inference. We introduce subsequence based features for
discriminating cognates from non-cognates. We show that subsequence based
features perform better than the state-of-the-art string similarity measures
for the purpose of cognate identification. We use the cognate judgments for the
purpose of phylogenetic inference and observe that these classifiers infer a
tree which is close to the gold standard tree. The contribution of this paper
is the use of subsequence features for cognate identification and to employ the
cognate judgments for phylogenetic inference.
</summary>
    <author>
      <name>Taraka Rama</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2466v1</id>
    <updated>2014-07-15T02:20:23Z</updated>
    <published>2014-07-15T02:20:23Z</published>
    <title>Controlled Natural Language Processing as Answer Set Programming: an
  Experiment</title>
    <summary>  Most controlled natural languages (CNLs) are processed with the help of a
pipeline architecture that relies on different software components. We
investigate in this paper in an experimental way how well answer set
programming (ASP) is suited as a unifying framework for parsing a CNL, deriving
a formal representation for the resulting syntax trees, and for reasoning with
that representation. We start from a list of input tokens in ASP notation and
show how this input can be transformed into a syntax tree using an ASP grammar
and then into reified ASP rules in form of a set of facts. These facts are then
processed by an ASP meta-interpreter that allows us to infer new knowledge.
</summary>
    <author>
      <name>Rolf Schwitter</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3731v2</id>
    <updated>2014-11-03T14:38:44Z</updated>
    <published>2014-08-16T10:09:28Z</published>
    <title>Unsupervised Keyword Extraction from Polish Legal Texts</title>
    <summary>  In this work, we present an application of the recently proposed unsupervised
keyword extraction algorithm RAKE to a corpus of Polish legal texts from the
field of public procurement. RAKE is essentially a language and domain
independent method. Its only language-specific input is a stoplist containing a
set of non-content words. The performance of the method heavily depends on the
choice of such a stoplist, which should be domain adopted. Therefore, we
complement RAKE algorithm with an automatic approach to selecting non-content
words, which is based on the statistical properties of term distribution.
</summary>
    <author>
      <name>Michał Jungiewicz</name>
    </author>
    <author>
      <name>Michał Łopuszyński</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-10888-9_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-10888-9_7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Volume 8686, Springer 2014, pp
  65-70</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.3731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4753v1</id>
    <updated>2014-08-20T18:40:13Z</updated>
    <published>2014-08-20T18:40:13Z</published>
    <title>Be Careful When Assuming the Obvious: Commentary on "The placement of
  the head that minimizes online memory: a complex systems approach"</title>
    <summary>  Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic
and diachronic nature of word order based on the assumption that memory costs
are a never decreasing function of distance and a few very general linguistic
assumptions. However, even these minimal and seemingly obvious assumptions are
not as safe as they appear in light of recent typological and psycholinguistic
evidence. The interaction of word order and memory has further depths to be
explored.
</summary>
    <author>
      <name>Phillip M. Alday</name>
    </author>
    <link href="http://arxiv.org/abs/1408.4753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5882v2</id>
    <updated>2014-09-03T03:09:02Z</updated>
    <published>2014-08-25T19:48:04Z</published>
    <title>Convolutional Neural Networks for Sentence Classification</title>
    <summary>  We report on a series of experiments with convolutional neural networks (CNN)
trained on top of pre-trained word vectors for sentence-level classification
tasks. We show that a simple CNN with little hyperparameter tuning and static
vectors achieves excellent results on multiple benchmarks. Learning
task-specific vectors through fine-tuning offers further gains in performance.
We additionally propose a simple modification to the architecture to allow for
the use of both task-specific and static vectors. The CNN models discussed
herein improve upon the state of the art on 4 out of 7 tasks, which include
sentiment analysis and question classification.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5882v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5882v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6181v1</id>
    <updated>2014-08-26T16:43:30Z</updated>
    <published>2014-08-26T16:43:30Z</published>
    <title>Resolving Lexical Ambiguity in Tensor Regression Models of Meaning</title>
    <summary>  This paper provides a method for improving tensor-based compositional
distributional models of meaning by the addition of an explicit disambiguation
step prior to composition. In contrast with previous research where this
hypothesis has been successfully tested against relatively simple compositional
models, in our work we use a robust model trained with linear regression. The
results we get in two experiments show the superiority of the prior
disambiguation method and suggest that the effectiveness of this approach is
model-independent.
</summary>
    <author>
      <name>Dimitri Kartsaklis</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACL 2014, Vol. 2:Short Papers, pp:212-217</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.6181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6788v2</id>
    <updated>2014-08-29T08:44:09Z</updated>
    <published>2014-08-28T17:29:55Z</published>
    <title>Strongly Incremental Repair Detection</title>
    <summary>  We present STIR (STrongly Incremental Repair detection), a system that
detects speech repairs and edit terms on transcripts incrementally with minimal
latency. STIR uses information-theoretic measures from n-gram models as its
principal decision features in a pipeline of classifiers detecting the
different stages of repairs. Results on the Switchboard disfluency tagged
corpus show utterance-final accuracy on a par with state-of-the-art incremental
repair detection methods, but with better incremental accuracy, faster
time-to-detection and less computational overhead. We evaluate its performance
using incremental metrics and propose new repair processing evaluation
standards.
</summary>
    <author>
      <name>Julian Hough</name>
    </author>
    <author>
      <name>Matthew Purver</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, EMNLP conference long paper 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.6788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01243v1</id>
    <updated>2015-01-06T17:27:40Z</updated>
    <published>2015-01-06T17:27:40Z</published>
    <title>Un résumeur à base de graphes, indépéndant de la langue</title>
    <summary>  In this paper we present REG, a graph-based approach for study a fundamental
problem of Natural Language Processing (NLP): the automatic text summarization.
The algorithm maps a document as a graph, then it computes the weight of their
sentences. We have applied this approach to summarize documents in three
languages.
</summary>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Javier Ramirez</name>
    </author>
    <author>
      <name>Iria da Cunha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, in French, 2 figures; International Workshop on African
  Human Language Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.01243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04324v1</id>
    <updated>2015-01-18T16:37:53Z</updated>
    <published>2015-01-18T16:37:53Z</published>
    <title>Phrase Based Language Model For Statistical Machine Translation</title>
    <summary>  We consider phrase based Language Models (LM), which generalize the commonly
used word level models. Similar concept on phrase based LMs appears in speech
recognition, which is rather specialized and thus less suitable for machine
translation (MT). In contrast to the dependency LM, we first introduce the
exhaustive phrase-based LMs tailored for MT use. Preliminary experimental
results show that our approach outperform word based LMs with the respect to
perplexity and translation quality.
</summary>
    <author>
      <name>Jia Xu</name>
    </author>
    <author>
      <name>Geliang Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. This version of the paper was submitted for review to EMNLP
  2013. The title, the idea and the content of this paper was presented by the
  first author in the machine translation group meeting at the MSRA-NLC lab
  (Microsoft Research Asia, Natural Language Computing) on July 16, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.04324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07005v1</id>
    <updated>2015-01-28T07:04:18Z</updated>
    <published>2015-01-28T07:04:18Z</published>
    <title>Survey:Natural Language Parsing For Indian Languages</title>
    <summary>  Syntactic parsing is a necessary task which is required for NLP applications
including machine translation. It is a challenging task to develop a
qualitative parser for morphological rich and agglutinative languages.
Syntactic analysis is used to understand the grammatical structure of a natural
language sentence. It outputs all the grammatical information of each word and
its constituent. Also issues related to it help us to understand the language
in a more detailed way. This literature survey is groundwork to understand the
different parser development for Indian languages and various approaches that
are used to develop such tools and techniques. This paper provides a survey of
research papers from well known journals and conferences.
</summary>
    <author>
      <name>Monika T. Makwana</name>
    </author>
    <author>
      <name>Deepak C. Vegda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00923v1</id>
    <updated>2015-04-03T19:57:06Z</updated>
    <published>2015-04-03T19:57:06Z</published>
    <title>A Unified Deep Neural Network for Speaker and Language Recognition</title>
    <summary>  Learned feature representations and sub-phoneme posteriors from Deep Neural
Networks (DNNs) have been used separately to produce significant performance
gains for speaker and language recognition tasks. In this work we show how
these gains are possible using a single DNN for both speaker and language
recognition. The unified DNN approach is shown to yield substantial performance
improvements on the the 2013 Domain Adaptation Challenge speaker recognition
task (55% reduction in EER for the out-of-domain condition) and on the NIST
2011 Language Recognition Evaluation (48% reduction in EER for the 30s test
condition).
</summary>
    <author>
      <name>Fred Richardson</name>
    </author>
    <author>
      <name>Douglas Reynolds</name>
    </author>
    <author>
      <name>Najim Dehak</name>
    </author>
    <link href="http://arxiv.org/abs/1504.00923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04666v1</id>
    <updated>2015-04-18T00:23:16Z</updated>
    <published>2015-04-18T00:23:16Z</published>
    <title>Unsupervised Dependency Parsing: Let's Use Supervised Parsers</title>
    <summary>  We present a self-training approach to unsupervised dependency parsing that
reuses existing supervised and unsupervised parsing algorithms. Our approach,
called `iterated reranking' (IR), starts with dependency trees generated by an
unsupervised parser, and iteratively improves these trees using the richer
probability models used in supervised parsing that are in turn trained on these
trees. Our system achieves 1.8% accuracy higher than the state-of-the-part
parser of Spitkovsky et al. (2013) on the WSJ corpus.
</summary>
    <author>
      <name>Phong Le</name>
    </author>
    <author>
      <name>Willem Zuidema</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.04666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04716v1</id>
    <updated>2015-04-18T13:28:59Z</updated>
    <published>2015-04-18T13:28:59Z</published>
    <title>Gap Analysis of Natural Language Processing Systems with respect to
  Linguistic Modality</title>
    <summary>  Modality is one of the important components of grammar in linguistics. It
lets speaker to express attitude towards, or give assessment or potentiality of
state of affairs. It implies different senses and thus has different
perceptions as per the context. This paper presents an account showing the gap
in the functionality of the current state of art Natural Language Processing
(NLP) systems. The contextual nature of linguistic modality is studied. In this
paper, the works and logical approaches employed by Natural Language Processing
systems dealing with modality are reviewed. It sees human cognition and
intelligence as multi-layered approach that can be implemented by intelligent
systems for learning. Lastly, current flow of research going on within this
field is talked providing futurology.
</summary>
    <author>
      <name>Vishal Shukla</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04751v1</id>
    <updated>2015-04-18T18:34:19Z</updated>
    <published>2015-04-18T18:34:19Z</published>
    <title>A Knowledge-poor Pronoun Resolution System for Turkish</title>
    <summary>  A pronoun resolution system which requires limited syntactic knowledge to
identify the antecedents of personal and reflexive pronouns in Turkish is
presented. As in its counterparts for languages like English, Spanish and
French, the core of the system is the constraints and preferences determined
empirically. In the evaluation phase, it performed considerably better than the
baseline algorithm used for comparison. The system is significant for its being
the first fully specified knowledge-poor computational framework for pronoun
resolution in Turkish where Turkish possesses different structural properties
from the languages for which knowledge-poor systems had been developed.
</summary>
    <author>
      <name>Dilek Küçük</name>
    </author>
    <author>
      <name>Meltem Turhan Yöndem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 6th Discourse Anaphora and Anaphora
  Resolution Colloquium (DAARC), 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.04751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06078v1</id>
    <updated>2015-04-23T08:28:01Z</updated>
    <published>2015-04-23T08:28:01Z</published>
    <title>x.ent: R Package for Entities and Relations Extraction based on
  Unsupervised Learning and Document Structure</title>
    <summary>  Relation extraction with accurate precision is still a challenge when
processing full text databases. We propose an approach based on cooccurrence
analysis in each document for which we used document organization to improve
accuracy of relation extraction. This approach is implemented in a R package
called \emph{x.ent}. Another facet of extraction relies on use of extracted
relation into a querying system for expert end-users. Two datasets had been
used. One of them gets interest from specialists of epidemiology in plant
health. For this dataset usage is dedicated to plant-disease exploration
through agricultural information news. An open-data platform exploits exports
from \emph{x.ent} and is publicly available.
</summary>
    <author>
      <name>Nicolas Turenne</name>
    </author>
    <author>
      <name>Tien Phan</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06650v1</id>
    <updated>2015-04-24T21:43:55Z</updated>
    <published>2015-04-24T21:43:55Z</published>
    <title>Learning Dictionaries for Named Entity Recognition using Minimal
  Supervision</title>
    <summary>  This paper describes an approach for automatic construction of dictionaries
for Named Entity Recognition (NER) using large amounts of unlabeled data and a
few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower
dimensional embeddings (representations) for candidate phrases and classify
these phrases using a small number of labeled examples. Our method achieves
16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER
respectively. We also show that by adding candidate phrase embeddings as
features in a sequence tagger gives better performance compared to using word
embeddings.
</summary>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Michael Collins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In 14th Conference of the European Chapter of the Association for
  Computational Linguistic, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06665v2</id>
    <updated>2015-04-28T16:36:13Z</updated>
    <published>2015-04-24T23:24:10Z</published>
    <title>Using Syntax-Based Machine Translation to Parse English into Abstract
  Meaning Representation</title>
    <summary>  We present a parser for Abstract Meaning Representation (AMR). We treat
English-to-AMR conversion within the framework of string-to-tree, syntax-based
machine translation (SBMT). To make this work, we transform the AMR structure
into a form suitable for the mechanics of SBMT and useful for modeling. We
introduce an AMR-specific language model and add data and features drawn from
semantic resources. Our resulting AMR parser improves upon state-of-the-art
results by 7 Smatch points.
</summary>
    <author>
      <name>Michael Pust</name>
    </author>
    <author>
      <name>Ulf Hermjakob</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <author>
      <name>Daniel Marcu</name>
    </author>
    <author>
      <name>Jonathan May</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06665v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06665v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08050v1</id>
    <updated>2015-04-30T00:35:32Z</updated>
    <published>2015-04-30T00:35:32Z</published>
    <title>Detecting Concept-level Emotion Cause in Microblogging</title>
    <summary>  In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead
of the mere word-level models, to discover causes of microblogging users'
diversified emotions on specific hot event. A modified topic-supervised biterm
topic model is utilized in CECM to detect emotion topics' in event-related
tweets, and then context-sensitive topical PageRank is utilized to detect
meaningful multiword expressions as emotion causes. Experimental results on a
dataset from Sina Weibo, one of the largest microblogging websites in China,
show CECM can better detect emotion causes than baseline methods.
</summary>
    <author>
      <name>Shuangyong Song</name>
    </author>
    <author>
      <name>Yao Meng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, to appear on WWW 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08102v1</id>
    <updated>2015-04-30T07:27:56Z</updated>
    <published>2015-04-30T07:27:56Z</published>
    <title>Detecting and ordering adjectival scalemates</title>
    <summary>  This paper presents a pattern-based method that can be used to infer
adjectival scales, such as &lt;lukewarm, warm, hot&gt;, from a corpus. Specifically,
the proposed method uses lexical patterns to automatically identify and order
pairs of scalemates, followed by a filtering phase in which unrelated pairs are
discarded. For the filtering phase, several different similarity measures are
implemented and compared. The model presented in this paper is evaluated using
the current standard, along with a novel evaluation set, and shown to be at
least as good as the current state-of-the-art.
</summary>
    <author>
      <name>Emiel van Miltenburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at MAPLEX 2015, February 9-10, Yamagata, Japan
  (http://lang.cs.tut.ac.jp/maplex2015/)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01127v1</id>
    <updated>2015-07-04T16:59:30Z</updated>
    <published>2015-07-04T16:59:30Z</published>
    <title>AutoExtend: Extending Word Embeddings to Embeddings for Synsets and
  Lexemes</title>
    <summary>  We present \textit{AutoExtend}, a system to learn embeddings for synsets and
lexemes. It is flexible in that it can take any word embeddings as input and
does not need an additional training corpus. The synset/lexeme embeddings
obtained live in the same vector space as the word embeddings. A sparse tensor
formalization guarantees efficiency and parallelizability. We use WordNet as a
lexical resource, but AutoExtend can be easily applied to other resources like
Freebase. AutoExtend achieves state-of-the-art performance on word similarity
and word sense disambiguation tasks.
</summary>
    <author>
      <name>Sascha Rothe</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3115/v1/P15-1173</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3115/v1/P15-1173" rel="related"/>
    <link href="http://arxiv.org/abs/1507.01127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01636v1</id>
    <updated>2015-07-06T22:25:55Z</updated>
    <published>2015-07-06T22:25:55Z</published>
    <title>Reflections on Sentiment/Opinion Analysis</title>
    <summary>  In this paper, we described possible directions for deeper understanding,
helping bridge the gap between psychology / cognitive science and computational
approaches in sentiment/opinion analysis literature. We focus on the opinion
holder's underlying needs and their resultant goals, which, in a utilitarian
model of sentiment, provides the basis for explaining the reason a sentiment
valence is held. While these thoughts are still immature, scattered,
unstructured, and even imaginary, we believe that these perspectives might
suggest fruitful avenues for various kinds of future work.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <link href="http://arxiv.org/abs/1507.01636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01839v2</id>
    <updated>2015-08-03T15:36:45Z</updated>
    <published>2015-07-07T15:20:36Z</published>
    <title>Dependency-based Convolutional Neural Networks for Sentence Embedding</title>
    <summary>  In sentence modeling and classification, convolutional neural network
approaches have recently achieved state-of-the-art results, but all such
efforts process word vectors sequentially and neglect long-distance
dependencies. To exploit both deep learning and linguistic structures, we
propose a tree-based convolutional neural network model which exploit various
long-distance relationships between words. Our model improves the sequential
baselines on all three sentiment and question classification tasks, and
achieves the highest published accuracy on TREC.
</summary>
    <author>
      <name>Mingbo Ma</name>
    </author>
    <author>
      <name>Liang Huang</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">this paper has been accepted by ACL 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01839v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01839v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02205v2</id>
    <updated>2015-08-16T18:09:26Z</updated>
    <published>2015-07-08T15:55:18Z</published>
    <title>Talking to the crowd: What do people react to in online discussions?</title>
    <summary>  This paper addresses the question of how language use affects community
reaction to comments in online discussion forums, and the relative importance
of the message vs. the messenger. A new comment ranking task is proposed based
on community annotated karma in Reddit discussions, which controls for topic
and timing of comments. Experimental work with discussion threads from six
subreddits shows that the importance of different types of language features
varies with the community of interest.
</summary>
    <author>
      <name>Aaron Jaech</name>
    </author>
    <author>
      <name>Victoria Zayats</name>
    </author>
    <author>
      <name>Hao Fang</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02205v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02205v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03077v2</id>
    <updated>2015-11-24T11:57:40Z</updated>
    <published>2015-07-11T08:54:45Z</published>
    <title>A new hybrid stemming algorithm for Persian</title>
    <summary>  Stemming has been an influential part in Information retrieval and search
engines. There have been tremendous endeavours in making stemmer that are both
efficient and accurate. Stemmers can have three method in stemming, Dictionary
based stemmer, statistical-based stemmers, and rule-based stemmers. This paper
aims at building a hybrid stemmer that uses both Dictionary based method and
rule-based method for stemming. This ultimately helps the efficacy and
accurateness of the stemmer.
</summary>
    <author>
      <name>Adel Rahimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.03077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03223v1</id>
    <updated>2015-07-12T12:14:19Z</updated>
    <published>2015-07-12T12:14:19Z</published>
    <title>Classifier-Based Text Simplification for Improved Machine Translation</title>
    <summary>  Machine Translation is one of the research fields of Computational
Linguistics. The objective of many MT Researchers is to develop an MT System
that produce good quality and high accuracy output translations and which also
covers maximum language pairs. As internet and Globalization is increasing day
by day, we need a way that improves the quality of translation. For this
reason, we have developed a Classifier based Text Simplification Model for
English-Hindi Machine Translation Systems. We have used support vector machines
and Na\"ive Bayes Classifier to develop this model. We have also evaluated the
performance of these classifiers.
</summary>
    <author>
      <name>Shruti Tyagi</name>
    </author>
    <author>
      <name>Deepti Chopra</name>
    </author>
    <author>
      <name>Iti Mathur</name>
    </author>
    <author>
      <name>Nisheeth Joshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of International Conference on Advances in Computer
  Engineering and Applications 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.03223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03462v1</id>
    <updated>2015-07-13T14:00:22Z</updated>
    <published>2015-07-13T14:00:22Z</published>
    <title>Supervised Hierarchical Classification for Student Answer Scoring</title>
    <summary>  This paper describes a hierarchical system that predicts one label at a time
for automated student response analysis. For the task, we build a
classification binary tree that delays more easily confused labels to later
stages using hierarchical processes. In particular, the paper describes how the
hierarchical classifier has been built and how the classification task has been
broken down into binary subtasks. It finally discusses the motivations and
fundamentals of such an approach.
</summary>
    <author>
      <name>Itziar Aldabe</name>
    </author>
    <author>
      <name>Oier Lopez de Lacalle</name>
    </author>
    <author>
      <name>Iñigo Lopez-Gazpio</name>
    </author>
    <author>
      <name>Montse Maritxalar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages with references</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.03462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03471v1</id>
    <updated>2015-07-13T14:27:16Z</updated>
    <published>2015-07-13T14:27:16Z</published>
    <title>Incremental LSTM-based Dialog State Tracker</title>
    <summary>  A dialog state tracker is an important component in modern spoken dialog
systems. We present an incremental dialog state tracker, based on LSTM
networks. It directly uses automatic speech recognition hypotheses to track the
state. We also present the key non-standard aspects of the model that bring its
performance close to the state-of-the-art and experimentally analyze their
contribution: including the ASR confidence scores, abstracting scarcely
represented values, including transcriptions in the training data, and model
averaging.
</summary>
    <author>
      <name>Lukas Zilka</name>
    </author>
    <author>
      <name>Filip Jurcicek</name>
    </author>
    <link href="http://arxiv.org/abs/1507.03471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05630v1</id>
    <updated>2015-07-20T20:01:44Z</updated>
    <published>2015-07-20T20:01:44Z</published>
    <title>Notes About a More Aware Dependency Parser</title>
    <summary>  In this paper I explain the reasons that led me to research and conceive a
novel technology for dependency parsing, mixing together the strengths of
data-driven transition-based and constraint-based approaches. In particular I
highlight the problem to infer the reliability of the results of a data-driven
transition-based parser, which is extremely important for high-level processes
that expect to use correct parsing results. I then briefly introduce a number
of notes about a new parser model I'm working on, capable to proceed with the
analysis in a "more aware" way, with a more "robust" concept of robustness.
</summary>
    <author>
      <name>Matteo Grella</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06021v1</id>
    <updated>2015-07-22T00:34:15Z</updated>
    <published>2015-07-22T00:34:15Z</published>
    <title>An Empirical Comparison of SVM and Some Supervised Learning Algorithms
  for Vowel recognition</title>
    <summary>  In this article, we conduct a study on the performance of some supervised
learning algorithms for vowel recognition. This study aims to compare the
accuracy of each algorithm. Thus, we present an empirical comparison between
five supervised learning classifiers and two combined classifiers: SVM, KNN,
Naive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms
were tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral
coefficients (MFCCs).
</summary>
    <author>
      <name>Rimah Amami</name>
    </author>
    <author>
      <name>Dorra Ben Ayed</name>
    </author>
    <author>
      <name>Noureddine Ellouze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">08 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08449v2</id>
    <updated>2016-05-19T14:57:45Z</updated>
    <published>2015-07-30T10:53:11Z</published>
    <title>One model, two languages: training bilingual parsers with harmonized
  treebanks</title>
    <summary>  We introduce an approach to train lexicalized parsers using bilingual corpora
obtained by merging harmonized treebanks of different languages, producing
parsers that can analyze sentences in either of the learned languages, or even
sentences that mix both. We test the approach on the Universal Dependency
Treebanks, training with MaltParser and MaltOptimizer. The results show that
these bilingual parsers are more than competitive, as most combinations not
only preserve accuracy, but some even achieve significant improvements over the
corresponding monolingual parsers. Preliminary experiments also show the
approach to be promising on texts with code-switching and when more languages
are added.
</summary>
    <author>
      <name>David Vilares</name>
    </author>
    <author>
      <name>Carlos Gómez-Rodríguez</name>
    </author>
    <author>
      <name>Miguel A. Alonso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08449v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08449v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08452v3</id>
    <updated>2016-09-07T16:43:38Z</updated>
    <published>2015-07-30T11:05:01Z</published>
    <title>Unsupervised Sentence Simplification Using Deep Semantics</title>
    <summary>  We present a novel approach to sentence simplification which departs from
previous work in two main ways. First, it requires neither hand written rules
nor a training corpus of aligned standard and simplified sentences. Second,
sentence splitting operates on deep semantic structure. We show (i) that the
unsupervised framework we propose is competitive with four state-of-the-art
supervised systems and (ii) that our semantic based approach allows for a
principled and effective handling of sentence splitting.
</summary>
    <author>
      <name>Shashi Narayan</name>
    </author>
    <author>
      <name>Claire Gardent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, INLG 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08452v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08452v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00189v1</id>
    <updated>2015-08-02T04:17:40Z</updated>
    <published>2015-08-02T04:17:40Z</published>
    <title>Class Vectors: Embedding representation of Document Classes</title>
    <summary>  Distributed representations of words and paragraphs as semantic embeddings in
high dimensional data are used across a number of Natural Language
Understanding tasks such as retrieval, translation, and classification. In this
work, we propose "Class Vectors" - a framework for learning a vector per class
in the same embedding space as the word and paragraph embeddings. Similarity
between these class vectors and word vectors are used as features to classify a
document to a class. In experiment on several sentiment analysis tasks such as
Yelp reviews and Amazon electronic product reviews, class vectors have shown
better or comparable results in classification while learning very meaningful
class embeddings.
</summary>
    <author>
      <name>Devendra Singh Sachan</name>
    </author>
    <author>
      <name>Shailesh Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00657v2</id>
    <updated>2015-08-11T17:33:47Z</updated>
    <published>2015-08-04T04:36:36Z</published>
    <title>Improved Transition-Based Parsing by Modeling Characters instead of
  Words with LSTMs</title>
    <summary>  We present extensions to a continuous-state dependency parsing method that
makes it applicable to morphologically rich languages. Starting with a
high-performance transition-based parser that uses long short-term memory
(LSTM) recurrent neural networks to learn representations of the parser state,
we replace lookup-based word representations with representations constructed
from the orthographic representations of the words, also using LSTMs. This
allows statistical sharing across word forms that are similar on the surface.
Experiments for morphologically rich languages show that the parsing model
benefits from incorporating the character-based encodings of words.
</summary>
    <author>
      <name>Miguel Ballesteros</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01067v1</id>
    <updated>2015-08-05T13:18:51Z</updated>
    <published>2015-08-05T13:18:51Z</published>
    <title>Topic Stability over Noisy Sources</title>
    <summary>  Topic modelling techniques such as LDA have recently been applied to speech
transcripts and OCR output. These corpora may contain noisy or erroneous texts
which may undermine topic stability. Therefore, it is important to know how
well a topic modelling algorithm will perform when applied to noisy data. In
this paper we show that different types of textual noise will have diverse
effects on the stability of different topic models. From these observations, we
propose guidelines for text corpus generation, with a focus on automatic speech
transcription. We also suggest topic model selection methods for noisy corpora.
</summary>
    <author>
      <name>Jing Su</name>
    </author>
    <author>
      <name>Oisín Boydell</name>
    </author>
    <author>
      <name>Derek Greene</name>
    </author>
    <author>
      <name>Gerard Lynch</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01346v1</id>
    <updated>2015-08-06T10:15:51Z</updated>
    <published>2015-08-06T10:15:51Z</published>
    <title>Word sense disambiguation: a survey</title>
    <summary>  In this paper, we made a survey on Word Sense Disambiguation (WSD). Near
about in all major languages around the world, research in WSD has been
conducted upto different extents. In this paper, we have gone through a survey
regarding the different approaches adopted in different research works, the
State of the Art in the performance in this domain, recent works in different
Indian languages and finally a survey in Bengali language. We have made a
survey on different competitions in this field and the bench mark results,
obtained from those competitions.
</summary>
    <author>
      <name>Alok Ranjan Pal</name>
    </author>
    <author>
      <name>Diganta Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijctcm.2015.5301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijctcm.2015.5301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Control Theory and Computer Modeling
  (IJCTCM) Vol.5, No.3, July 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.01346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01476v1</id>
    <updated>2015-08-06T18:02:54Z</updated>
    <published>2015-08-06T18:02:54Z</published>
    <title>Hyponymy extraction of domain ontology concept based on ccrfs and
  hierarchy clustering</title>
    <summary>  Concept hierarchy is the backbone of ontology, and the concept hierarchy
acquisition has been a hot topic in the field of ontology learning. this paper
proposes a hyponymy extraction method of domain ontology concept based on
cascaded conditional random field(CCRFs) and hierarchy clustering. It takes
free text as extracting object, adopts CCRFs identifying the domain concepts.
First the low layer of CCRFs is used to identify simple domain concept, then
the results are sent to the high layer, in which the nesting concepts are
recognized. Next we adopt hierarchy clustering to identify the hyponymy
relation between domain ontology concepts. The experimental results demonstrate
the proposed method is efficient.
</summary>
    <author>
      <name>Qiang Zhan</name>
    </author>
    <author>
      <name>Chunhong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01718v1</id>
    <updated>2015-08-07T15:06:13Z</updated>
    <published>2015-08-07T15:06:13Z</published>
    <title>Study of Phonemes Confusions in Hierarchical Automatic Phoneme
  Recognition System</title>
    <summary>  In this paper, we have analyzed the impact of confusions on the robustness of
phoneme recognitions system. The confusions are detected at the pronunciation
and the confusions matrices of the phoneme recognizer. The confusions show that
some similarities between phonemes at the pronunciation affect significantly
the recognition rates. This paper proposes to understand those confusions in
order to improve the performance of the phoneme recognition system by isolating
the problematic phonemes. Confusion analysis leads to build a new hierarchical
recognizer using new phoneme distribution and the information from the
confusion matrices. This new hierarchical phoneme recognition system shows
significant improvements of the recognition rates on TIMIT database.
</summary>
    <author>
      <name>Rimah Amami</name>
    </author>
    <author>
      <name>Noureddine Ellouze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">08 pages in Journal of Convergence Information Technology 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.01718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02142v1</id>
    <updated>2015-08-10T07:02:49Z</updated>
    <published>2015-08-10T07:02:49Z</published>
    <title>Feature-based Decipherment for Large Vocabulary Machine Translation</title>
    <summary>  Orthographic similarities across languages provide a strong signal for
probabilistic decipherment, especially for closely related language pairs. The
existing decipherment models, however, are not well-suited for exploiting these
orthographic similarities. We propose a log-linear model with latent variables
that incorporates orthographic similarity features. Maximum likelihood training
is computationally expensive for the proposed log-linear model. To address this
challenge, we perform approximate inference via MCMC sampling and contrastive
divergence. Our results show that the proposed log-linear model with
contrastive divergence scales to large vocabularies and outperforms the
existing generative decipherment models by exploiting the orthographic
features.
</summary>
    <author>
      <name>Iftekhar Naim</name>
    </author>
    <author>
      <name>Daniel Gildea</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03721v1</id>
    <updated>2015-08-15T11:16:39Z</updated>
    <published>2015-08-15T11:16:39Z</published>
    <title>A Comparative Study on Regularization Strategies for Embedding-based
  Neural Networks</title>
    <summary>  This paper aims to compare different regularization strategies to address a
common phenomenon, severe overfitting, in embedding-based neural networks for
NLP. We chose two widely studied neural models and tasks as our testbed. We
tried several frequently applied or newly proposed regularization strategies,
including penalizing weights (embeddings excluded), penalizing embeddings,
re-embedding words, and dropout. We also emphasized on incremental
hyperparameter tuning, and combining different regularizations. The results
provide a picture on tuning hyperparameters for neural NLP models.
</summary>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Yunchuan Chen</name>
    </author>
    <author>
      <name>Yangyang Lu</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP '15</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03854v1</id>
    <updated>2015-08-16T18:27:25Z</updated>
    <published>2015-08-16T18:27:25Z</published>
    <title>Online Representation Learning in Recurrent Neural Language Models</title>
    <summary>  We investigate an extension of continuous online learning in recurrent neural
network language models. The model keeps a separate vector representation of
the current unit of text being processed and adaptively adjusts it after each
prediction. The initial experiments give promising results, indicating that the
method is able to increase language modelling accuracy, while also decreasing
the parameters needed to store the model along with the computation required at
each step.
</summary>
    <author>
      <name>Marek Rei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/D15-1026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/D15-1026" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04257v2</id>
    <updated>2015-12-30T08:29:54Z</updated>
    <published>2015-08-18T09:29:22Z</published>
    <title>Learning Meta-Embeddings by Using Ensembles of Embedding Sets</title>
    <summary>  Word embeddings -- distributed representations of words -- in deep learning
are beneficial for many tasks in natural language processing (NLP). However,
different embedding sets vary greatly in quality and characteristics of the
captured semantics. Instead of relying on a more advanced algorithm for
embedding learning, this paper proposes an ensemble approach of combining
different public embedding sets with the aim of learning meta-embeddings.
Experiments on word similarity and analogy tasks and on part-of-speech tagging
show better performance of meta-embeddings compared to individual embedding
sets. One advantage of meta-embeddings is the increased vocabulary coverage. We
will release our meta-embeddings publicly.
</summary>
    <author>
      <name>Wenpeng Yin</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.04257v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04257v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05051v1</id>
    <updated>2015-08-20T17:21:50Z</updated>
    <published>2015-08-20T17:21:50Z</published>
    <title>Auto-Sizing Neural Networks: With Applications to n-gram Language Models</title>
    <summary>  Neural networks have been shown to improve performance across a range of
natural-language tasks. However, designing and training them can be
complicated. Frequently, researchers resort to repeated experimentation to pick
optimal settings. In this paper, we address the issue of choosing the correct
number of units in hidden layers. We introduce a method for automatically
adjusting network size by pruning out hidden units through $\ell_{\infty,1}$
and $\ell_{2,1}$ regularization. We apply this method to language modeling and
demonstrate its ability to correctly choose the number of hidden units while
maintaining perplexity. We also include these models in a machine translation
decoder and show that these smaller neural models maintain the significant
improvements of their unpruned versions.
</summary>
    <author>
      <name>Kenton Murray</name>
    </author>
    <author>
      <name>David Chiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05154v2</id>
    <updated>2015-09-02T17:26:24Z</updated>
    <published>2015-08-21T00:25:51Z</published>
    <title>Posterior calibration and exploratory analysis for natural language
  processing models</title>
    <summary>  Many models in natural language processing define probabilistic distributions
over linguistic structures. We argue that (1) the quality of a model' s
posterior distribution can and should be directly evaluated, as to whether
probabilities correspond to empirical frequencies, and (2) NLP uncertainty can
be projected not only to pipeline components, but also to exploratory data
analysis, telling a user when to trust and not trust the NLP analysis. We
present a method to analyze calibration, and apply it to compare the
miscalibration of several commonly used models. We also contribute a
coreference sampling algorithm that can create confidence intervals for a
political event extraction task.
</summary>
    <author>
      <name>Khanh Nguyen</name>
    </author>
    <author>
      <name>Brendan O'Connor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages (including supplementary information), proceedings of EMNLP
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05154v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05154v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05817v1</id>
    <updated>2015-08-24T14:15:39Z</updated>
    <published>2015-08-24T14:15:39Z</published>
    <title>Echoes of Persuasion: The Effect of Euphony in Persuasive Communication</title>
    <summary>  While the effect of various lexical, syntactic, semantic and stylistic
features have been addressed in persuasive language from a computational point
of view, the persuasive effect of phonetics has received little attention. By
modeling a notion of euphony and analyzing four datasets comprising persuasive
and non-persuasive sentences in different domains (political speeches, movie
quotes, slogans and tweets), we explore the impact of sounds on different forms
of persuasiveness. We conduct a series of analyses and prediction experiments
within and across datasets. Our results highlight the positive role of phonetic
devices on persuasion.
</summary>
    <author>
      <name>Marco Guerini</name>
    </author>
    <author>
      <name>Gözde Özbal</name>
    </author>
    <author>
      <name>Carlo Strapparava</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05902v1</id>
    <updated>2015-08-24T18:14:34Z</updated>
    <published>2015-08-24T18:14:34Z</published>
    <title>A Framework for Comparing Groups of Documents</title>
    <summary>  We present a general framework for comparing multiple groups of documents. A
bipartite graph model is proposed where document groups are represented as one
node set and the comparison criteria are represented as the other node set.
Using this model, we present basic algorithms to extract insights into
similarities and differences among the document groups. Finally, we demonstrate
the versatility of our framework through an analysis of NSF funding programs
for basic research.
</summary>
    <author>
      <name>Arun S. Maiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; 2015 Conference on Empirical Methods in Natural Language
  Processing (EMNLP '15)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06491v2</id>
    <updated>2017-04-12T19:51:09Z</updated>
    <published>2015-08-26T13:44:54Z</published>
    <title>Alignment-based compositional semantics for instruction following</title>
    <summary>  This paper describes an alignment-based model for interpreting natural
language instructions in context. We approach instruction following as a search
over plans, scoring sequences of actions conditioned on structured observations
of text and the environment. By explicitly modeling both the low-level
compositional structure of individual actions and the high-level structure of
full plans, we are able to learn both grounded representations of sentence
meaning and pragmatic constraints on interpretation. To demonstrate the model's
flexibility, we apply it to a diverse set of benchmark tasks. On every task, we
outperform strong task-specific baselines, and achieve several new
state-of-the-art results.
</summary>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in proceedings of EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06491v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06491v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06669v1</id>
    <updated>2015-08-26T21:25:25Z</updated>
    <published>2015-08-26T21:25:25Z</published>
    <title>Component-Enhanced Chinese Character Embeddings</title>
    <summary>  Distributed word representations are very useful for capturing semantic
information and have been successfully applied in a variety of NLP tasks,
especially on English. In this work, we innovatively develop two
component-enhanced Chinese character embedding models and their bigram
extensions. Distinguished from English word embeddings, our models explore the
compositions of Chinese characters, which often serve as semantic indictors
inherently. The evaluations on both word similarity and text classification
demonstrate the effectiveness of our models.
</summary>
    <author>
      <name>Yanran Li</name>
    </author>
    <author>
      <name>Wenjie Li</name>
    </author>
    <author>
      <name>Fei Sun</name>
    </author>
    <author>
      <name>Sujian Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, conference, EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07555v1</id>
    <updated>2015-08-30T11:22:38Z</updated>
    <published>2015-08-30T11:22:38Z</published>
    <title>An Event Network for Exploring Open Information</title>
    <summary>  In this paper, an event network is presented for exploring open information,
where linguistic units about an event are organized for analysing. The process
is divided into three steps: document event detection, event network
construction and event network analysis. First, by implementing event detection
or tracking, documents are retrospectively (or on-line) organized into document
events. Secondly, for each of the document event, linguistic units are
extracted and combined into event networks. Thirdly, various analytic methods
are proposed for event network analysis. In our application methodologies are
presented for exploring open information.
</summary>
    <author>
      <name>Yanping Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1508.07555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01073v1</id>
    <updated>2016-01-06T04:00:50Z</updated>
    <published>2016-01-06T04:00:50Z</published>
    <title>Multi-Way, Multilingual Neural Machine Translation with a Shared
  Attention Mechanism</title>
    <summary>  We propose multi-way, multilingual neural machine translation. The proposed
approach enables a single neural translation model to translate between
multiple languages, with a number of parameters that grows only linearly with
the number of languages. This is made possible by having a single attention
mechanism that is shared across all language pairs. We train the proposed
multi-way, multilingual model on ten language pairs from WMT'15 simultaneously
and observe clear performance improvements over models trained on only one
language pair. In particular, we observe that the proposed model significantly
improves the translation quality of low-resource language pairs.
</summary>
    <author>
      <name>Orhan Firat</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01085v1</id>
    <updated>2016-01-06T06:03:17Z</updated>
    <published>2016-01-06T06:03:17Z</published>
    <title>Incorporating Structural Alignment Biases into an Attentional Neural
  Translation Model</title>
    <summary>  Neural encoder-decoder models of machine translation have achieved impressive
results, rivalling traditional translation models. However their modelling
formulation is overly simplistic, and omits several key inductive biases built
into traditional models. In this paper we extend the attentional neural
translation model to include structural biases from word based alignment
models, including positional bias, Markov conditioning, fertility and agreement
over translation directions. We show improvements over a baseline attentional
model and standard phrase-based model over several language pairs, evaluating
on difficult languages in a low resource setting.
</summary>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <author>
      <name>Cong Duy Vu Hoang</name>
    </author>
    <author>
      <name>Ekaterina Vymolova</name>
    </author>
    <author>
      <name>Kaisheng Yao</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Gholamreza Haffari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.01085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01280v2</id>
    <updated>2016-06-06T21:06:55Z</updated>
    <published>2016-01-06T19:13:12Z</published>
    <title>Language to Logical Form with Neural Attention</title>
    <summary>  Semantic parsing aims at mapping natural language to machine interpretable
meaning representations. Traditional approaches rely on high-quality lexicons,
manually-built templates, and linguistic features which are either domain- or
representation-specific. In this paper we present a general method based on an
attention-enhanced encoder-decoder model. We encode input utterances into
vector representations, and generate their logical forms by conditioning the
output sequences or trees on the encoding vectors. Experimental results on four
datasets show that our approach performs competitively without using
hand-engineered features and is easy to adapt across domains and meaning
representations.
</summary>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.01280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01705v4</id>
    <updated>2016-06-07T23:25:51Z</updated>
    <published>2016-01-07T21:21:59Z</published>
    <title>Learning to Compose Neural Networks for Question Answering</title>
    <summary>  We describe a question answering model that applies to both images and
structured knowledge bases. The model uses natural language strings to
automatically assemble neural networks from a collection of composable modules.
Parameters for these modules are learned jointly with network-assembly
parameters via reinforcement learning, with only (world, question, answer)
triples as supervision. Our approach, which we term a dynamic neural model
network, achieves state-of-the-art results on benchmark datasets in both visual
and structured domains.
</summary>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01705v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01705v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02502v1</id>
    <updated>2016-01-11T16:12:32Z</updated>
    <published>2016-01-11T16:12:32Z</published>
    <title>Trans-gram, Fast Cross-lingual Word-embeddings</title>
    <summary>  We introduce Trans-gram, a simple and computationally-efficient method to
simultaneously learn and align wordembeddings for a variety of languages, using
only monolingual data and a smaller set of sentence-aligned data. We use our
new method to compute aligned wordembeddings for twenty-one languages using
English as a pivot language. We show that some linguistic features are aligned
across languages for which we do not have aligned data, even though those
properties do not exist in the pivot language. We also achieve state of the art
results on standard cross-lingual text classification and word translation
tasks.
</summary>
    <author>
      <name>Jocelyn Coulmance</name>
    </author>
    <author>
      <name>Jean-Marc Marty</name>
    </author>
    <author>
      <name>Guillaume Wenzek</name>
    </author>
    <author>
      <name>Amine Benhalloum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04012v1</id>
    <updated>2016-01-15T17:33:39Z</updated>
    <published>2016-01-15T17:33:39Z</published>
    <title>Detecting and Extracting Events from Text Documents</title>
    <summary>  Events of various kinds are mentioned and discussed in text documents,
whether they are books, news articles, blogs or microblog feeds. The paper
starts by giving an overview of how events are treated in linguistics and
philosophy. We follow this discussion by surveying how events and associated
information are handled in computationally. In particular, we look at how
textual documents can be mined to extract events and ancillary information.
These days, it is mostly through the application of various machine learning
techniques. We also discuss applications of event detection and extraction
systems, particularly in summarization, in the medical domain and in the
context of Twitter posts. We end the paper with a discussion of challenges and
future directions.
</summary>
    <author>
      <name>Jugal Kalita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is work in progress. Please email jkalita@uccs.edu with any
  comments for improvement</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.05768v1</id>
    <updated>2016-01-21T20:19:31Z</updated>
    <published>2016-01-21T20:19:31Z</published>
    <title>Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA</title>
    <summary>  This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky
issues of syntactic ambiguity reduction and word sense disambiguation as well
as providing innovative and original solutions for constituent generation and
constraints management. To provide an insight into how it operates, the
system's general architecture and components, as well as its lexical, syntactic
and semantic resources are described. After that, the paper addresses the
mechanism that performs selective parsing through an interaction between
syntactic and semantic information, leading the parser to a coherent and
accurate interpretation of the input text.
</summary>
    <author>
      <name>Daniel Christen</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07124v1</id>
    <updated>2016-01-26T18:19:00Z</updated>
    <published>2016-01-26T18:19:00Z</published>
    <title>LIA-RAG: a system based on graphs and divergence of probabilities
  applied to Speech-To-Text Summarization</title>
    <summary>  This paper aims to introduces a new algorithm for automatic speech-to-text
summarization based on statistical divergences of probabilities and graphs. The
input is a text from speech conversations with noise, and the output a compact
text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus
are very encouraging
</summary>
    <author>
      <name>Elvys Linhares Pontes</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Andréa Carneiro Linhares</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, CCCS Multiling 2015 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00367v1</id>
    <updated>2016-02-01T02:53:41Z</updated>
    <published>2016-02-01T02:53:41Z</published>
    <title>Efficient Character-level Document Classification by Combining
  Convolution and Recurrent Layers</title>
    <summary>  Document classification tasks were primarily tackled at word level. Recent
research that works with character-level inputs shows several benefits over
word-level approaches such as natural incorporation of morphemes and better
handling of rare words. We propose a neural network architecture that utilizes
both convolution and recurrent layers to efficiently encode character inputs.
We validate the proposed model on eight large scale document classification
tasks and compare with character-level convolution-only models. It achieves
comparable performances with much less parameters.
</summary>
    <author>
      <name>Yijun Xiao</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <link href="http://arxiv.org/abs/1602.00367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00812v2</id>
    <updated>2016-08-26T07:04:29Z</updated>
    <published>2016-02-02T07:35:02Z</published>
    <title>The Grail theorem prover: Type theory for syntax and semantics</title>
    <summary>  As the name suggests, type-logical grammars are a grammar formalism based on
logic and type theory. From the prespective of grammar design, type-logical
grammars develop the syntactic and semantic aspects of linguistic phenomena
hand-in-hand, letting the desired semantics of an expression inform the
syntactic type and vice versa. Prototypical examples of the successful
application of type-logical grammars to the syntax-semantics interface include
coordination, quantifier scope and extraction.This chapter describes the Grail
theorem prover, a series of tools for designing and testing grammars in various
modern type-logical grammars which functions as a tool . All tools described in
this chapter are freely available.
</summary>
    <author>
      <name>Richard Moot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI, CNRS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Modern Perspectives in Type Theoretical Semantics, Springer, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.00812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01428v1</id>
    <updated>2016-02-03T19:44:37Z</updated>
    <published>2016-02-03T19:44:37Z</published>
    <title>"Draw My Topics": Find Desired Topics fast from large scale of Corpus</title>
    <summary>  We develop the "Draw My Topics" toolkit, which provides a fast way to
incorporate social scientists' interest into standard topic modelling. Instead
of using raw corpus with primitive processing as input, an algorithm based on
Vector Space Model and Conditional Entropy are used to connect social
scientists' willingness and unsupervised topic models' output. Space for users'
adjustment on specific corpus of their interest is also accommodated. We
demonstrate the toolkit's use on the Diachronic People's Daily Corpus in
Chinese.
</summary>
    <author>
      <name>Jason Dou</name>
    </author>
    <author>
      <name>Ni Sun</name>
    </author>
    <author>
      <name>Xiaojun Zou</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01925v2</id>
    <updated>2016-05-21T08:08:21Z</updated>
    <published>2016-02-05T04:26:38Z</published>
    <title>Massively Multilingual Word Embeddings</title>
    <summary>  We introduce new methods for estimating and evaluating embeddings of words in
more than fifty languages in a single shared embedding space. Our estimation
methods, multiCluster and multiCCA, use dictionaries and monolingual data; they
do not require parallel data. Our new evaluation method, multiQVEC-CCA, is
shown to correlate better than previous ones with two downstream tasks (text
categorization and parsing). We also describe a web portal for evaluation that
will facilitate further research in this area, along with open-source releases
of all our methods.
</summary>
    <author>
      <name>Waleed Ammar</name>
    </author>
    <author>
      <name>George Mulcaire</name>
    </author>
    <author>
      <name>Yulia Tsvetkov</name>
    </author>
    <author>
      <name>Guillaume Lample</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03483v1</id>
    <updated>2016-02-10T18:49:58Z</updated>
    <published>2016-02-10T18:49:58Z</published>
    <title>Learning Distributed Representations of Sentences from Unlabelled Data</title>
    <summary>  Unsupervised methods for learning distributed representations of words are
ubiquitous in today's NLP research, but far less is known about the best ways
to learn distributed phrase or sentence representations from unlabelled data.
This paper is a systematic comparison of models that learn such
representations. We find that the optimal approach depends critically on the
intended application. Deeper, more complex models are preferable for
representations to be used in supervised systems, but shallow log-linear models
work best for building representation spaces that can be decoded with simple
spatial distance metrics. We also propose two new unsupervised
representation-learning objectives designed to optimise the trade-off between
training time, domain portability and performance.
</summary>
    <author>
      <name>Felix Hill</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Anna Korhonen</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03606v1</id>
    <updated>2016-02-11T02:39:21Z</updated>
    <published>2016-02-11T02:39:21Z</published>
    <title>Variations of the Similarity Function of TextRank for Automated
  Summarization</title>
    <summary>  This article presents new alternatives to the similarity function for the
TextRank algorithm for automatic summarization of texts. We describe the
generalities of the algorithm and the different functions we propose. Some of
these variants achieve a significative improvement using the same metrics and
dataset as the original publication.
</summary>
    <author>
      <name>Federico Barrios</name>
    </author>
    <author>
      <name>Federico López</name>
    </author>
    <author>
      <name>Luis Argerich</name>
    </author>
    <author>
      <name>Rosa Wachenchauzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures. Presented at the Argentine Symposium on
  Artificial Intelligence (ASAI) 2015 - 44 JAIIO (September 2015)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">44 JAIIO - ASAI 2015 - ISSN: 2451-7585, pages 65-72</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.03606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04375v2</id>
    <updated>2016-04-05T01:17:56Z</updated>
    <published>2016-02-13T20:13:48Z</published>
    <title>Science Question Answering using Instructional Materials</title>
    <summary>  We provide a solution for elementary science test using instructional
materials. We posit that there is a hidden structure that explains the
correctness of an answer given the question and instructional materials and
present a unified max-margin framework that learns to find these hidden
structures (given a corpus of question-answer pairs and instructional
materials), and uses what it learns to answer novel elementary science
questions. Our evaluation shows that our framework outperforms several strong
baselines.
</summary>
    <author>
      <name>Mrinmaya Sachan</name>
    </author>
    <author>
      <name>Avinava Dubey</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected that the science QA dataset is NOT freely available</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.04375v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04375v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06064v3</id>
    <updated>2016-02-25T02:00:23Z</updated>
    <published>2016-02-19T07:27:49Z</published>
    <title>On Training Bi-directional Neural Network Language Model with Noise
  Contrastive Estimation</title>
    <summary>  We propose to train bi-directional neural network language model(NNLM) with
noise contrastive estimation(NCE). Experiments are conducted on a rescore task
on the PTB data set. It is shown that NCE-trained bi-directional NNLM
outperformed the one trained by conventional maximum likelihood training. But
still(regretfully), it did not out-perform the baseline uni-directional NNLM.
</summary>
    <author>
      <name>Tianxing He</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Jasha Droppo</name>
    </author>
    <author>
      <name>Kai Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1602.06064v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06064v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06289v2</id>
    <updated>2018-03-08T18:45:32Z</updated>
    <published>2016-02-19T20:48:19Z</published>
    <title>Learning to SMILE(S)</title>
    <summary>  This paper shows how one can directly apply natural language processing (NLP)
methods to classification problems in cheminformatics. Connection between these
seemingly separate fields is shown by considering standard textual
representation of compound, SMILES. The problem of activity prediction against
a target protein is considered, which is a crucial part of computer aided drug
design process. Conducted experiments show that this way one can not only
outrank state of the art results of hand crafted representations but also gets
direct structural insights into the way decisions are made.
</summary>
    <author>
      <name>Stanisław Jastrzębski</name>
    </author>
    <author>
      <name>Damian Leśniak</name>
    </author>
    <author>
      <name>Wojciech Marian Czarnecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a workshop contribution to ICLR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06289v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06289v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07776v4</id>
    <updated>2016-10-12T04:47:45Z</updated>
    <published>2016-02-25T02:42:58Z</published>
    <title>Recurrent Neural Network Grammars</title>
    <summary>  We introduce recurrent neural network grammars, probabilistic models of
sentences with explicit phrase structure. We explain efficient inference
procedures that allow application to both parsing and language modeling.
Experiments show that they provide better parsing in English than any single
previously published supervised generative model and better language modeling
than state-of-the-art sequential RNNs in English and Chinese.
</summary>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Adhiguna Kuncoro</name>
    </author>
    <author>
      <name>Miguel Ballesteros</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NAACL 2016 (contains corrigendum)</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07776v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07776v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08657v2</id>
    <updated>2017-07-31T17:19:25Z</updated>
    <published>2016-02-28T02:25:22Z</published>
    <title>QuotationFinder - Searching for Quotations and Allusions in Greek and
  Latin Texts and Establishing the Degree to Which a Quotation or Allusion
  Matches Its Source</title>
    <summary>  The software programs generally used with the TLG (Thesaurus Linguae Graecae)
and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well
suited for finding quotations and allusions. QuotationFinder uses more
sophisticated criteria as it ranks search results based on how closely they
match the source text, listing search results with literal quotations first and
loose verbal parallels last.
</summary>
    <author>
      <name>Luc Herren</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.46298/jdmdh.1389</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.46298/jdmdh.1389" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Data Mining &amp; Digital Humanities, Special Issue on
  Computer-Aided Processing of Intertextuality in Ancient Languages, Project
  presentations (August 2, 2017) jdmdh:1389</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08741v1</id>
    <updated>2016-02-28T16:58:01Z</updated>
    <published>2016-02-28T16:58:01Z</published>
    <title>Gibberish Semantics: How Good is Russian Twitter in Word Semantic
  Similarity Task?</title>
    <summary>  The most studied and most successful language models were developed and
evaluated mainly for English and other close European languages, such as
French, German, etc. It is important to study applicability of these models to
other languages. The use of vector space models for Russian was recently
studied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models
were evaluated against word semantic similarity task. For our knowledge Twitter
was not considered as a corpus for this task, with this work we fill the gap.
Results for vectors trained on Twitter corpus are comparable in accuracy with
other single-corpus trained models, although the best performance is currently
achieved by combination of multiple corpora.
</summary>
    <author>
      <name>Nikolay N. Vasiliev</name>
    </author>
    <link href="http://arxiv.org/abs/1602.08741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08742v3</id>
    <updated>2016-09-24T17:35:29Z</updated>
    <published>2016-02-28T16:59:46Z</published>
    <title>Optimizing the Learning Order of Chinese Characters Using a Novel
  Topological Sort Algorithm</title>
    <summary>  We present a novel algorithm for optimizing the order in which Chinese
characters are learned, one that incorporates the benefits of learning them in
order of usage frequency and in order of their hierarchal structural
relationships. We show that our work outperforms previously published orders
and algorithms. Our algorithm is applicable to any scheduling task where nodes
have intrinsic differences in importance and must be visited in topological
order.
</summary>
    <author>
      <name>James C. Loach</name>
    </author>
    <author>
      <name>Jinzhao Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0163623</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0163623" rel="related"/>
    <link href="http://arxiv.org/abs/1602.08742v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08742v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00112v1</id>
    <updated>2016-07-30T12:39:19Z</updated>
    <published>2016-07-30T12:39:19Z</published>
    <title>Supervised Attentions for Neural Machine Translation</title>
    <summary>  In this paper, we improve the attention or alignment accuracy of neural
machine translation by utilizing the alignments of training sentence pairs. We
simply compute the distance between the machine attentions and the "true"
alignments, and minimize this cost in the training procedure. Our experiments
on large-scale Chinese-to-English task show that our model improves both
translation and alignment qualities significantly over the large-vocabulary
neural machine translation system, and even beats a state-of-the-art
traditional syntax-based system.
</summary>
    <author>
      <name>Haitao Mi</name>
    </author>
    <author>
      <name>Zhiguo Wang</name>
    </author>
    <author>
      <name>Abe Ittycheriah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. In Proceedings of EMNLP 2016. arXiv admin note: text overlap
  with arXiv:1605.03148</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00470v2</id>
    <updated>2017-01-03T16:49:39Z</updated>
    <published>2016-08-01T15:27:16Z</published>
    <title>Labeling Topics with Images using Neural Networks</title>
    <summary>  Topics generated by topic models are usually represented by lists of $t$
terms or alternatively using short phrases and images. The current
state-of-the-art work on labeling topics using images selects images by
re-ranking a small set of candidates for a given topic. In this paper, we
present a more generic method that can estimate the degree of association
between any arbitrary pair of an unseen topic and image using a deep neural
network. Our method has better runtime performance $O(n)$ compared to $O(n^2)$
for the current state-of-the-art method, and is also significantly more
accurate.
</summary>
    <author>
      <name>Nikolaos Aletras</name>
    </author>
    <author>
      <name>Arpit Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00508v2</id>
    <updated>2017-05-27T04:01:13Z</updated>
    <published>2016-08-01T17:51:03Z</published>
    <title>Blind phoneme segmentation with temporal prediction errors</title>
    <summary>  Phonemic segmentation of speech is a critical step of speech recognition
systems. We propose a novel unsupervised algorithm based on sequence prediction
models such as Markov chains and recurrent neural network. Our approach
consists in analyzing the error profile of a model trained to predict speech
features frame-by-frame. Specifically, we try to learn the dynamics of speech
in the MFCC space and hypothesize boundaries from local maxima in the
prediction error. We evaluate our system on the TIMIT dataset, with
improvements over similar methods.
</summary>
    <author>
      <name>Paul Michel</name>
    </author>
    <author>
      <name>Okko Räsänen</name>
    </author>
    <author>
      <name>Roland Thiollière</name>
    </author>
    <author>
      <name>Emmanuel Dupoux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages 3 figures. Presented at ACL SRW 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00508v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00508v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00929v1</id>
    <updated>2016-08-02T18:45:53Z</updated>
    <published>2016-08-02T18:45:53Z</published>
    <title>Efficient Segmental Cascades for Speech Recognition</title>
    <summary>  Discriminative segmental models offer a way to incorporate flexible feature
functions into speech recognition. However, their appeal has been limited by
their computational requirements, due to the large number of possible segments
to consider. Multi-pass cascades of segmental models introduce features of
increasing complexity in different passes, where in each pass a segmental model
rescores lattices produced by a previous (simpler) segmental model. In this
paper, we explore several ways of making segmental cascades efficient and
practical: reducing the feature set in the first pass, frame subsampling, and
various pruning approaches. In experiments on phonetic recognition, we find
that with a combination of such techniques, it is possible to maintain
competitive performance while greatly reducing decoding, pruning, and training
time.
</summary>
    <author>
      <name>Hao Tang</name>
    </author>
    <author>
      <name>Weiran Wang</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <author>
      <name>Karen Livescu</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02784v2</id>
    <updated>2017-11-17T19:53:13Z</updated>
    <published>2016-08-09T12:26:19Z</published>
    <title>Canonical Correlation Inference for Mapping Abstract Scenes to Text</title>
    <summary>  We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an "abstract scene".
</summary>
    <author>
      <name>Nikos Papasarantopoulos</name>
    </author>
    <author>
      <name>Helen Jiang</name>
    </author>
    <author>
      <name>Shay B. Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, accepted to AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03000v1</id>
    <updated>2016-08-09T23:05:03Z</updated>
    <published>2016-08-09T23:05:03Z</published>
    <title>Neural Generation of Regular Expressions from Natural Language with
  Minimal Domain Knowledge</title>
    <summary>  This paper explores the task of translating natural language queries into
regular expressions which embody their meaning. In contrast to prior work, the
proposed neural model does not utilize domain-specific crafting, learning to
translate directly from a parallel corpus. To fully explore the potential of
neural models, we propose a methodology for collecting a large corpus of
regular expression, natural language pairs. Our resulting model achieves a
performance gain of 19.6% over previous state-of-the-art models.
</summary>
    <author>
      <name>Nicholas Locascio</name>
    </author>
    <author>
      <name>Karthik Narasimhan</name>
    </author>
    <author>
      <name>Eduardo DeLeon</name>
    </author>
    <author>
      <name>Nate Kushman</name>
    </author>
    <author>
      <name>Regina Barzilay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be published in EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03448v1</id>
    <updated>2016-08-11T13:10:02Z</updated>
    <published>2016-08-11T13:10:02Z</published>
    <title>Sex, drugs, and violence</title>
    <summary>  Automatically detecting inappropriate content can be a difficult NLP task,
requiring understanding context and innuendo, not just identifying specific
keywords. Due to the large quantity of online user-generated content, automatic
detection is becoming increasingly necessary. We take a largely unsupervised
approach using a large corpus of narratives from a community-based
self-publishing website and a small segment of crowd-sourced annotations. We
explore topic modelling using latent Dirichlet allocation (and a variation),
and use these to regress appropriateness ratings, effectively automating rating
for suitability. The results suggest that certain topics inferred may be useful
in detecting latent inappropriateness -- yielding recall up to 96% and low
regression errors.
</summary>
    <author>
      <name>Stefania Raimondo</name>
    </author>
    <author>
      <name>Frank Rudzicz</name>
    </author>
    <link href="http://arxiv.org/abs/1608.03448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03764v1</id>
    <updated>2016-08-12T11:58:00Z</updated>
    <published>2016-08-12T11:58:00Z</published>
    <title>Extracting Biological Pathway Models From NLP Event Representations</title>
    <summary>  This paper describes an an open-source software system for the automatic
conversion of NLP event representations to system biology structured data
interchange formats such as SBML and BioPAX. It is part of a larger effort to
make results of the NLP community available for system biology pathway
modelers.
</summary>
    <author>
      <name>Michael Spranger</name>
    </author>
    <author>
      <name>Sucheendra K. Palaniappan</name>
    </author>
    <author>
      <name>Samik Ghosh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2015 Workshop on Biomedical Natural Language
  Processing (BioNLP 2015), pages 42-51. Association for Computational
  Linguistics</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.03764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03767v1</id>
    <updated>2016-08-12T12:10:24Z</updated>
    <published>2016-08-12T12:10:24Z</published>
    <title>Measuring the State of the Art of Automated Pathway Curation Using Graph
  Algorithms - A Case Study of the mTOR Pathway</title>
    <summary>  This paper evaluates the difference between human pathway curation and
current NLP systems. We propose graph analysis methods for quantifying the gap
between human curated pathway maps and the output of state-of-the-art automatic
NLP systems. Evaluation is performed on the popular mTOR pathway. Based on
analyzing where current systems perform well and where they fail, we identify
possible avenues for progress.
</summary>
    <author>
      <name>Michael Spranger</name>
    </author>
    <author>
      <name>Sucheendra K. Palaniappan</name>
    </author>
    <author>
      <name>Samik Ghosh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 15th Workshop on Biomedical Natural Language
  Processing, Berlin, Germany, 2016, pages 119-127. Association for
  Computational Linguistics</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.03767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04147v1</id>
    <updated>2016-08-14T22:34:22Z</updated>
    <published>2016-08-14T22:34:22Z</published>
    <title>Numerically Grounded Language Models for Semantic Error Correction</title>
    <summary>  Semantic error detection and correction is an important task for applications
such as fact checking, speech-to-text or grammatical error correction. Current
approaches generally focus on relatively shallow semantics and do not account
for numeric quantities. Our approach uses language models grounded in numbers
within the text. Such groundings are easily achieved for recurrent neural
language model architectures, which can be further conditioned on incomplete
background knowledge bases. Our evaluation on clinical reports shows that
numerical grounding improves perplexity by 33% and F1 for semantic error
correction by 5 points when compared to ungrounded approaches. Conditioning on
a knowledge base yields further improvements.
</summary>
    <author>
      <name>Georgios P. Spithourakis</name>
    </author>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04434v1</id>
    <updated>2016-08-15T23:09:21Z</updated>
    <published>2016-08-15T23:09:21Z</published>
    <title>Natural Language Processing using Hadoop and KOSHIK</title>
    <summary>  Natural language processing, as a data analytics related technology, is used
widely in many research areas such as artificial intelligence, human language
processing, and translation. At present, due to explosive growth of data, there
are many challenges for natural language processing. Hadoop is one of the
platforms that can process the large amount of data required for natural
language processing. KOSHIK is one of the natural language processing
architectures, and utilizes Hadoop and contains language processing components
such as Stanford CoreNLP and OpenNLP. This study describes how to build a
KOSHIK platform with the relevant tools, and provides the steps to analyze wiki
data. Finally, it evaluates and discusses the advantages and disadvantages of
the KOSHIK architecture, and gives recommendations on improving the processing
performance.
</summary>
    <author>
      <name>Emre Erturk</name>
    </author>
    <author>
      <name>Hong Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05243v1</id>
    <updated>2016-08-18T11:41:45Z</updated>
    <published>2016-08-18T11:41:45Z</published>
    <title>Multilingual Modal Sense Classification using a Convolutional Neural
  Network</title>
    <summary>  Modal sense classification (MSC) is a special WSD task that depends on the
meaning of the proposition in the modal's scope. We explore a CNN architecture
for classifying modal sense in English and German. We show that CNNs are
superior to manually designed feature-based classifiers and a standard NN
classifier. We analyze the feature maps learned by the CNN and identify known
and previously unattested linguistic features. We benchmark the CNN on a
standard WSD task, where it compares favorably to models using
sense-disambiguated target vectors.
</summary>
    <author>
      <name>Ana Marasović</name>
    </author>
    <author>
      <name>Anette Frank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version, accepted at the 1st Workshop on Representation
  Learning for NLP, held in conjunction with ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05777v1</id>
    <updated>2016-08-20T03:43:29Z</updated>
    <published>2016-08-20T03:43:29Z</published>
    <title>Topic Sensitive Neural Headline Generation</title>
    <summary>  Neural models have recently been used in text summarization including
headline generation. The model can be trained using a set of document-headline
pairs. However, the model does not explicitly consider topical similarities and
differences of documents. We suggest to categorizing documents into various
topics so that documents within the same topic are similar in content and share
similar summarization patterns. Taking advantage of topic information of
documents, we propose topic sensitive neural headline generation model. Our
model can generate more accurate summaries guided by document topics. We test
our model on LCSTS dataset, and experiments show that our method outperforms
other baselines on each topic and achieves the state-of-art performance.
</summary>
    <author>
      <name>Lei Xu</name>
    </author>
    <author>
      <name>Ziyun Wang</name>
    </author>
    <author>
      <name> Ayana</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06757v1</id>
    <updated>2016-08-24T09:06:14Z</updated>
    <published>2016-08-24T09:06:14Z</published>
    <title>Robust Named Entity Recognition in Idiosyncratic Domains</title>
    <summary>  Named entity recognition often fails in idiosyncratic domains. That causes a
problem for depending tasks, such as entity linking and relation extraction. We
propose a generic and robust approach for high-recall named entity recognition.
Our approach is easy to train and offers strong generalization over diverse
domain-specific language, such as news documents (e.g. Reuters) or biomedical
text (e.g. Medline). Our approach is based on deep contextual sequence learning
and utilizes stacked bidirectional LSTM networks. Our model is trained with
only few hundred labeled sentences and does not rely on further external
knowledge. We report from our results F1 scores in the range of 84-94% on
standard datasets.
</summary>
    <author>
      <name>Sebastian Arnold</name>
    </author>
    <author>
      <name>Felix A. Gers</name>
    </author>
    <author>
      <name>Torsten Kilias</name>
    </author>
    <author>
      <name>Alexander Löser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07076v1</id>
    <updated>2016-08-25T10:43:56Z</updated>
    <published>2016-08-25T10:43:56Z</published>
    <title>A Context-aware Natural Language Generator for Dialogue Systems</title>
    <summary>  We present a novel natural language generation system for spoken dialogue
systems capable of entraining (adapting) to users' way of speaking, providing
contextually appropriate responses. The generator is based on recurrent neural
networks and the sequence-to-sequence approach. It is fully trainable from data
which include preceding context along with responses to be generated. We show
that the context-aware generator yields significant improvements over the
baseline in both automatic metrics and a human pairwise preference test.
</summary>
    <author>
      <name>Ondřej Dušek</name>
    </author>
    <author>
      <name>Filip Jurčíček</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/W16-3622</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/W16-3622" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a short paper for SIGDIAL 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the SIGDIAL 2016 Conference, pages 185-190, Los
  Angeles, USA, 13-15 September 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.07076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07115v1</id>
    <updated>2016-08-25T12:44:05Z</updated>
    <published>2016-08-25T12:44:05Z</published>
    <title>Aligning Packed Dependency Trees: a theory of composition for
  distributional semantics</title>
    <summary>  We present a new framework for compositional distributional semantics in
which the distributional contexts of lexemes are expressed in terms of anchored
packed dependency trees. We show that these structures have the potential to
capture the full sentential contexts of a lexeme and provide a uniform basis
for the composition of distributional knowledge in a way that captures both
mutual disambiguation and generalization.
</summary>
    <author>
      <name>David Weir</name>
    </author>
    <author>
      <name>Julie Weeds</name>
    </author>
    <author>
      <name>Jeremy Reffin</name>
    </author>
    <author>
      <name>Thomas Kober</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Special issue of Computational Linguistics - Formal
  Distributional Semantics</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08927v1</id>
    <updated>2016-08-31T16:23:07Z</updated>
    <published>2016-08-31T16:23:07Z</published>
    <title>The Generalized Smallest Grammar Problem</title>
    <summary>  The Smallest Grammar Problem -- the problem of finding the smallest
context-free grammar that generates exactly one given sequence -- has never
been successfully applied to grammatical inference. We investigate the reasons
and propose an extended formulation that seeks to minimize non-recursive
grammars, instead of straight-line programs. In addition, we provide very
efficient algorithms that approximate the minimization problem of this class of
grammars. Our empirical evaluation shows that we are able to find smaller
models than the current best approximations to the Smallest Grammar Problem on
standard benchmarks, and that the inferred rules capture much better the
syntactic structure of natural language.
</summary>
    <author>
      <name>Payam Siyari</name>
    </author>
    <author>
      <name>Matthias Gallé</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00718v1</id>
    <updated>2016-08-31T15:43:27Z</updated>
    <published>2016-08-31T15:43:27Z</published>
    <title>Convolutional Neural Networks for Text Categorization: Shallow
  Word-level vs. Deep Character-level</title>
    <summary>  This paper reports the performances of shallow word-level convolutional
neural networks (CNN), our earlier work (2015), on the eight datasets with
relatively large training data that were used for testing the very deep
character-level CNN in Conneau et al. (2016). Our findings are as follows. The
shallow word-level CNNs achieve better error rates than the error rates
reported in Conneau et al., though the results should be interpreted with some
consideration due to the unique pre-processing of Conneau et al. The shallow
word-level CNN uses more parameters and therefore requires more storage than
the deep character-level CNN; however, the shallow word-level CNN computes much
faster.
</summary>
    <author>
      <name>Rie Johnson</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01933v1</id>
    <updated>2016-09-07T10:59:58Z</updated>
    <published>2016-09-07T10:59:58Z</published>
    <title>Sentiment Classification of Food Reviews</title>
    <summary>  Sentiment analysis of reviews is a popular task in natural language
processing. In this work, the goal is to predict the score of food reviews on a
scale of 1 to 5 with two recurrent neural networks that are carefully tuned. As
for baseline, we train a simple RNN for classification. Then we extend the
baseline to GRU. In addition, we present two different methods to deal with
highly skewed data, which is a common problem for reviews. Models are evaluated
using accuracies.
</summary>
    <author>
      <name>Hua Feng</name>
    </author>
    <author>
      <name>Ruixi Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1609.01933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03204v1</id>
    <updated>2016-09-11T19:51:46Z</updated>
    <published>2016-09-11T19:51:46Z</published>
    <title>On the Similarities Between Native, Non-native and Translated Texts</title>
    <summary>  We present a computational analysis of three language varieties: native,
advanced non-native, and translation. Our goal is to investigate the
similarities and differences between non-native language productions and
translations, contrasting both with native language. Using a collection of
computational methods we establish three main results: (1) the three types of
texts are easily distinguishable; (2) non-native language and translations are
closer to each other than each of them is to native language; and (3) some of
these characteristics depend on the source or native language, while others do
not, reflecting, perhaps, unified principles that similarly affect translations
and non-native language.
</summary>
    <author>
      <name>Ella Rabinovich</name>
    </author>
    <author>
      <name>Sergiu Nisioi</name>
    </author>
    <author>
      <name>Noam Ordan</name>
    </author>
    <author>
      <name>Shuly Wintner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL2016, 12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03441v2</id>
    <updated>2017-06-05T18:46:40Z</updated>
    <published>2016-09-12T15:16:43Z</published>
    <title>Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing</title>
    <summary>  We present a dependency parser implemented as a single deep neural network
that reads orthographic representations of words and directly generates
dependencies and their labels. Unlike typical approaches to parsing, the model
doesn't require part-of-speech (POS) tagging of the sentences. With proper
regularization and additional supervision achieved with multitask learning we
reach state-of-the-art performance on Slavic languages from the Universal
Dependencies treebank: with no linguistic features other than characters, our
parser is as accurate as a transition- based system trained on perfect POS
tags.
</summary>
    <author>
      <name>Jan Chorowski</name>
    </author>
    <author>
      <name>Michał Zapotoczny</name>
    </author>
    <author>
      <name>Paweł Rychlikowski</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03441v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03441v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04186v1</id>
    <updated>2016-09-14T09:31:40Z</updated>
    <published>2016-09-14T09:31:40Z</published>
    <title>Neural Machine Translation with Supervised Attention</title>
    <summary>  The attention mechanisim is appealing for neural machine translation, since
it is able to dynam- ically encode a source sentence by generating a alignment
between a target word and source words. Unfortunately, it has been proved to be
worse than conventional alignment models in aligment accuracy. In this paper,
we analyze and explain this issue from the point view of re- ordering, and
propose a supervised attention which is learned with guidance from conventional
alignment models. Experiments on two Chinese-to-English translation tasks show
that the super- vised attention mechanism yields better alignments leading to
substantial gains over the standard attention based NMT.
</summary>
    <author>
      <name>Lemao Liu</name>
    </author>
    <author>
      <name>Masao Utiyama</name>
    </author>
    <author>
      <name>Andrew Finch</name>
    </author>
    <author>
      <name>Eiichiro Sumita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was submitted into COLING2016 on July 10, and it is under
  review</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04325v1</id>
    <updated>2016-09-14T15:58:55Z</updated>
    <published>2016-09-14T15:58:55Z</published>
    <title>Transliteration in Any Language with Surrogate Languages</title>
    <summary>  We introduce a method for transliteration generation that can produce
transliterations in every language. Where previous results are only as
multilingual as Wikipedia, we show how to use training data from Wikipedia as
surrogate training for any language. Thus, the problem becomes one of ranking
Wikipedia languages in order of suitability with respect to a target language.
We introduce several task-specific methods for ranking languages, and show that
our approach is comparable to the oracle ceiling, and even outperforms it in
some cases.
</summary>
    <author>
      <name>Stephen Mayhew</name>
    </author>
    <author>
      <name>Christos Christodoulopoulos</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04779v1</id>
    <updated>2016-09-15T19:07:17Z</updated>
    <published>2016-09-15T19:07:17Z</published>
    <title>Characterizing the Language of Online Communities and its Relation to
  Community Reception</title>
    <summary>  This work investigates style and topic aspects of language in online
communities: looking at both utility as an identifier of the community and
correlation with community reception of content. Style is characterized using a
hybrid word and part-of-speech tag n-gram language model, while topic is
represented using Latent Dirichlet Allocation. Experiments with several Reddit
forums show that style is a better indicator of community identity than topic,
even for communities organized around specific topics. Further, there is a
positive correlation between the community reception to a contribution and the
style similarity to that community, but not so for topic similarity.
</summary>
    <author>
      <name>Trang Tran</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05935v2</id>
    <updated>2017-01-25T08:30:10Z</updated>
    <published>2016-09-19T20:52:44Z</published>
    <title>Advances in All-Neural Speech Recognition</title>
    <summary>  This paper advances the design of CTC-based all-neural (or end-to-end) speech
recognizers. We propose a novel symbol inventory, and a novel iterated-CTC
method in which a second system is used to transform a noisy initial output
into a cleaner version. We present a number of stabilization and initialization
methods we have found useful in training these networks. We evaluate our system
on the commonly used NIST 2000 conversational telephony test set, and
significantly exceed the previously published performance of similar systems,
both with and without the use of an external language model and decoding
technology.
</summary>
    <author>
      <name>G. Zweig</name>
    </author>
    <author>
      <name>C. Yu</name>
    </author>
    <author>
      <name>J. Droppo</name>
    </author>
    <author>
      <name>A. Stolcke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2017.7953069</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2017.7953069" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE ICASSP, March 2017, pp. 4805-4809</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.05935v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05935v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06082v1</id>
    <updated>2016-09-20T10:23:47Z</updated>
    <published>2016-09-20T10:23:47Z</published>
    <title>Learning Robust Representations of Text</title>
    <summary>  Deep neural networks have achieved remarkable results across many language
processing tasks, however these methods are highly sensitive to noise and
adversarial attacks. We present a regularization based method for limiting
network sensitivity to its inputs, inspired by ideas from computer vision, thus
learning models that are more robust. Empirical evaluation over a range of
sentiment datasets with a convolutional neural network shows that, compared to
a baseline model and the dropout method, our method achieves superior
performance over noisy inputs and out-of-domain data.
</summary>
    <author>
      <name>Yitong Li</name>
    </author>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <author>
      <name>Timothy Baldwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages with 2 pages reference, 2 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07035v1</id>
    <updated>2016-09-22T15:53:04Z</updated>
    <published>2016-09-22T15:53:04Z</published>
    <title>Abstractive Meeting Summarization UsingDependency Graph Fusion</title>
    <summary>  Automatic summarization techniques on meeting conversations developed so far
have been primarily extractive, resulting in poor summaries. To improve this,
we propose an approach to generate abstractive summaries by fusing important
content from several utterances. Any meeting is generally comprised of several
discussion topic segments. For each topic segment within a meeting
conversation, we aim to generate a one sentence summary from the most important
utterances using an integer linear programming-based sentence fusion approach.
Experimental results show that our method can generate more informative
summaries than the baselines.
</summary>
    <author>
      <name>Siddhartha Banerjee</name>
    </author>
    <author>
      <name>Prasenjit Mitra</name>
    </author>
    <author>
      <name>Kazunari Sugiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WWW '15 Companion Proceedings of the 24th International Conference on
  World Wide Web, Pages 5-6. arXiv admin note: substantial text overlap with
  arXiv:1609.07033</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07053v2</id>
    <updated>2016-10-31T18:33:13Z</updated>
    <published>2016-09-22T16:34:00Z</published>
    <title>Semantic Tagging with Deep Residual Networks</title>
    <summary>  We propose a novel semantic tagging task, sem-tagging, tailored for the
purpose of multilingual semantic parsing, and present the first tagger using
deep residual networks (ResNets). Our tagger uses both word and character
representations and includes a novel residual bypass architecture. We evaluate
the tagset both intrinsically on the new task of semantic tagging, as well as
on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an
auxiliary loss function predicting our semantic tags, significantly outperforms
prior results on English Universal Dependencies POS tagging (95.71% accuracy on
UD v1.2 and 95.67% accuracy on UD v1.3).
</summary>
    <author>
      <name>Johannes Bjerva</name>
    </author>
    <author>
      <name>Barbara Plank</name>
    </author>
    <author>
      <name>Johan Bos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COLING 2016, camera ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07222v1</id>
    <updated>2016-09-23T03:35:27Z</updated>
    <published>2016-09-23T03:35:27Z</published>
    <title>Deep Multi-Task Learning with Shared Memory</title>
    <summary>  Neural network based models have achieved impressive results on various
specific tasks. However, in previous works, most models are learned separately
based on single-task supervised objectives, which often suffer from
insufficient training data. In this paper, we propose two deep architectures
which can be trained jointly on multiple related tasks. More specifically, we
augment neural model with an external memory, which is shared by several tasks.
Experiments on two groups of text classification tasks show that our proposed
architectures can improve the performance of a task with the help of other
related tasks.
</summary>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by emnlp2016. arXiv admin note: text overlap with
  arXiv:1605.05101</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07451v1</id>
    <updated>2016-09-23T18:12:12Z</updated>
    <published>2016-09-23T18:12:12Z</published>
    <title>AMR-to-text generation as a Traveling Salesman Problem</title>
    <summary>  The task of AMR-to-text generation is to generate grammatical text that
sustains the semantic meaning for a given AMR graph. We at- tack the task by
first partitioning the AMR graph into smaller fragments, and then generating
the translation for each fragment, before finally deciding the order by solving
an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy
classifier is trained to estimate the traveling costs, and a TSP solver is used
to find the optimized solution. The final model reports a BLEU score of 22.44
on the SemEval-2016 Task8 dataset.
</summary>
    <author>
      <name>Linfeng Song</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <author>
      <name>Xiaochang Peng</name>
    </author>
    <author>
      <name>Zhiguo Wang</name>
    </author>
    <author>
      <name>Daniel Gildea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07680v1</id>
    <updated>2016-09-24T23:22:50Z</updated>
    <published>2016-09-24T23:22:50Z</published>
    <title>Existence of Hierarchies and Human's Pursuit of Top Hierarchy Lead to
  Power Law</title>
    <summary>  The power law is ubiquitous in natural and social phenomena, and is
considered as a universal relationship between the frequency and its rank for
diverse social systems. However, a general model is still lacking to interpret
why these seemingly unrelated systems share great similarity. Through a
detailed analysis of natural language texts and simulation experiments based on
the proposed 'Hierarchical Selection Model', we found that the existence of
hierarchies and human's pursuit of top hierarchy lead to the power law.
Further, the power law is a statistical and emergent performance of
hierarchies, and it is the universality of hierarchies that contributes to the
ubiquity of the power law.
</summary>
    <author>
      <name>Shuiyuan Yu</name>
    </author>
    <author>
      <name>Junying Liang</name>
    </author>
    <author>
      <name>Haitao Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07701v1</id>
    <updated>2016-09-25T05:07:55Z</updated>
    <published>2016-09-25T05:07:55Z</published>
    <title>Large-Scale Machine Translation between Arabic and Hebrew: Available
  Corpora and Initial Results</title>
    <summary>  Machine translation between Arabic and Hebrew has so far been limited by a
lack of parallel corpora, despite the political and cultural importance of this
language pair. Previous work relied on manually-crafted grammars or pivoting
via English, both of which are unsatisfactory for building a scalable and
accurate MT system. In this work, we compare standard phrase-based and neural
systems on Arabic-Hebrew translation. We experiment with tokenization by
external tools and sub-word modeling by character-level neural models, and show
that both methods lead to improved translation performance, with a small
advantage to the neural models.
</summary>
    <author>
      <name>Yonatan Belinkov</name>
    </author>
    <author>
      <name>James Glass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SeMaT 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07756v1</id>
    <updated>2016-09-25T15:10:16Z</updated>
    <published>2016-09-25T15:10:16Z</published>
    <title>A Factorized Model for Transitive Verbs in Compositional Distributional
  Semantics</title>
    <summary>  We present a factorized compositional distributional semantics model for the
representation of transitive verb constructions. Our model first produces
(subject, verb) and (verb, object) vector representations based on the
similarity of the nouns in the construction to each of the nouns in the
vocabulary and the tendency of these nouns to take the subject and object roles
of the verb. These vectors are then combined into a final (subject,verb,object)
representation through simple vector operations. On two established tasks for
the transitive verb construction our model outperforms recent previous work.
</summary>
    <author>
      <name>Lilach Edelstein</name>
    </author>
    <author>
      <name>Roi Reichart</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08139v1</id>
    <updated>2016-09-26T19:50:59Z</updated>
    <published>2016-09-26T19:50:59Z</published>
    <title>An Unsupervised Probability Model for Speech-to-Translation Alignment of
  Low-Resource Languages</title>
    <summary>  For many low-resource languages, spoken language resources are more likely to
be annotated with translations than with transcriptions. Translated speech data
is potentially valuable for documenting endangered languages or for training
speech translation systems. A first step towards making use of such data would
be to automatically align spoken words with their translations. We present a
model that combines Dyer et al.'s reparameterization of IBM Model 2
(fast-align) and k-means clustering using Dynamic Time Warping as a distance
metric. The two components are trained jointly using expectation-maximization.
In an extremely low-resource scenario, our model performs significantly better
than both a neural model and a strong baseline.
</summary>
    <author>
      <name>Antonios Anastasopoulos</name>
    </author>
    <author>
      <name>David Chiang</name>
    </author>
    <author>
      <name>Long Duong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08210v1</id>
    <updated>2016-09-26T22:12:50Z</updated>
    <published>2016-09-26T22:12:50Z</published>
    <title>Learning to Translate for Multilingual Question Answering</title>
    <summary>  In multilingual question answering, either the question needs to be
translated into the document language, or vice versa. In addition to direction,
there are multiple methods to perform the translation, four of which we explore
in this paper: word-based, 10-best, context-based, and grammar-based. We build
a feature for each combination of translation direction and method, and train a
model that learns optimal feature weights. On a large forum dataset consisting
of posts in English, Arabic, and Chinese, our novel learn-to-translate approach
was more effective than a strong baseline (p&lt;0.05): translating all text into
English, then training a classifier based only on English (original or
translated) text.
</summary>
    <author>
      <name>Ferhan Ture</name>
    </author>
    <author>
      <name>Elizabeth Boschee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. To appear in EMNLP'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08293v1</id>
    <updated>2016-09-27T07:38:29Z</updated>
    <published>2016-09-27T07:38:29Z</published>
    <title>The Effects of Data Size and Frequency Range on Distributional Semantic
  Models</title>
    <summary>  This paper investigates the effects of data size and frequency range on
distributional semantic models. We compare the performance of a number of
representative models for several test settings over data of varying sizes, and
over test items of various frequency. Our results show that neural
network-based models underperform when the data is small, and that the most
reliable model over data of varying sizes and frequency ranges is the inverted
factorized model.
</summary>
    <author>
      <name>Magnus Sahlgren</name>
    </author>
    <author>
      <name>Alessandro Lenci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08445v1</id>
    <updated>2016-09-27T13:50:13Z</updated>
    <published>2016-09-27T13:50:13Z</published>
    <title>AP16-OL7: A Multilingual Database for Oriental Languages and A Language
  Recognition Baseline</title>
    <summary>  We present the AP16-OL7 database which was released as the training and test
data for the oriental language recognition (OLR) challenge on APSIPA 2016.
Based on the database, a baseline system was constructed on the basis of the
i-vector model. We report the baseline results evaluated in various metrics
defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a
reasonable data resource for multilingual research.
</summary>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Difei Tang</name>
    </author>
    <author>
      <name>Qing Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">APSIPA ASC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08667v3</id>
    <updated>2016-10-31T20:30:15Z</updated>
    <published>2016-09-27T21:00:26Z</published>
    <title>Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
    <summary>  Coreference resolution systems are typically trained with heuristic loss
functions that require careful tuning. In this paper we instead apply
reinforcement learning to directly optimize a neural mention-ranking model for
coreference evaluation metrics. We experiment with two approaches: the
REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective.
We find the latter to be more effective, resulting in significant improvements
over the current state-of-the-art on the English and Chinese portions of the
CoNLL 2012 Shared Task.
</summary>
    <author>
      <name>Kevin Clark</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08667v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08667v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08777v2</id>
    <updated>2016-10-28T16:08:36Z</updated>
    <published>2016-09-28T05:41:18Z</published>
    <title>Character Sequence Models for ColorfulWords</title>
    <summary>  We present a neural network architecture to predict a point in color space
from the sequence of characters in the color's name. Using large scale
color--name pairs obtained from an online color design forum, we evaluate our
model on a "color Turing test" and find that, given a name, the colors
predicted by our model are preferred by annotators to color names created by
humans. Our datasets and demo system are available online at colorlab.us.
</summary>
    <author>
      <name>Kazuya Kawakami</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Bryan R. Routledge</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09004v2</id>
    <updated>2016-10-28T15:28:29Z</updated>
    <published>2016-09-28T16:51:56Z</published>
    <title>Byte-based Language Identification with Deep Convolutional Networks</title>
    <summary>  We report on our system for the shared task on discriminating between similar
languages (DSL 2016). The system uses only byte representations in a deep
residual network (ResNet). The system, named ResIdent, is trained only on the
data released with the task (closed training). We obtain 84.88% accuracy on
subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A
large difference in accuracy on development data can be observed with
relatively minor changes in our network's architecture and hyperparameters. We
therefore expect fine-tuning of these parameters to yield higher accuracies.
</summary>
    <author>
      <name>Johannes Bjerva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. Adapted reviewer comments. arXiv admin note: text overlap
  with arXiv:1609.07053</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09019v1</id>
    <updated>2016-09-28T17:58:23Z</updated>
    <published>2016-09-28T17:58:23Z</published>
    <title>Psychologically Motivated Text Mining</title>
    <summary>  Natural language processing techniques are increasingly applied to identify
social trends and predict behavior based on large text collections. Existing
methods typically rely on surface lexical and syntactic information. Yet,
research in psychology shows that patterns of human conceptualisation, such as
metaphorical framing, are reliable predictors of human expectations and
decisions. In this paper, we present a method to learn patterns of metaphorical
framing from large text collections, using statistical techniques. We apply the
method to data in three different languages and evaluate the identified
patterns, demonstrating their psychological validity.
</summary>
    <author>
      <name>Ekaterina Shutova</name>
    </author>
    <author>
      <name>Patricia Lichtenstein</name>
    </author>
    <link href="http://arxiv.org/abs/1609.09019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09171v2</id>
    <updated>2016-10-08T15:16:57Z</updated>
    <published>2016-09-29T01:53:08Z</published>
    <title>Empirical Evaluation of RNN Architectures on Sentence Classification
  Task</title>
    <summary>  Recurrent Neural Networks have achieved state-of-the-art results for many
problems in NLP and two most popular RNN architectures are Tail Model and
Pooling Model. In this paper, a hybrid architecture is proposed and we present
the first empirical study using LSTMs to compare performance of the three RNN
structures on sentence classification task. Experimental results show that the
Max Pooling Model or Hybrid Max Pooling Model achieves the best performance on
most datasets, while Tail Model does not outperform other models.
</summary>
    <author>
      <name>Lei Shen</name>
    </author>
    <author>
      <name>Junlin Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1609.09171v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09171v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09315v1</id>
    <updated>2016-09-29T12:20:13Z</updated>
    <published>2016-09-29T12:20:13Z</published>
    <title>Semantic Parsing with Semi-Supervised Sequential Autoencoders</title>
    <summary>  We present a novel semi-supervised approach for sequence transduction and
apply it to semantic parsing. The unsupervised component is based on a
generative model in which latent sentences generate the unpaired logical forms.
We apply this method to a number of semantic parsing tasks focusing on domains
with limited access to labelled training data and extend those datasets with
synthetically generated logical forms.
</summary>
    <author>
      <name>Tomáš Kočiský</name>
    </author>
    <author>
      <name>Gábor Melis</name>
    </author>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Wang Ling</name>
    </author>
    <author>
      <name>Phil Blunsom</name>
    </author>
    <author>
      <name>Karl Moritz Hermann</name>
    </author>
    <link href="http://arxiv.org/abs/1609.09315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09552v1</id>
    <updated>2016-09-30T00:01:27Z</updated>
    <published>2016-09-30T00:01:27Z</published>
    <title>Controlling Output Length in Neural Encoder-Decoders</title>
    <summary>  Neural encoder-decoder models have shown great success in many sequence
generation tasks. However, previous work has not investigated situations in
which we would like to control the length of encoder-decoder outputs. This
capability is crucial for applications such as text summarization, in which we
have to generate concise summaries with a desired length. In this paper, we
propose methods for controlling the output sequence length for neural
encoder-decoder models: two decoding-based methods and two learning-based
methods. Results show that our learning-based methods have the capability to
control length without degrading summary quality in a summarization task.
</summary>
    <author>
      <name>Yuta Kikuchi</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Ryohei Sasano</name>
    </author>
    <author>
      <name>Hiroya Takamura</name>
    </author>
    <author>
      <name>Manabu Okumura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. To appear in EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09580v1</id>
    <updated>2016-09-30T03:20:52Z</updated>
    <published>2016-09-30T03:20:52Z</published>
    <title>Referential Uncertainty and Word Learning in High-dimensional,
  Continuous Meaning Spaces</title>
    <summary>  This paper discusses lexicon word learning in high-dimensional meaning spaces
from the viewpoint of referential uncertainty. We investigate various
state-of-the-art Machine Learning algorithms and discuss the impact of scaling,
representation and meaning space structure. We demonstrate that current Machine
Learning techniques successfully deal with high-dimensional meaning spaces. In
particular, we show that exponentially increasing dimensions linearly impact
learner performance and that referential uncertainty from word sensitivity has
no impact.
</summary>
    <author>
      <name>Michael Spranger</name>
    </author>
    <author>
      <name>Katrien Beuls</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as Spranger, M. and Beuls, K. (2016). Referential
  uncertainty and word learning in high-dimensional, continuous meaning spaces.
  In Hafner, V. and Pitti, A., editors, Development and Learning and Epigenetic
  Robotics (ICDL-Epirob), 2016 Joint IEEE International Conferences on, 2016.
  IEEE</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00126v1</id>
    <updated>2016-11-01T04:48:09Z</updated>
    <published>2016-11-01T04:48:09Z</published>
    <title>Improving Twitter Sentiment Classification via Multi-Level
  Sentiment-Enriched Word Embeddings</title>
    <summary>  Most of existing work learn sentiment-specific word representation for
improving Twitter sentiment classification, which encoded both n-gram and
distant supervised tweet sentiment information in learning process. They assume
all words within a tweet have the same sentiment polarity as the whole tweet,
which ignores the word its own sentiment polarity. To address this problem, we
propose to learn sentiment-specific word embedding by exploiting both lexicon
resource and distant supervised information. We develop a multi-level
sentiment-enriched word embedding learning method, which uses parallel
asymmetric neural network to model n-gram, word level sentiment and tweet level
sentiment in learning process. Experiments on standard benchmarks show our
approach outperforms state-of-the-art methods.
</summary>
    <author>
      <name>Shufeng Xiong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2017.11.023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2017.11.023" rel="related"/>
    <link href="http://arxiv.org/abs/1611.00126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00354v1</id>
    <updated>2016-11-01T19:56:36Z</updated>
    <published>2016-11-01T19:56:36Z</published>
    <title>Faster decoding for subword level Phrase-based SMT between related
  languages</title>
    <summary>  A common and effective way to train translation systems between related
languages is to consider sub-word level basic units. However, this increases
the length of the sentences resulting in increased decoding time. The increase
in length is also impacted by the specific choice of data format for
representing the sentences as subwords. In a phrase-based SMT framework, we
investigate different choices of decoder parameters as well as data format and
their impact on decoding time and translation accuracy. We suggest best options
for these settings that significantly improve decoding time with little impact
on the translation accuracy.
</summary>
    <author>
      <name>Anoop Kunchukuttan</name>
    </author>
    <author>
      <name>Pushpak Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at VarDial3 (Third Workshop on NLP for Similar Languages,
  Varieties and Dialects) collocated with COLING 2016; 7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00995v1</id>
    <updated>2016-11-03T13:14:52Z</updated>
    <published>2016-11-03T13:14:52Z</published>
    <title>An empirical study for Vietnamese dependency parsing</title>
    <summary>  This paper presents an empirical comparison of different dependency parsers
for Vietnamese, which has some unusual characteristics such as copula drop and
verb serialization. Experimental results show that the neural network-based
parsers perform significantly better than the traditional parsers. We report
the highest parsing scores published to date for Vietnamese with the labeled
attachment score (LAS) at 73.53% and the unlabeled attachment score (UAS) at
80.66%.
</summary>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <author>
      <name>Mark Dras</name>
    </author>
    <author>
      <name>Mark Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 14th Annual Workshop of the
  Australasian Language Technology Association</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01101v1</id>
    <updated>2016-11-03T17:41:47Z</updated>
    <published>2016-11-03T17:41:47Z</published>
    <title>CogALex-V Shared Task: ROOT18</title>
    <summary>  In this paper, we describe ROOT 18, a classifier using the scores of several
unsupervised distributional measures as features to discriminate between
semantically related and unrelated words, and then to classify the related
pairs according to their semantic relation (i.e. synonymy, antonymy, hypernymy,
part-whole meronymy). Our classifier participated in the CogALex-V Shared Task,
showing a solid performance on the first subtask, but a poor performance on the
second subtask. The low scores reported on the second subtask suggest that
distributional measures are not sufficient to discriminate between multiple
semantic relations at once.
</summary>
    <author>
      <name>Emmanuele Chersoni</name>
    </author>
    <author>
      <name>Giulia Rambelli</name>
    </author>
    <author>
      <name>Enrico Santus</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01487v3</id>
    <updated>2017-04-11T08:51:27Z</updated>
    <published>2016-11-04T18:42:47Z</published>
    <title>Morphological Inflection Generation with Hard Monotonic Attention</title>
    <summary>  We present a neural model for morphological inflection generation which
employs a hard attention mechanism, inspired by the nearly-monotonic alignment
commonly found between the characters in a word and the characters in its
inflection. We evaluate the model on three previously studied morphological
inflection generation datasets and show that it provides state of the art
results in various setups compared to previous neural and non-neural
approaches. Finally we present an analysis of the continuous representations
learned by both the hard and soft attention \cite{bahdanauCB14} models for the
task, shedding some light on the features such models extract.
</summary>
    <author>
      <name>Roee Aharoni</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a long paper in ACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01487v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01487v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01747v1</id>
    <updated>2016-11-06T09:50:24Z</updated>
    <published>2016-11-06T09:50:24Z</published>
    <title>A Compare-Aggregate Model for Matching Text Sequences</title>
    <summary>  Many NLP tasks including machine comprehension, answer selection and text
entailment require the comparison between sequences. Matching the important
units between sequences is a key to solve these problems. In this paper, we
present a general "compare-aggregate" framework that performs word-level
matching followed by aggregation using Convolutional Neural Networks. We
particularly focus on the different comparison functions we can use to match
two vectors. We use four different datasets to evaluate the model. We find that
some simple comparison functions based on element-wise operations can work
better than standard neural network and neural tensor network.
</summary>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Jing Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01783v1</id>
    <updated>2016-11-06T14:00:14Z</updated>
    <published>2016-11-06T14:00:14Z</published>
    <title>Domain Adaptation For Formant Estimation Using Deep Learning</title>
    <summary>  In this paper we present a domain adaptation technique for formant estimation
using a deep network. We first train a deep learning network on a small read
speech dataset. We then freeze the parameters of the trained network and use
several different datasets to train an adaptation layer that makes the obtained
network universal in the sense that it works well for a variety of speakers and
speech domains with very different characteristics. We evaluated our adapted
network on three datasets, each of which has different speaker characteristics
and speech styles. The performance of our method compares favorably with
alternative methods for formant estimation.
</summary>
    <author>
      <name>Yehoshua Dissen</name>
    </author>
    <author>
      <name>Joseph Keshet</name>
    </author>
    <author>
      <name>Jacob Goldberger</name>
    </author>
    <author>
      <name>Cynthia Clopper</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01884v3</id>
    <updated>2017-06-05T03:47:15Z</updated>
    <published>2016-11-07T03:39:52Z</published>
    <title>AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text
  Classification</title>
    <summary>  Recently deeplearning models have been shown to be capable of making
remarkable performance in sentences and documents classification tasks. In this
work, we propose a novel framework called AC-BLSTM for modeling sentences and
documents, which combines the asymmetric convolution neural network (ACNN) with
the Bidirectional Long Short-Term Memory network (BLSTM). Experiment results
demonstrate that our model achieves state-of-the-art results on five tasks,
including sentiment analysis, question type classification, and subjectivity
classification. In order to further improve the performance of AC-BLSTM, we
propose a semi-supervised learning framework called G-AC-BLSTM for text
classification by combining the generative model with AC-BLSTM.
</summary>
    <author>
      <name>Depeng Liang</name>
    </author>
    <author>
      <name>Yongdong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01884v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01884v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02025v1</id>
    <updated>2016-11-07T12:47:25Z</updated>
    <published>2016-11-07T12:47:25Z</published>
    <title>Presenting a New Dataset for the Timeline Generation Problem</title>
    <summary>  The timeline generation task summarises an entity's biography by selecting
stories representing key events from a large pool of relevant documents. This
paper addresses the lack of a standard dataset and evaluative methodology for
the problem. We present and make publicly available a new dataset of 18,793
news articles covering 39 entities. For each entity, we provide a gold standard
timeline and a set of entity-related articles. We propose ROUGE as an
evaluation metric and validate our dataset by showing that top Google results
outperform straw-man baselines.
</summary>
    <author>
      <name>Xavier Holt</name>
    </author>
    <author>
      <name>Will Radford</name>
    </author>
    <author>
      <name>Ben Hachey</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02027v1</id>
    <updated>2016-11-07T12:51:22Z</updated>
    <published>2016-11-07T12:51:22Z</published>
    <title>:telephone::person::sailboat::whale::okhand:; or "Call me Ishmael" - How
  do you translate emoji?</title>
    <summary>  We report on an exploratory analysis of Emoji Dick, a project that leverages
crowdsourcing to translate Melville's Moby Dick into emoji. This distinctive
use of emoji removes textual context, and leads to a varying translation
quality. In this paper, we use statistical word alignment and part-of-speech
tagging to explore how people use emoji. Despite these simple methods, we
observed differences in token and part-of-speech distributions. Experiments
also suggest that semantics are preserved in the translation, and repetition is
more common in emoji.
</summary>
    <author>
      <name>Will Radford</name>
    </author>
    <author>
      <name>Andrew Chisholm</name>
    </author>
    <author>
      <name>Ben Hachey</name>
    </author>
    <author>
      <name>Bo Han</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02988v1</id>
    <updated>2016-11-09T15:49:31Z</updated>
    <published>2016-11-09T15:49:31Z</published>
    <title>Distant supervision for emotion detection using Facebook reactions</title>
    <summary>  We exploit the Facebook reaction feature in a distant supervised fashion to
train a support vector machine classifier for emotion detection, using several
feature combinations and combining different Facebook pages. We test our models
on existing benchmarks for emotion detection and show that employing only
information that is derived completely automatically, thus without relying on
any handcrafted lexicon as it's usually done, we can achieve competitive
results. The results also show that there is large room for improvement,
especially by gearing the collection of Facebook pages, with a view to the
target domain.
</summary>
    <author>
      <name>Chris Pool</name>
    </author>
    <author>
      <name>Malvina Nissim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on Computational Modeling of People's
  Opinions, Personality, and Emotions in Social Media (PEOPLES 2016), held in
  conjunction with COLING 2016, Osaka, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03057v1</id>
    <updated>2016-11-09T19:39:15Z</updated>
    <published>2016-11-09T19:39:15Z</published>
    <title>When silver glitters more than gold: Bootstrapping an Italian
  part-of-speech tagger for Twitter</title>
    <summary>  We bootstrap a state-of-the-art part-of-speech tagger to tag Italian Twitter
data, in the context of the Evalita 2016 PoSTWITA shared task. We show that
training the tagger on native Twitter data enriched with little amounts of
specifically selected gold data and additional silver-labelled data scraped
from Facebook, yields better results than using large amounts of manually
annotated data from a mix of genres.
</summary>
    <author>
      <name>Barbara Plank</name>
    </author>
    <author>
      <name>Malvina Nissim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 5th Evaluation Campaign of Natural Language
  Processing and Speech Tools for Italian (EVALITA 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03279v1</id>
    <updated>2016-11-10T12:22:43Z</updated>
    <published>2016-11-10T12:22:43Z</published>
    <title>Tracing metaphors in time through self-distance in vector spaces</title>
    <summary>  From a diachronic corpus of Italian, we build consecutive vector spaces in
time and use them to compare a term's cosine similarity to itself in different
time spans. We assume that a drop in similarity might be related to the
emergence of a metaphorical sense at a given time. Similarity-based
observations are matched to the actual year when a figurative meaning was
documented in a reference dictionary and through manual inspection of corpus
occurrences.
</summary>
    <author>
      <name>Marco Del Tredici</name>
    </author>
    <author>
      <name>Malvina Nissim</name>
    </author>
    <author>
      <name>Andrea Zaninello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Third Italian Conference on Computational
  Linguistics (CLIC 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04033v1</id>
    <updated>2016-11-12T18:41:58Z</updated>
    <published>2016-11-12T18:41:58Z</published>
    <title>1.5 billion words Arabic Corpus</title>
    <summary>  This study is an attempt to build a contemporary linguistic corpus for Arabic
language. The corpus produced, is a text corpus includes more than five million
newspaper articles. It contains over a billion and a half words in total, out
of which, there is about three million unique words. The data were collected
from newspaper articles in ten major news sources from eight Arabic countries,
over a period of fourteen years. The corpus was encoded with two types of
encoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two
mark-up languages, namely: SGML, and XML.
</summary>
    <author>
      <name>Ibrahim Abu El-khair</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04125v1</id>
    <updated>2016-11-13T12:32:20Z</updated>
    <published>2016-11-13T12:32:20Z</published>
    <title>Joint Representation Learning of Text and Knowledge for Knowledge Graph
  Completion</title>
    <summary>  Joint representation learning of text and knowledge within a unified semantic
space enables us to perform knowledge graph completion more accurately. In this
work, we propose a novel framework to embed words, entities and relations into
the same continuous vector space. In this model, both entity and relation
embeddings are learned by taking knowledge graph and plain text into
consideration. In experiments, we evaluate the joint learning model on three
tasks including entity prediction, relation prediction and relation
classification from text. The experiment results show that our model can
significantly and consistently improve the performance on the three tasks as
compared with other baselines.
</summary>
    <author>
      <name>Xu Han</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04230v1</id>
    <updated>2016-11-14T02:44:14Z</updated>
    <published>2016-11-14T02:44:14Z</published>
    <title>SummaRuNNer: A Recurrent Neural Network based Sequence Model for
  Extractive Summarization of Documents</title>
    <summary>  We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model
for extractive summarization of documents and show that it achieves performance
better than or comparable to state-of-the-art. Our model has the additional
advantage of being very interpretable, since it allows visualization of its
predictions broken up by abstract features such as information content,
salience and novelty. Another novel contribution of our work is abstractive
training of our extractive model that can train on human generated reference
summaries alone, eliminating the need for sentence-level extractive labels.
</summary>
    <author>
      <name>Ramesh Nallapati</name>
    </author>
    <author>
      <name>Feifei Zhai</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at AAAI 2017, The Thirty-First AAAI Conference on
  Artificial Intelligence (AAAI-2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04358v2</id>
    <updated>2016-11-15T14:41:23Z</updated>
    <published>2016-11-14T12:24:27Z</published>
    <title>Character-level Convolutional Network for Text Classification Applied to
  Chinese Corpus</title>
    <summary>  This article provides an interesting exploration of character-level
convolutional neural network solving Chinese corpus text classification
problem. We constructed a large-scale Chinese language dataset, and the result
shows that character-level convolutional neural network works better on Chinese
corpus than its corresponding pinyin format dataset. This is the first time
that character-level convolutional neural network applied to text
classification problem.
</summary>
    <author>
      <name>Weijie Huang</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc Thesis, 44 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04361v1</id>
    <updated>2016-11-14T12:36:07Z</updated>
    <published>2016-11-14T12:36:07Z</published>
    <title>Attending to Characters in Neural Sequence Labeling Models</title>
    <summary>  Sequence labeling architectures use word embeddings for capturing similarity,
but suffer when handling previously unseen or rare words. We investigate
character-level extensions to such models and propose a novel architecture for
combining alternative word representations. By using an attention mechanism,
the model is able to dynamically decide how much information to use from a
word- or character-level component. We evaluated different architectures on a
range of sequence labeling datasets, and character-level extensions were found
to improve performance on every benchmark. In addition, the proposed
attention-based architecture delivered the best results even with a smaller
number of trainable parameters.
</summary>
    <author>
      <name>Marek Rei</name>
    </author>
    <author>
      <name>Gamal K. O. Crichton</name>
    </author>
    <author>
      <name>Sampo Pyysalo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of COLING 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04798v1</id>
    <updated>2016-11-15T11:47:42Z</updated>
    <published>2016-11-15T11:47:42Z</published>
    <title>Toward Multilingual Neural Machine Translation with Universal Encoder
  and Decoder</title>
    <summary>  In this paper, we present our first attempts in building a multilingual
Neural Machine Translation framework under a unified approach. We are then able
to employ attention-based NMT for many-to-many multilingual translation tasks.
Our approach does not require any special treatment on the network architecture
and it allows us to learn minimal number of free parameters in a standard way
of training. Our approach has shown its effectiveness in an under-resourced
translation scenario with considerable improvements up to 2.6 BLEU points. In
addition, the approach has achieved interesting and promising results when
applied in the translation task that there is no direct parallel corpus between
source and target languages.
</summary>
    <author>
      <name>Thanh-Le Ha</name>
    </author>
    <author>
      <name>Jan Niehues</name>
    </author>
    <author>
      <name>Alexander Waibel</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04841v2</id>
    <updated>2017-01-15T03:59:14Z</updated>
    <published>2016-11-14T10:53:40Z</published>
    <title>Quantitative Entropy Study of Language Complexity</title>
    <summary>  We study the entropy of Chinese and English texts, based on characters in
case of Chinese texts and based on words for both languages. Significant
differences are found between the languages and between different personal
styles of debating partners. The entropy analysis points in the direction of
lower entropy, that is of higher complexity. Such a text analysis would be
applied for individuals of different styles, a single individual at different
age, as well as different groups of the population.
</summary>
    <author>
      <name>R. R. Xie</name>
    </author>
    <author>
      <name>W. B. Deng</name>
    </author>
    <author>
      <name>D. J. Wang</name>
    </author>
    <author>
      <name>L. P. Csernai</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04841v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04841v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04953v2</id>
    <updated>2016-11-25T16:38:30Z</updated>
    <published>2016-11-15T17:38:10Z</published>
    <title>End-to-End Neural Sentence Ordering Using Pointer Network</title>
    <summary>  Sentence ordering is one of important tasks in NLP. Previous works mainly
focused on improving its performance by using pair-wise strategy. However, it
is nontrivial for pair-wise models to incorporate the contextual sentence
information. In addition, error prorogation could be introduced by using the
pipeline strategy in pair-wise models. In this paper, we propose an end-to-end
neural approach to address the sentence ordering problem, which uses the
pointer network (Ptr-Net) to alleviate the error propagation problem and
utilize the whole contextual information. Experimental results show the
effectiveness of the proposed model. Source codes and dataset of this paper are
available.
</summary>
    <author>
      <name>Jingjing Gong</name>
    </author>
    <author>
      <name>Xinchi Chen</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04953v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04953v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.05104v2</id>
    <updated>2016-12-17T06:47:05Z</updated>
    <published>2016-11-16T00:53:01Z</published>
    <title>A Way out of the Odyssey: Analyzing and Combining Recent Insights for
  LSTMs</title>
    <summary>  LSTMs have become a basic building block for many deep NLP models. In recent
years, many improvements and variations have been proposed for deep sequence
models in general, and LSTMs in particular. We propose and analyze a series of
augmentations and modifications to LSTM networks resulting in improved
performance for text classification datasets. We observe compounding
improvements on traditional LSTMs using Monte Carlo test-time model averaging,
average pooling, and residual connections, along with four other suggested
modifications. Our analysis provides a simple, reliable, and high quality
baseline model.
</summary>
    <author>
      <name>Shayne Longpre</name>
    </author>
    <author>
      <name>Sabeek Pradhan</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <link href="http://arxiv.org/abs/1611.05104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.05527v1</id>
    <updated>2016-11-17T01:43:01Z</updated>
    <published>2016-11-17T01:43:01Z</published>
    <title>Automatic Node Selection for Deep Neural Networks using Group Lasso
  Regularization</title>
    <summary>  We examine the effect of the Group Lasso (gLasso) regularizer in selecting
the salient nodes of Deep Neural Network (DNN) hidden layers by applying a
DNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of
gLasso regularization, one for outgoing weight vectors and another for incoming
weight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096
nodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment
results demonstrate that our DNN training, in which the gLasso regularizer was
embedded, successfully selected the hidden layer nodes that are necessary and
sufficient for achieving high classification power.
</summary>
    <author>
      <name>Tsubasa Ochiai</name>
    </author>
    <author>
      <name>Shigeki Matsuda</name>
    </author>
    <author>
      <name>Hideyuki Watanabe</name>
    </author>
    <author>
      <name>Shigeru Katagiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.05527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07954v2</id>
    <updated>2017-05-31T01:32:26Z</updated>
    <published>2016-11-23T19:51:34Z</published>
    <title>Emergent Predication Structure in Hidden State Vectors of Neural Readers</title>
    <summary>  A significant number of neural architectures for reading comprehension have
recently been developed and evaluated on large cloze-style datasets. We present
experiments supporting the emergence of "predication structure" in the hidden
state vectors of these readers. More specifically, we provide evidence that the
hidden state vectors represent atomic formulas $\Phi[c]$ where $\Phi$ is a
semantic property (predicate) and $c$ is a constant symbol entity identifier.
</summary>
    <author>
      <name>Hai Wang</name>
    </author>
    <author>
      <name>Takeshi Onishi</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <author>
      <name>David McAllester</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Repl4NLP: 2nd Workshop on Representation Learning for
  NLP</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07954v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07954v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08765v1</id>
    <updated>2016-11-26T23:39:41Z</updated>
    <published>2016-11-26T23:39:41Z</published>
    <title>Fill it up: Exploiting partial dependency annotations in a minimum
  spanning tree parser</title>
    <summary>  Unsupervised models of dependency parsing typically require large amounts of
clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect
supervision (e.g. language universals and rules) can help, but we show that
obtaining small amounts of direct supervision - here, partial dependency
annotations - provides a strong balance between zero and full supervision. We
adapt the unsupervised ConvexMST dependency parser to learn from partial
dependencies expressed in the Graph Fragment Language. With less than 24 hours
of total annotation, we obtain 7% and 17% absolute improvement in unlabeled
dependency scores for English and Spanish, respectively, compared to the same
parser using only universal grammar constraints.
</summary>
    <author>
      <name>Liang Sun</name>
    </author>
    <author>
      <name>Jason Mielens</name>
    </author>
    <author>
      <name>Jason Baldridge</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08813v1</id>
    <updated>2016-11-27T09:53:36Z</updated>
    <published>2016-11-27T09:53:36Z</published>
    <title>Semi Supervised Preposition-Sense Disambiguation using Multilingual Data</title>
    <summary>  Prepositions are very common and very ambiguous, and understanding their
sense is critical for understanding the meaning of the sentence. Supervised
corpora for the preposition-sense disambiguation task are small, suggesting a
semi-supervised approach to the task. We show that signals from unannotated
multilingual data can be used to improve supervised preposition-sense
disambiguation. Our approach pre-trains an LSTM encoder for predicting the
translation of a preposition, and then incorporates the pre-trained encoder as
a component in a supervised classification system, and fine-tunes it for the
task. The multilingual signals consistently improve results on two
preposition-sense datasets.
</summary>
    <author>
      <name>Hila Gonen</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; COLING 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09100v1</id>
    <updated>2016-11-28T12:57:07Z</updated>
    <published>2016-11-28T12:57:07Z</published>
    <title>Learning to Compose Words into Sentences with Reinforcement Learning</title>
    <summary>  We use reinforcement learning to learn tree-structured neural networks for
computing representations of natural language sentences. In contrast with prior
work on tree-structured models in which the trees are either provided as input
or predicted using supervision from explicit treebank annotations, the tree
structures in this work are optimized to improve performance on a downstream
task. Experiments demonstrate the benefit of learning task-specific composition
orders, outperforming both sequential encoders and recursive encoders based on
treebank annotations. We analyze the induced trees and show that while they
discover some linguistically intuitive structures (e.g., noun phrases, simple
verb phrases), they are different than conventional English syntactic
structures.
</summary>
    <author>
      <name>Dani Yogatama</name>
    </author>
    <author>
      <name>Phil Blunsom</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <author>
      <name>Wang Ling</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09405v1</id>
    <updated>2016-11-28T22:03:22Z</updated>
    <published>2016-11-28T22:03:22Z</published>
    <title>An End-to-End Architecture for Keyword Spotting and Voice Activity
  Detection</title>
    <summary>  We propose a single neural network architecture for two tasks: on-line
keyword spotting and voice activity detection. We develop novel inference
algorithms for an end-to-end Recurrent Neural Network trained with the
Connectionist Temporal Classification loss function which allow our model to
achieve high accuracy on both keyword spotting and voice activity detection
without retraining. In contrast to prior voice activity detection models, our
architecture does not require aligned training data and uses the same
parameters as the keyword spotting model. This allows us to deploy a high
quality voice activity detector with no additional memory or maintenance
requirements.
</summary>
    <author>
      <name>Chris Lengerich</name>
    </author>
    <author>
      <name>Awni Hannun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2016 End-to-End Learning for Speech and Audio Processing
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09441v1</id>
    <updated>2016-11-29T00:22:13Z</updated>
    <published>2016-11-29T00:22:13Z</published>
    <title>Sentiment Analysis for Twitter : Going Beyond Tweet Text</title>
    <summary>  Analysing sentiment of tweets is important as it helps to determine the
users' opinion. Knowing people's opinion is crucial for several purposes
starting from gathering knowledge about customer base, e-governance,
campaigning and many more. In this report, we aim to develop a system to detect
the sentiment from tweets. We employ several linguistic features along with
some other external sources of information to detect the sentiment of a tweet.
We show that augmenting the 140 character-long tweet with information harvested
from external urls shared in the tweet as well as Social Media features
enhances the sentiment prediction accuracy significantly.
</summary>
    <author>
      <name>Lahari Poddar</name>
    </author>
    <author>
      <name>Kishaloy Halder</name>
    </author>
    <author>
      <name>Xianyan Jia</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09703v1</id>
    <updated>2016-11-29T16:20:24Z</updated>
    <published>2016-11-29T16:20:24Z</published>
    <title>Semantic Parsing of Mathematics by Context-based Learning from Aligned
  Corpora and Theorem Proving</title>
    <summary>  We study methods for automated parsing of informal mathematical expressions
into formal ones, a main prerequisite for deep computer understanding of
informal mathematical texts. We propose a context-based parsing approach that
combines efficient statistical learning of deep parse trees with their semantic
pruning by type checking and large-theory automated theorem proving. We show
that the methods very significantly improve on previous results in parsing
theorems from the Flyspeck corpus.
</summary>
    <author>
      <name>Cezary Kaliszyk</name>
    </author>
    <author>
      <name>Josef Urban</name>
    </author>
    <author>
      <name>Jiří Vyskočil</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09799v1</id>
    <updated>2016-11-29T19:23:41Z</updated>
    <published>2016-11-29T19:23:41Z</published>
    <title>Geometry of Compositionality</title>
    <summary>  This paper proposes a simple test for compositionality (i.e., literal usage)
of a word or phrase in a context-specific way. The test is computationally
simple, relying on no external resources and only uses a set of trained word
vectors. Experiments show that the proposed method is competitive with state of
the art and displays high accuracy in context-specific compositionality
detection of a variety of natural language phenomena (idiomaticity, sarcasm,
metaphor) for different datasets in multiple languages. The key insight is to
connect compositionality to a curious geometric property of word embeddings,
which is of independent interest.
</summary>
    <author>
      <name>Hongyu Gong</name>
    </author>
    <author>
      <name>Suma Bhat</name>
    </author>
    <author>
      <name>Pramod Viswanath</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00138v2</id>
    <updated>2017-02-13T23:40:09Z</updated>
    <published>2016-12-31T16:41:43Z</published>
    <title>Cutting-off Redundant Repeating Generations for Neural Abstractive
  Summarization</title>
    <summary>  This paper tackles the reduction of redundant repeating generation that is
often observed in RNN-based encoder-decoder models. Our basic idea is to
jointly estimate the upper-bound frequency of each target vocabulary in the
encoder and control the output words based on the estimation in the decoder.
Our method shows significant improvement over a strong RNN-based
encoder-decoder baseline and achieved its best results on an abstractive
summarization benchmark.
</summary>
    <author>
      <name>Jun Suzuki</name>
    </author>
    <author>
      <name>Masaaki Nagata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, a draft version of EACL-2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.00138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00728v1</id>
    <updated>2017-01-03T15:49:35Z</updated>
    <published>2017-01-03T15:49:35Z</published>
    <title>On (Commercial) Benefits of Automatic Text Summarization Systems in the
  News Domain: A Case of Media Monitoring and Media Response Analysis</title>
    <summary>  In this work, we present the results of a systematic study to investigate the
(commercial) benefits of automatic text summarization systems in a real world
scenario. More specifically, we define a use case in the context of media
monitoring and media response analysis and claim that even using a simple
query-based extractive approach can dramatically save the processing time of
the employees without significantly reducing the quality of their work.
</summary>
    <author>
      <name>Pashutan Modaresi</name>
    </author>
    <author>
      <name>Philipp Gross</name>
    </author>
    <author>
      <name>Siavash Sefidrodi</name>
    </author>
    <author>
      <name>Mirja Eckhof</name>
    </author>
    <author>
      <name>Stefan Conrad</name>
    </author>
    <link href="http://arxiv.org/abs/1701.00728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01614v1</id>
    <updated>2017-01-06T12:28:15Z</updated>
    <published>2017-01-06T12:28:15Z</published>
    <title>Enumeration of Extractive Oracle Summaries</title>
    <summary>  To analyze the limitations and the future directions of the extractive
summarization paradigm, this paper proposes an Integer Linear Programming (ILP)
formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also
propose an algorithm that enumerates all of the oracle summaries for a set of
reference summaries to exploit F-measures that evaluate which system summaries
contain how many sentences that are extracted as an oracle summary. Our
experimental results obtained from Document Understanding Conference (DUC)
corpora demonstrated the following: (1) room still exists to improve the
performance of extractive summarization; (2) the F-measures derived from the
enumerated oracle summaries have significantly stronger correlations with human
judgment than those derived from single oracle summaries.
</summary>
    <author>
      <name>Tsutomu Hirao</name>
    </author>
    <author>
      <name>Masaaki Nishino</name>
    </author>
    <author>
      <name>Jun Suzuki</name>
    </author>
    <author>
      <name>Masaaki Nagata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01623v1</id>
    <updated>2017-01-06T12:54:48Z</updated>
    <published>2017-01-06T12:54:48Z</published>
    <title>Cross-Lingual Dependency Parsing with Late Decoding for Truly
  Low-Resource Languages</title>
    <summary>  In cross-lingual dependency annotation projection, information is often lost
during transfer because of early decoding. We present an end-to-end graph-based
neural network dependency parser that can be trained to reproduce matrices of
edge scores, which can be directly projected across word alignments. We show
that our approach to cross-lingual dependency parsing is not only simpler, but
also achieves an absolute improvement of 2.25% averaged across 10 languages
compared to the previous state of the art.
</summary>
    <author>
      <name>Michael Sejr Schlichtkrull</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published at EACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01811v1</id>
    <updated>2017-01-07T09:58:49Z</updated>
    <published>2017-01-07T09:58:49Z</published>
    <title>Structural Attention Neural Networks for improved sentiment analysis</title>
    <summary>  We introduce a tree-structured attention neural network for sentences and
small phrases and apply it to the problem of sentiment classification. Our
model expands the current recursive models by incorporating structural
information around a node of a syntactic tree using both bottom-up and top-down
information propagation. Also, the model utilizes structural attention to
identify the most salient representations during the construction of the
syntactic tree. To our knowledge, the proposed models achieve state of the art
performance on the Stanford Sentiment Treebank dataset.
</summary>
    <author>
      <name>Filippos Kokkinos</name>
    </author>
    <author>
      <name>Alexandros Potamianos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to EACL2017 for review</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02481v3</id>
    <updated>2017-05-08T03:19:20Z</updated>
    <published>2017-01-10T08:59:38Z</published>
    <title>Implicitly Incorporating Morphological Information into Word Embedding</title>
    <summary>  In this paper, we propose three novel models to enhance word embedding by
implicitly using morphological information. Experiments on word similarity and
syntactic analogy show that the implicit models are superior to traditional
explicit ones. Our models outperform all state-of-the-art baselines and
significantly improve the performance on both tasks. Moreover, our performance
on the smallest corpus is similar to the performance of CBOW on the corpus
which is five times the size of ours. Parameter analysis indicates that the
implicit models can supplement semantic information during the word embedding
training process.
</summary>
    <author>
      <name>Yang Xu</name>
    </author>
    <author>
      <name>Jiawei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02481v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02481v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02795v1</id>
    <updated>2017-01-10T21:45:56Z</updated>
    <published>2017-01-10T21:45:56Z</published>
    <title>Bidirectional American Sign Language to English Translation</title>
    <summary>  We outline a bidirectional translation system that converts sentences from
American Sign Language (ASL) to English, and vice versa. To perform machine
translation between ASL and English, we utilize a generative approach.
Specifically, we employ an adjustment to the IBM word-alignment model 1 (IBM
WAM1), where we define language models for English and ASL, as well as a
translation model, and attempt to generate a translation that maximizes the
posterior distribution defined by these models. Then, using these models, we
are able to quantify the concepts of fluency and faithfulness of a translation
between languages.
</summary>
    <author>
      <name>Hardie Cate</name>
    </author>
    <author>
      <name>Zeshan Hussain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02810v2</id>
    <updated>2017-03-06T15:54:27Z</updated>
    <published>2017-01-10T23:32:43Z</published>
    <title>OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
    <summary>  We describe an open-source toolkit for neural machine translation (NMT). The
toolkit prioritizes efficiency, modularity, and extensibility with the goal of
supporting NMT research into model architectures, feature representations, and
source modalities, while maintaining competitive performance and reasonable
training requirements. The toolkit consists of modeling and translation
support, as well as detailed pedagogical documentation about the underlying
techniques.
</summary>
    <author>
      <name>Guillaume Klein</name>
    </author>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Yuntian Deng</name>
    </author>
    <author>
      <name>Jean Senellart</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Report for http://opennmt.net</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02810v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02810v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03129v1</id>
    <updated>2017-01-11T19:22:56Z</updated>
    <published>2017-01-11T19:22:56Z</published>
    <title>De-identification In practice</title>
    <summary>  We report our effort to identify the sensitive information, subset of data
items listed by HIPAA (Health Insurance Portability and Accountability), from
medical text using the recent advances in natural language processing and
machine learning techniques. We represent the words with high dimensional
continuous vectors learned by a variant of Word2Vec called Continous Bag Of
Words (CBOW). We feed the word vectors into a simple neural network with a Long
Short-Term Memory (LSTM) architecture. Without any attempts to extract manually
crafted features and considering that our medical dataset is too small to be
fed into neural network, we obtained promising results. The results thrilled us
to think about the larger scale of the project with precise parameter tuning
and other possible improvements.
</summary>
    <author>
      <name>Besat Kassaie</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03163v1</id>
    <updated>2017-01-11T20:56:29Z</updated>
    <published>2017-01-11T20:56:29Z</published>
    <title>Parsing Universal Dependencies without training</title>
    <summary>  We propose UDP, the first training-free parser for Universal Dependencies
(UD). Our algorithm is based on PageRank and a small set of head attachment
rules. It features two-step decoding to guarantee that function words are
attached as leaf nodes. The parser requires no training, and it is competitive
with a delexicalized transfer system. UDP offers a linguistically sound
unsupervised alternative to cross-lingual parsing for UD, which can be used as
a baseline for such systems. The parser has very few parameters and is
distinctly robust to domain change across languages.
</summary>
    <author>
      <name>Héctor Martínez Alonso</name>
    </author>
    <author>
      <name>Željko Agić</name>
    </author>
    <author>
      <name>Barbara Plank</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EACL 2017, 8+2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03214v2</id>
    <updated>2017-02-03T21:46:12Z</updated>
    <published>2017-01-12T02:37:09Z</published>
    <title>An Empirical Comparison of Simple Domain Adaptation Methods for Neural
  Machine Translation</title>
    <summary>  In this paper, we propose a novel domain adaptation method named "mixed fine
tuning" for neural machine translation (NMT). We combine two existing
approaches namely fine tuning and multi domain NMT. We first train an NMT model
on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus
which is a mix of the in-domain and out-of-domain corpora. All corpora are
augmented with artificial tags to indicate specific domains. We empirically
compare our proposed method against fine tuning and multi domain methods and
discuss its benefits and shortcomings.
</summary>
    <author>
      <name>Chenhui Chu</name>
    </author>
    <author>
      <name>Raj Dabre</name>
    </author>
    <author>
      <name>Sadao Kurohashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03214v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03214v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03329v2</id>
    <updated>2017-01-26T23:39:25Z</updated>
    <published>2017-01-12T13:03:49Z</published>
    <title>A Data-Oriented Model of Literary Language</title>
    <summary>  We consider the task of predicting how literary a text is, with a gold
standard from human ratings. Aside from a standard bigram baseline, we apply
rich syntactic tree fragments, mined from the training set, and a series of
hand-picked features. Our model is the first to distinguish degrees of highly
and less literary novels using a variety of lexical and syntactic features, and
explains 76.0 % of the variation in literary ratings.
</summary>
    <author>
      <name>Andreas van Cranenburgh</name>
    </author>
    <author>
      <name>Rens Bod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in EACL 2017, 11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of EACL 2017, pp. 1228-1238</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.03329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03434v1</id>
    <updated>2017-01-12T17:51:12Z</updated>
    <published>2017-01-12T17:51:12Z</published>
    <title>SMARTies: Sentiment Models for Arabic Target Entities</title>
    <summary>  We consider entity-level sentiment analysis in Arabic, a morphologically rich
language with increasing resources. We present a system that is applied to
complex posts written in response to Arabic newspaper articles. Our goal is to
identify important entity "targets" within the post along with the polarity
expressed about each target. We achieve significant improvements over multiple
baselines, demonstrating that the use of specific morphological representations
improves the performance of identifying both important targets and their
sentiment, and that the use of distributional semantic clusters further boosts
performances for these representations, especially when richer linguistic
resources are not available.
</summary>
    <author>
      <name>Noura Farra</name>
    </author>
    <author>
      <name>Kathleen McKeown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Proceedings of the European Chapter of the
  Association for Computational Linguistics (EACL 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03492v1</id>
    <updated>2017-01-12T20:14:52Z</updated>
    <published>2017-01-12T20:14:52Z</published>
    <title>Scalable, Trie-based Approximate Entity Extraction for Real-Time
  Financial Transaction Screening</title>
    <summary>  Financial institutions have to screen their transactions to ensure that they
are not affiliated with terrorism entities. Developing appropriate solutions to
detect such affiliations precisely while avoiding any kind of interruption to
large amount of legitimate transactions is essential. In this paper, we present
building blocks of a scalable solution that may help financial institutions to
build their own software to extract terrorism entities out of both structured
and unstructured financial messages in real time and with approximate
similarity matching approach.
</summary>
    <author>
      <name>Emrah Budur</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03682v1</id>
    <updated>2017-01-13T14:20:06Z</updated>
    <published>2017-01-13T14:20:06Z</published>
    <title>LIDE: Language Identification from Text Documents</title>
    <summary>  The increase in the use of microblogging came along with the rapid growth on
short linguistic data. On the other hand deep learning is considered to be the
new frontier to extract meaningful information out of large amount of raw data
in an automated manner. In this study, we engaged these two emerging fields to
come up with a robust language identifier on demand, namely Language
Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which
is comparable to the maximum reported accuracy of 95.54% achieved so far.
</summary>
    <author>
      <name>Priyank Mathur</name>
    </author>
    <author>
      <name>Arkajyoti Misra</name>
    </author>
    <author>
      <name>Emrah Budur</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04056v1</id>
    <updated>2017-01-15T15:10:29Z</updated>
    <published>2017-01-15T15:10:29Z</published>
    <title>Dialog Context Language Modeling with Recurrent Neural Networks</title>
    <summary>  In this work, we propose contextual language models that incorporate dialog
level discourse information into language modeling. Previous works on
contextual language model treat preceding utterances as a sequence of inputs,
without considering dialog interactions. We design recurrent neural network
(RNN) based contextual language models that specially track the interactions
between speakers in a dialog. Experiment results on Switchboard Dialog Act
Corpus show that the proposed model outperforms conventional single turn based
RNN language model by 3.3% on perplexity. The proposed models also demonstrate
advantageous performance over other competitive contextual language models.
</summary>
    <author>
      <name>Bing Liu</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05581v1</id>
    <updated>2017-01-19T19:58:26Z</updated>
    <published>2017-01-19T19:58:26Z</published>
    <title>Leveraging Cognitive Features for Sentiment Analysis</title>
    <summary>  Sentiments expressed in user-generated short text and sentences are nuanced
by subtleties at lexical, syntactic, semantic and pragmatic levels. To address
this, we propose to augment traditional features used for sentiment analysis
and sarcasm detection, with cognitive features derived from the eye-movement
patterns of readers. Statistical classification using our enhanced feature set
improves the performance (F-score) of polarity detection by a maximum of 3.7%
and 9.3% on two datasets, over the systems that use only traditional features.
We perform feature significance analysis, and experiment on a held-out dataset,
showing that cognitive features indeed empower sentiment analyzers to handle
complex constructs.
</summary>
    <author>
      <name>Abhijit Mishra</name>
    </author>
    <author>
      <name>Diptesh Kanojia</name>
    </author>
    <author>
      <name>Seema Nagar</name>
    </author>
    <author>
      <name>Kuntal Dey</name>
    </author>
    <author>
      <name>Pushpak Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The SIGNLL Conference on Computational Natural Language Learning
  (CoNLL 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07880v1</id>
    <updated>2017-01-26T21:18:32Z</updated>
    <published>2017-01-26T21:18:32Z</published>
    <title>emLam -- a Hungarian Language Modeling baseline</title>
    <summary>  This paper aims to make up for the lack of documented baselines for Hungarian
language modeling. Various approaches are evaluated on three publicly available
Hungarian corpora. Perplexity values comparable to models of similar-sized
English corpora are reported. A new, freely downloadable Hungar- ian benchmark
corpus is introduced.
</summary>
    <author>
      <name>Dávid Márk Nemeskey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Additional resources: - the emLam repository:
  https://github.com/DavidNemeskey/emLam - the emLam corpus:
  http://hlt.bme.hu/en/resources/emLam</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 13th Conference on Hungarian Computational
  Linguistics (MSZNY), pp. 91-102. Szeged, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.07880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08071v2</id>
    <updated>2018-07-05T16:12:22Z</updated>
    <published>2017-01-27T14:50:36Z</published>
    <title>Emotion Recognition From Speech With Recurrent Neural Networks</title>
    <summary>  In this paper the task of emotion recognition from speech is considered.
Proposed approach uses deep recurrent neural network trained on a sequence of
acoustic features calculated over small speech intervals. At the same time
special probabilistic-nature CTC loss function allows to consider long
utterances containing both emotional and neutral parts. The effectiveness of
such an approach is shown in two ways. Firstly, the comparison with recent
advances in this field is carried out. Secondly, human performance on the same
task is measured. Both criteria show the high quality of the proposed method.
</summary>
    <author>
      <name>Vladimir Chernykh</name>
    </author>
    <author>
      <name>Pavel Prikhodko</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08071v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08071v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08198v1</id>
    <updated>2017-01-27T21:28:57Z</updated>
    <published>2017-01-27T21:28:57Z</published>
    <title>Adversarial Evaluation of Dialogue Models</title>
    <summary>  The recent application of RNN encoder-decoder models has resulted in
substantial progress in fully data-driven dialogue systems, but evaluation
remains a challenge. An adversarial loss could be a way to directly evaluate
the extent to which generated dialogue responses sound like they came from a
human. This could reduce the need for human evaluation, while more directly
evaluating on a generative task. In this work, we investigate this idea by
training an RNN to discriminate a dialogue model's samples from human-generated
samples. Although we find some evidence this setup could be viable, we also
note that many issues remain in its practical application. We discuss both
aspects and conclude that future work is warranted.
</summary>
    <author>
      <name>Anjuli Kannan</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08533v1</id>
    <updated>2017-01-30T10:28:49Z</updated>
    <published>2017-01-30T10:28:49Z</published>
    <title>Graph-Based Semi-Supervised Conditional Random Fields For Spoken
  Language Understanding Using Unaligned Data</title>
    <summary>  We experiment graph-based Semi-Supervised Learning (SSL) of Conditional
Random Fields (CRF) for the application of Spoken Language Understanding (SLU)
on unaligned data. The aligned labels for examples are obtained using IBM
Model. We adapt a baseline semi-supervised CRF by defining new feature set and
altering the label propagation algorithm. Our results demonstrate that our
proposed approach significantly improves the performance of the supervised
model by utilizing the knowledge gained from the graph.
</summary>
    <author>
      <name>Mohammad Aliannejadi</name>
    </author>
    <author>
      <name>Masoud Kiaeeha</name>
    </author>
    <author>
      <name>Shahram Khadivi</name>
    </author>
    <author>
      <name>Saeed Shiry Ghidary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop of The Australasian Language Technology Association</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00052v1</id>
    <updated>2017-03-31T20:39:38Z</updated>
    <published>2017-03-31T20:39:38Z</published>
    <title>One-Shot Neural Cross-Lingual Transfer for Paradigm Completion</title>
    <summary>  We present a novel cross-lingual transfer method for paradigm completion, the
task of mapping a lemma to its inflected forms, using a neural encoder-decoder
model, the state of the art for the monolingual task. We use labeled data from
a high-resource language to increase performance on a low-resource language. In
experiments on 21 language pairs from four different language families, we
obtain up to 58% higher accuracy than without transfer and show that even
zero-shot and one-shot learning are possible. We further find that the degree
of language relatedness strongly influences the ability to transfer
morphological knowledge.
</summary>
    <author>
      <name>Katharina Kann</name>
    </author>
    <author>
      <name>Ryan Cotterell</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00380v1</id>
    <updated>2017-04-02T22:36:56Z</updated>
    <published>2017-04-02T22:36:56Z</published>
    <title>Word-Alignment-Based Segment-Level Machine Translation Evaluation using
  Word Embeddings</title>
    <summary>  One of the most important problems in machine translation (MT) evaluation is
to evaluate the similarity between translation hypotheses with different
surface forms from the reference, especially at the segment level. We propose
to use word embeddings to perform word alignment for segment-level MT
evaluation. We performed experiments with three types of alignment methods
using word embeddings. We evaluated our proposed methods with various
translation datasets. Experimental results show that our proposed methods
outperform previous word embeddings-based methods.
</summary>
    <author>
      <name>Junki Matsuo</name>
    </author>
    <author>
      <name>Mamoru Komachi</name>
    </author>
    <author>
      <name>Katsuhito Sudoh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00405v2</id>
    <updated>2017-04-20T01:55:26Z</updated>
    <published>2017-04-03T02:10:19Z</published>
    <title>Syntax Aware LSTM Model for Chinese Semantic Role Labeling</title>
    <summary>  As for semantic role labeling (SRL) task, when it comes to utilizing parsing
information, both traditional methods and recent recurrent neural network (RNN)
based methods use the feature engineering way. In this paper, we propose Syntax
Aware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies
according to dependency parsing information in order to model parsing
information directly in an architecture engineering way instead of feature
engineering way. We experimentally demonstrate that SA-LSTM gains more
improvement from the model architecture. Furthermore, SA-LSTM outperforms the
state-of-the-art on CPB 1.0 significantly according to Student t-test
($p&lt;0.05$).
</summary>
    <author>
      <name>Feng Qian</name>
    </author>
    <author>
      <name>Lei Sha</name>
    </author>
    <author>
      <name>Baobao Chang</name>
    </author>
    <author>
      <name>Lu-chen Liu</name>
    </author>
    <author>
      <name>Ming Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00514v2</id>
    <updated>2017-04-26T16:48:49Z</updated>
    <published>2017-04-03T10:25:22Z</published>
    <title>Multi-Task Learning of Keyphrase Boundary Classification</title>
    <summary>  Keyphrase boundary classification (KBC) is the task of detecting keyphrases
in scientific articles and labelling them with respect to predefined types.
Although important in practice, this task is so far underexplored, partly due
to the lack of labelled data. To overcome this, we explore several auxiliary
tasks, including semantic super-sense tagging and identification of multi-word
expressions, and cast the task as a multi-task learning problem with deep
recurrent neural networks. Our multi-task models perform significantly better
than previous state of the art approaches on two scientific KBC datasets,
particularly for long keyphrases.
</summary>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00514v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00514v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00924v2</id>
    <updated>2018-09-30T03:11:29Z</updated>
    <published>2017-04-04T09:08:46Z</published>
    <title>Japanese Sentiment Classification using a Tree-Structured Long
  Short-Term Memory with Attention</title>
    <summary>  Previous approaches to training syntax-based sentiment classification models
required phrase-level annotated corpora, which are not readily available in
many languages other than English. Thus, we propose the use of tree-structured
Long Short-Term Memory with an attention mechanism that pays attention to each
subtree of the parse tree. Experimental results indicate that our model
achieves the state-of-the-art performance in a Japanese sentiment
classification task.
</summary>
    <author>
      <name>Ryosuke Miyazaki</name>
    </author>
    <author>
      <name>Mamoru Komachi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; PACLIC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00939v1</id>
    <updated>2017-04-04T10:01:47Z</updated>
    <published>2017-04-04T10:01:47Z</published>
    <title>Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring
  Sentiment towards Brands from Financial News Headlines</title>
    <summary>  In this paper, we describe a methodology to infer Bullish or Bearish
sentiment towards companies/brands. More specifically, our approach leverages
affective lexica and word embeddings in combination with convolutional neural
networks to infer the sentiment of financial news headlines towards a target
company. Such architecture was used and evaluated in the context of the SemEval
2017 challenge (task 5, subtask 2), in which it obtained the best performance.
</summary>
    <author>
      <name>Youness Mansar</name>
    </author>
    <author>
      <name>Lorenzo Gatti</name>
    </author>
    <author>
      <name>Sira Ferradans</name>
    </author>
    <author>
      <name>Marco Guerini</name>
    </author>
    <author>
      <name>Jacopo Staiano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure; accepted for publication at the International
  Workshop on Semantic Evaluation (SemEval-2017) to be held in conjunction with
  ACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
