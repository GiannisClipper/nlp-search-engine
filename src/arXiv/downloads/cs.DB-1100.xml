<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.DB%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.DB&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/YLmBib4NLVJqouNmSeLO7GG+F98</id>
  <updated>2025-03-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">9742</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0709.4655v1</id>
    <updated>2007-09-28T17:08:39Z</updated>
    <published>2007-09-28T17:08:39Z</published>
    <title>Mining for trees in a graph is NP-complete</title>
    <summary>  Mining for trees in a graph is shown to be NP-complete.
</summary>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/0709.4655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.4655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2764v2</id>
    <updated>2014-08-11T19:55:33Z</updated>
    <published>2010-09-14T20:15:14Z</published>
    <title>A Blink Tree latch method and protocol to support synchronous node
  deletion</title>
    <summary>  A Blink Tree latch method and protocol supports synchronous node deletion in
a high concurrency environment. Full source code is available.
</summary>
    <author>
      <name>Karl Malbrain</name>
    </author>
    <link href="http://arxiv.org/abs/1009.2764v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2764v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1575v1</id>
    <updated>2013-01-06T04:03:29Z</updated>
    <published>2013-01-06T04:03:29Z</published>
    <title>BigDB: Automatic Machine Learning Optimizer</title>
    <summary>  In this short vision paper, we introduce a machine learning optimizer for
data management and describe its architecture and main functionality.
</summary>
    <author>
      <name>Anna Pyayt</name>
    </author>
    <author>
      <name>Michael Gubanov</name>
    </author>
    <link href="http://arxiv.org/abs/1301.1575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5313v1</id>
    <updated>2013-03-21T15:56:54Z</updated>
    <published>2013-03-21T15:56:54Z</published>
    <title>Incremental Maintenance for Leapfrog Triejoin</title>
    <summary>  We present an incremental maintenance algorithm for leapfrog triejoin. The
algorithm maintains rules in time proportional (modulo log factors) to the edit
distance between leapfrog triejoin traces.
</summary>
    <author>
      <name>Todd L. Veldhuizen</name>
    </author>
    <link href="http://arxiv.org/abs/1303.5313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6778v1</id>
    <updated>2014-06-26T06:09:26Z</updated>
    <published>2014-06-26T06:09:26Z</published>
    <title>Performance Comparison of Two Streaming Data Clustering Algorithms</title>
    <summary>  The weighted fuzzy c-mean clustering algorithm and weighted fuzzy
c-mean-adaptive cluster number are extension of traditional fuzzy c-mean
Algorithm to stream data clustering algorithm.
</summary>
    <author>
      <name>Chandrakant Mahobiya</name>
    </author>
    <author>
      <name>M. Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4468v1</id>
    <updated>2014-08-19T20:14:34Z</updated>
    <published>2014-08-19T20:14:34Z</published>
    <title>Undecidability of Finite Model Reasoning in DLFD</title>
    <summary>  We resolve an open problem concerning finite logical implication for path
functional dependencies (PFDs).
</summary>
    <author>
      <name>David Toman</name>
    </author>
    <author>
      <name>Grant Weddell</name>
    </author>
    <link href="http://arxiv.org/abs/1408.4468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00295v1</id>
    <updated>2016-06-01T14:07:56Z</updated>
    <published>2016-06-01T14:07:56Z</published>
    <title>A Review of Star Schema Benchmark</title>
    <summary>  This paper examines the Star Schema Benchmark, an alternative to the flawed
TPC-H decision support system and presents reasons why this benchmark should be
adopted over the industry standard for decision support systems.
</summary>
    <author>
      <name>Jimi Sanchez</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01229v1</id>
    <updated>2016-05-04T11:27:45Z</updated>
    <published>2016-05-04T11:27:45Z</published>
    <title>The lifecycle of provenance metadata and its associated challenges and
  opportunities</title>
    <summary>  This chapter outlines some of the challenges and opportunities associated
with adopting provenance principles and standards in a variety of disciplines,
including data publication and reuse, and information sciences.
</summary>
    <author>
      <name>Paolo Missier</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.14810v1</id>
    <updated>2023-07-27T12:30:27Z</updated>
    <published>2023-07-27T12:30:27Z</published>
    <title>A Differential Datalog Interpreter</title>
    <summary>  Redacted by arXiv admins
</summary>
    <author>
      <name>Matthew Stephenson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: This submission has been removed due to violation
  of arXiv author policy</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.14810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.14810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.13914v1</id>
    <updated>2024-03-20T18:33:33Z</updated>
    <published>2024-03-20T18:33:33Z</published>
    <title>Database Dependencies and Formal Concept Analysis</title>
    <summary>  This is an account of the characterization of database dependencies with
Formal Concept Analysis.
</summary>
    <author>
      <name>Jaume Baixeries</name>
    </author>
    <link href="http://arxiv.org/abs/2403.13914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.13914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0; H.1.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402051v2</id>
    <updated>2004-05-11T20:56:14Z</updated>
    <published>2004-02-20T19:45:56Z</published>
    <title>Nested Intervals Tree Encoding with Continued Fractions</title>
    <summary>  We introduce a new variation of Tree Encoding with Nested Intervals, find
connections with Materialized Path, and suggest a method for moving parts of
the hierarchy.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402051v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402051v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501053v3</id>
    <updated>2005-02-04T01:12:52Z</updated>
    <published>2005-01-21T20:07:32Z</published>
    <title>Relational Algebra as non-Distributive Lattice</title>
    <summary>  We reduce the set of classic relational algebra operators to two binary
operations: natural join and generalized union. We further demonstrate that
this set of operators is relationally complete and honors lattice axioms.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0501053v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501053v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701165v1</id>
    <updated>2007-01-26T00:23:07Z</updated>
    <published>2007-01-26T00:23:07Z</published>
    <title>Petascale Computational Systems</title>
    <summary>  Computational science is changing to be data intensive. Super-Computers must
be balanced systems; not just CPU farms but also petascale IO and networking
arrays. Anyone building CyberInfrastructure should allocate resources to
support a balanced Tier-1 through Tier-3 design.
</summary>
    <author>
      <name>Gordon Bell</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex Szalay</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703103v3</id>
    <updated>2009-06-30T00:07:21Z</updated>
    <published>2007-03-22T03:31:27Z</published>
    <title>Concept of a Value in Multilevel Security Databases</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Jia Tao</name>
    </author>
    <author>
      <name>Shashi Gadia</name>
    </author>
    <author>
      <name>Tsz Shing Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703103v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703103v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4380v4</id>
    <updated>2020-07-30T19:49:01Z</updated>
    <published>2012-03-20T10:41:29Z</published>
    <title>Analyzing closed frequent itemsets with convex polytopes</title>
    <summary>  Frequent itemsets form a polytope and can be found and analyzed with Linear
Programming.
</summary>
    <author>
      <name>Natalia Vanetik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as another paper with different data model</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.4380v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4380v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01817v2</id>
    <updated>2015-01-12T10:34:27Z</updated>
    <published>2015-01-08T12:28:45Z</published>
    <title>The Hunt for a Red Spider: Conjunctive Query Determinacy Is Undecidable</title>
    <summary>  We solve a well known, long-standing open problem in relational databases
theory, showing that the conjunctive query determinacy problem (in its
"unrestricted" version) is undecidable.
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LICS.2015.35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LICS.2015.35" rel="related"/>
    <link href="http://arxiv.org/abs/1501.01817v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01817v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00707v1</id>
    <updated>2016-01-05T00:17:21Z</updated>
    <published>2016-01-05T00:17:21Z</published>
    <title>A Survey of RDF Data Management Systems</title>
    <summary>  RDF is increasingly being used to encode data for the semantic web and for
data exchange. There have been a large number of works that address RDF data
management. In this paper we provide an overview of these works.
</summary>
    <author>
      <name>M. Tamer Özsu</name>
    </author>
    <link href="http://arxiv.org/abs/1601.00707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05136v2</id>
    <updated>2017-06-26T17:05:39Z</updated>
    <published>2017-04-17T21:58:45Z</published>
    <title>The Causality/Repair Connection in Databases: Causality-Programs</title>
    <summary>  In this work, answer-set programs that specify repairs of databases are used
as a basis for solving computational and reasoning problems about causes for
query answers from databases.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proc. SUM'17 as short paper, 7-pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03317v1</id>
    <updated>2017-06-11T07:28:48Z</updated>
    <published>2017-06-11T07:28:48Z</published>
    <title>Fault Tolerant Consensus Agreement Algorithm</title>
    <summary>  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
</summary>
    <author>
      <name>Marius Rafailescu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00740v1</id>
    <updated>2016-06-02T16:07:15Z</updated>
    <published>2016-06-02T16:07:15Z</published>
    <title>The Meaning of Null in Databases and Programming Languages</title>
    <summary>  The meaning of null in relational databases is a major source of confusion
not only among database users but also among database textbook writers. The
purpose of this article is to examine what database nulls could mean and to
make some modest suggestions about how to reduce the confusion.
</summary>
    <author>
      <name>Kenneth Baclawski</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02622v1</id>
    <updated>2018-05-07T17:11:39Z</updated>
    <published>2018-05-07T17:11:39Z</published>
    <title>Provenance for Interactive Visualizations</title>
    <summary>  We highlight the connections between data provenance and interactive
visualizations. To do so, we first incrementally add interactions to a
visualization and show how these interactions are readily expressible in terms
of provenance. We then describe how an interactive visualization system that
natively supports provenance can be easily extended with novel interactions.
</summary>
    <author>
      <name>Fotis Psallidas</name>
    </author>
    <author>
      <name>Eugene Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.03161v1</id>
    <updated>2021-05-07T10:59:16Z</updated>
    <published>2021-05-07T10:59:16Z</published>
    <title>Open Data Portal Germany (OPAL) Projektergebnisse</title>
    <summary>  In the Open Data Portal Germany (OPAL) project, a pipeline of the following
data refinement steps has been developed: requirements analysis, data
acquisition, analysis, conversion, integration and selection. 800,000 datasets
in DCAT format have been produced.
</summary>
    <author>
      <name>Adrian Wilke</name>
    </author>
    <author>
      <name>Axel-Cyrille Ngonga Ngomo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in German</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.03161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04108v1</id>
    <updated>2017-12-12T03:02:24Z</updated>
    <published>2017-12-12T03:02:24Z</published>
    <title>Incremental View Maintenance for Property Graph Queries</title>
    <summary>  This paper discusses the challenges of incremental view maintenance for
property graph queries. We select a subset of property graph queries and
present an approach that uses nested relational algebra to allow incremental
evaluation.
</summary>
    <author>
      <name>Gábor Szárnyas</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2101v1</id>
    <updated>2014-01-09T17:55:33Z</updated>
    <published>2014-01-09T17:55:33Z</published>
    <title>NoSQL Databases</title>
    <summary>  In this document, I present the main notions of NoSQL databases and compare
four selected products (Riak, MongoDB, Cassandra, Neo4J) according to their
capabilities with respect to consistency, availability, and partition
tolerance, as well as performance. I also propose a few criteria for selecting
the right tool for the right situation.
</summary>
    <author>
      <name>Massimo Carro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">57 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.12469v1</id>
    <updated>2019-03-29T12:28:28Z</updated>
    <published>2019-03-29T12:28:28Z</published>
    <title>Corrigendum to "Counting Database Repairs that Satisfy Conjunctive
  Queries with Self-Joins"</title>
    <summary>  The helping Lemma 7 in [Maslowski and Wijsen, ICDT, 2014] is false. The lemma
is used in (and only in) the proof of Theorem 3 of that same paper. In this
corrigendum, we provide a new proof for the latter theorem.
</summary>
    <author>
      <name>Jef Wijsen</name>
    </author>
    <link href="http://arxiv.org/abs/1903.12469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12531v1</id>
    <updated>2019-12-28T21:58:43Z</updated>
    <published>2019-12-28T21:58:43Z</published>
    <title>A framework supporting imprecise queries and data</title>
    <summary>  This technical report provides some lightweight introduction and some generic
use case scenarios motivating the definition of a database supporting
uncertainties in both queries and data. This technical report is only providing
the logical framework, which implementation is going to be provided in the
final paper.
</summary>
    <author>
      <name>Giacomo Bergami</name>
    </author>
    <link href="http://arxiv.org/abs/1912.12531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.08874v1</id>
    <updated>2024-11-13T18:52:24Z</updated>
    <published>2024-11-13T18:52:24Z</published>
    <title>A Decidable Case of Query Determinacy: Project-Select Views</title>
    <summary>  Query determinacy is decidable for project-select views and a
project-select-join query with no self joins, as long as the selection
predicates are in a first-order theory for which satisfiability is decidable.
</summary>
    <author>
      <name>Wen Zhang</name>
    </author>
    <author>
      <name>Aurojit Panda</name>
    </author>
    <author>
      <name>Mooly Sagiv</name>
    </author>
    <author>
      <name>Scott Shenker</name>
    </author>
    <link href="http://arxiv.org/abs/2411.08874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.08874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106046v1</id>
    <updated>2001-06-21T16:33:18Z</updated>
    <published>2001-06-21T16:33:18Z</published>
    <title>Expressing the cone radius in the relational calculus with real
  polynomial constraints</title>
    <summary>  We show that there is a query expressible in first-order logic over the reals
that returns, on any given semi-algebraic set A, for every point a radius
around which A is conical. We obtain this result by combining famous results
from calculus and real algebraic geometry, notably Sard's theorem and Thom's
first isotopy lemma, with recent algorithmic results by Rannou.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306034v1</id>
    <updated>2003-06-07T15:09:16Z</updated>
    <published>2003-06-07T15:09:16Z</published>
    <title>A ROOT/IO Based Software Framework for CMS</title>
    <summary>  The implementation of persistency in the Compact Muon Solenoid (CMS) Software
Framework uses the core I/O functionality of ROOT. We will discuss the current
ROOT/IO implementation, its evolution from the prior Objectivity/DB
implementation, and the plans and ongoing work for the conversion to "POOL",
provided by the LHC Computing Grid (LCG) persistency project.
</summary>
    <author>
      <name>William Tanenbaum</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONFC0303241:TUKT010,2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0306034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310012v1</id>
    <updated>2003-10-08T00:18:31Z</updated>
    <published>2003-10-08T00:18:31Z</published>
    <title>A Formal Comparison of Visual Web Wrapper Generators</title>
    <summary>  We study the core fragment of the Elog wrapping language used in the Lixto
system (a visual wrapper generator) and formally compare Elog to other wrapping
languages proposed in the literature.
</summary>
    <author>
      <name>Georg Gottlob</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 0 figures, second part of long version of PODS 2002 paper
  "Monadic Datalog and the Expressive Power of Languages for Web Information
  Extraction". First part accepted for JACM</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1, F.4.1, F.4.3, H.2.3, I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406029v1</id>
    <updated>2004-06-17T09:11:49Z</updated>
    <published>2004-06-17T09:11:49Z</published>
    <title>Subset Queries in Relational Databases</title>
    <summary>  In this paper, we motivated the need for relational database systems to
support subset query processing. We defined new operators in relational
algebra, and new constructs in SQL for expressing subset queries. We also
illustrated the applicability of subset queries through different examples
expressed using extended SQL statements and relational algebra expressions. Our
aim is to show the utility of subset queries for next generation applications.
</summary>
    <author>
      <name>Satyanarayana R Valluri</name>
    </author>
    <author>
      <name>Kamalakar Karlapalem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.3, H2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410038v1</id>
    <updated>2004-10-16T13:14:54Z</updated>
    <published>2004-10-16T13:14:54Z</published>
    <title>Frequent Knot Discovery</title>
    <summary>  We explore the possibility of applying the framework of frequent pattern
mining to a class of continuous objects appearing in nature, namely knots. We
introduce the frequent knot mining problem and present a solution. The key
observation is that a database consisting of knots can be transformed into a
transactional database. This observation is based on the Prime Decomposition
Theorem of knots.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, recreational data mining</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410070v1</id>
    <updated>2004-10-26T17:00:40Z</updated>
    <published>2004-10-26T17:00:40Z</published>
    <title>Using image partitions in 4th Dimension</title>
    <summary>  I have plotted an image by using mathematical functions in the Database "4th
Dimension". I'm going to show an alternative method to: detect which sector has
been clicked; highlight it and combine it with other sectors already
highlighted; store the graph information in an efficient way; load and splat
image layers to reconstruct the stored graph.
</summary>
    <author>
      <name>Giovanni Gasparri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505059v1</id>
    <updated>2005-05-23T14:19:24Z</updated>
    <published>2005-05-23T14:19:24Z</published>
    <title>Consistent query answers on numerical databases under aggregate
  constraints</title>
    <summary>  The problem of extracting consistent information from relational databases
violating integrity constraints on numerical data is addressed. In particular,
aggregate constraints defined as linear inequalities on aggregate-sum queries
on input data are considered. The notion of repair as consistent set of updates
at attribute-value level is exploited, and the characterization of several
complexity issues related to repairing data and computing consistent query
answers is provided.
</summary>
    <author>
      <name>Sergio Flesca</name>
    </author>
    <author>
      <name>Filippo Furfaro</name>
    </author>
    <author>
      <name>Francesco Parisi</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0505059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602039v1</id>
    <updated>2006-02-10T12:21:42Z</updated>
    <published>2006-02-10T12:21:42Z</published>
    <title>Path Summaries and Path Partitioning in Modern XML Databases</title>
    <summary>  We study the applicability of XML path summaries in the context of
current-day XML databases. We find that summaries provide an excellent basis
for optimizing data access methods, which furthermore mixes very well with
path-partitioned stores. We provide practical algorithms for building and
exploiting summaries, and prove its benefits through extensive experiments.
</summary>
    <author>
      <name>Andrei Arion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Bonifati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Ioana Manolescu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Pugliese</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0602039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603044v2</id>
    <updated>2006-03-15T04:54:50Z</updated>
    <published>2006-03-10T18:11:49Z</published>
    <title>First Steps in Relational Lattice</title>
    <summary>  Relational lattice reduces the set of six classic relational algebra
operators to two binary lattice operations: natural join and inner union. We
give an introduction to this theory with emphasis on formal algebraic laws. New
results include Spight distributivity criteria and its applications to query
transformations.
</summary>
    <author>
      <name>Marshall Spight</name>
    </author>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603044v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603044v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612102v2</id>
    <updated>2007-01-13T06:51:37Z</updated>
    <published>2006-12-20T21:11:05Z</published>
    <title>The Dichotomy of Conjunctive Queries on Probabilistic Structures</title>
    <summary>  We show that for every conjunctive query, the complexity of evaluating it on
a probabilistic database is either \PTIME or #\P-complete, and we give an
algorithm for deciding whether a given conjunctive query is \PTIME or
#\P-complete. The dichotomy property is a fundamental result on query
evaluation on probabilistic databases and it gives a complete classification of
the complexity of conjunctive queries.
</summary>
    <author>
      <name>Nilesh Dalvi</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701161v1</id>
    <updated>2007-01-25T23:51:22Z</updated>
    <published>2007-01-25T23:51:22Z</published>
    <title>Thousands of DebitCredit Transactions-Per-Second: Easy and Inexpensive</title>
    <summary>  A $2k computer can execute about 8k transactions per second. This is 80x more
than one of the largest US bank's 1970's traffic - it approximates the total US
1970's financial transaction volume. Very modest modern computers can easily
solve yesterday's problems.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Charles Levine</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701162v1</id>
    <updated>2007-01-25T23:57:15Z</updated>
    <published>2007-01-25T23:57:15Z</published>
    <title>A Measure of Transaction Processing 20 Years Later</title>
    <summary>  This provides a retrospective of the paper "A Measure of Transaction
Processing" published in 1985. It shows that transaction processing peak
performance and price-peformance have improved about 100,000x respectively and
that sort/sequential performance has approximately doubled each year (so a
million fold improvement) even though processor performance plateaued in 1995.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article appeared in the IEEE Data Engineering, Fall 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3404v2</id>
    <updated>2009-06-09T17:03:07Z</updated>
    <published>2008-03-24T13:59:53Z</published>
    <title>Some results on $\mathbb{R}$-computable structures</title>
    <summary>  This survey paper examines the effective model theory obtained with the BSS
model of real number computation. It treats the following topics: computable
ordinals, satisfaction of computable infinitary formulas, forcing as a
construction technique, effective categoricity, effective topology, and
relations with other models for the effective theory of uncountable structures.
</summary>
    <author>
      <name>Wesley Calvert</name>
    </author>
    <author>
      <name>John E. Porter</name>
    </author>
    <link href="http://arxiv.org/abs/0803.3404v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3404v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1593v2</id>
    <updated>2008-06-19T05:30:49Z</updated>
    <published>2008-05-12T08:52:16Z</published>
    <title>On the Probability Distribution of Superimposed Random Codes</title>
    <summary>  A systematic study of the probability distribution of superimposed random
codes is presented through the use of generating functions. Special attention
is paid to the cases of either uniformly distributed but not necessarily
independent or non uniform but independent bit structures. Recommendations for
optimal coding strategies are derived.
</summary>
    <author>
      <name>Bernd Günther</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2008.924658</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2008.924658" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Inf. Theory, 54(7):3206--3210, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.1593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3795v1</id>
    <updated>2008-07-24T05:24:34Z</updated>
    <published>2008-07-24T05:24:34Z</published>
    <title>Relational Lattice Axioms</title>
    <summary>  Relational lattice is a formal mathematical model for Relational algebra. It
reduces the set of six classic relational algebra operators to two: natural
join and inner union. We continue to investigate Relational lattice properties
with emphasis onto axiomatic definition. New results include additional axioms,
equational definition for set difference (more generally anti-join), and case
study demonstrating application of the relational lattice theory for query
transformations.
</summary>
    <author>
      <name>Marshall Spight</name>
    </author>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.3795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.4620v1</id>
    <updated>2008-07-29T11:22:01Z</updated>
    <published>2008-07-29T11:22:01Z</published>
    <title>A Compositional Query Algebra for Second-Order Logic and Uncertain
  Databases</title>
    <summary>  World-set algebra is a variable-free query language for uncertain databases.
It constitutes the core of the query language implemented in MayBMS, an
uncertain database system. This paper shows that world-set algebra captures
exactly second-order logic over finite structures, or equivalently, the
polynomial hierarchy. The proofs also imply that world-set algebra is closed
under composition, a previously open problem.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.4620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.4620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0438v1</id>
    <updated>2008-12-02T09:00:27Z</updated>
    <published>2008-12-02T09:00:27Z</published>
    <title>An Introduction to Knowledge Management</title>
    <summary>  Knowledge has been lately recognized as one of the most important assets of
organizations. Managing knowledge has grown to be imperative for the success of
a company. This paper presents an overview of Knowledge Management and various
aspects of secure knowledge management. A case study of knowledge management
activities at Tata Steel is also discussed
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <link href="http://arxiv.org/abs/0812.0438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.4986v1</id>
    <updated>2008-12-29T23:14:00Z</updated>
    <published>2008-12-29T23:14:00Z</published>
    <title>An Array Algebra</title>
    <summary>  This is a proposal of an algebra which aims at distributed array processing.
The focus lies on re-arranging and distributing array data, which may be
multi-dimensional. The context of the work is scientific processing; thus, the
core science operations are assumed to be taken care of in external libraries
or languages. A main design driver is the desire to carry over some of the
strategies of the relational algebra into the array domain.
</summary>
    <author>
      <name>Albrecht Schmidt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Five pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.4986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.4986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1059v1</id>
    <updated>2009-03-05T18:33:52Z</updated>
    <published>2009-03-05T18:33:52Z</published>
    <title>Home Heating Systems Design using PHP and MySQL Databases</title>
    <summary>  This paper presents the use of a computer application based on a MySQL
database, managed by PHP programs, allowing the selection of a heating device
using coefficient-based calculus.
</summary>
    <author>
      <name>Tiberiu Marius Karnyanszky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages (121-128), 5th "Actualities and Perspectives in Hard and
  Soft", 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series 5 (2007), 121-128</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.1059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4880v1</id>
    <updated>2010-01-27T09:33:54Z</updated>
    <published>2010-01-27T09:33:54Z</published>
    <title>The WebContent XML Store</title>
    <summary>  In this article, we describe the XML storage system used in the WebContent
project. We begin by advocating the use of an XML database in order to store
WebContent documents, and we present two different ways of storing and querying
these documents : the use of a centralized XML database and the use of a P2P
XML database.
</summary>
    <author>
      <name>Benjamin Nguyen</name>
    </author>
    <author>
      <name>Spyros Zoupanos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Must be compiled with pdflatex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RFIA 2010 Workshop "Sources Ouvertes et Services"</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.4880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4718v1</id>
    <updated>2010-04-27T05:56:24Z</updated>
    <published>2010-04-27T05:56:24Z</published>
    <title>A Data Cleansing Method for Clustering Large-scale Transaction Databases</title>
    <summary>  In this paper, we emphasize the need for data cleansing when clustering
large-scale transaction databases and propose a new data cleansing method that
improves clustering quality and performance. We evaluate our data cleansing
method through a series of experiments. As a result, the clustering quality and
performance were significantly improved by up to 165% and 330%, respectively.
</summary>
    <author>
      <name>Woong-Kee Loh</name>
    </author>
    <author>
      <name>Yang-Sae Moon</name>
    </author>
    <author>
      <name>Jun-Gyu Kang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transinf.E93.D.3120</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transinf.E93.D.3120" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0198v1</id>
    <updated>2010-05-03T06:36:10Z</updated>
    <published>2010-05-03T06:36:10Z</published>
    <title>Personnalisation de Systèmes OLAP Annotés</title>
    <summary>  This paper deals with personalization of annotated OLAP systems. Data
constellation is extended to support annotations and user preferences.
Annotations reflect the decision-maker experience whereas user preferences
enable users to focus on the most interesting data. User preferences allow
annotated contextual recommendations helping the decision-maker during his/her
multidimensional navigations.
</summary>
    <author>
      <name>Houssem Jerbi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Geneviève Pujolle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">XXVIII\`eme Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'10, Marseille : France (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0575v1</id>
    <updated>2010-06-03T07:59:15Z</updated>
    <published>2010-06-03T07:59:15Z</published>
    <title>XQ2P: Efficient XQuery P2P Time Series Processing</title>
    <summary>  In this demonstration, we propose a model for the management of XML time
series (TS), using the new XQuery 1.1 window operator. We argue that
centralized computation is slow, and demonstrate XQ2P, our prototype of
efficient XQuery P2P TS computation in the context of financial analysis of
large data sets (&gt;1M values).
</summary>
    <author>
      <name>Bogdan Butnaru</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Nguyen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Gardarin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Yeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bases de Donn\'ees Avanc\'ees (D\'emonstration), Namur : Belgium
  (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2077v1</id>
    <updated>2010-06-10T16:16:37Z</updated>
    <published>2010-06-10T16:16:37Z</published>
    <title>Multidimensi Pada Data Warehouse Dengan Menggunakan Rumus Kombinasi</title>
    <summary>  Multidimensional in data warehouse is a compulsion and become the most
important for information delivery, without multidimensional data warehouse is
incomplete. Multidimensional give the able to analyze business measurement in
many different ways. Multidimensional is also synonymous with online analytical
processing (OLAP).
</summary>
    <author>
      <name>H. L. H Spits Warnars</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2nd National Seminar Information Technology Application
  (SNATI) 2006, University of Islam Indonesia, pp. J1-J6, Yogyakarta, 17 June
  2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.2077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1339v1</id>
    <updated>2010-08-07T13:02:01Z</updated>
    <published>2010-08-07T13:02:01Z</published>
    <title>Removal of Communication Gap</title>
    <summary>  This research is about an online forum designed and developed to improve the
communication process between alumni, new, old and upcoming students. In this
research paper we present targeted problems, designed architecture, used
technologies in development and final end product in detail.
</summary>
    <author>
      <name>Zeeshan Ahmed</name>
    </author>
    <author>
      <name>Sudhir Ganti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the 5th Virtual Conference of the EU-funded FP6 I*PROMS Network of
  Excellence on Innovative Production Machines and Systems, IPROMS 2009, 6-17
  July, 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.1339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5619v1</id>
    <updated>2011-08-26T07:06:03Z</updated>
    <published>2011-08-26T07:06:03Z</published>
    <title>Modification of GTD from Flat File Format to OLAP for Data Mining</title>
    <summary>  This document is part of original research work by the authors in a bid to
explore new fields for applying Data Mining Techniques. The sample data is part
of a large data set from University of Maryland (UMD) and outlines how more
meaningful patterns can be discovered by preprocessing the data in the form of
OLAP cubes.
</summary>
    <author>
      <name>Karanjit Singh</name>
    </author>
    <author>
      <name>Shuchita Bhasin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GTD, OLAP, Data Mining, Terror Databases</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.0242v1</id>
    <updated>2012-02-01T18:46:41Z</updated>
    <published>2012-02-01T18:46:41Z</published>
    <title>Weak Forms of Monotonicity and Coordination-Freeness</title>
    <summary>  Our earlier work titled: "Win-move is Coordination-Free (Sometimes)" has
shown that the classes of queries that can be distributedly computed in a
coordination-free manner form a strict hierarchy depending on the assumptions
of the model for distributed computations. In this paper, we further
characterize these classes by revealing a tight relationship between them and
novel weakened forms of monotonicity.
</summary>
    <author>
      <name>Daniel Zinn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Early Research Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.0242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.0242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6560v1</id>
    <updated>2012-07-27T14:49:18Z</updated>
    <published>2012-07-27T14:49:18Z</published>
    <title>Covering Rough Sets From a Topological Point of View</title>
    <summary>  Covering-based rough set theory is an extension to classical rough set. The
main purpose of this paper is to study covering rough sets from a topological
point of view. The relationship among upper approximations based on topological
spaces are explored.
</summary>
    <author>
      <name>Nguyen Duc Thuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Theory and Engineering, Vol. 1,
  No. 5, December, 2009, 606-609</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.6560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3756v3</id>
    <updated>2012-12-18T16:36:22Z</updated>
    <published>2012-09-17T19:10:38Z</published>
    <title>Incomplete Information in RDF</title>
    <summary>  We extend RDF with the ability to represent property values that exist, but
are unknown or partially known, using constraints. Following ideas from the
incomplete information literature, we develop a semantics for this extension of
RDF, called RDFi, and study SPARQL query evaluation in this framework.
</summary>
    <author>
      <name>Charalampos Nikolaou</name>
    </author>
    <author>
      <name>Manolis Koubarakis</name>
    </author>
    <link href="http://arxiv.org/abs/1209.3756v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3756v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3913v1</id>
    <updated>2012-09-18T11:35:57Z</updated>
    <published>2012-09-18T11:35:57Z</published>
    <title>Keyspace: A Consistently Replicated, Highly-Available Key-Value Store</title>
    <summary>  This paper describes the design and architecture of Keyspace, a distributed
key-value store offering strong consistency, fault-tolerance and high
availability. The source code is available under the open-source AGPL license
for Linux, Windows and BSD-like platforms. As of 2012, Keyspace is no longer
undergoing active development.
</summary>
    <author>
      <name>Márton Trencséni</name>
    </author>
    <author>
      <name>Attila Gazsó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2354v1</id>
    <updated>2012-11-10T21:43:40Z</updated>
    <published>2012-11-10T21:43:40Z</published>
    <title>Privacy Preserving Web Query Log Publishing: A Survey on Anonymization
  Techniques</title>
    <summary>  Releasing Web query logs which contain valuable information for research or
marketing, can breach the privacy of search engine users. Therefore rendering
query logs to limit linking a query to an individual while preserving the data
usefulness for analysis, is an important research problem. This survey provides
an overview and discussion on the recent studies on this direction.
</summary>
    <author>
      <name>Amin Milani Fard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.2354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1877v1</id>
    <updated>2013-04-06T11:20:24Z</updated>
    <published>2013-04-06T11:20:24Z</published>
    <title>Privacy-preserving Data Mining, Sharing and Publishing</title>
    <summary>  The goal of the paper is to present different approaches to
privacy-preserving data sharing and publishing in the context of e-health care
systems. In particular, the literature review on technical issues in privacy
assurance and current real-life high complexity implementation of medical
system that assumes proper data sharing mechanisms are presented in the paper.
</summary>
    <author>
      <name>Katarzyna Pasierb</name>
    </author>
    <author>
      <name>Tomasz Kajdanowicz</name>
    </author>
    <author>
      <name>Przemyslaw Kazienko</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Medical Informatics &amp; Technologies, Vol. 18, pp. 69-76,
  2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.1877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2637v2</id>
    <updated>2013-06-19T18:47:43Z</updated>
    <published>2013-04-09T15:40:21Z</published>
    <title>Containment of Nested Regular Expressions</title>
    <summary>  Nested regular expressions (NREs) have been proposed as a powerful formalism
for querying RDFS graphs, but research in a more general graph database context
has been scarce, and static analysis results are currently lacking. In this
paper we investigate the problem of containment of NREs, and show that it can
be solved in PSPACE, i.e., the same complexity as the problem of containment of
regular expressions or regular path queries (RPQs).
</summary>
    <author>
      <name>Juan L. Reutter</name>
    </author>
    <link href="http://arxiv.org/abs/1304.2637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5566v1</id>
    <updated>2013-04-20T00:25:50Z</updated>
    <published>2013-04-20T00:25:50Z</published>
    <title>A Markov Model for Ontology Alignment</title>
    <summary>  The explosion of available data along with the need to integrate and utilize
that data has led to a pressing interest in data integration techniques. In
terms of Semantic Web technologies, Ontology Alignment is a key step in the
process of integrating heterogeneous knowledge bases. In this paper, we present
the Edge Confidence technique, a modification and improvement over the popular
Similarity Flooding technique for Ontology Alignment.
</summary>
    <author>
      <name>Michael E. Cotterell</name>
    </author>
    <author>
      <name>Terrance Medina</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1927v1</id>
    <updated>2013-07-07T22:20:27Z</updated>
    <published>2013-07-07T22:20:27Z</published>
    <title>Link Based Session Reconstruction: Finding All Maximal Paths</title>
    <summary>  This paper introduces a new method for the session construction problem,
which is the first main step of the web usage mining process. Through
experiments, it is shown that when our new technique is used, it outperforms
previous approaches in web usage mining applications such as next-page
prediction.
</summary>
    <author>
      <name>Murat Ali Bayir</name>
    </author>
    <author>
      <name>Ismail Hakki Toroslu</name>
    </author>
    <link href="http://arxiv.org/abs/1307.1927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5777v1</id>
    <updated>2014-05-22T14:51:13Z</updated>
    <published>2014-05-22T14:51:13Z</published>
    <title>An Analytical Survey of Provenance Sanitization</title>
    <summary>  Security is likely becoming a critical factor in the future adoption of
provenance technology, because of the risk of inadvertent disclosure of
sensitive information. In this survey paper we review the state of the art in
secure provenance, considering mechanisms for controlling access, and the
extent to which these mechanisms preserve provenance integrity. We examine
seven systems or approaches, comparing features and identifying areas for
future work.
</summary>
    <author>
      <name>James Cheney</name>
    </author>
    <author>
      <name>Roly Perera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, IPAW 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2081v1</id>
    <updated>2014-08-09T11:09:00Z</updated>
    <published>2014-08-09T11:09:00Z</published>
    <title>On the BDD/FC Conjecture</title>
    <summary>  Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are
two properties of sets of datalog rules and tuple generating dependencies
(known as Datalog +/- programs), which recently attracted some attention. We
conjecture that the first of these properties implies the second, and support
this conjecture by some evidence proving, among other results, that it holds
true for all theories over binary signature.
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05039v1</id>
    <updated>2015-01-21T02:41:55Z</updated>
    <published>2015-01-21T02:41:55Z</published>
    <title>Defining Data Science</title>
    <summary>  Data science is gaining more and more and widespread attention, but no
consensus viewpoint on what data science is has emerged. As a new science, its
objects of study and scientific issues should not be covered by established
sciences. Data in cyberspace have formed what we call datanature. In the
present paper, data science is defined as the science of exploring datanature.
</summary>
    <author>
      <name>Yangyong Zhu</name>
    </author>
    <author>
      <name>Yun Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/1501.05039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01708v1</id>
    <updated>2015-07-07T08:43:59Z</updated>
    <published>2015-07-07T08:43:59Z</published>
    <title>Typing Regular Path Query Languages for Data Graphs</title>
    <summary>  Regular path query languages for data graphs are essentially \emph{untyped}.
The lack of type information greatly limits the optimization opportunities for
query engines and makes application development more complex. In this paper we
discuss a simple, yet expressive, schema language for edge-labelled data
graphs. This schema language is, then, used to define a query type inference
approach with good precision properties.
</summary>
    <author>
      <name>Dario Colazzo</name>
    </author>
    <author>
      <name>Carlo Sartiani</name>
    </author>
    <link href="http://arxiv.org/abs/1507.01708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02650v1</id>
    <updated>2016-01-11T21:18:59Z</updated>
    <published>2016-01-11T21:18:59Z</published>
    <title>Inference rules for RDF(S) and OWL in N3Logic</title>
    <summary>  This paper presents inference rules for Resource Description Framework (RDF),
RDF Schema (RDFS) and Web Ontology Language (OWL). Our formalization is based
on Notation 3 Logic, which extended RDF by logical symbols and created Semantic
Web logic for deductive RDF graph stores. We also propose OWL-P that is a
lightweight formalism of OWL and supports soft inferences by omitting complex
language constructs.
</summary>
    <author>
      <name>Dominik Tomaszuk</name>
    </author>
    <link href="http://arxiv.org/abs/1601.02650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00791v1</id>
    <updated>2016-09-03T04:35:57Z</updated>
    <published>2016-09-03T04:35:57Z</published>
    <title>Reprowd: Crowdsourced Data Processing Made Reproducible</title>
    <summary>  Crowdsourcing is a multidisciplinary research area including disciplines like
artificial intelligence, human-computer interaction, database, and social
science. To facilitate cooperation across disciplines, reproducibility is a
crucial factor, but unfortunately, it has not gotten enough attention in the
HCOMP community. In this paper, we present Reprowd, a system aiming to make it
easy to reproduce crowdsourced data processing research. We have open sourced
Reprowd at http://sfu-db.github.io/reprowd/.
</summary>
    <author>
      <name>Ruochen Jiang</name>
    </author>
    <author>
      <name>Jiannan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HCOMP 2016 Work in Progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09062v1</id>
    <updated>2016-09-29T03:01:44Z</updated>
    <published>2016-09-29T03:01:44Z</published>
    <title>A Study on Altering PostgreSQL from Multi-Processes Structure to
  Multi-Threads Structure</title>
    <summary>  How to altering PostgreSQL database from multi-processes structure to
multi-threads structure is a difficult problem. In the paper, we bring forward
a comprehensive alteration scheme. Especially, put rational methods to account
for three difficult points: semaphores, signal processing and global variables.
At last, applied the scheme successfully to modify a famous open source DBMS.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <link href="http://arxiv.org/abs/1609.09062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04182v1</id>
    <updated>2017-01-16T06:22:24Z</updated>
    <published>2017-01-16T06:22:24Z</published>
    <title>hMDAP: A Hybrid Framework for Multi-paradigm Data Analytical Processing
  on Spark</title>
    <summary>  We propose hMDAP, a hybrid framework for large-scale data analytical
processing on Spark, to support multi-paradigm process (incl. OLAP, machine
learning, and graph analysis etc.) in distributed environments. The framework
features a three-layer data process module and a business process module which
controls the former. We will demonstrate the strength of hMDAP by using traffic
scenarios in a real world.
</summary>
    <author>
      <name>Xiaowang Zhang</name>
    </author>
    <author>
      <name>Jiahui Zhang</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08028v1</id>
    <updated>2017-01-27T12:30:07Z</updated>
    <published>2017-01-27T12:30:07Z</published>
    <title>Biomedical Data Warehouses</title>
    <summary>  The aim of this article is to present an overview of the existing biomedical
data warehouses and to discuss the issues and future trends in this area. We
illustrate this topic by presenting the design of an innovative, complex data
warehouse for personal, anticipative medicine.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Emerson Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:0809.2688</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopaedia of Healthcare Information Systems, IGI Publishing,
  pp.149-156, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08052v1</id>
    <updated>2017-01-27T13:42:54Z</updated>
    <published>2017-01-27T13:42:54Z</published>
    <title>Database Benchmarks</title>
    <summary>  The aim of this article is to present an overview of the major families of
state-of-the-art data-base benchmarks, namely: relational benchmarks, object
and object-relational benchmarks, XML benchmarks, and decision-support
benchmarks, and to discuss the issues, tradeoffs and future trends in database
benchmarking. We particularly focus on XML and decision-support benchmarks,
which are currently the most innovative tools that are developed in this area.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Information Science and Technology, Second
  Edition, IGI Publishing, pp.950-954, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08634v1</id>
    <updated>2017-01-30T15:10:07Z</updated>
    <published>2017-01-30T15:10:07Z</published>
    <title>Data Processing Benchmarks</title>
    <summary>  The aim of this article is to present an overview of the major families of
state-of-the-art data processing benchmarks, namely transaction processing
benchmarks and decision support benchmarks. We also address the newer trends in
cloud benchmarking. Finally, we discuss the issues, tradeoffs and future trends
for data processing benchmarks.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1701.08052</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Information Science and Technology, Third Edition,
  pp.146-152, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06494v1</id>
    <updated>2019-09-14T00:36:13Z</updated>
    <published>2019-09-14T00:36:13Z</published>
    <title>Transactional Smart Contracts in Blockchain Systems</title>
    <summary>  This paper presents TXSC, a framework that provides smart contract developers
with transaction primitives. These primitives allow developers to write smart
contracts without the need to reason about the anomalies that can arise due to
concurrent smart contract function executions.
</summary>
    <author>
      <name>Victor Zakhary</name>
    </author>
    <author>
      <name>Divyakant Agrawal</name>
    </author>
    <author>
      <name>Amr El Abbadi</name>
    </author>
    <link href="http://arxiv.org/abs/1909.06494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.00694v1</id>
    <updated>2020-06-01T03:36:27Z</updated>
    <published>2020-06-01T03:36:27Z</published>
    <title>F-IVM: Learning over Fast-Evolving Relational Data</title>
    <summary>  F-IVM is a system for real-time analytics such as machine learning
applications over training datasets defined by queries over fast-evolving
relational databases. We will demonstrate F-IVM for three such applications:
model selection, Chow-Liu trees, and ridge linear regression.
</summary>
    <author>
      <name>Milos Nikolic</name>
    </author>
    <author>
      <name>Haozhe Zhang</name>
    </author>
    <author>
      <name>Ahmet Kara</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD DEMO 2020, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.00694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08470v2</id>
    <updated>2019-10-21T14:07:45Z</updated>
    <published>2017-09-13T15:39:03Z</published>
    <title>An efficient clustering algorithm from the measure of local Gaussian
  distribution</title>
    <summary>  In this paper, I will introduce a fast and novel clustering algorithm based
on Gaussian distribution and it can guarantee the separation of each cluster
centroid as a given parameter, $d_s$. The worst run time complexity of this
algorithm is approximately $\sim$O$(T\times N \times \log(N))$ where $T$ is the
iteration steps and $N$ is the number of features.
</summary>
    <author>
      <name>Yuan-Yen Tai</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09003v1</id>
    <updated>2017-09-19T07:59:41Z</updated>
    <published>2017-09-19T07:59:41Z</published>
    <title>CASP-DM: Context Aware Standard Process for Data Mining</title>
    <summary>  We propose an extension of the Cross Industry Standard Process for Data
Mining (CRISPDM) which addresses specific challenges of machine learning and
data mining for context and model reuse handling. This new general
context-aware process model is mapped with CRISP-DM reference model proposing
some new or enhanced outputs.
</summary>
    <author>
      <name>Fernando Martínez-Plumed</name>
    </author>
    <author>
      <name>Lidia Contreras-Ochando</name>
    </author>
    <author>
      <name>Cèsar Ferri</name>
    </author>
    <author>
      <name>Peter Flach</name>
    </author>
    <author>
      <name>José Hernández-Orallo</name>
    </author>
    <author>
      <name>Meelis Kull</name>
    </author>
    <author>
      <name>Nicolas Lachiche</name>
    </author>
    <author>
      <name>María José Ramírez-Quintana</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09353v1</id>
    <updated>2019-01-27T11:18:06Z</updated>
    <published>2019-01-27T11:18:06Z</published>
    <title>Subsumption of Weakly Well-Designed SPARQL Patterns is Undecidable</title>
    <summary>  Weakly well-designed SPARQL patterns is a recent generalisation of
well-designed patterns, which preserve good computational properties but also
capture almost all patterns that appear in practice. Subsumption is one of
static analysis problems for SPARQL, along with equivalence and containment. In
this paper we show that subsumption is undecidable for weakly well-designed
patterns, which is in stark contrast to well-designed patterns, and to
equivalence and containment.
</summary>
    <author>
      <name>Mark Kaminski</name>
    </author>
    <author>
      <name>Egor V. Kostylev</name>
    </author>
    <link href="http://arxiv.org/abs/1901.09353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01633v1</id>
    <updated>2019-11-05T06:03:16Z</updated>
    <published>2019-11-05T06:03:16Z</published>
    <title>On the Importance of Location Privacy for Users of Location Based
  Applications</title>
    <summary>  Do people care about their location privacy while using location-based
service apps? This paper aims to answer this question and several other
hypotheses through a survey, and review the privacy preservation techniques.
Our results indicate that privacy is indeed an influential factor in the
selection of location-based apps by users.
</summary>
    <author>
      <name>Sina Shaham</name>
    </author>
    <author>
      <name>Saba Rafieian</name>
    </author>
    <author>
      <name>Ming Ding</name>
    </author>
    <author>
      <name>Mahyar Shirvanimoghaddam</name>
    </author>
    <author>
      <name>Zihuai Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4916v2</id>
    <updated>2012-03-09T10:40:21Z</updated>
    <published>2011-03-25T07:10:50Z</published>
    <title>Detection of Spatial Changes using Spatial Data Mining</title>
    <summary>  The Change detection based on analysis and samples are analyzed. Land
use/cover change detection based on SDM is discussed.
</summary>
    <author>
      <name>B. G. Kodge</name>
    </author>
    <author>
      <name>P. S. Hiremath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Information Mining, ISSN: 0975-3265, Volume 2, Issue
  2, 2010, pp-14-18</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.4916v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4916v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6985v3</id>
    <updated>2014-10-16T14:56:52Z</updated>
    <published>2014-03-27T11:54:27Z</published>
    <title>A Fast Minimal Infrequent Itemset Mining Algorithm</title>
    <summary>  A novel fast algorithm for finding quasi identifiers in large datasets is
presented. Performance measurements on a broad range of datasets demonstrate
substantial reductions in run-time relative to the state of the art and the
scalability of the algorithm to realistically-sized datasets up to several
million records.
</summary>
    <author>
      <name>Kostyantyn Demchuk</name>
    </author>
    <author>
      <name>Douglas J. Leith</name>
    </author>
    <link href="http://arxiv.org/abs/1403.6985v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6985v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1671v2</id>
    <updated>2014-12-10T10:20:23Z</updated>
    <published>2014-12-04T14:12:23Z</published>
    <title>Chases and Bag-Set Certain Answers</title>
    <summary>  In this paper we show that the chase technique is powerful enough to capture
the bag-set semantics of conjunctive queries over IDBs and IDs and TGDs. In
addition, we argue that in such cases it provides efficient (LogSpace) query
evaluation algorithms and that, moreover, it can serve as a basis for
evaluating some restricted classes of aggregate queries under incomplete
information.
</summary>
    <author>
      <name>Camilo Thorne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pp</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.1671v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1671v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07950v1</id>
    <updated>2015-06-26T03:24:39Z</updated>
    <published>2015-06-26T03:24:39Z</published>
    <title>Bag-of-Features Image Indexing and Classification in Microsoft SQL
  Server Relational Database</title>
    <summary>  This paper presents a novel relational database architecture aimed to visual
objects classification and retrieval. The framework is based on the
bag-of-features image representation model combined with the Support Vector
Machine classification and is integrated in a Microsoft SQL Server database.
</summary>
    <author>
      <name>Marcin Korytkowski</name>
    </author>
    <author>
      <name>Rafal Scherer</name>
    </author>
    <author>
      <name>Pawel Staszewski</name>
    </author>
    <author>
      <name>Piotr Woldan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CYBConf.2015.7175981</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CYBConf.2015.7175981" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),
  Gdynia, Poland, 24-26 June 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.07950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07180v1</id>
    <updated>2016-04-25T09:39:57Z</updated>
    <published>2016-04-25T09:39:57Z</published>
    <title>Observing and Recommending from a Social Web with Biases</title>
    <summary>  The research question this report addresses is: how, and to what extent,
those directly involved with the design, development and employment of a
specific black box algorithm can be certain that it is not unlawfully
discriminating (directly and/or indirectly) against particular persons with
protected characteristics (e.g. gender, race and ethnicity)?
</summary>
    <author>
      <name>Steffen Staab</name>
    </author>
    <author>
      <name>Sophie Stalla-Bourdillon</name>
    </author>
    <author>
      <name>Laura Carmichael</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report, University of Southampton, March 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.5.0; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07514v1</id>
    <updated>2016-12-22T10:05:45Z</updated>
    <published>2016-12-22T10:05:45Z</published>
    <title>Getting Started with PATSTAT Register</title>
    <summary>  This paper provides a technical introduction to the PATSTAT Register
database, which contains bibliographical, procedural and legal status data on
patent applications handled by the European Patent Office. It presents eight
MySQL queries that cover some of the most relevant aspects of the database for
research purposes. It targets academic researchers and practitioners who are
familiar with the PATSTAT database and the MySQL language.
</summary>
    <author>
      <name>Gaetan de Rassenfosse</name>
    </author>
    <author>
      <name>Martin Kracker</name>
    </author>
    <author>
      <name>Gianluca Tarasconi</name>
    </author>
    <link href="http://arxiv.org/abs/1612.07514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01298v1</id>
    <updated>2017-03-03T18:17:53Z</updated>
    <published>2017-03-03T18:17:53Z</published>
    <title>Defining Domain-Independent Discovery Informatics</title>
    <summary>  This paper presents a personal account of the early legacy of discovery
informatics, especially surrounding the first published definition of
domain-independent DI. The state of DI is traced across various reference
sources and the literature on the fourth paradigm of the scientific method.
Observations are offered on DI, concluding that it will retain its appeal as a
highly apt descriptor for research and practice activities that are inherent in
our human nature.
</summary>
    <author>
      <name>William W. Agresti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08685v2</id>
    <updated>2017-04-01T07:27:18Z</updated>
    <published>2017-03-25T12:48:55Z</published>
    <title>Thespis: Actor-Based Middleware for Causal Consistency</title>
    <summary>  This paper provides a survey of the current state of the art in
Causally-Consistent data stores. Furthermore, we present the design of Thespis,
a middleware that innovatively leverages the Actor model to implement causal
consistency over an industry-standard data store.
</summary>
    <author>
      <name>Carl Camilleri</name>
    </author>
    <author>
      <name>Joseph Vella</name>
    </author>
    <author>
      <name>Vitezslav Nezval</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">need to withdraw for corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04265v1</id>
    <updated>2018-05-11T07:52:23Z</updated>
    <published>2018-05-11T07:52:23Z</published>
    <title>Scripting Relational Database Engine Using Transducer</title>
    <summary>  We allow database user to script a parallel relational database engine with a
procedural language. Procedural language code is executed as a user defined
relational query operator called transducer. Transducer is tightly integrated
with relation engine, including query optimizer, query executor and can be
executed in parallel like other query operators. With transducer, we can
efficiently execute queries that are very difficult to express in SQL. As
example, we show how to run time series and graph queries, etc, within a
parallel relational database.
</summary>
    <author>
      <name>Feng Tian</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.06300v1</id>
    <updated>2020-07-13T10:37:21Z</updated>
    <published>2020-07-13T10:37:21Z</published>
    <title>Synthetic Dataset Generation with Itemset-Based Generative Models</title>
    <summary>  This paper proposes three different data generators, tailored to
transactional datasets, based on existing itemset-based generative models. All
these generators are intuitive and easy to implement and show satisfactory
performance. The quality of each generator is assessed by means of three
different methods that capture how well the original dataset structure is
preserved.
</summary>
    <author>
      <name>Christian Lezcano</name>
    </author>
    <author>
      <name>Marta Arias</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISSREW.2019.00086</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISSREW.2019.00086" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Symposium on Software Reliability Engineering
  Workshops (ISSREW@RDSA 2019), Oct 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.06300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11926v1</id>
    <updated>2021-05-21T14:09:35Z</updated>
    <published>2021-05-21T14:09:35Z</published>
    <title>ConQuer-92 -- The revised report on the conceptual query language LISA-D</title>
    <summary>  In this report the conceptual query language ConQuer-92 is introduced. This
query language serves as the backbone of InfoAssistant's query facilities.
Furthermore, this language can also be used for the specification of derivation
rules (e.g. subtype defining rules) and textual constraints in InfoModeler.
This report is solely concerned with a formal definition, and the explanation
thereof, of ConQuer-92. The implementation of ConQuer-92 in SQL-92 will be
treated in a separate report.
</summary>
    <author>
      <name>H. A. Proper</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12507v1</id>
    <updated>2021-05-26T12:18:32Z</updated>
    <published>2021-05-26T12:18:32Z</published>
    <title>Cost models for geo-distributed massively parallel streaming analytics</title>
    <summary>  This report is part of the DataflowOpt project on optimization of modern
dataflows and aims to introduce a data quality-aware cost model that covers the
following aspects in combination: (1) heterogeneity in compute nodes, (2)
geo-distribution, (3) massive parallelism, (4) complex DAGs and (5) streaming
applications. Such a cost model can be then leveraged to devise cost-based
optimization solutions that deal with task placement and operator
configuration.
</summary>
    <author>
      <name>Anna-Valentini Michailidou</name>
    </author>
    <author>
      <name>Anastasios Gounaris</name>
    </author>
    <author>
      <name>Konstantinos Tsichlas</name>
    </author>
    <link href="http://arxiv.org/abs/2105.12507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01682v1</id>
    <updated>2016-03-05T05:39:46Z</updated>
    <published>2016-03-05T05:39:46Z</published>
    <title>Frequent-Itemset Mining using Locality-Sensitive Hashing</title>
    <summary>  The Apriori algorithm is a classical algorithm for the frequent itemset
mining problem. A significant bottleneck in Apriori is the number of I/O
operation involved, and the number of candidates it generates. We investigate
the role of LSH techniques to overcome these problems, without adding much
computational overhead. We propose randomized variations of Apriori that are
based on asymmetric LSH defined over Hamming distance and Jaccard similarity.
</summary>
    <author>
      <name>Debajyoti Bera</name>
    </author>
    <author>
      <name>Rameshwar Pratap</name>
    </author>
    <link href="http://arxiv.org/abs/1603.01682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09617v2</id>
    <updated>2020-05-20T14:09:11Z</updated>
    <published>2020-05-19T17:46:34Z</published>
    <title>Unlocking New York City Crime Insights using Relational Database
  Embeddings</title>
    <summary>  This version withdrawn by arXiv administrators because the author did not
have the right to agree to our license at the time of submission.
</summary>
    <author>
      <name>Apoorva Nitsure</name>
    </author>
    <author>
      <name>Rajesh Bordawekar</name>
    </author>
    <author>
      <name>Jose Neves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: This version withdrawn by arXiv administrators
  because the author did not have the right to agree to our license at the time
  of submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.09617v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09617v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.01934v1</id>
    <updated>2020-11-03T10:50:05Z</updated>
    <published>2020-11-03T10:50:05Z</published>
    <title>Palette diagram: A Python package for visualization of collective
  categorical data</title>
    <summary>  Categorical data, wherein a numerical quantity is assigned to each category
(nominal variable), are ubiquitous in data science. A palette diagram is a
visualization tool for a large number of categorical datasets, each comprising
several categories.
</summary>
    <author>
      <name>Chihiro Noguchi</name>
    </author>
    <author>
      <name>Tatsuro Kawamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.01934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.01934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03447v1</id>
    <updated>2017-02-11T19:18:41Z</updated>
    <published>2017-02-11T19:18:41Z</published>
    <title>A Collective, Probabilistic Approach to Schema Mapping: Appendix</title>
    <summary>  In this appendix we provide additional supplementary material to "A
Collective, Probabilistic Approach to Schema Mapping." We include an additional
extended example, supplementary experiment details, and proof for the
complexity result stated in the main paper.
</summary>
    <author>
      <name>Angelika Kimmig</name>
    </author>
    <author>
      <name>Alex Memory</name>
    </author>
    <author>
      <name>Renee J. Miller</name>
    </author>
    <author>
      <name>Lise Getoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the appendix to the paper "A Collective, Probabilistic
  Approach to Schema Mapping" accepted to ICDE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01349v1</id>
    <updated>2019-05-03T19:36:55Z</updated>
    <published>2019-05-03T19:36:55Z</published>
    <title>Adaptive filter ordering in Spark</title>
    <summary>  This report describes a technical methodology to render the Apache Spark
execution engine adaptive. It presents the engineering solutions, which
specifically target to adaptively reorder predicates in data streams with
evolving statistics. The system extension developed is available as an
open-source prototype. Indicative experimental results show its overhead and
sensitivity to tuning parameters.
</summary>
    <author>
      <name>Nikodimos Nikolaidis</name>
    </author>
    <author>
      <name>Anastasios Gounaris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10268v1</id>
    <updated>2019-08-27T15:22:28Z</updated>
    <published>2019-08-27T15:22:28Z</published>
    <title>Answering Summation Queries for Numerical Attributes under Differential
  Privacy</title>
    <summary>  In this work we explore the problem of answering a set of sum queries under
Differential Privacy. This is a little understood, non-trivial problem
especially in the case of numerical domains. We show that traditional
techniques from the literature are not always the best choice and a more
rigorous approach is necessary to develop low error algorithms.
</summary>
    <author>
      <name>Yikai Wu</name>
    </author>
    <author>
      <name>David Pujol</name>
    </author>
    <author>
      <name>Ios Kotsogiannis</name>
    </author>
    <author>
      <name>Ashwin Machanavajjhala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TPDP 2019, 7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07781v1</id>
    <updated>2021-06-14T22:26:22Z</updated>
    <published>2021-06-14T22:26:22Z</published>
    <title>On Declare MAX-SAT and a finite Herbrand Base for data-aware logs</title>
    <summary>  This technical report provides some lightweight introduction motivating the
definition of an alignment of log traces against Data-Aware Declare Models
potentially containing correlation conditions. This technical report is only
providing the intuition of the logical framework as a feasibility study for a
future formalization and experiment section.
</summary>
    <author>
      <name>Giacomo Bergami</name>
    </author>
    <link href="http://arxiv.org/abs/2106.07781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.12996v1</id>
    <updated>2021-10-25T14:39:42Z</updated>
    <published>2021-10-25T14:39:42Z</published>
    <title>PREC: semantic translation of property graphs</title>
    <summary>  Converting property graphs to RDF graphs allows to enhance the
interoperability of knowledge graphs. But existing tools perform the same
conversion for every graph, regardless of its content. In this paper, we
propose PREC, a user-configured conversion of property graphs to RDF graphs to
better capture the semantics of the content.
</summary>
    <author>
      <name>Julian Bruyat</name>
    </author>
    <author>
      <name>Pierre-Antoine Champin</name>
    </author>
    <author>
      <name>Lionel Médini</name>
    </author>
    <author>
      <name>Frédérique Laforest</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 1st Workshop on Squaring the Circles on Graphs at
  SEMANTiCS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.12996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.12996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05664v1</id>
    <updated>2022-01-14T20:48:08Z</updated>
    <published>2022-01-14T20:48:08Z</published>
    <title>Demonstration of PI2: Interactive Visualization Interface Generation for
  SQL Analysis in Notebook</title>
    <summary>  We demonstrate PI2, the first notebook extension that can automatically
generate interactive visualization interfaces during SQL-based analyses.
</summary>
    <author>
      <name>Jeffrey Tao</name>
    </author>
    <author>
      <name>Yiru Chen</name>
    </author>
    <author>
      <name>Eugene Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3514221.3520153</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3514221.3520153" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2107.08203</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD '22: Proceedings of the 2022 International Conference on
  Management of Data</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.05664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.06351v1</id>
    <updated>2022-02-13T16:01:27Z</updated>
    <published>2022-02-13T16:01:27Z</published>
    <title>Comparing Flexible Skylines And Top-k Queries: Which Is the Best
  Alternative?</title>
    <summary>  The question of how to get the best results out of the data we have is an
everlasting problem in data science. The two main approaches to tackle the
problem are top-k queries and skyline queries. Since their introduction, a new
paradigm called flexible skylines has emerged. The aim of this survey is to
provide a solid comparison between the new and the old approaches,
understanding and exploring their differences and similarities.
</summary>
    <author>
      <name>Flavio Rizzoglio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.06351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.06351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5077v1</id>
    <updated>2014-10-19T14:10:50Z</updated>
    <published>2014-10-19T14:10:50Z</published>
    <title>On the Provenance of Linked Data Statistics</title>
    <summary>  As the amount of linked data published on the web grows, attempts are being
made to describe and measure it. However even basic statistics about a graph,
such as its size, are difficult to express in a uniform and predictable way. In
order to be able to sensibly interpret a statistic it is necessary to know how
it was calculate. In this paper we survey the nature of the problem and outline
a strategy for addressing it.
</summary>
    <author>
      <name>William Waites</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02781v1</id>
    <updated>2015-03-10T06:23:56Z</updated>
    <published>2015-03-10T06:23:56Z</published>
    <title>Unravelling Graph-Exchange File Formats</title>
    <summary>  A graph is used to represent data in which the relationships between the
objects in the data are at least as important as the objects themselves. Over
the last two decades nearly a hundred file formats have been proposed or used
to provide portable access to such data. This paper seeks to review these
formats, and provide some insight to both reduce the ongoing creation of
unnecessary formats, and guide the development of new formats where needed.
</summary>
    <author>
      <name>Matthew Roughan</name>
    </author>
    <author>
      <name>Jonathan Tuke</name>
    </author>
    <link href="http://arxiv.org/abs/1503.02781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00384v1</id>
    <updated>2015-11-02T05:31:42Z</updated>
    <published>2015-11-02T05:31:42Z</published>
    <title>Z Specification for the W3C Editor's Draft Core SHACL Semantics</title>
    <summary>  This article provides a formalization of the W3C Draft Core SHACL Semantics
specification using Z notation. This formalization exercise has identified a
number of quality issues in the draft. It has also established that the
recursive definitions in the draft are well-founded. Further formal validation
of the draft will require the use of an executable specification technology.
</summary>
    <author>
      <name>Arthur Ryman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">57 pages, Invited Expert contribution to the W3C RDF Data Shapes
  Working Group</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.00384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04315v1</id>
    <updated>2016-10-14T03:19:54Z</updated>
    <published>2016-10-14T03:19:54Z</published>
    <title>The multiset semantics of SPARQL patterns</title>
    <summary>  The paper determines the algebraic and logic structure of the multiset
semantics of the core patterns of SPARQL. We prove that the fragment formed by
AND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both,
the intuitive multiset relational algebra (projection, selection, natural join,
arithmetic union and except), and the multiset non-recursive Datalog with safe
negation.
</summary>
    <author>
      <name>Renzo Angles</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended and updated version of the paper accepted at the
  International Semantic Web Conference 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00036v1</id>
    <updated>2017-12-29T21:02:12Z</updated>
    <published>2017-12-29T21:02:12Z</published>
    <title>An introduction to Graph Data Management</title>
    <summary>  A graph database is a database where the data structures for the schema
and/or instances are modeled as a (labeled)(directed) graph or generalizations
of it, and where querying is expressed by graph-oriented operations and type
constructors. In this article we present the basic notions of graph databases,
give an historical overview of its main development, and study the main current
systems that implement them.
</summary>
    <author>
      <name>Renzo Angles</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-96193-4_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-96193-4_1" rel="related"/>
    <link href="http://arxiv.org/abs/1801.00036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03445v1</id>
    <updated>2018-09-10T16:33:11Z</updated>
    <published>2018-09-10T16:33:11Z</published>
    <title>A collection of database industrial techniques and optimization
  approaches of database operations</title>
    <summary>  Databases play an essential role in our society today. Databases are embedded
in sectors like corporations, institutions, and government organizations, among
others. These databases are used for our video and audio streaming platforms,
social gaming, finances, cloud storage, e-commerce, healthcare, economy, etc.
It is therefore imperative that we learn how to properly execute database
operations and efficiently implement methodologies so that we may optimize the
performance of databases.
</summary>
    <author>
      <name>Jasper Kyle Catapang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1439511</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1439511" rel="related"/>
    <link href="http://arxiv.org/abs/1809.03445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.08621v1</id>
    <updated>2019-03-20T17:07:11Z</updated>
    <published>2019-03-20T17:07:11Z</published>
    <title>Column2Vec: Structural Understanding via Distributed Representations of
  Database Schemas</title>
    <summary>  We present Column2Vec, a distributed representation of database columns based
on column metadata. Our distributed representation has several applications.
Using known names for groups of columns (i.e., a table name), we train a model
to generate an appropriate name for columns in an unnamed table. We demonstrate
the viability of our approach using schema information collected from open
source applications on GitHub.
</summary>
    <author>
      <name>Michael J. Mior</name>
    </author>
    <author>
      <name>Alexander G. Ororbia II</name>
    </author>
    <link href="http://arxiv.org/abs/1903.08621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.08621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.07693v3</id>
    <updated>2022-11-25T23:12:15Z</updated>
    <published>2019-04-16T14:09:51Z</published>
    <title>Frequent Itemset Mining using QUBO</title>
    <summary>  In this paper we propose a R-step approximation to solve frequent itemset
mining on quantum hardware like quantum annealing or QAOA. The idea is to
search for the set of items where the minimal 2-item frequency is maximal. This
can be represented as a maximum clique problem.
</summary>
    <author>
      <name>Jonas Nüßlein</name>
    </author>
    <link href="http://arxiv.org/abs/1904.07693v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07693v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04991v1</id>
    <updated>2019-10-11T06:41:09Z</updated>
    <published>2019-10-11T06:41:09Z</published>
    <title>Sub-query Fragmentation for Query Analysis and Data Caching in the
  Distributed Environment</title>
    <summary>  When data stores and users are distributed geographically, it is essential to
organize distributed data cache points at ideal locations to minimize data
transfers. To answer this, we are developing an adaptive distributed data
caching framework that can identify suitable data chunks to cache and move
across a network of community cache locations.
</summary>
    <author>
      <name>Santhilata Kuppili Venkata</name>
    </author>
    <author>
      <name>Katarzyna Musial</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 18 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.04991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08352v1</id>
    <updated>2019-12-16T21:28:44Z</updated>
    <published>2019-12-16T21:28:44Z</published>
    <title>Manifesto for Improved Foundations of Relational Model</title>
    <summary>  Normalized relations extended with inherited attributes can be more faithful
to reality and support logical navigation free queries, properties available at
present only through specific views. Adding inherited attributes can be
nonetheless always less procedural than to define any such views. Present
schemes should even typically suffice for relations with foreign keys.
Implementing extended relations on popular DBSs appears also simple. Relational
model should evolve accordingly, for benefit of likely millions of DBAs,
clients, developers.
</summary>
    <author>
      <name>Witold Litwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.08352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.08657v1</id>
    <updated>2020-08-19T20:15:32Z</updated>
    <published>2020-08-19T20:15:32Z</published>
    <title>LMFAO: An Engine for Batches of Group-By Aggregates</title>
    <summary>  LMFAO is an in-memory optimization and execution engine for large batches of
group-by aggregates over joins. Such database workloads capture the
data-intensive computation of a variety of data science applications.
  We demonstrate LMFAO for three popular models: ridge linear regression with
batch gradient descent, decision trees with CART, and clustering with Rk-means.
</summary>
    <author>
      <name>Maximilian Schleich</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.08657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.08657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.05142v1</id>
    <updated>2021-09-11T00:02:02Z</updated>
    <published>2021-09-11T00:02:02Z</published>
    <title>Discovering Technology Gaps using the IntSight Knowledge Navigator</title>
    <summary>  Knowledge analysis is an important application of knowledge graphs. In this
paper, we present a complex knowledge analysis problem that discovers the gaps
in the technology areas of interest to an organization. Our knowledge graph is
developed on a heterogeneous data management platform. The analysis combines
semantic search, graph analytics, and polystore query optimization.
</summary>
    <author>
      <name>Aurpon Gupta</name>
    </author>
    <author>
      <name>Subhasis Dasgupta</name>
    </author>
    <author>
      <name>Snehasis Sinha</name>
    </author>
    <author>
      <name>Amarnath Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/2109.05142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.05142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07278v1</id>
    <updated>2022-06-15T03:38:01Z</updated>
    <published>2022-06-15T03:38:01Z</published>
    <title>Nebula Graph: An open source distributed graph database</title>
    <summary>  This paper introduces the recent work of Nebula Graph, an open-source,
distributed, scalable, and native graph database. We present a system design
trade-off and a comprehensive overview of Nebula Graph internals, including
graph data models, partitioning strategies, secondary indexes, optimizer rules,
storage-side transactions, graph query languages, observability, graph
processing frameworks, and visualization tool-kits. In addition, three sets of
large-scale graph b
</summary>
    <author>
      <name>Min Wu</name>
    </author>
    <author>
      <name>Xinglu Yi</name>
    </author>
    <author>
      <name>Hui Yu</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Yujue Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2206.07278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.09198v1</id>
    <updated>2022-07-19T11:15:15Z</updated>
    <published>2022-07-19T11:15:15Z</published>
    <title>Consistent Query Answering for Expressive Constraints under
  Tuple-Deletion Semantics</title>
    <summary>  We study consistent query answering in relational databases. We consider an
expressive class of schema constraints that generalizes both tuple-generating
dependencies and equality-generating dependencies. We establish the complexity
of consistent query answering and repair checking under tuple-deletion
semantics for different fragments of the above constraint language. In
particular, we identify new subclasses of constraints in which the above
problems are tractable or even first-order rewritable.
</summary>
    <author>
      <name>Lorenzo Marconi</name>
    </author>
    <author>
      <name>Riccardo Rosati</name>
    </author>
    <link href="http://arxiv.org/abs/2207.09198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00782v1</id>
    <updated>2022-07-26T14:36:23Z</updated>
    <published>2022-07-26T14:36:23Z</published>
    <title>Tree edit distance for hierarchical data compatible with HMIL paradigm</title>
    <summary>  We define edit distance for hierarchically structured data compatible with
the hierarchical multi-instance learning paradigm. Example of such data is
dataset represented in JSON format where inner Array objects are interpreted as
unordered bags of elements. We prove correct analytical properties of the
defined distance.
</summary>
    <author>
      <name>Břetislav Šopík</name>
    </author>
    <author>
      <name>Tomáš Strenáčik</name>
    </author>
    <link href="http://arxiv.org/abs/2208.00782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.01613v1</id>
    <updated>2022-08-02T17:44:36Z</updated>
    <published>2022-08-02T17:44:36Z</published>
    <title>Principles of Query Visualization</title>
    <summary>  Query Visualization (QV) is the problem of transforming a given query into a
graphical representation that helps humans understand its meaning. This task is
notably different from designing a Visual Query Language (VQL) that helps a
user compose a query. This article discusses the principles of relational query
visualization and its potential for simplifying user interactions with
relational data.
</summary>
    <author>
      <name>Wolfgang Gatterbauer</name>
    </author>
    <author>
      <name>Cody Dunne</name>
    </author>
    <author>
      <name>H. V. Jagadish</name>
    </author>
    <author>
      <name>Mirek Riedewald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 12 figures, preprint for IEEE Data Engineering Bulletin</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.01613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09305v1</id>
    <updated>2022-08-19T12:33:03Z</updated>
    <published>2022-08-19T12:33:03Z</published>
    <title>Real and simulated CBM data interacting with an ESCAPE datalake</title>
    <summary>  Integration of the ESCAPE and CBM software environment. The ESCAPE datalake
are utilized by the CBM experiment for the storage, distribution and retrieval
of real SIS18 and simulated SIS100 particle physics data.
</summary>
    <author>
      <name>E. Clerkin</name>
    </author>
    <author>
      <name>P. -N. Kramp</name>
    </author>
    <author>
      <name>P. -A. Loizeau</name>
    </author>
    <author>
      <name>M. Szuba</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15120/GSI-2022-00599</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15120/GSI-2022-00599" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CBM Progress Report 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.09305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15711v1</id>
    <updated>2022-10-27T18:21:43Z</updated>
    <published>2022-10-27T18:21:43Z</published>
    <title>Re-looking at the View Update Problem</title>
    <summary>  Relational databases have always had a means for creating a pseudo-table,
called a view, defined by a query. Views are like tables in most ways, except
that they are read-only and cannot be updated. The problem of how to update
views has attracted a lot of attention in the 1980s but is unsolved.
  The best approach from that time was by Bancilhon and Spyratos. I use one of
their overlooked theorems and find a number of simple solutions for common
relational operators.
</summary>
    <author>
      <name>Terry Brennan</name>
    </author>
    <link href="http://arxiv.org/abs/2210.15711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03656v1</id>
    <updated>2023-01-09T19:56:47Z</updated>
    <published>2023-01-09T19:56:47Z</published>
    <title>Towards Multifaceted Human-Centered AI</title>
    <summary>  Human-centered AI workflows involve stakeholders with multiple roles
interacting with each other and automated agents to accomplish diverse tasks.
In this paper, we call for a holistic view when designing support mechanisms,
such as interaction paradigms, interfaces, and systems, for these multifaceted
workflows.
</summary>
    <author>
      <name>Sajjadur Rahman</name>
    </author>
    <author>
      <name>Hannah Kim</name>
    </author>
    <author>
      <name>Dan Zhang</name>
    </author>
    <author>
      <name>Estevam Hruschka</name>
    </author>
    <author>
      <name>Eser Kandogan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Human-Centered AI at NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09927v1</id>
    <updated>2023-02-20T11:46:50Z</updated>
    <published>2023-02-20T11:46:50Z</published>
    <title>NHtapDB: Native HTAP Databases</title>
    <summary>  Native database (1) provides a near-data machine learning framework to
facilitate generating real-time business insight, and predefined change
thresholds will trigger online training and deployment of new models, and (2)
offers a mixed-format store to guarantee the performance of HTAP workloads,
especially the hybrid workloads that consist of OLAP queries in-between online
transactions. We make rigorous test plans for native database with an enhanced
state-of-the-art HTAP benchmark.
</summary>
    <author>
      <name>Guoxin Kang</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Simin Chen</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.11748v1</id>
    <updated>2023-03-21T11:14:24Z</updated>
    <published>2023-03-21T11:14:24Z</published>
    <title>Database Technology Evolution</title>
    <summary>  This paper reviews suggestions for changes to database technology coming from
the work of many researchers, particularly those working with evolving big
data. We discuss new approaches to remote data access and standards that better
provide for durability and auditability in settings including business and
scientific computing. We propose ways in which the language standards could
evolve, with proof-of-concept implementations on Github.
</summary>
    <author>
      <name>Malcolm Crowe</name>
    </author>
    <author>
      <name>Fritz Laux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal On Advances in Software, volume 15, numbers
  3 and 4, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.11748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.11748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.13010v2</id>
    <updated>2023-07-05T21:33:33Z</updated>
    <published>2023-04-25T17:30:05Z</published>
    <title>Unstructured and structured data: Can we have the best of both worlds
  with large language models?</title>
    <summary>  This paper presents an opinion on the potential of using large language
models to query on both unstructured and structured data. It also outlines some
research challenges related to the topic of building question-answering systems
for both types of data.
</summary>
    <author>
      <name>Wang-Chiew Tan</name>
    </author>
    <link href="http://arxiv.org/abs/2304.13010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.13010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08766v1</id>
    <updated>2023-06-14T22:28:17Z</updated>
    <published>2023-06-14T22:28:17Z</published>
    <title>A Copernican Revolution in Data</title>
    <summary>  Half a century ago, Charles Bachman foresaw the significance and centrality
of data in the digital world. In this short paper, we delve into the evolution
of these ideas within the database community over the past decades. We believe
that this historical analysis helps deepen our comprehension of the fundamental
changes undergoing our discipline and provides insights into the future
trajectory of our field.
</summary>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <link href="http://arxiv.org/abs/2306.08766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.09374v1</id>
    <updated>2023-06-15T04:08:23Z</updated>
    <published>2023-06-15T04:08:23Z</published>
    <title>From Database Repairs to Causality in Databases and Beyond</title>
    <summary>  We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contributed paper associated to keynote presentation at BDA 2022. To
  appear in special issue of Springer TLDKS. arXiv admin note: substantial text
  overlap with arXiv:2106.10562</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.09374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.09374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13486v1</id>
    <updated>2023-06-23T13:02:19Z</updated>
    <published>2023-06-23T13:02:19Z</published>
    <title>Relational Playground: Teaching the Duality of Relational Algebra and
  SQL</title>
    <summary>  Students in introductory data management courses are often taught how to
write queries in SQL. This is a useful and practical skill, but it gives
limited insight into how queries are processed by relational database engines.
In contrast, relational algebra is a commonly used internal representation of
queries by database engines, but can be challenging for students to grasp. We
developed a tool we call Relational Playground for database students to explore
the connection between relational algebra and SQL.
</summary>
    <author>
      <name>Michael Mior</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3596673.3596978</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3596673.3596978" rel="related"/>
    <link href="http://arxiv.org/abs/2306.13486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.04601v1</id>
    <updated>2023-10-06T21:37:26Z</updated>
    <published>2023-10-06T21:37:26Z</published>
    <title>Eight Transaction Papers by Jim Gray</title>
    <summary>  This article is a summary of eight of Jim Gray's transaction papers. It was
written at the invitation of Pat Helland to be a chapter of a forthcoming book
in the ACM Turing Award winners' series, "Curiosity, Clarity, and Caring: How
Jim Gray's Passion for Learning, Teaching, and People Changed Computing."
</summary>
    <author>
      <name>Philip A. Bernstein</name>
    </author>
    <link href="http://arxiv.org/abs/2310.04601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.04601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13680v1</id>
    <updated>2023-12-11T07:28:46Z</updated>
    <published>2023-12-11T07:28:46Z</published>
    <title>A parallel algorithm for automated labeling of large time series</title>
    <summary>  This article presents the PaSTiLa algorithm for automated labeling of large
time series on a cluster with GPUs. The method automatically selects snippet
length values based on the new proposed criterion and allows to search for
patterns with high performance. Experiments showed high accuracy of pattern
search and the advantage of the method compared to analogues.
</summary>
    <author>
      <name>Andrey Goglachev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.13680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.07481v1</id>
    <updated>2024-03-12T10:14:33Z</updated>
    <published>2024-03-12T10:14:33Z</published>
    <title>Generalised Graph Grammars for Natural Language Processing</title>
    <summary>  This seminal paper proposes a new query language for graph matching and
rewriting overcoming {the declarative} limitation of Cypher while outperforming
{Neo4j} on graph matching and rewriting by at least one order of magnitude. We
exploited columnar databases (KnoBAB) to represent graphs using the Generalised
Semistructured Model.
</summary>
    <author>
      <name>Oliver Robert Fox</name>
    </author>
    <author>
      <name>Giacomo Bergami</name>
    </author>
    <link href="http://arxiv.org/abs/2403.07481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.07481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.06035v1</id>
    <updated>2024-04-09T05:46:36Z</updated>
    <published>2024-04-09T05:46:36Z</published>
    <title>PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs</title>
    <summary>  pm4py is a process mining library for Python implementing several process
mining (PM) artifacts and algorithms. It also offers methods to integrate PM
with large language models (LLMs). This paper examines how the current
paradigms of PM on LLM are implemented in pm4py, identifying challenges such as
privacy, hallucinations, and the context window limit.
</summary>
    <author>
      <name>Alessandro Berti</name>
    </author>
    <link href="http://arxiv.org/abs/2404.06035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.06035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.17434v1</id>
    <updated>2024-03-15T06:31:56Z</updated>
    <published>2024-03-15T06:31:56Z</published>
    <title>Efficient Search in Graph Edit Distance: Metric Search Trees vs. Brute
  Force Verification</title>
    <summary>  This report evaluates the efficiency of Graph Edit Distance (GED) computation
for graph similarity search, comparing Cascading Metric Trees (CMT) with
brute-force verification. Despite the anticipated advantages of CMT, our
findings indicate it does not consistently outperform brute-force methods in
speed. The study, based on graph data from PubChem, suggests that the
computational complexity of GED-based GSS remains a challenge.
</summary>
    <author>
      <name>Wenqi Marshall Guo</name>
    </author>
    <author>
      <name>Jeffrey Uhlmann</name>
    </author>
    <link href="http://arxiv.org/abs/2405.17434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.17434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.07502v1</id>
    <updated>2024-07-10T09:40:20Z</updated>
    <published>2024-07-10T09:40:20Z</published>
    <title>Understanding the Semantic SQL Transducer</title>
    <summary>  Nowadays we observe an evolving landscape of data management and analytics,
emphasising the significance of meticulous data management practices, semantic
modelling, and bridging business-technical divides, to optimise data
utilisation and enhance value from datasets in modern data environments. In
this paper we introduce and explain the basic formalisation of the Semantic SQL
Transducer, a well-founded but practical tool providing the materialised
lossless conceptual view of an arbitrary relational source data, contributing
to a knowledge-centric data stack.
</summary>
    <author>
      <name>Théo Abgrall</name>
    </author>
    <author>
      <name>Enrico Franconi</name>
    </author>
    <link href="http://arxiv.org/abs/2407.07502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.07502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.09566v1</id>
    <updated>2024-07-06T15:20:20Z</updated>
    <published>2024-07-06T15:20:20Z</published>
    <title>Implementing the draft Graph Query Language Standard</title>
    <summary>  The International Standards Organization (ISO) is developing a new standard
for Graph Query Language, with a particular focus on graph patterns with
repeating paths. The Linked Database Benchmark Council (LDBC) has developed
benchmarks to test proposed implementations. Their Financial Benchmark includes
a novel requirement for truncation of results. This paper presents an
open-source implementation of the benchmark workloads and truncation.
</summary>
    <author>
      <name>Malcolm Crowe</name>
    </author>
    <author>
      <name>Fritz Laux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IARIA, 2024. ISBN: 978-1-68558-138-1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.09566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.09566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.20431v1</id>
    <updated>2024-07-29T21:42:07Z</updated>
    <published>2024-07-29T21:42:07Z</published>
    <title>Limitations of Validity Intervals in Data Freshness Management</title>
    <summary>  In data-intensive real-time applications, such as smart transportation and
manufacturing, ensuring data freshness is essential, as using obsolete data can
lead to negative outcomes. Validity intervals serve as the standard means to
specify freshness requirements in real-time databases. In this paper, we bring
attention to significant drawbacks of validity intervals that have largely been
unnoticed and introduce a new definition of data freshness, while discussing
future research directions to address these limitations.
</summary>
    <author>
      <name>Kyoung-Don Kang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.20431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.20431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.05512v1</id>
    <updated>2024-09-09T11:10:45Z</updated>
    <published>2024-09-09T11:10:45Z</published>
    <title>DatAasee -- A Metadata-Lake as Metadata Catalog for a Virtual Data-Lake</title>
    <summary>  Metadata management for distributed data sources is a long-standing but
ever-growing problem. To counter this challenge in a research-data and
library-oriented setting, this work constructs a data architecture, derived
from the data-lake: the metadata-lake. A proof-of-concept implementation of
this proposed metadata system is presented and evaluated as well.
</summary>
    <author>
      <name>Christian Himpe</name>
    </author>
    <link href="http://arxiv.org/abs/2409.05512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.05512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.04101v1</id>
    <updated>2024-12-05T12:10:49Z</updated>
    <published>2024-12-05T12:10:49Z</published>
    <title>Database Theory + X: Database Visualization</title>
    <summary>  We draw a connection between data modeling and visualization, namely that a
visualization specification defines a mapping from database constraints to
visual representations of those constraints. Using this formalism, we show how
many visualization design decisions are, in fact, data modeling choices and
extend data visualization from single-dataset visualizations to database
visualization
</summary>
    <author>
      <name>Eugene Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2412.04101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.04101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18699v1</id>
    <updated>2024-12-24T23:17:57Z</updated>
    <published>2024-12-24T23:17:57Z</published>
    <title>Market Basket Analysis Using Rule-Based Algorithms and Data Mining
  Techniques</title>
    <summary>  The research identifies association rules that can inform marketing
strategies and enhance operational efficiency. A structured methodology is
applied to extract and interpret meaningful relationships within transactional
data, emphasizing their implications for managerial decision-making. By
demonstrating the potential of data mining to transform raw data into valuable
business insights, this paper provides a framework for using analytical tools
to improve customer engagement and competitive positioning.
</summary>
    <author>
      <name>Marina Kholod</name>
    </author>
    <author>
      <name>Nikita Mokrenko</name>
    </author>
    <link href="http://arxiv.org/abs/2412.18699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.11162v1</id>
    <updated>2025-01-19T20:29:50Z</updated>
    <published>2025-01-19T20:29:50Z</published>
    <title>Query Repairs</title>
    <summary>  We formalize and study the problem of repairing database queries based on
user feedback in the form of a collection of labeled examples. We propose a
framework based on the notion of a proximity pre-order, and we investigate and
compare query repairs for conjunctive queries (CQs) using different such
pre-orders. The proximity pre-orders we consider are based on query containment
and on distance metrics for CQs.
</summary>
    <author>
      <name>Balder ten Cate</name>
    </author>
    <author>
      <name>Phokion Kolaitis</name>
    </author>
    <author>
      <name>Carsten Lutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of ICDT 2025 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.11162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.11162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902017v2</id>
    <updated>1999-02-11T18:31:10Z</updated>
    <published>1999-02-09T04:02:55Z</published>
    <title>Not Available</title>
    <summary>  withdrawn by author
</summary>
    <author>
      <name>Not Available</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9902017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Not Available" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0717v1</id>
    <updated>2014-10-02T21:31:12Z</updated>
    <published>2014-10-02T21:31:12Z</published>
    <title>Fast, memory efficient low-rank approximation of SimRank</title>
    <summary>  SimRank is a well-known similarity measure between graph vertices.
  In this paper novel low-rank approximation of SimRank is proposed.
</summary>
    <author>
      <name>I. V. Oseledets</name>
    </author>
    <author>
      <name>G. V. Ovchinnikov</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.07022v1</id>
    <updated>2024-05-11T14:15:13Z</updated>
    <published>2024-05-11T14:15:13Z</published>
    <title>DTMamba : Dual Twin Mamba for Time Series Forecasting</title>
    <summary>  We utilized the Mamba model for time series data prediction tasks, and the
experimental results indicate that our model performs well.
</summary>
    <author>
      <name>Zexue Wu</name>
    </author>
    <author>
      <name>Yifeng Gong</name>
    </author>
    <author>
      <name>Aoqian Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.07022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.07022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809004v1</id>
    <updated>1998-09-02T00:49:25Z</updated>
    <published>1998-09-02T00:49:25Z</published>
    <title>Performance / Price Sort</title>
    <summary>  NTsort is an external sort on WindowsNT 5.0. It has minimal functionality but
excellent price performance. In particular, running on mail-order hardware it
can sort 1.5 GB for a penny. For commercially available sorts, Postman Sort
from Robert Ramey Software Development has elapsed time performance comparable
to NTsort, while using less processor time. It can sort 1.27 GB for a penny
(12.7 million records.) These sorts set new price-performance records. This
paper documents this and proposes that the PennySort benchmark be revised to
Performance/Price sort: a simple GB/$ sort metric based on a two-pass external
sort.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Joshua Coates</name>
    </author>
    <author>
      <name>Chris Nyberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original word file at:
  http://research.microsoft.com/~gray/PennySort.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.5;H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809005v1</id>
    <updated>1998-09-02T01:49:32Z</updated>
    <published>1998-09-02T01:49:32Z</published>
    <title>The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules
  of Thumb</title>
    <summary>  Simple economic and performance arguments suggest appropriate lifetimes for
main memory pages and suggest optimal page sizes. The fundamental tradeoffs are
the prices and bandwidths of RAMs and disks. The analysis indicates that with
today's technology, five minutes is a good lifetime for randomly accessed
pages, one minute is a good lifetime for two-pass sequentially accessed pages,
and 16 KB is a good size for index pages. These rules-of-thumb change in
predictable ways as technology ratios change. They also motivate the importance
of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Goetz Graefe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original document at:
  http://research.microsoft.com/~gray/5_min_rule_SIGMOD.doc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGMOD Record 26(4): 63-68 (1997)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809033v2</id>
    <updated>1998-09-25T16:12:20Z</updated>
    <published>1998-09-18T21:24:23Z</published>
    <title>Efficient Retrieval of Similar Time Sequences Using DFT</title>
    <summary>  We propose an improvement of the known DFT-based indexing technique for fast
retrieval of similar time sequences. We use the last few Fourier coefficients
in the distance computation without storing them in the index since every
coefficient at the end is the complex conjugate of a coefficient at the
beginning and as strong as its counterpart. We show analytically that this
observation can accelerate the search time of the index by more than a factor
of two. This result was confirmed by our experiments, which were carried out on
real stock prices and synthetic data.
</summary>
    <author>
      <name>Davood Rafiei</name>
    </author>
    <author>
      <name>Alberto Mendelzon</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 5th Intl. Conf. on Foundations of Data
  Organizations and Algorithms (FODO '98), November 1998, Kobe, Japan</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809033v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809033v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2;H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912015v1</id>
    <updated>1999-12-22T15:25:55Z</updated>
    <published>1999-12-22T15:25:55Z</published>
    <title>Comparative Analysis of Five XML Query Languages</title>
    <summary>  XML is becoming the most relevant new standard for data representation and
exchange on the WWW. Novel languages for extracting and restructuring the XML
content have been proposed, some in the tradition of database query languages
(i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query
language has yet been decided, but the discussion is ongoing within the World
Wide Web Consortium and within many academic institutions and Internet-related
major companies. We present a comparison of five, representative query
languages for XML, highlighting their common features and differences.
</summary>
    <author>
      <name>Angela Bonifati</name>
    </author>
    <author>
      <name>Stefano Ceri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TeX v3.1415, 17 pages, 6 figures, to be published in ACM Sigmod
  Record, March 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9912015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.2.3; I.7; I.7.1; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110032v1</id>
    <updated>2001-10-16T19:05:24Z</updated>
    <published>2001-10-16T19:05:24Z</published>
    <title>A logic-based approach to data integration</title>
    <summary>  An important aspect of data integration involves answering queries using
various resources rather than by accessing database relations. The process of
transforming a query from the database relations to the resources is often
referred to as query folding or answering queries using views, where the views
are the resources. We present a uniform approach that includes as special cases
much of the previous work on this subject. Our approach is logic-based using
resolution. We deal with integrity constraints, negation, and recursion also
within this framework.
</summary>
    <author>
      <name>J. Grant</name>
    </author>
    <author>
      <name>J. Minker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages, Accepted for publication in the Theory and Practice of
  Logic Programming</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2, I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112007v2</id>
    <updated>2002-11-30T01:15:08Z</updated>
    <published>2001-12-07T15:40:11Z</published>
    <title>A Tight Upper Bound on the Number of Candidate Patterns</title>
    <summary>  In the context of mining for frequent patterns using the standard levelwise
algorithm, the following question arises: given the current level and the
current set of frequent patterns, what is the maximal number of candidate
patterns that can be generated on the next level? We answer this question by
providing a tight upper bound, derived from a combinatorial result from the
sixties by Kruskal and Katona. Our result is useful to reduce the number of
database scans.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0112007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112011v2</id>
    <updated>2003-02-05T10:05:18Z</updated>
    <published>2001-12-10T15:50:47Z</published>
    <title>Interactive Constrained Association Rule Mining</title>
    <summary>  We investigate ways to support interactive mining sessions, in the setting of
association rule mining. In such sessions, users specify conditions (queries)
on the associations to be generated. Our approach is a combination of the
integration of querying conditions inside the mining phase, and the incremental
querying of already generated associations. We present several concrete
algorithms and compare their performance.
</summary>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary report on this work was presented at the Second
  International Conference on Knowledge Discovery and Data Mining (DaWaK 2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0112011v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112011v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202001v1</id>
    <updated>2002-02-01T05:00:24Z</updated>
    <published>2002-02-01T05:00:24Z</published>
    <title>The Deductive Database System LDL++</title>
    <summary>  This paper describes the LDL++ system and the research advances that have
enabled its design and development. We begin by discussing the new nonmonotonic
and nondeterministic constructs that extend the functionality of the LDL++
language, while preserving its model-theoretic and fixpoint semantics. Then, we
describe the execution model and the open architecture designed to support
these new constructs and to facilitate the integration with existing DBMSs and
applications. Finally, we describe the lessons learned by using LDL++ on
various tested applications, such as middleware and datamining.
</summary>
    <author>
      <name>Faiz Arni</name>
    </author>
    <author>
      <name>KayLiang Ong</name>
    </author>
    <author>
      <name>Shalom Tsur</name>
    </author>
    <author>
      <name>Haixun Wang</name>
    </author>
    <author>
      <name>Carlo Zaniolo</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0202001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202037v2</id>
    <updated>2003-10-13T12:29:04Z</updated>
    <published>2002-02-25T19:35:12Z</published>
    <title>Towards practical meta-querying</title>
    <summary>  We describe a meta-querying system for databases containing queries in
addition to ordinary data. In the context of such databases, a meta-query is a
query about queries. Representing stored queries in XML, and using the standard
XML manipulation language XSLT as a sublanguage, we show that just a few
features need to be added to SQL to turn it into a fully-fledged meta-query
language. The good news is that these features can be directly supported by
extensible database technology.
</summary>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <author>
      <name>Stijn Vansummeren</name>
    </author>
    <author>
      <name>Gottfried Vossen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2004.04.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2004.04.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Includes a new section "Experimental performance evaluation"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems, Volume 30, Issue 4 , June 2005, Pages 317-332</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0202037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203027v1</id>
    <updated>2002-03-27T03:35:12Z</updated>
    <published>2002-03-27T03:35:12Z</published>
    <title>The Algorithms of Updating Sequential Patterns</title>
    <summary>  Because the data being mined in the temporal database will evolve with time,
many researchers have focused on the incremental mining of frequent sequences
in temporal database. In this paper, we propose an algorithm called IUS, using
the frequent and negative border sequences in the original database for
incremental sequence mining. To deal with the case where some data need to be
updated from the original database, we present an algorithm called DUS to
maintain sequential patterns in the updated database. We also define the
negative border sequence threshold: Min_nbd_supp to control the number of
sequences in the negative border.
</summary>
    <author>
      <name>Qingguo Zheng</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <author>
      <name>Shilong Ma</name>
    </author>
    <author>
      <name>Weifeng Lv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Second SIAM Data mining2002: workshop HPDM</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0203027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0203027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204010v1</id>
    <updated>2002-04-05T22:24:33Z</updated>
    <published>2002-04-05T22:24:33Z</published>
    <title>On the Computational Complexity of Consistent Query Answers</title>
    <summary>  We consider here the problem of obtaining reliable, consistent information
from inconsistent databases -- databases that do not have to satisfy given
integrity constraints. We use the notion of consistent query answer -- a query
answer which is true in every (minimal) repair of the database. We provide a
complete classification of the computational complexity of consistent answers
to first-order queries w.r.t. functional dependencies and denial constraints.
We show how the complexity depends on the {\em type} of the constraints
considered, their {\em number}, and the {\em size} of the query. We obtain
several new PTIME cases, using new algorithms.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204038v1</id>
    <updated>2002-04-16T16:03:18Z</updated>
    <published>2002-04-16T16:03:18Z</published>
    <title>Technology For Information Engineering (TIE): A New Way of Storing,
  Retrieving and Analyzing Information</title>
    <summary>  The theoretical foundations of a new model and paradigm (called TIE) for data
storage and access are introduced. Associations between data elements are
stored in a single Matrix table, which is usually kept entirely in RAM for
quick access. The model ties together a very intuitive "guided" GUI to the
Matrix structure, allowing extremely easy complex searches through the data.
Although it is an "Associative Model" in that it stores the data associations
separately from the data itself, in contrast to other implementations of that
model TIE guides the user to only the available information ensuring that every
search is always fruitful. Very many diverse applications of the technology are
reviewed.
</summary>
    <author>
      <name>Jerzy Lewak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages. Introduces the theoretical foundation for associative
  information</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206023v1</id>
    <updated>2002-06-15T12:08:12Z</updated>
    <published>2002-06-15T12:08:12Z</published>
    <title>Relational Association Rules: getting WARMeR</title>
    <summary>  In recent years, the problem of association rule mining in transactional data
has been well studied. We propose to extend the discovery of classical
association rules to the discovery of association rules of conjunctive queries
in arbitrary relational data, inspired by the WARMR algorithm, developed by
Dehaspe and Toivonen, that discovers association rules over a limited set of
conjunctive queries. Conjunctive query evaluation in relational databases is
well understood, but still poses some great challenges when approached from a
discovery viewpoint in which patterns are generated and evaluated with respect
to some well defined search space and pruning operators.
</summary>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0206023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211042v1</id>
    <updated>2002-11-29T04:28:56Z</updated>
    <published>2002-11-29T04:28:56Z</published>
    <title>Database Repairs and Analytic Tableaux</title>
    <summary>  In this article, we characterize in terms of analytic tableaux the repairs of
inconsistent relational databases, that is databases that do not satisfy a
given set of integrity constraints. For this purpose we provide closing and
opening criteria for branches in tableaux that are built for database instances
and their integrity constraints. We use the tableaux based characterization as
a basis for consistent query answering, that is for retrieving from the
database answers to queries that are consistent wrt the integrity constraints.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Camilla Schwind</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of paper appeared in Proc. FOIKS02. Submitted by
  invitation to AMAI journal. Uses packages: llncs.cls, amssymb.sty,
  parsetree.sty. 31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2; F4; I2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212017v1</id>
    <updated>2002-12-09T17:29:12Z</updated>
    <published>2002-12-09T17:29:12Z</published>
    <title>Classes of Spatiotemporal Objects and Their Closure Properties</title>
    <summary>  We present a data model for spatio-temporal databases. In this model
spatio-temporal data is represented as a finite union of objects described by
means of a spatial reference object, a temporal object and a geometric
transformation function that determines the change or movement of the reference
object in time.
  We define a number of practically relevant classes of spatio-temporal
objects, and give complete results concerning closure under Boolean set
operators for these classes. Since only few classes are closed under all set
operators, we suggest an extension of the model, which leads to better closure
properties, and therefore increased practical applicability. We also discuss a
normal form for this extended data model.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Sofie Haesevoets</name>
    </author>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <author>
      <name>Peter Revesz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0212017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301009v1</id>
    <updated>2003-01-13T05:34:39Z</updated>
    <published>2003-01-13T05:34:39Z</published>
    <title>A Script Language for Data Integration in Database</title>
    <summary>  A Script Language in this paper is designed to transform the original data
into the target data by the computing formula. The Script Language can be
translated into the corresponding SQL Language, and the computation is finally
implemented by the first type of dynamic SQL. The Script Language has the
operations of insert, update, delete, union, intersect, and minus for the table
in the database.The Script Language is edited by a text file and you can easily
modify the computing formula in the text file to deal with the situations when
the computing formula have been changed. So you only need modify the text of
the script language, but needn't change the programs that have complied.
</summary>
    <author>
      <name>Qingguo Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0301009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306066v1</id>
    <updated>2003-06-13T14:33:53Z</updated>
    <published>2003-06-13T14:33:53Z</published>
    <title>The COMPASS Event Store in 2002</title>
    <summary>  COMPASS, the fixed-target experiment at CERN studying the structure of the
nucleon and spectroscopy, collected over 260 TB during summer 2002 run. All
these data, together with reconstructed events information, were put from the
beginning in a database infrastructure based on Objectivity/DB and on the
hierarchical storage manager CASTOR. The experience in the usage of the
database is reviewed and the evolution of the system outlined.
</summary>
    <author>
      <name>Venicio Duic</name>
    </author>
    <author>
      <name>Massimo Lamanna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNS.2004.832645</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNS.2004.832645" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 conference: "Computing in High Energy and Nuclear
  Physics" (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT011</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307073v1</id>
    <updated>2003-07-31T11:18:51Z</updated>
    <published>2003-07-31T11:18:51Z</published>
    <title>Search and Navigation in Relational Databases</title>
    <summary>  We present a new application for keyword search within relational databases,
which uses a novel algorithm to solve the join discovery problem by finding
Memex-like trails through the graph of foreign key dependencies. It differs
from previous efforts in the algorithms used, in the presentation mechanism and
in the use of primary-key only database queries at query-time to maintain a
fast response for users. We present examples using the DBLP data set.
</summary>
    <author>
      <name>Richard Wheeldon</name>
    </author>
    <author>
      <name>Mark Levene</name>
    </author>
    <author>
      <name>Kevin Keenoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3;H.4;H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308014v2</id>
    <updated>2004-03-03T13:43:33Z</updated>
    <published>2003-08-06T14:12:11Z</published>
    <title>On the expressive power of semijoin queries</title>
    <summary>  The semijoin algebra is the variant of the relational algebra obtained by
replacing the join operator by the semijoin operator. We provide an
Ehrenfeucht-Fraiss\'{e} game, characterizing the discerning power of the
semijoin algebra. This game gives a method for showing that queries are not
expressible in the semijoin algebra.
</summary>
    <author>
      <name>Dirk Leinders</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Belgium</arxiv:affiliation>
    </author>
    <author>
      <name>Jerzy Tyszkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Warsaw University</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Belgium</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, to appear in Information Processing Letters; added results
  that more clearly delineate the expressive power of SA, added a section that
  discusses the impact of order on the expressive power of SA, deemphasized the
  discussion on the relationship with GF</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312042v1</id>
    <updated>2003-12-18T17:43:43Z</updated>
    <published>2003-12-18T17:43:43Z</published>
    <title>Declarative Semantics for Active Rules</title>
    <summary>  In this paper we analyze declarative deterministic and non-deterministic
semantics for active rules. In particular we consider several (partial) stable
model semantics, previously defined for deductive rules, such as well-founded,
max deterministic, unique total stable model, total stable model, and maximal
stable model semantics. The semantics of an active program AP is given by first
rewriting it into a deductive program P, then computing a model M defining the
declarative semantics of P and, finally, applying `consistent' updates
contained in M to the source database. The framework we propose permits a
natural integration of deductive and active rules and can also be applied to
queries with function symbols or to queries over infinite databases.
</summary>
    <author>
      <name>Sergio Flesca</name>
    </author>
    <author>
      <name>Sergio Greco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, 1(1): 43-69, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.3.1; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401014v1</id>
    <updated>2004-01-18T03:09:04Z</updated>
    <published>2004-01-18T03:09:04Z</published>
    <title>Nested Intervals with Farey Fractions</title>
    <summary>  Relational Databases are universally conceived as an advance over their
predecessors Network and Hierarchical models. Superior in every querying
respect, they turned out to be surprisingly incomplete when modeling transitive
dependencies. Almost every couple of months a question how to model a tree in
the database surfaces at comp.database.theory newsgroup. This article completes
a series of articles exploring Nested Intervals Model. Previous articles
introduced tree encoding with Binary Rational Numbers. However, binary encoding
grows exponentially, both in breadth and in depth. In this article, we'll
leverage Farey fractions in order to overcome this problem. We'll also
demonstrate that our implementation scales to a tree with 1M nodes.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401015v1</id>
    <updated>2004-01-20T19:26:35Z</updated>
    <published>2004-01-20T19:26:35Z</published>
    <title>Query Answering in Peer-to-Peer Data Exchange Systems</title>
    <summary>  The problem of answering queries posed to a peer who is a member of a
peer-to-peer data exchange system is studied. The answers have to be consistent
wrt to both the local semantic constraints and the data exchange constraints
with other peers; and must also respect certain trust relationships between
peers. A semantics for peer consistent answers under exchange constraints and
trust relationships is introduced and some techniques for obtaining those
answers are presented.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Loreto Bravo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;F.4.1;I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402016v1</id>
    <updated>2004-02-09T19:13:17Z</updated>
    <published>2004-02-09T19:13:17Z</published>
    <title>Perspects in astrophysical databases</title>
    <summary>  Astrophysics has become a domain extremely rich of scientific data. Data
mining tools are needed for information extraction from such large datasets.
This asks for an approach to data management emphasizing the efficiency and
simplicity of data access; efficiency is obtained using multidimensional access
methods and simplicity is achieved by properly handling metadata. Moreover,
clustering and classification techniques on large datasets pose additional
requirements in terms of computation and memory scalability and
interpretability of results. In this study we review some possible solutions.
</summary>
    <author>
      <name>M. Frailis</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>V. Roberto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2004.02.024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2004.02.024" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A338 (2004) 54-59</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0402016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403014v2</id>
    <updated>2004-03-12T14:33:23Z</updated>
    <published>2004-03-11T06:30:30Z</published>
    <title>Search Efficiency in Indexing Structures for Similarity Searching</title>
    <summary>  Similarity searching finds application in a wide variety of domains including
multilingual databases, computational biology, pattern recognition and text
retrieval. Similarity is measured in terms of a distance function, edit
distance, in general metric spaces, which is expensive to compute. Indexing
techniques can be used reduce the number of distance computations. We present
an analysis of various existing similarity indexing structures for the same.
The performance obtained using the index structures studied was found to be
unsatisfactory . We propose an indexing technique that combines the features of
clustering with M tree(MTB) and the results indicate that this gives better
performance.
</summary>
    <author>
      <name>Girish Motwani</name>
    </author>
    <author>
      <name>Sandhya G. Nair</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0403014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406004v1</id>
    <updated>2004-06-02T12:55:04Z</updated>
    <published>2004-06-02T12:55:04Z</published>
    <title>Application of Business Intelligence In Banks (Pakistan)</title>
    <summary>  The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).
</summary>
    <author>
      <name>Muhammad Nadeem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Szabist</arxiv:affiliation>
    </author>
    <author>
      <name>Syed Ata Hussain Jaffri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Szabist</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406060v1</id>
    <updated>2004-06-29T16:09:10Z</updated>
    <published>2004-06-29T16:09:10Z</published>
    <title>Well-Definedness and Semantic Type-Checking in the Nested Relational
  Calculus and XQuery</title>
    <summary>  Two natural decision problems regarding the XML query language XQuery are
well-definedness and semantic type-checking. We study these problems in the
setting of a relational fragment of XQuery. We show that well-definedness and
semantic type-checking are undecidable, even in the positive-existential case.
Nevertheless, for a ``pure'' variant of XQuery, in which no identification is
made between an item and the singleton containing that item, the problems
become decidable. We also consider the analogous problems in the setting of the
nested relational calculus.
</summary>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <author>
      <name>Dirk Van Gucht</name>
    </author>
    <author>
      <name>Stijn Vansummeren</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0406060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407007v1</id>
    <updated>2004-07-02T15:44:32Z</updated>
    <published>2004-07-02T15:44:32Z</published>
    <title>The semijoin algebra and the guarded fragment</title>
    <summary>  The semijoin algebra is the variant of the relational algebra obtained by
replacing the join operator by the semijoin operator. We discuss some
interesting connections between the semijoin algebra and the guarded fragment
of first-order logic. We also provide an Ehrenfeucht-Fraisse game,
characterizing the discerning power of the semijoin algebra. This game gives a
method for showing that certain queries are not expressible in the semijoin
algebra.
</summary>
    <author>
      <name>Dirk Leinders</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Diepenbeek, Belgium</arxiv:affiliation>
    </author>
    <author>
      <name>Jerzy Tyszkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Informatics, Warsaw University, Warsaw, Poland</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Diepenbeek, Belgium</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0407007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3;F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409020v1</id>
    <updated>2004-09-11T11:02:30Z</updated>
    <published>2004-09-11T11:02:30Z</published>
    <title>A Generalized Disjunctive Paraconsistent Data Model for Negative and
  Disjunctive Information</title>
    <summary>  This paper presents a generalization of the disjunctive paraconsistent
relational data model in which disjunctive positive and negative information
can be represented explicitly and manipulated. There are situations where the
closed world assumption to infer negative facts is not valid or undesirable and
there is a need to represent and reason with negation explicitly. We consider
explicit disjunctive negation in the context of disjunctive databases as there
is an interesting interplay between these two types of information. Generalized
disjunctive paraconsistent relation is introduced as the main structure in this
model. The relational algebra is appropriately generalized to work on
generalized disjunctive paraconsistent relations and their correctness is
established.
</summary>
    <author>
      <name>Haibin Wang</name>
    </author>
    <author>
      <name>Yuanchun He</name>
    </author>
    <author>
      <name>Rajshekhar Sunderraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0409020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410053v1</id>
    <updated>2004-10-20T08:50:22Z</updated>
    <published>2004-10-20T08:50:22Z</published>
    <title>An Extended Generalized Disjunctive Paraconsistent Data Model for
  Disjunctive Information</title>
    <summary>  This paper presents an extension of generalized disjunctive paraconsistent
relational data model in which pure disjunctive positive and negative
information as well as mixed disjunctive positive and negative information can
be represented explicitly and manipulated. We consider explicit mixed
disjunctive information in the context of disjunctive databases as there is an
interesting interplay between these two types of information. Extended
generalized disjunctive paraconsistent relation is introduced as the main
structure in this model. The relational algebra is appropriately generalized to
work on extended generalized disjunctive paraconsistent relations and their
correctness is established.
</summary>
    <author>
      <name>Haibin Wang</name>
    </author>
    <author>
      <name>Hao Tian</name>
    </author>
    <author>
      <name>Rajshekhar Sunderraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502009v1</id>
    <updated>2005-02-02T03:26:40Z</updated>
    <published>2005-02-02T03:26:40Z</published>
    <title>Performance Considerations for Gigabyte per Second Transcontinental
  Disk-to-Disk File Transfers</title>
    <summary>  Moving data from CERN to Pasadena at a gigabyte per second using the next
generation Internet requires good networking and good disk IO. Ten Gbps
Ethernet and OC192 links are in place, so now it is simply a matter of
programming. This report describes our preliminary work and measurements in
configuring the disk subsystem for this effort. Using 24 SATA disks at each
endpoint we are able to locally read and write an NTFS volume is striped across
24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher
performance and higher-capacity systems deliver up to 3.5 GBps.
</summary>
    <author>
      <name>Peter Kukol</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502011v1</id>
    <updated>2005-02-02T04:40:55Z</updated>
    <published>2005-02-02T04:40:55Z</published>
    <title>Where the Rubber Meets the Sky: Bridging the Gap between Databases and
  Science</title>
    <summary>  Scientists in all domains face a data avalanche - both from better
instruments and from improved simulations. We believe that computer science
tools and computer scientists are in a position to help all the sciences by
building tools and developing techniques to manage, analyze, and visualize
peta-scale scientific information. This article is summarizes our experiences
over the last seven years trying to bridge the gap between database technology
and the needs of the astronomy community in building the World-Wide Telescope.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Data Engineering Bulletin, Vol 27.4, Dec. 2004, pp. 3-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0502011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503081v1</id>
    <updated>2005-03-29T13:31:01Z</updated>
    <published>2005-03-29T13:31:01Z</published>
    <title>An Optimization Model for Outlier Detection in Categorical Data</title>
    <summary>  The task of outlier detection is to find small groups of data objects that
are exceptional when compared with rest large amount of data. Detection of such
outliers is important for many applications such as fraud detection and
customer migration. Most existing methods are designed for numeric data. They
will encounter problems with real-life applications that contain categorical
data. In this paper, we formally define the problem of outlier detection in
categorical data as an optimization problem from a global viewpoint. Moreover,
we present a local-search heuristic based algorithm for efficiently finding
feasible solutions. Experimental results on real datasets and large synthetic
datasets demonstrate the superiority of our model and algorithm.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503092v1</id>
    <updated>2005-03-31T19:43:31Z</updated>
    <published>2005-03-31T19:43:31Z</published>
    <title>Monotonic and Nonmonotonic Preference Revision</title>
    <summary>  We study here preference revision, considering both the monotonic case where
the original preferences are preserved and the nonmonotonic case where the new
preferences may override the original ones. We use a relational framework in
which preferences are represented using binary relations (not necessarily
finite). We identify several classes of revisions that preserve order axioms,
for example the axioms of strict partial or weak orders. We consider
applications of our results to preference querying in relational databases.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Joyce Song</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0503092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505002v1</id>
    <updated>2005-04-29T22:28:31Z</updated>
    <published>2005-04-29T22:28:31Z</published>
    <title>Tight Lower Bounds for Query Processing on Streaming and External Memory
  Data</title>
    <summary>  We study a clean machine model for external memory and stream processing. We
show that the number of scans of the external data induces a strict hierarchy
(as long as work space is sufficiently small, e.g., polylogarithmic in the size
of the input). We also show that neither joins nor sorting are feasible if the
product of the number $r(n)$ of scans of the external memory and the size
$s(n)$ of the internal memory buffers is sufficiently small, e.g., of size
$o(\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath
evaluation and filtering.
</summary>
    <author>
      <name>Martin Grohe</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures, to appear in Proc. ICALP 2005; extended version
  with appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506063v1</id>
    <updated>2005-06-14T23:10:47Z</updated>
    <published>2005-06-14T23:10:47Z</published>
    <title>Priority-Based Conflict Resolution in Inconsistent Relational Databases</title>
    <summary>  We study here the impact of priorities on conflict resolution in inconsistent
relational databases. We extend the framework of repairs and consistent query
answers. We propose a set of postulates that an extended framework should
satisfy and consider two instantiations of the framework: (locally preferred)
l-repairs and (globally preferred) g-repairs. We study the relationships
between them and the impact each notion of repair has on the computational
complexity of repair checking and consistent query answers.
</summary>
    <author>
      <name>Slawomir Staworko</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0506063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605124v1</id>
    <updated>2006-05-26T16:41:15Z</updated>
    <published>2006-05-26T16:41:15Z</published>
    <title>Semantics and Complexity of SPARQL</title>
    <summary>  SPARQL is the W3C candidate recommendation query language for RDF. In this
paper we address systematically the formal study of SPARQL, concentrating in
its graph pattern facility. We consider for this study a fragment without
literals and a simple version of filters which encompasses all the main issues
yet is simple to formalize. We provide a compositional semantics, prove there
are normal forms, prove complexity bounds, among others that the evaluation of
SPARQL patterns is PSPACE-complete, compare our semantics to an alternative
operational semantics, give simple and natural conditions when both semantics
coincide and discuss optimizations procedures.
</summary>
    <author>
      <name>Jorge Perez</name>
    </author>
    <author>
      <name>Marcelo Arenas</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0605124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605127v1</id>
    <updated>2006-05-27T00:51:46Z</updated>
    <published>2006-05-27T00:51:46Z</published>
    <title>Analyzing Large Collections of Electronic Text Using OLAP</title>
    <summary>  Computer-assisted reading and analysis of text has various applications in
the humanities and social sciences. The increasing size of many electronic text
archives has the advantage of a more complete analysis but the disadvantage of
taking longer to obtain results. On-Line Analytical Processing is a method used
to store and quickly analyze multidimensional data. By storing text analysis
information in an OLAP system, a user can obtain solutions to inquiries in a
matter of seconds as opposed to minutes, hours, or even days. This analysis is
user-driven allowing various users the freedom to pursue their own direction of
research.
</summary>
    <author>
      <name>Steven Keith</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0605127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606094v1</id>
    <updated>2006-06-22T09:46:48Z</updated>
    <published>2006-06-22T09:46:48Z</published>
    <title>On Typechecking Top-Down XML Tranformations: Fixed Input or Output
  Schemas</title>
    <summary>  Typechecking consists of statically verifying whether the output of an XML
transformation always conforms to an output type for documents satisfying a
given input type. In this general setting, both the input and output schema as
well as the transformation are part of the input for the problem. However,
scenarios where the input or output schema can be considered to be fixed, are
quite common in practice. In the present work, we investigate the computational
complexity of the typechecking problem in the latter setting.
</summary>
    <author>
      <name>Wim Martens</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Marc Gyssens</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0606094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607013v1</id>
    <updated>2006-07-05T18:37:12Z</updated>
    <published>2006-07-05T18:37:12Z</published>
    <title>Database Querying under Changing Preferences</title>
    <summary>  We present here a formal foundation for an iterative and incremental approach
to constructing and evaluating preference queries. Our main focus is on query
modification: a query transformation approach which works by revising the
preference relation in the query. We provide a detailed analysis of the cases
where the order-theoretic properties of the preference relation are preserved
by the revision. We consider a number of different revision operators: union,
prioritized and Pareto composition. We also formulate algebraic laws that
enable incremental evaluation of preference queries. Finally, we consider two
variations of the basic framework: finite restrictions of preference relations
and weak-order extensions of strict partial order preference relations.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610020v2</id>
    <updated>2006-10-06T23:03:01Z</updated>
    <published>2006-10-04T23:21:11Z</published>
    <title>XString: XML as a String</title>
    <summary>  Extensible markup language (XML) is a technology that has been much hyped, so
that XML has become an industry buzzword. Behind the hype is a powerful
technology for data representation in a platform independent manner. As a text
document, however, XML suffers from being too bloated, and requires an XML
parser to access and manipulate it. XString is an encoding method for XML, in
essence, a markup language's markup language. XString gives the benefit of
compressing XML, and allows for easy manipulation and processing of XML source
as a very long string.
</summary>
    <author>
      <name>William F. Gilreath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27-pages, 2-tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612114v1</id>
    <updated>2006-12-21T22:29:07Z</updated>
    <published>2006-12-21T22:29:07Z</published>
    <title>Demaq: A Foundation for Declarative XML Message Processing</title>
    <summary>  This paper gives an overview of Demaq, an XML message processing system
operating on the foundation of transactional XML message queues. We focus on
the syntax and semantics of its fully declarative, rule-based application
language and demonstrate our message-based programming paradigm in the context
of a case study. Further, we discuss optimization opportunities for executing
Demaq programs.
</summary>
    <author>
      <name>Alexander Böhm</name>
    </author>
    <author>
      <name>Carl-Christian Kanne</name>
    </author>
    <author>
      <name>Guido Moerkotte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701157v1</id>
    <updated>2007-01-25T22:53:48Z</updated>
    <published>2007-01-25T22:53:48Z</published>
    <title>A Critique of ANSI SQL Isolation Levels</title>
    <summary>  ANSI SQL-92 defines Isolation Levels in terms of phenomena: Dirty Reads,
Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and
the ANSI SQL definitions fail to characterize several popular isolation levels,
including the standard locking implementations of the levels. Investigating the
ambiguities of the phenomena leads to clearer definitions; in addition new
phenomena that better characterize isolation types are introduced. An important
multiversion isolation type, Snapshot Isolation, is defined.
</summary>
    <author>
      <name>Hal Berenson</name>
    </author>
    <author>
      <name>Phil Bernstein</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Jim Melton</name>
    </author>
    <author>
      <name>Elizabeth O'Neil</name>
    </author>
    <author>
      <name>Patrick O'Neil</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM SIGMOD 95, pp. 1-10, San Jose CA, June 1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701160v1</id>
    <updated>2007-01-25T23:05:40Z</updated>
    <published>2007-01-25T23:05:40Z</published>
    <title>Supporting Finite Element Analysis with a Relational Database Backend,
  Part II: Database Design and Access</title>
    <summary>  This is Part II of a three article series on using databases for Finite
Element Analysis (FEA). It discusses (1) db design, (2) data loading, (3)
typical use cases during grid building, (4) typical use cases during simulation
(get and put), (5) typical use cases during analysis (also done in Part III)
and some performance measures of these cases. It argues that using a database
is simpler to implement than custom data schemas, has better performance
because it can use data parallelism, and better supports FEA modularity and
tool evolution because database schema evolution, data independence, and
self-defining data.
</summary>
    <author>
      <name>Gerd Heber</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701163v1</id>
    <updated>2007-01-26T00:00:37Z</updated>
    <published>2007-01-26T00:00:37Z</published>
    <title>Using Table Valued Functions in SQL Server 2005 To Implement a Spatial
  Data Library</title>
    <summary>  This article explains how to add spatial search functions (point-near-point
and point in polygon) to Microsoft SQL Server 2005 using C# and table-valued
functions. It is possible to use this library to add spatial search to your
application without writing any special code. The library implements the
public-domain C# Hierarchical Triangular Mesh (HTM) algorithms from Johns
Hopkins University. That C# library is connected to SQL Server 2005 via a set
of scalar-valued and table-valued functions. These functions act as a spatial
index.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex Szalay</name>
    </author>
    <author>
      <name>Gyorgy Fekete</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701171v1</id>
    <updated>2007-01-26T05:11:20Z</updated>
    <published>2007-01-26T05:11:20Z</published>
    <title>The Zones Algorithm for Finding Points-Near-a-Point or Cross-Matching
  Spatial Datasets</title>
    <summary>  Zones index an N-dimensional Euclidian or metric space to efficiently support
points-near-a-point queries either within a dataset or between two datasets.
The approach uses relational algebra and the B-Tree mechanism found in almost
all relational database systems. Hence, the Zones Algorithm gives a
portable-relational implementation of points-near-point, spatial cross-match,
and self-match queries. This article corrects some mistakes in an earlier
article we wrote on the Zones Algorithm and describes some algorithmic
improvements. The Appendix includes an implementation of point-near-point,
self-match, and cross-match using the USGS city and stream gauge database.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Maria A. Nieto-Santisteban</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702012v1</id>
    <updated>2007-02-01T20:52:13Z</updated>
    <published>2007-02-01T20:52:13Z</published>
    <title>Plagiarism Detection in arXiv</title>
    <summary>  We describe a large-scale application of methods for finding plagiarism in
research document collections. The methods are applied to a collection of
284,834 documents collected by arXiv.org over a 14 year period, covering a few
different research disciplines. The methodology efficiently detects a variety
of problematic author behaviors, and heuristics are developed to reduce the
number of false positives. The methods are also efficient enough to implement
as a real-time submission screen for a collection many times larger.
</summary>
    <author>
      <name>Daria Sorokina</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Simeon Warner</name>
    </author>
    <author>
      <name>Paul Ginsparg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDM.2006.126</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDM.2006.126" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sixth International Conference on Data Mining (ICDM'06), Dec 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702075v1</id>
    <updated>2007-02-13T11:13:29Z</updated>
    <published>2007-02-13T11:13:29Z</published>
    <title>Firebird Database Backup by Serialized Database Table Dump</title>
    <summary>  This paper presents a simple data dump and load utility for Firebird
databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load,
for dumping and loading respectively, retrieves each database table using
kinterbasdb and serializes the data using marshal module. This utility has two
advantages over the standard Firebird database backup utility, gbak. Firstly,
it is able to backup and restore single database tables which might help to
recover corrupted databases. Secondly, the output is in text-coded format (from
marshal module) making it more resilient than a compressed text backup, as in
the case of using gbak.
</summary>
    <author>
      <name>Maurice HT Ling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ling, Maurice HT. 2007. Firebird Database Backup by Serialized
  Database Table Dump. The Python Papers 2 (1): 10-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7; E.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1288v1</id>
    <updated>2007-07-09T15:52:02Z</updated>
    <published>2007-07-09T15:52:02Z</published>
    <title>Espaces de représentation multidimensionnels dédiés à la
  visualisation</title>
    <summary>  In decision-support systems, the visual component is important for On Line
Analysis Processing (OLAP). In this paper, we propose a new approach that faces
the visualization problem due to data sparsity. We use the results of a
Multiple Correspondence Analysis (MCA) to reduce the negative effect of
sparsity by organizing differently data cube cells. Our approach does not
reduce sparsity, however it tries to build relevant representation spaces where
facts are efficiently gathered. In order to evaluate our approach, we propose
an homogeneity criterion based on geometric neighborhood of cells. The obtained
experimental results have shown the efficiency of our method.
</summary>
    <author>
      <name>Riadh Ben Messaoud</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Cécile Favre</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1304v1</id>
    <updated>2007-07-09T16:58:14Z</updated>
    <published>2007-07-09T16:58:14Z</published>
    <title>Un index de jointure pour les entrepôts de données XML</title>
    <summary>  XML data warehouses form an interesting basis for decision-support
applications that exploit heterogeneous data from multiple sources. However,
XML-native database systems currently bear limited performances and it is
necessary to research ways to optimize them. In this paper, we propose a new
index that is specifically adapted to the multidimensional architecture of XML
warehouses and eliminates join operations, while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our index, even when queries are complex.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1306v1</id>
    <updated>2007-07-09T17:23:31Z</updated>
    <published>2007-07-09T17:23:31Z</published>
    <title>Sélection simultanée d'index et de vues matérialisées</title>
    <summary>  Indices and materialized views are physical structures that accelerate data
access in data warehouses. However, these data structures generate some
maintenance overhead. They also share the same storage space. The existing
studies about index and materialized view selection consider these structures
separately. In this paper, we adopt the opposite stance and couple index and
materialized view selection to take into account the interactions between them
and achieve an efficient storage space sharing. We develop cost models that
evaluate the respective benefit of indexing and view materialization. These
cost models are then exploited by a greedy algorithm to select a relevant
configuration of indices and materialized views. Experimental results show that
our strategy performs better than the independent selection of indices and
materialized views.
</summary>
    <author>
      <name>Nora Maiz</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2717v1</id>
    <updated>2007-08-20T20:08:53Z</updated>
    <published>2007-08-20T20:08:53Z</published>
    <title>Aggregation Languages for Moving Object and Places of Interest Data</title>
    <summary>  We address aggregate queries over GIS data and moving object data, where
non-spatial data are stored in a data warehouse. We propose a formal data model
and query language to express complex aggregate queries. Next, we study the
compression of trajectory data, produced by moving objects, using the notions
of stops and moves. We show that stops and moves are expressible in our query
language and we consider a fragment of this language, consisting of regular
expressions to talk about temporally ordered sequences of stops and moves. This
fragment can be used to efficiently express data mining and pattern matching
tasks over trajectory data.
</summary>
    <author>
      <name>Leticia Gomez</name>
    </author>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.1404v1</id>
    <updated>2007-10-07T08:22:53Z</updated>
    <published>2007-10-07T08:22:53Z</published>
    <title>Performance Comparison of Persistence Frameworks</title>
    <summary>  One of the essential and most complex components in the software development
process is the database. The complexity increases when the "orientation" of the
interacting components differs. A persistence framework moves the program data
in its most natural form to and from a permanent data store, the database. Thus
a persistence framework manages the database and the mapping between the
database and the objects. This paper compares the performance of two
persistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking
database. The performance of both of these tools in single and multi-user
environments are evaluated.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <author>
      <name>Ashwin a K</name>
    </author>
    <link href="http://arxiv.org/abs/0710.1404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.1404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2604v1</id>
    <updated>2007-10-13T11:47:14Z</updated>
    <published>2007-10-13T11:47:14Z</published>
    <title>Efficient Skyline Querying with Variable User Preferences on Nominal
  Attributes</title>
    <summary>  Current skyline evaluation techniques assume a fixed ordering on the
attributes. However, dynamic preferences on nominal attributes are more
realistic in known applications. In order to generate online response for any
such preference issued by a user, we propose two methods of different
characteristics. The first one is a semi-materialization method and the second
is an adaptive SFS method. Finally, we conduct experiments to show the
efficiency of our proposed algorithms.
</summary>
    <author>
      <name>Raymond Chi-Wing Wong</name>
    </author>
    <author>
      <name>Ada Wai-chee Fu</name>
    </author>
    <author>
      <name>Jian Pei</name>
    </author>
    <author>
      <name>Yip Sing Ho</name>
    </author>
    <author>
      <name>Tai Wong</name>
    </author>
    <author>
      <name>Yubao Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.0131v1</id>
    <updated>2007-12-30T14:29:17Z</updated>
    <published>2007-12-30T14:29:17Z</published>
    <title>Two-Level Concept-Oriented Data Model</title>
    <summary>  In this paper we describe a new approach to data modelling called the
concept-oriented model (CoM). This model is based on the formalism of nested
ordered sets which uses inclusion relation to produce hierarchical structure of
sets and ordering relation to produce multi-dimensional structure among its
elements. Nested ordered set is defined as an ordered set where an each element
can be itself an ordered set. Ordering relation in CoM is used to define data
semantics and operations with data such as projection and de-projection. This
data model can be applied to very different problems and the paper describes
some its uses such grouping with aggregation and multi-dimensional analysis.
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Mathematics and Computer Science, Academy of Sciences
  of Moldova, Technical Report RT0006, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.0131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.0131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0137v3</id>
    <updated>2009-03-31T14:41:43Z</updated>
    <published>2008-02-01T14:47:24Z</published>
    <title>Fault-Tolerant Partial Replication in Large-Scale Database Systems</title>
    <summary>  We investigate a decentralised approach to committing transactions in a
replicated database, under partial replication. Previous protocols either
re-execute transactions entirely and/or compute a total order of transactions.
In contrast, ours applies update values, and orders only conflicting
transactions. It results that transactions execute faster, and distributed
databases commit in small committees. Both effects contribute to preserve
scalability as the number of databases and transactions increase. Our algorithm
ensures serializability, and is live and safe in spite of faults.
</summary>
    <author>
      <name>Pierre Sutra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Shapiro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0802.0137v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0137v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3171v2</id>
    <updated>2008-04-27T19:32:31Z</updated>
    <published>2008-04-20T03:23:38Z</published>
    <title>Optimization Approach for Detecting the Critical Data on a Database</title>
    <summary>  Through purposeful introduction of malicious transactions (tracking
transactions) into randomly select nodes of a (database) graph, soiled and
clean segments are identified. Soiled and clean measures corresponding those
segments are then computed. These measures are used to repose the problem of
critical database elements detection as an optimization problem over the graph.
This method is universally applicable over a large class of graphs (including
directed, weighted, disconnected, cyclic) that occur in several contexts of
databases. A generalization argument is presented which extends the critical
data problem to abstract settings.
</summary>
    <author>
      <name>Prashanth Alluvada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 3 tables. corrected typos, added remarks</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3171v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3171v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0075v1</id>
    <updated>2008-05-31T14:49:00Z</updated>
    <published>2008-05-31T14:49:00Z</published>
    <title>An Experimental Investigation of XML Compression Tools</title>
    <summary>  This paper presents an extensive experimental study of the state-of-the-art
of XML compression tools. The study reports the behavior of nine XML
compressors using a large corpus of XML documents which covers the different
natures and scales of XML documents. In addition to assessing and comparing the
performance characteristics of the evaluated XML compression tools, the study
tries to assess the effectiveness and practicality of using these tools in the
real world. Finally, we provide some guidelines and recommen- dations which are
useful for helping developers and users for making an effective decision for
selecting the most suitable XML compression tool for their needs.
</summary>
    <author>
      <name>Sherif Sakr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://xmlcompbench.sourceforge.net/</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.0075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3115v1</id>
    <updated>2008-06-19T02:06:14Z</updated>
    <published>2008-06-19T02:06:14Z</published>
    <title>Using rational numbers to key nested sets</title>
    <summary>  This report details the generation and use of tree node ordering keys in a
single relational database table. The keys for each node are calculated from
the keys of its parent, in such a way that the sort order places every node in
the tree before all of its descendants and after all siblings having a lower
index. The calculation from parent keys to child keys is simple, and reversible
in the sense that the keys of every ancestor of a node can be calculated from
that node's keys without having to consult the database.
  Proofs of the above properties of the key encoding process and of its
correspondence to a finite continued fraction form are provided.
</summary>
    <author>
      <name>Dan Hazel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technology One</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2083v3</id>
    <updated>2009-01-19T15:22:33Z</updated>
    <published>2008-08-15T03:14:55Z</published>
    <title>Histogram-Aware Sorting for Enhanced Word-Aligned Compression in Bitmap
  Indexes</title>
    <summary>  Bitmap indexes must be compressed to reduce input/output costs and minimize
CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use
techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. These techniques are sensitive to the order of the rows: a
simple lexicographical sort can divide the index size by 9 and make indexes
several times faster. We investigate reordering heuristics based on computed
attribute-value histograms. Simply permuting the columns of the table based on
these histograms can increase the sorting efficiency by 40%.
</summary>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proceedings of DOLAP 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.2083v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2083v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1971v1</id>
    <updated>2008-09-11T12:20:00Z</updated>
    <published>2008-09-11T12:20:00Z</published>
    <title>Knowledge and Metadata Integration for Warehousing Complex Data</title>
    <summary>  With the ever-growing availability of so-called complex data, especially on
the Web, decision-support systems such as data warehouses must store and
process data that are not only numerical or symbolic. Warehousing and analyzing
such data requires the joint exploitation of metadata and domain-related
knowledge, which must thereby be integrated. In this paper, we survey the types
of knowledge and metadata that are needed for managing complex data, discuss
the issue of knowledge and metadata integration, and propose a CWM-compliant
integration solution that we incorporate into an XML complex data warehousing
framework we previously designed.
</summary>
    <author>
      <name>Jean-Christian Ralaivao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference on Information Systems Technology and
  its Applications (ISTA 07), Kharkiv : Ukraine (2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1981v1</id>
    <updated>2008-09-11T12:44:10Z</updated>
    <published>2008-09-11T12:44:10Z</published>
    <title>A Join Index for XML Data Warehouses</title>
    <summary>  XML data warehouses form an interesting basis for decision-support
applications that exploit complex data. However, native-XML database management
systems (DBMSs) currently bear limited performances and it is necessary to
research for ways to optimize them. In this paper, we propose a new join index
that is specifically adapted to the multidimensional architecture of XML
warehouses. It eliminates join operations while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our join index. They also show that
native XML DBMSs can compete with XML-compatible, relational DBMSs when
warehousing and analyzing XML data.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2008 International Conference on Information Resources Management
  (Conf-IRM 08), Niagra Falls : Canada (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.5582v2</id>
    <updated>2008-11-03T23:33:20Z</updated>
    <published>2008-10-31T19:25:02Z</published>
    <title>Anonymizing Unstructured Data</title>
    <summary>  In this paper we consider the problem of anonymizing datasets in which each
individual is associated with a set of items that constitute private
information about the individual. Illustrative datasets include market-basket
datasets and search engine query logs. We formalize the notion of k-anonymity
for set-valued data as a variant of the k-anonymity model for traditional
relational datasets. We define an optimization problem that arises from this
definition of anonymity and provide O(klogk) and O(1)-approximation algorithms
for the same. We demonstrate applicability of our algorithms to the America
Online query log dataset.
</summary>
    <author>
      <name>Rajeev Motwani</name>
    </author>
    <author>
      <name>Shubha U. Nabar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.5582v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.5582v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2117v1</id>
    <updated>2008-11-13T14:12:57Z</updated>
    <published>2008-11-13T14:12:57Z</published>
    <title>Disjunctive Databases for Representing Repairs</title>
    <summary>  This paper addresses the problem of representing the set of repairs of a
possibly inconsistent database by means of a disjunctive database.
Specifically, the class of denial constraints is considered. We show that,
given a database and a set of denial constraints, there exists a (unique)
disjunctive database, called canonical, which represents the repairs of the
database w.r.t. the constraints and is contained in any other disjunctive
database with the same set of minimal models. We propose an algorithm for
computing the canonical disjunctive database. Finally, we study the size of the
canonical disjunctive database in the presence of functional dependencies for
both repairs and cardinality-based repairs.
</summary>
    <author>
      <name>Cristian Molinaro</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/0811.2117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.3301v2</id>
    <updated>2009-06-10T17:37:32Z</updated>
    <published>2008-11-20T16:22:05Z</published>
    <title>Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound</title>
    <summary>  The Dynamic Time Warping (DTW) is a popular similarity measure between time
series. The DTW fails to satisfy the triangle inequality and its computation
requires quadratic time. Hence, to find closest neighbors quickly, we use
bounding techniques. We can avoid most DTW computations with an inexpensive
lower bound (LB Keogh). We compare LB Keogh with a tighter lower bound (LB
Improved). We find that LB Improved-based search is faster. As an example, our
approach is 2-3 times faster over random-walk and shape time series.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patcog.2008.11.030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patcog.2008.11.030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Pattern Recognition on November 20th, 2008</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Daniel Lemire, Faster Retrieval with a Two-Pass
  Dynamic-Time-Warping Lower Bound, Pattern Recognition 42(9): 2169-2180 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.3301v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.3301v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3715v1</id>
    <updated>2008-12-19T07:52:46Z</updated>
    <published>2008-12-19T07:52:46Z</published>
    <title>Business processes integration and performance indicators in a PLM</title>
    <summary>  In an economic environment more and more competitive, the effective
management of information and knowledge is a strategic issue for industrial
enterprises. In the global marketplace, companies must use reactive strategies
and reduce their products development cycle. In this context, the PLM (Product
Lifecycle Management) is considered as a key component of the information
system. The aim of this paper is to present an approach to integrate Business
Processes in a PLM system. This approach is implemented in automotive sector
with second-tier subcontractor
</summary>
    <author>
      <name>Aurélie Bissay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Pernelle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Abdelaziz Bouras</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">APMS'08, Espoo : Finlande (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.3715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3072v1</id>
    <updated>2009-03-18T04:54:54Z</updated>
    <published>2009-03-18T04:54:54Z</published>
    <title>Spatial Skyline Queries: An Efficient Geometric Algorithm</title>
    <summary>  As more data-intensive applications emerge, advanced retrieval semantics,
such as ranking or skylines, have attracted attention. Geographic information
systems are such an application with massive spatial data. Our goal is to
efficiently support skyline queries over massive spatial data. To achieve this
goal, we first observe that the best known algorithm VS2, despite its claim,
may fail to deliver correct results. In contrast, we present a simple and
efficient algorithm that computes the correct results. To validate the
effectiveness and efficiency of our algorithm, we provide an extensive
empirical comparison of our algorithm and VS2 in several aspects.
</summary>
    <author>
      <name>Wanbin Son</name>
    </author>
    <author>
      <name>Mu-Woong Lee</name>
    </author>
    <author>
      <name>Hee-Kap Ahn</name>
    </author>
    <author>
      <name>Seung-won Hwang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, uses LNCS format</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4305v2</id>
    <updated>2009-06-22T14:55:54Z</updated>
    <published>2009-03-25T11:39:31Z</published>
    <title>Evaluation d'une requete en SQL</title>
    <summary>  The objective of this paper is to show how the interrogation processor
responds to SQL interrogation. The interrogation processor is split into two
parts. The first, called the interrogation compiler translates an SQL query
into a plan of physical execution. The second, called evaluation query runs the
execution plan.
</summary>
    <author>
      <name>Diana Sophia Codat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, exposed on 4th International Conferences "Actualities and
  Perspectives on Hardware and Software" - APHS2007, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 99-104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.4305v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4305v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3312v1</id>
    <updated>2009-04-21T18:38:25Z</updated>
    <published>2009-04-21T18:38:25Z</published>
    <title>HybridMiner: Mining Maximal Frequent Itemsets Using Hybrid Database
  Representation Approach</title>
    <summary>  In this paper we present a novel hybrid (arraybased layout and vertical
bitmap layout) database representation approach for mining complete Maximal
Frequent Itemset (MFI) on sparse and large datasets. Our work is novel in terms
of scalability, item search order and two horizontal and vertical projection
techniques. We also present a maximal algorithm using this hybrid database
representation approach. Different experimental results on real and sparse
benchmark datasets show that our approach is better than previous state of art
maximal algorithms.
</summary>
    <author>
      <name>Shariq Bashir</name>
    </author>
    <author>
      <name>Abdul Rauf Baig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/INMIC.2005.334484</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/INMIC.2005.334484" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages In the proceedings of 9th IEEE-INMIC 2005, Karachi, Pakistan,
  2005</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.3312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4138v1</id>
    <updated>2009-05-26T08:52:42Z</updated>
    <published>2009-05-26T08:52:42Z</published>
    <title>Faster estimation of the correlation fractal dimension using
  box-counting</title>
    <summary>  Fractal dimension is widely adopted in spatial databases and data mining,
among others as a measure of dataset skewness. State-of-the-art algorithms for
estimating the fractal dimension exhibit linear runtime complexity whether
based on box-counting or approximation schemes. In this paper, we revisit a
correlation fractal dimension estimation algorithm that redundantly rescans the
dataset and, extending that work, we propose another linear, yet faster and as
accurate method, which completes in a single pass.
</summary>
    <author>
      <name>Christos Attikos</name>
    </author>
    <author>
      <name>Michael Doumpos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, to appear in BCI 2009 - 4th Balkan Conference in Informatics</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.4138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4605v1</id>
    <updated>2009-05-28T10:29:20Z</updated>
    <published>2009-05-28T10:29:20Z</published>
    <title>Techniques for Securing Data Exchange between a Database Server and a
  Client Program</title>
    <summary>  The goal of the presented work is to illustrate a method by which the data
exchange between a standalone computer software and a shared database server
can be protected of unauthorized interceptation of the traffic in Internet
network, a transport network for data managed by those two systems,
interceptation by which an attacker could gain illegetimate access to the
database, threatening this way the data integrity and compromising the
database.
</summary>
    <author>
      <name>Ovidiu Crista</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, exposed on 5th International Conference "Actualities and
  Perspectives on Hardware and Software" - APHS2009, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),95-100</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.4605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0910v1</id>
    <updated>2009-06-04T13:28:51Z</updated>
    <published>2009-06-04T13:28:51Z</published>
    <title>On the Challenges of Collaborative Data Processing</title>
    <summary>  The last 30 years have seen the creation of a variety of electronic
collaboration tools for science and business. Some of the best-known
collaboration tools support text editing (e.g., wikis). Wikipedia's success
shows that large-scale collaboration can produce highly valuable content.
Meanwhile much structured data is being collected and made publicly available.
We have never had access to more powerful databases and statistical packages.
Is large-scale collaborative data analysis now possible? Using a quantitative
analysis of Web 2.0 data visualization sites, we find evidence that at least
moderate open collaboration occurs. We then explore some of the limiting
factors of collaboration over data.
</summary>
    <author>
      <name>Sylvie Noel</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear as a chapter in an upcoming book (Collaborative Information
  Behavior)</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4927v1</id>
    <updated>2009-06-26T13:24:57Z</updated>
    <published>2009-06-26T13:24:57Z</published>
    <title>Fast Probabilistic Ranking under x-Relation Model</title>
    <summary>  The probabilistic top-k queries based on the interplay of score and
probability, under the possible worlds semantic, become an important research
issue that considers both score and uncertainty on the same basis. In the
literature, many different probabilistic top-k queries are proposed. Almost all
of them need to compute the probability of a tuple t_i to be ranked at the j-th
position across the entire set of possible worlds. The cost of such computing
is the dominant cost and is known as O(kn^2), where n is the size of dataset.
In this paper, we propose a new novel algorithm that computes such probability
in O(kn).
</summary>
    <author>
      <name>Lijun Chang</name>
    </author>
    <author>
      <name>Jeffrey Xu Yu</name>
    </author>
    <author>
      <name>Lu Qin</name>
    </author>
    <link href="http://arxiv.org/abs/0906.4927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1763v1</id>
    <updated>2009-09-09T18:09:06Z</updated>
    <published>2009-09-09T18:09:06Z</published>
    <title>Remembrance: The Unbearable Sentience of Being Digital</title>
    <summary>  We introduce a world vision in which data is endowed with memory. In this
data-centric systems paradigm, data items can be enabled to retain all or some
of their previous values. We call this ability "remembrance" and posit that it
empowers significant leaps in the security, availability, and general
operational dimensions of systems. With the explosion in cheap, fast memories
and storage, large-scale remembrance will soon become practical. Here, we
introduce and explore the advantages of such a paradigm and the challenges in
making it a reality.
</summary>
    <author>
      <name>Ragib Hasan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois</arxiv:affiliation>
    </author>
    <author>
      <name>Radu Sion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <author>
      <name>Marianne Winslett</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1782v1</id>
    <updated>2009-09-09T18:10:33Z</updated>
    <published>2009-09-09T18:10:33Z</published>
    <title>Principles for Inconsistency</title>
    <summary>  Data consistency is very desirable because strong semantic properties make it
easier to write correct programs that perform as users expect. However, there
are good reasons why consistency may have to be weakened to achieve other
business goals. In this CIDR 2009 Perspectives paper, we present real-world
reasons inconsistency may be necessary, offer principles for managing
inconsistency coherently, and describe implementation approaches we are
investigating for sustainably scalable systems that offer comprehensible user
experiences despite inconsistency.
</summary>
    <author>
      <name>Shel Finkelstein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAP</arxiv:affiliation>
    </author>
    <author>
      <name>Dean Jacobs</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAP</arxiv:affiliation>
    </author>
    <author>
      <name>Rainer Brendle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAP</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4412v1</id>
    <updated>2009-09-24T11:09:15Z</updated>
    <published>2009-09-24T11:09:15Z</published>
    <title>Algorithm for Spatial Clustering with Obstacles</title>
    <summary>  In this paper, we propose an efficient clustering technique to solve the
problem of clustering in the presence of obstacles. The proposed algorithm
divides the spatial area into rectangular cells. Each cell is associated with
statistical information that enables us to label the cell as dense or
non-dense. We also label each cell as obstructed (i.e. intersects any obstacle)
or non-obstructed. Then the algorithm finds the regions (clusters) of
connected, dense, non-obstructed cells. Finally, the algorithm finds a center
for each such region and returns those centers as centers of the relatively
dense regions (clusters) in the spatial area.
</summary>
    <author>
      <name>Mohamed E. El-Sharkawi</name>
    </author>
    <author>
      <name>Mohamed A. El-Zawawy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. 2002 ICICIS Int. Conference on Intelligent Computing and
  Information Systems (ICICIS02), Cairo, Egypt, June 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.4412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3600v1</id>
    <updated>2009-11-18T16:59:38Z</updated>
    <published>2009-11-18T16:59:38Z</published>
    <title>"Almost automatic" and semantic integration of XML Schemas at various
  "severity" levels</title>
    <summary>  This paper presents a novel approach for the integration of a set of XML
Schemas. The proposed approach is specialized for XML, is almost automatic,
semantic and "light". As a further, original, peculiarity, it is parametric
w.r.t. a "severity" level against which the integration task is performed. The
paper describes the approach in all details, illustrates various theoretical
results, presents the experiments we have performed for testing it and,
finally, compares it with various related approaches already proposed in the
literature.
</summary>
    <author>
      <name>P. De Meo</name>
    </author>
    <author>
      <name>G. Quattrone</name>
    </author>
    <author>
      <name>G. Terracina</name>
    </author>
    <author>
      <name>D. Ursino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 Figures, 3 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the International Conference on Cooperative Information
  Systems (CoopIS 2003), pages 4 -21, Taormina, Italy, 2003. Lecture Notes in
  Computer Science, Springer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.3600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2134v1</id>
    <updated>2009-12-11T00:58:32Z</updated>
    <published>2009-12-11T00:58:32Z</published>
    <title>Enterprise Multi-Branch Database Synchronization with MSMQ</title>
    <summary>  When we talk about databases there have always been problems concerning data
synchronization. The latter is a technique for maintaining consistency among
different copies of data (often called replicas). In general, there is no
universal solution to this problem and often a particular situation requires a
particular approach driven by specific conditions. This paper presents an
approach tackling the issue of data synchronization in a distributed
multi-branch enterprise database. The proposed solution is based on MSMQ
(Microsoft Message Queue), a mechanism for asynchronous messaging.
</summary>
    <author>
      <name>Emil Vassev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.2134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2822v1</id>
    <updated>2009-12-15T08:55:12Z</updated>
    <published>2009-12-15T08:55:12Z</published>
    <title>Data management in Systems biology II - Outlook towards the semantic web</title>
    <summary>  The benefit of using ontologies, defined by the respective data standards, is
shown. It is presented how ontologies can be used for the semantic enrichment
of data and how this can contribute to the vision of the semantic web to become
true. The problems existing today on the way to a true semantic web are
pinpointed, different semantic web standards, tools and development frameworks
are overlooked and an outlook towards artificial intelligence and agents for
searching and mining the data in the semantic web are given, paving the way
from data management to information and in the end true knowledge management
systems.
</summary>
    <author>
      <name>Gerhard Mayer</name>
    </author>
    <link href="http://arxiv.org/abs/0912.2822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3494v1</id>
    <updated>2010-01-20T08:02:20Z</updated>
    <published>2010-01-20T08:02:20Z</published>
    <title>Proposing a New Method for Query Processing Adaption in DataBase</title>
    <summary>  This paper proposes a multi agent system by compiling two technologies, query
processing optimization and agents which contains features of personalized
queries and adaption with changing of requirements. This system uses a new
algorithm based on modeling of users' long-term requirements and also GA to
gather users' query data. Experimented Result shows more adaption capability
for presented algorithm in comparison with classic algorithms.
</summary>
    <author>
      <name>Mohammad-Reza Feizi-Derakhshi</name>
    </author>
    <author>
      <name>Hasan Asil</name>
    </author>
    <author>
      <name>Amir Asil</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Vol. 2, Issue 1, January 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4892v1</id>
    <updated>2010-01-27T10:38:51Z</updated>
    <published>2010-01-27T10:38:51Z</published>
    <title>Janus: Automatic Ontology Builder from XSD Files</title>
    <summary>  The construction of a reference ontology for a large domain still remains an
hard human task. The process is sometimes assisted by software tools that
facilitate the information extraction from a textual corpus. Despite of the
great use of XML Schema files on the internet and especially in the B2B domain,
tools that offer a complete semantic analysis of XML schemas are really rare.
In this paper we introduce Janus, a tool for automatically building a reference
knowledge base starting from XML Schema files. Janus also provides different
useful views to simplify B2B application integration.
</summary>
    <author>
      <name>Ivan Bedini</name>
    </author>
    <author>
      <name>Benjamin Nguyen</name>
    </author>
    <author>
      <name>Georges Gardarin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the World Wide Web Conference (WWW), Beijin, China,
  April 2008 (Developper Track), 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.4892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1010v1</id>
    <updated>2010-03-04T10:31:01Z</updated>
    <published>2010-03-04T10:31:01Z</published>
    <title>Verifying Recursive Active Documents with Positive Data Tree Rewriting</title>
    <summary>  This paper proposes a data tree-rewriting framework for modeling evolving
documents. The framework is close to Guarded Active XML, a platform used for
handling XML repositories evolving through web services. We focus on automatic
verification of properties of evolving documents that can contain data from an
infinite domain. We establish the boundaries of decidability, and show that
verification of a {\em positive} fragment that can handle recursive service
calls is decidable. We also consider bounded model-checking in our data
tree-rewriting framework and show that it is $\nexptime$-complete.
</summary>
    <author>
      <name>Blaise Genest</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Irisa</arxiv:affiliation>
    </author>
    <author>
      <name>Anca Muscholl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Zhilin Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1003.1010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2682v1</id>
    <updated>2010-03-13T06:22:07Z</updated>
    <published>2010-03-13T06:22:07Z</published>
    <title>Table manipulation in simplicial databases</title>
    <summary>  In \cite{Spi}, we developed a category of databases in which the schema of a
database is represented as a simplicial set. Each simplex corresponds to a
table in the database. There, our main concern was to find a categorical
formulation of databases; the simplicial nature of the schemas was to some
degree unexpected and unexploited.
  In the present note, we show how to use this geometric formulation
effectively on a computer. If we think of each simplex as a polygonal tile, we
can imagine assembling custom databases by mixing and matching tiles. Queries
on this database can be performed by drawing paths through the resulting tile
formations, selecting records at the start-point of this path and retrieving
corresponding records at its end-point.
</summary>
    <author>
      <name>David I. Spivak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages.</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.2682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4418v1</id>
    <updated>2010-03-23T13:55:40Z</updated>
    <published>2010-03-23T13:55:40Z</published>
    <title>Evaluation of Query Generators for Entity Search Engines</title>
    <summary>  Dynamic web applications such as mashups need efficient access to web data
that is only accessible via entity search engines (e.g. product or publication
search engines). However, most current mashup systems and applications only
support simple keyword searches for retrieving data from search engines. We
propose the use of more powerful search strategies building on so-called query
generators. For a given set of entities query generators are able to
automatically determine a set of search queries to retrieve these entities from
an entity search engine. We demonstrate the usefulness of query generators for
on-demand web data integration and evaluate the effectiveness and efficiency of
query generators for a challenging real-world integration scenario.
</summary>
    <author>
      <name>Stefan Endrullis</name>
    </author>
    <author>
      <name>Andreas Thor</name>
    </author>
    <author>
      <name>Erhard Rahm</name>
    </author>
    <link href="http://arxiv.org/abs/1003.4418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4827v1</id>
    <updated>2010-03-25T09:23:18Z</updated>
    <published>2010-03-25T09:23:18Z</published>
    <title>Tuple-based abstract data types: full parallelism</title>
    <summary>  Commutativity has the same inherent limitations as compatibility. Then, it is
worth conceiving simple concurrency control techniques. We propose a restricted
form of commutativity which increases parallelism without incurring a higher
overhead than compatibility. Advantages of our proposition are: (1)
commutativity of operations is determined at compile-time, (2) run-time
checking is as efficient as for compatibility, (3) neither commutativity
relations, (4) nor inverse operations, need to be specified, and (5) log space
utilization is reduced.
</summary>
    <author>
      <name>José Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Carmelo Malta</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Computer and Information Sciences
  (ISCIS'92), Antalya : Turkey (1992)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4828v1</id>
    <updated>2010-03-25T09:33:35Z</updated>
    <published>2010-03-25T09:33:35Z</published>
    <title>A framework for designing concurrent and recoverable abstract data types
  based on commutativity</title>
    <summary>  In this paper, we try to focus the reader's interest on the problems that
transactional systems have to resolve for taking advantage of commutativity in
a serializable and recoverable way. Our framework is, (as others), based on the
use of conditional commutativity on abstract date types. We present new
features that have not been found in the literature hitherto, that both
increase concurrency and simplify recovery.
</summary>
    <author>
      <name>Carmelo Malta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>José Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Computer and Information Sciences
  (ISCIS'91), Side : Turkey (1991)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4830v1</id>
    <updated>2010-03-25T09:52:39Z</updated>
    <published>2010-03-25T09:52:39Z</published>
    <title>Limits of Commutativity on Abstract Data Types</title>
    <summary>  We present some formal properties of (symmetrical) commutativity, the major
criterion used in transactional systems, which allow us to fully understand its
advantages and disadvantages. The main result is that commutativity is subject
to the same limitation as compatibility for arbitrary objects. However,
commutativity has also a number of attracting properties, one of which is
related to recovery and, to our knowledge, has not been exploited in the
literature. Advantages and disadvantages are illustrated on abstract data types
of interest. We also show how limits of commutativity have been circumvented,
which gives guidelines for doing so (or not!).
</summary>
    <author>
      <name>Carmelo Malta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>José Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th International Conference on Information Systems and Management
  of Data (CISMOD'92), Bangalore : India (1992)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5350v1</id>
    <updated>2010-03-28T08:11:05Z</updated>
    <published>2010-03-28T08:11:05Z</published>
    <title>An Improved Algorithm for Generating Database Transactions from
  Relational Algebra Specifications</title>
    <summary>  Alloy is a lightweight modeling formalism based on relational algebra. In
prior work with Fisler, Giannakopoulos, Krishnamurthi, and Yoo, we have
presented a tool, Alchemy, that compiles Alloy specifications into
implementations that execute against persistent databases. The foundation of
Alchemy is an algorithm for rewriting relational algebra formulas into code for
database transactions. In this paper we report on recent progress in improving
the robustness and efficiency of this transformation.
</summary>
    <author>
      <name>Daniel J. Dougherty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.21.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.21.7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 21, 2010, pp. 77-89</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1747v1</id>
    <updated>2010-04-10T22:23:27Z</updated>
    <published>2010-04-10T22:23:27Z</published>
    <title>Mobile Database System: Role of Mobility on the Query Processing</title>
    <summary>  The rapidly expanding technology of mobile communication will give mobile
users capability of accessing information from anywhere and any time. The
wireless technology has made it possible to achieve continuous connectivity in
mobile environment. When the query is specified as continuous, the requesting
mobile user can obtain continuously changing result. In order to provide
accurate and timely outcome to requesting mobile user, the locations of moving
object has to be closely monitored. The objective of paper is to discuss the
problem related to the role of personal and terminal mobility and query
processing in the mobile environment.
</summary>
    <author>
      <name>Samidha Dwivedi Sharma</name>
    </author>
    <author>
      <name>Dr. R. S. Kasana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS, Vol. 7 No. 3, March 2010, 211-216</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3272v1</id>
    <updated>2010-04-19T18:18:43Z</updated>
    <published>2010-04-19T18:18:43Z</published>
    <title>Database Reverse Engineering based on Association Rule Mining</title>
    <summary>  Maintaining a legacy database is a difficult task especially when system
documentation is poor written or even missing. Database reverse engineering is
an attempt to recover high-level conceptual design from the existing database
instances. In this paper, we propose a technique to discover conceptual schema
using the association mining technique. The discovered schema corresponds to
the normalization at the third normal form, which is a common practice in many
business organizations. Our algorithm also includes the rule filtering
heuristic to solve the problem of exponential growth of discovered rules
inherited with the association mining technique.
</summary>
    <author>
      <name>Nattapon Pannurat</name>
    </author>
    <author>
      <name>Nittaya Kerdprasop</name>
    </author>
    <author>
      <name>Kittisak Kerdprasop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Database-Reverse-Engineering-based-on-Association-Rule-Mining.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4216v1</id>
    <updated>2010-04-23T20:04:57Z</updated>
    <published>2010-04-23T20:04:57Z</published>
    <title>Symmetric M-tree</title>
    <summary>  The M-tree is a paged, dynamically balanced metric access method that
responds gracefully to the insertion of new objects. To date, no algorithm has
been published for the corresponding Delete operation. We believe this to be
non-trivial because of the design of the M-tree's Insert algorithm. We propose
a modification to Insert that overcomes this problem and give the corresponding
Delete algorithm. The performance of the tree is comparable to the M-tree and
offers additional benefits in terms of supported operations, which we briefly
discuss.
</summary>
    <author>
      <name>Alan P. Sexton</name>
    </author>
    <author>
      <name>Richard Swinbank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0201v1</id>
    <updated>2010-05-03T06:40:36Z</updated>
    <published>2010-05-03T06:40:36Z</published>
    <title>Personnalisation de bases de données multidimensionnelles</title>
    <summary>  This paper deals with decision support systems resting on multidimensional
modelling of data. Moreover, we intend to offer a set of concepts and
mechanisms for personalized multidimensional database specifications. This
personalization consists in associating weights to different components of a
multidimensional schema. Personalization specifications are specified through
the use of a language based on the principle of Event Condition Action. This
personalisation determines multidimensional data display as well as their
analyses (with the use of drilling or rotating operations).
</summary>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Zurfluh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'07, Perros-Guirec : France (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0212v1</id>
    <updated>2010-05-03T07:43:22Z</updated>
    <published>2010-05-03T07:43:22Z</published>
    <title>Construction graphique d'entrepôts et de magasins de données</title>
    <summary>  Nowadays, decisional systems have became a significant research topic in
databases. Data warehouses and data marts are the main elements of such
systems. This paper presents our decisional support system. We present
graphical interfaces which help the administrator to build data warehouses and
data marts. We present a data warehouse building interface based on an
object-oriented conceptual model. This model allows the warehouse data
historisation at three levels: attribute, class and environment. Also, we
present a data mart building interface which allows warehouse data to be
reorganised through a multidimensional object-oriented model.
</summary>
    <author>
      <name>Frédéric Bret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'99, La Garde : France (1999)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0213v1</id>
    <updated>2010-05-03T07:43:25Z</updated>
    <published>2010-05-03T07:43:25Z</published>
    <title>Algèbre OLAP et langage graphique</title>
    <summary>  This article deals with OLAP systems based on multidimensional model. The
conceptual model we provide, represents data through a constellation
(multi-facts) composed of several multi-hierarchy dimensions. In this model,
data are displayed through multidimensional tables. We define a query algebra
handling these tables. This user oriented algebra is composed of a closure core
of OLAP operators as soon as advanced operators dedicated to complex analysis.
Finally, we specify a graphical OLAP language based on this algebra. This
language facilitates analyses of decision makers.
</summary>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Zurfluh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'06, Hammamet : Tunisie (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0214v1</id>
    <updated>2010-05-03T07:43:45Z</updated>
    <published>2010-05-03T07:43:45Z</published>
    <title>Modélisation et extraction de données pour un entrepôt objet</title>
    <summary>  This paper describes an object-oriented model for designing complex and
time-variant data warehouse data. The main contribution is the warehouse class
concept, which extends the class concept by temporal and archive filters as
well as a mapping function. Filters allow the keeping of relevant data changes
whereas the mapping function defines the warehouse class schema from a global
data source schema. The approach take into account static properties as well as
dynamic properties. The behaviour extraction is based on the use-matrix
concept.
</summary>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Zurfluh Gilles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bases de donn\'ees avanc\'ees (BDA 2000), Blois : France (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0217v1</id>
    <updated>2010-05-03T07:47:20Z</updated>
    <published>2010-05-03T07:47:20Z</published>
    <title>Analyse multigraduelle OLAP</title>
    <summary>  Decisional systems are based on multidimensional databases improving OLAP
analyses. The paper describes a new OLAP operator named "BLEND" to perform
multigradual analyses. The operation transforms multidimensional structures
during querying in order to analyse measures according to various granularity
levels, which are reorganised into a single parameter. We study valid
combinations of the operation in the context of strict hierarchies. First
experimentations implement the operation in an R-OLAP framework showing the
slight cost of this operation.
</summary>
    <author>
      <name>Gilles Hubert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Revue des nouvelles technologies - RNTI, E-15 (2009) 253-258</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0220v1</id>
    <updated>2010-05-03T07:49:14Z</updated>
    <published>2010-05-03T07:49:14Z</published>
    <title>Elaboration d'entrepôts de données complexes</title>
    <summary>  In this paper, we study the data warehouse modelling used in decision support
systems. We provide an object-oriented data warehouse model allowing data
warehouse description as a central repository of relevant, complex and temporal
data. Our model integrates three concepts such as warehouse object, environment
and warehouse class. Each warehouse object is composed of one current state,
several past states (modelling its detailed evolutions) and several archive
states (modelling its evolutions within a summarised form). The environment
concept defines temporal parts in the data warehouse schema with significant
granularities (attribute, class, graph). Finally, we provide five functions
aiming at defining the data warehouse structures and two functions allowing the
warehouse class inheritance hierarchy organisation.
</summary>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'00, Lyon : France (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0224v1</id>
    <updated>2010-05-03T07:55:33Z</updated>
    <published>2010-05-03T07:55:33Z</published>
    <title>Towards Conceptual Multidimensional Design in Decision Support Systems</title>
    <summary>  Multidimensional databases support efficiently on-line analytical processing
(OLAP). In this paper, we depict a model dedicated to multidimensional
databases. The approach we present designs decisional information through a
constellation of facts and dimensions. Each dimension is possibly shared
between several facts and it is organised according to multiple hierarchies. In
addition, we define a comprehensive query algebra regrouping the more popular
multidimensional operations in current commercial systems and research
approaches. We introduce new operators dedicated to a constellation. Finally,
we describe a prototype that allows managers to query constellations of facts,
dimensions and multiple hierarchies.
</summary>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th East-European Conference on Advances in Databases and
  Information Systems - ADBIS'01, Vilnius : Lithuania (2001)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5433v1</id>
    <updated>2010-05-29T07:35:23Z</updated>
    <published>2010-05-29T07:35:23Z</published>
    <title>A Data Warehouse Assistant Design System Based on Clover Model</title>
    <summary>  Nowadays, Data Warehouse (DW) plays a crucial role in the process of decision
making. However, their design remains a very delicate and difficult task either
for expert or users. The goal of this paper is to propose a new approach based
on the clover model, destined to assist users to design a DW. The proposed
approach is based on two main steps. The first one aims to guide users in their
choice of DW schema model. The second one aims to finalize the chosen model by
offering to the designer views related to former successful DW design
experiences.
</summary>
    <author>
      <name>Nouha Arfaoui</name>
    </author>
    <author>
      <name>Jalel Akaichi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2010.2204</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2010.2204" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages, IJDMS</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems 2.2 (2010)
  57-71</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5514v1</id>
    <updated>2010-05-30T11:13:08Z</updated>
    <published>2010-05-30T11:13:08Z</published>
    <title>Managing Semantic Loss during Query Reformulation in Peer Data
  Management Systems</title>
    <summary>  In this paper we deal with the notion of semantic loss in Peer Data
Management Systems (PDMS) queries. We define such a notion and we give a
mechanism that discovers semantic loss in a PDMS network. Next, we propose an
algorithm that addresses the problem of restoring such a loss. Further
evaluation of our proposed algorithm is an ongoing work
</summary>
    <author>
      <name>Yannis Delveroudis</name>
    </author>
    <author>
      <name>Paraskevas V. Lekeas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SWOD.2007.353199</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SWOD.2007.353199" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SWOD '07 Proceedings of the 2007 IEEE International Workshop on
  Databases for Next Generation Researchers</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.5514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1309v1</id>
    <updated>2010-06-07T17:47:30Z</updated>
    <published>2010-06-07T17:47:30Z</published>
    <title>Using Grid Files for a Relational Database Management System</title>
    <summary>  This paper describes our experience with using Grid files as the main storage
organization for a relational database management system. We primarily focus on
the following two aspects. (i) Strategies for implementing grid files
efficiently. (ii) Methods for efficiency evaluating queries posed to a database
organized using grid files.
</summary>
    <author>
      <name>S. M. Joshi</name>
    </author>
    <author>
      <name>S. Sanyal</name>
    </author>
    <author>
      <name>S. Banerjee</name>
    </author>
    <author>
      <name>S. Srikumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 Pages, 5 Figures, 2 tables, This Paper was referred to in the
  seminal Paper by J. Nievergelt, H. Hinterberger,K.C. Sevcik, The Grid File:
  An Adaptable, Symmetric Multikey File Structure ACM Transactions on Database
  Systems (TODS), Volume 9, Issue 1, March, 1984. Pages: 38-71, ISSN:
  0362-5915, as [Reference 12]</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.1309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4833v1</id>
    <updated>2010-06-24T16:47:28Z</updated>
    <published>2010-06-24T16:47:28Z</published>
    <title>A Generic Storage API</title>
    <summary>  We present a generic API suitable for provision of highly generic storage
facilities that can be tailored to produce various individually customised
storage infrastructures. The paper identifies a candidate set of minimal
storage system building blocks, which are sufficiently simple to avoid
encapsulating policy where it cannot be customised by applications, and
composable to build highly flexible storage architectures. Four main generic
components are defined: the store, the namer, the caster and the interpreter.
It is hypothesised that these are sufficiently general that they could act as
building blocks for any information storage and retrieval system. The essential
characteristics of each are defined by an interface, which may be implemented
by multiple implementing classes.
</summary>
    <author>
      <name>Graham Kirby</name>
    </author>
    <author>
      <name>Evangelos Zirintsis</name>
    </author>
    <author>
      <name>Alan Dearle</name>
    </author>
    <author>
      <name>Ron Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACSC 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1337v1</id>
    <updated>2010-08-07T12:55:26Z</updated>
    <published>2010-08-07T12:55:26Z</published>
    <title>PDM based I-SOAS Data Warehouse Design</title>
    <summary>  This research paper briefly describes the industrial contributions of Product
Data Management in any organization's technical and managerial data management.
Then focusing on some current major PDM based problems i.e. Static and
Unintelligent Search, Platform Independent System and Successful PDM System
Implementation, briefly presents a semantic based solution i.e. I-SOAS. Majorly
this research paper is about to present and discuss the contributions of I-SOAS
in any organization's technical and system data management.
</summary>
    <author>
      <name>Zeeshan Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">", In the proceedings of FIFTH International Conference on
  Statistical Sciences: Mathematics, Paper ID 125, ISBN 978-969-8858-04-9, Vol.
  17, 23-25 January 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.1337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1715v5</id>
    <updated>2011-11-24T15:10:52Z</updated>
    <published>2010-08-10T14:05:28Z</published>
    <title>The universality of iterated hashing over variable-length strings</title>
    <summary>  Iterated hash functions process strings recursively, one character at a time.
At each iteration, they compute a new hash value from the preceding hash value
and the next character. We prove that iterated hashing can be pairwise
independent, but never 3-wise independent. We show that it can be almost
universal over strings much longer than the number of hash values; we bound the
maximal string length given the collision probability.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2011.11.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2011.11.009" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics 160 (4-5), 604--617 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.1715v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1715v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0397v1</id>
    <updated>2010-09-02T11:41:29Z</updated>
    <published>2010-09-02T11:41:29Z</published>
    <title>Mobile Information Collectors' Trajectory Data Warehouse Design</title>
    <summary>  To analyze complex phenomena which involve moving objects, Trajectory Data
Warehouse (TDW) seems to be an answer for many recent decision problems related
to various professions (physicians, commercial representatives, transporters,
ecologists ...) concerned with mobility. This work aims to make trajectories as
a first class concept in the trajectory data conceptual model and to design a
TDW, in which data resulting from mobile information collectors' trajectory are
gathered. These data will be analyzed, according to trajectory characteristics,
for decision making purposes, such as new products commercialization, new
commerce implementation, etc.
</summary>
    <author>
      <name>wided oueslati</name>
    </author>
    <author>
      <name>jalel akaichi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">international journal of managing information technology august
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0827v1</id>
    <updated>2010-09-04T11:29:55Z</updated>
    <published>2010-09-04T11:29:55Z</published>
    <title>A Novel Watermarking Scheme for Detecting and Recovering Distortions in
  Database Tables</title>
    <summary>  In this paper a novel fragile watermarking scheme is proposed to detect,
localize and recover malicious modifications in relational databases. In the
proposed scheme, all tuples in the database are first securely divided into
groups. Then watermarks are embedded and verified group-by-group independently.
By using the embedded watermark, we are able to detect and localize the
modification made to the database and even we recover the true data from the
database modified locations. Our experimental results show that this scheme is
so qualified; i.e. distortion detection and true data recovery both are
performed successfully.
</summary>
    <author>
      <name>Hamed khataeimaragheh</name>
    </author>
    <author>
      <name>Hassan Rashidi</name>
    </author>
    <link href="http://arxiv.org/abs/1009.0827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5149v1</id>
    <updated>2010-09-27T03:10:28Z</updated>
    <published>2010-09-27T03:10:28Z</published>
    <title>Towards an incremental maintenance of cyclic association rules</title>
    <summary>  Recently, the cyclic association rules have been introduced in order to
discover rules from items characterized by their regular variation over time.
In real life situations, temporal databases are often appended or updated.
Rescanning the whole database every time is highly expensive while existing
incremental mining techniques can efficiently solve such a problem. In this
paper, we propose an incremental algorithm for cyclic association rules
maintenance. The carried out experiments of our proposal stress on its
efficiency and performance.
</summary>
    <author>
      <name>Eya ben Ahmed</name>
    </author>
    <author>
      <name>Mohamed Salah Gouider</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems (IJDMS),
  November 2010, Volume 2, Number 4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.5149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.1147v1</id>
    <updated>2010-10-06T13:03:18Z</updated>
    <published>2010-10-06T13:03:18Z</published>
    <title>XML Query Processing and Query Languges: A Survey</title>
    <summary>  Today's database is associated with interoperability between different
domains and applications. This consequently results in the importance of data
portability in database. XML format fits the requirements and it has been
increasingly used for serving applications across different domains and
purposes. However, querying XML document effectively and efficiently is still a
challenging issue. This paper discusses query processing issues on XML and
reviews proposed solutions for querying XML databases by various authors.
</summary>
    <author>
      <name>Mikael Fernandus Simalango</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 2 tables, written in 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.1147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.1147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3615v1</id>
    <updated>2010-10-18T14:38:31Z</updated>
    <published>2010-10-18T14:38:31Z</published>
    <title>Scalable XML Collaborative Editing with Undo short paper</title>
    <summary>  Commutative Replicated Data-Type (CRDT) is a new class of algorithms that
ensures scalable consistency of replicated data. It has been successfully
applied to collaborative editing of texts without complex concurrency control.
In this paper, we present a CRDT to edit XML data. Compared to existing
approaches for XML collaborative editing, our approach is more scalable and
handles all the XML editing aspects : elements, contents, attributes and undo.
Indeed, undo is recognized as an important feature for collaborative editing
that allows to overcome system complexity through error recovery or
collaborative conflict resolution.
</summary>
    <author>
      <name>Stéphane Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Urso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Weiss</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1010.3615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.3615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1660v1</id>
    <updated>2010-12-08T00:51:37Z</updated>
    <published>2010-12-08T00:51:37Z</published>
    <title>Provenance and evidence in UniProtKB</title>
    <summary>  The primary mission of UniProt is to support biological research by
maintaining a stable, comprehensive, fully classified, richly and accurately
annotated protein sequence knowledgebase, with extensive cross-references to
external resources, that is freely available to the scientific community. To
enable users of the knowledgebase to accurately assess the reliability of the
information contained in this resource, the evidence for and provenance of the
information must be recorded. This paper discusses the user requirements for
this kind of metadata and the manner in which UniProtKB records it.
</summary>
    <author>
      <name>Jerven Bolleman</name>
    </author>
    <author>
      <name>Alain Gateau</name>
    </author>
    <author>
      <name>Sebastien Gehant</name>
    </author>
    <author>
      <name>Nicole Redaschi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1898v1</id>
    <updated>2010-12-09T00:03:31Z</updated>
    <published>2010-12-09T00:03:31Z</published>
    <title>Ontology Usage at ZFIN</title>
    <summary>  The Zebrafish Model Organism Database (ZFIN) provides a Web resource of
zebrafish genomic, genetic, developmental, and phenotypic data. Four different
ontologies are currently used to annotate data to the most specific term
available facilitating a better comparison between inter-species data. In
addition, ontologies are used to help users find and cluster data more quickly
without the need of knowing the exact technical name for a term.
</summary>
    <author>
      <name>Doug Howe</name>
    </author>
    <author>
      <name>Christian Pich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2858v1</id>
    <updated>2010-12-13T20:48:12Z</updated>
    <published>2010-12-13T20:48:12Z</published>
    <title>Relational transducers for declarative networking</title>
    <summary>  Motivated by a recent conjecture concerning the expressiveness of declarative
networking, we propose a formal computation model for "eventually consistent"
distributed querying, based on relational transducers. A tight link has been
conjectured between coordination-freeness of computations, and monotonicity of
the queries expressed by such computations. Indeed, we propose a formal
definition of coordination-freeness and confirm that the class of monotone
queries is captured by coordination-free transducer networks.
Coordination-freeness is a semantic property, but the syntactic class that we
define of "oblivious" transducers also captures the same class of monotone
queries. Transducer networks that are not coordination-free are much more
powerful.
</summary>
    <author>
      <name>Tom Ameloot</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1989284.1989321</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1989284.1989321" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">30th ACM Symposium on Principles of Database Systems, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.2858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.0952v1</id>
    <updated>2011-02-04T15:59:12Z</updated>
    <published>2011-02-04T15:59:12Z</published>
    <title>Pattern tree-based XOLAP rollup operator for XML complex hierarchies</title>
    <summary>  With the rise of XML as a standard for representing business data, XML data
warehousing appears as a suitable solution for decision-support applications.
In this context, it is necessary to allow OLAP analyses on XML data cubes.
Thus, XQuery extensions are needed. To define a formal framework and allow
much-needed performance optimizations on analytical queries expressed in
XQuery, defining an algebra is desirable. However, XML-OLAP (XOLAP) algebras
from the literature still largely rely on the relational model. Hence, we
propose in this paper a rollup operator based on a pattern tree in order to
handle multidimensional XML data expressed within complex hierarchies.
</summary>
    <author>
      <name>Marouane Hachicha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Machine and Web Intelligence (ICMWI
  10), Algiers : Algeria (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.0952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.0952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4163v1</id>
    <updated>2011-04-21T03:50:34Z</updated>
    <published>2011-04-21T03:50:34Z</published>
    <title>Data Mining : A prediction of performer or underperformer using
  classification</title>
    <summary>  Now a day's students have a large set of data having precious information
hidden. Data mining technique can help to find this hidden information. In this
paper, data mining techniques name Byes classification method is used on these
data to help an institution. Institutions can find those students who are
consistently perform well. This study will help to institution reduce the drop
put ratio to a significant level and improve the performance level of the
institution.
</summary>
    <author>
      <name>Umesh Kumar Pandey</name>
    </author>
    <author>
      <name>Saurabh Pal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJCSIT) International Journal of Computer Science and Information
  Technology, Vol. 2(2), 2011, 686-690</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.4163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4899v1</id>
    <updated>2011-04-26T12:25:56Z</updated>
    <published>2011-04-26T12:25:56Z</published>
    <title>Data Base Mappings and Theory of Sketches</title>
    <summary>  In this paper we will present the two basic operations for database schemas
used in database mapping systems (separation and Data Federation), and we will
explain why the functorial semantics for database mappings needed a new base
category instead of usual Set category. Successively, it is presented a
definition of the graph G for a schema database mapping system, and the
definition of its sketch category Sch(G). Based on this framework we presented
functorial semantics for database mapping systems with the new base category
DB.
</summary>
    <author>
      <name>Zoran Majkic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.4899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5951v1</id>
    <updated>2011-05-30T11:58:42Z</updated>
    <published>2011-05-30T11:58:42Z</published>
    <title>Performance of Short-Commit in Extreme Database Environment</title>
    <summary>  Atomic commit protocols are used where data integrity is more important than
data availability. Two-Phase commit (2PC) is a standard commit protocol for
commercial database management systems. To reduce certain drawbacks in 2PC
protocol people have suggested different variance of this protocol.
Short-Commit protocol is developed with an objective to achieve low cost
transaction commitment cost with non-blocking capability. In this paper we have
briefly explained short-commit protocol executing pattern. Experimental
analysis and results are presented to support the claim that short-commit can
work efficiently in extreme database environment.
</summary>
    <author>
      <name>Muhammad Tayyab Shahzad</name>
    </author>
    <author>
      <name>Muhammad Rizwan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages. International Journal of Database Management Systems, ISSN
  : 0975-5705 (Online); International Journal of Database Management Systems
  (IJDMS)2011, 0975-5985 (Print)</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.0831v1</id>
    <updated>2011-08-03T12:36:23Z</updated>
    <published>2011-08-03T12:36:23Z</published>
    <title>Towards Spatio-Temporal SOLAP</title>
    <summary>  The integration of Geographic Information Systems (GIS) and On-Line
Analytical Processing (OLAP), denoted SOLAP, is aimed at exploring and
analyzing spatial data. In real-world SOLAP applications, spatial and
non-spatial data are subject to changes. In this paper we present a temporal
query language for SOLAP, called TPiet-QL, supporting so-called discrete
changes (for example, in land use or cadastral applications there are
situations where parcels are merged or split). TPiet-QL allows expressing
integrated GIS-OLAP queries in an scenario where spatial objects change across
time.
</summary>
    <author>
      <name>Pablo Bisceglia</name>
    </author>
    <author>
      <name>Leticia Gomez</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.0831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0617v1</id>
    <updated>2011-09-03T12:04:00Z</updated>
    <published>2011-09-03T12:04:00Z</published>
    <title>Metadata Challenge for Query Processing Over Heterogeneous Wireless
  Sensor Network</title>
    <summary>  Wireless sensor networks become integral part of our life. These networks can
be used for monitoring the data in various domain due to their flexibility and
functionality. Query processing and optimization in the WSN is a very
challenging task because of their energy and memory constraint. In this paper,
first our focus is to review the different approaches that have significant
impacts on the development of query processing techniques for WSN. Finally, we
aim to illustrate the existing approach in popular query processing engines
with future research challenges in query optimization.
</summary>
    <author>
      <name>C. Komalavalli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jagan Institute of Management Studies, Rohini, New Delhi</arxiv:affiliation>
    </author>
    <author>
      <name>Chetna Laroiya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jagan Insitute of Management Studies, Rohini, New Delhi</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  3, No. 4, August 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1202v1</id>
    <updated>2011-09-06T14:32:05Z</updated>
    <published>2011-09-06T14:32:05Z</published>
    <title>Data Mining Techniques: A Source for Consumer Behavior Analysis</title>
    <summary>  Various studies on consumer purchasing behaviors have been presented and used
in real problems. Data mining techniques are expected to be a more effective
tool for analyzing consumer behaviors. However, the data mining method has
disadvantages as well as advantages. Therefore, it is important to select
appropriate techniques to mine databases. The objective of this paper is to
know consumer behavior, his psychological condition at the time of purchase and
how suitable data mining method apply to improve conventional method. Moreover,
in an experiment, association rule is employed to mine rules for trusted
customers using sales data in a super market industry
</summary>
    <author>
      <name>Abhijit Raorane</name>
    </author>
    <author>
      <name>R. V. Kulkarni</name>
    </author>
    <link href="http://arxiv.org/abs/1109.1202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6299v1</id>
    <updated>2011-09-28T19:04:18Z</updated>
    <published>2011-09-28T19:04:18Z</published>
    <title>Sensitivity Analysis for Declarative Relational Query Languages with
  Ordinal Ranks</title>
    <summary>  We present sensitivity analysis for results of query executions in a
relational model of data extended by ordinal ranks. The underlying model of
data results from the ordinary Codd's model of data in which we consider
ordinal ranks of tuples in data tables expressing degrees to which tuples match
queries. In this setting, we show that ranks assigned to tuples are insensitive
to small changes, i.e., small changes in the input data do not yield large
changes in the results of queries.
</summary>
    <author>
      <name>Radim Belohlavek</name>
    </author>
    <author>
      <name>Lucie Urbanova</name>
    </author>
    <author>
      <name>Vilem Vychodil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper will appear in Proceedings of the 19th International
  Conference on Applications of Declarative Programming and Knowledge
  Management (INAP 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.6299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3017v1</id>
    <updated>2011-10-13T18:01:39Z</updated>
    <published>2011-10-13T18:01:39Z</published>
    <title>Towards a Query Language for the Web of Data (A Vision Paper)</title>
    <summary>  Research on querying the Web of Data is still in its infancy. In this paper,
we provide an initial set of general features that we envision should be
considered in order to define a query language for the Web of Data.
Furthermore, for each of these features, we pose questions that have not been
addressed before in the context of querying the Web of Data. We believe that
addressing these questions and studying these features may guide the next 10
years of research on the Web of Data.
</summary>
    <author>
      <name>Juan Sequeda</name>
    </author>
    <author>
      <name>Olaf Hartig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5639v1</id>
    <updated>2011-11-23T22:28:38Z</updated>
    <published>2011-11-23T22:28:38Z</published>
    <title>A New Technique to Backup and Restore DBMS using XML and .NET
  Technologies</title>
    <summary>  In this paper, we proposed a new technique for backing up and restoring
different Database Management Systems (DBMS). The technique is enabling to
backup and restore a part of or the whole database using a unified interface
using ASP.NET and XML technologies. It presents a Web Solution allowing the
administrators to do their jobs from everywhere, locally or remotely. To show
the importance of our solution, we have taken two case studies, oracle 11g and
SQL Server 2008.
</summary>
    <author>
      <name>Seifedine Kadry</name>
    </author>
    <author>
      <name>Mohamad Smaili</name>
    </author>
    <author>
      <name>Hussam Kassem</name>
    </author>
    <author>
      <name>Hassan Hayek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computer Science and Engineering Vol. 02,
  No. 04, 2010, 1092-1102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5687v1</id>
    <updated>2011-11-24T07:52:59Z</updated>
    <published>2011-11-24T07:52:59Z</published>
    <title>Coron : Plate-forme d'extraction de connaissances dans les bases de
  données</title>
    <summary>  Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge.
</summary>
    <author>
      <name>Baptiste Ducatel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Mehdi Kaytoue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Florent Marcuola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Amedeo Napoli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laszlo Szathmary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">17\`eme conf\'erence en Reconnaissance des Formes et Intelligence
  Artificielle (2010) 883-884</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5690v1</id>
    <updated>2011-11-24T07:56:18Z</updated>
    <published>2011-11-24T07:56:18Z</published>
    <title>The Coron System</title>
    <summary>  Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge.
</summary>
    <author>
      <name>Mehdi Kaytoue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Florent Marcuola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Amedeo Napoli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laszlo Szathmary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Jean Villerd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">8th International Conference on Formal Concept Analsis (ICFCA)
  (2010) 55--58</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3134v1</id>
    <updated>2011-12-14T07:12:31Z</updated>
    <published>2011-12-14T07:12:31Z</published>
    <title>Proposing Cluster_Similarity Method in Order to Find as Much Better
  Similarities in Databases</title>
    <summary>  Different ways of entering data into databases result in duplicate records
that cause increasing of databases' size. This is a fact that we cannot ignore
it easily. There are several methods that are used for this purpose. In this
paper, we have tried to increase the accuracy of operations by using cluster
similarity instead of direct similarity of fields. So that clustering is done
on fields of database and according to accomplished clustering on fields,
similarity degree of records is obtained. In this method by using present
information in database, more logical similarity is obtained for deficient
information that in general, the method of cluster similarity could improve
operations 24% compared with previous methods.
</summary>
    <author>
      <name>Mohammad-Reza Feizi-Derakhshi</name>
    </author>
    <author>
      <name>Azade Roohany</name>
    </author>
    <link href="http://arxiv.org/abs/1112.3134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3807v1</id>
    <updated>2012-02-16T22:00:09Z</updated>
    <published>2012-02-16T22:00:09Z</published>
    <title>An Adaptive Mechanism for Accurate Query Answering under Differential
  Privacy</title>
    <summary>  We propose a novel mechanism for answering sets of count- ing queries under
differential privacy. Given a workload of counting queries, the mechanism
automatically selects a different set of "strategy" queries to answer
privately, using those answers to derive answers to the workload. The main
algorithm proposed in this paper approximates the optimal strategy for any
workload of linear counting queries. With no cost to the privacy guarantee, the
mechanism improves significantly on prior approaches and achieves near-optimal
error for many workloads, when applied under (\epsilon, \delta)-differential
privacy. The result is an adaptive mechanism which can help users achieve good
utility without requiring that they reason carefully about the best formulation
of their task.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012. arXiv admin note: substantial text overlap with
  arXiv:1103.1367</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.3807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0056v1</id>
    <updated>2012-03-01T00:17:13Z</updated>
    <published>2012-03-01T00:17:13Z</published>
    <title>SharedDB: Killing One Thousand Queries With One Stone</title>
    <summary>  Traditional database systems are built around the query-at-a-time model. This
approach tries to optimize performance in a best-effort way. Unfortunately,
best effort is not good enough for many modern applications. These applications
require response time guarantees in high load situations. This paper describes
the design of a new database architecture that is based on batching queries and
shared computation across possibly hundreds of concurrent queries and updates.
Performance experiments with the TPC-W benchmark show that the performance of
our implementation, SharedDB, is indeed robust across a wide range of dynamic
workloads.
</summary>
    <author>
      <name>Georgios Giannikis</name>
    </author>
    <author>
      <name>Gustavo Alonso</name>
    </author>
    <author>
      <name>Donald Kossmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  526-537 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.2672v1</id>
    <updated>2012-03-12T23:19:09Z</updated>
    <published>2012-03-12T23:19:09Z</published>
    <title>FDB: A Query Engine for Factorised Relational Databases</title>
    <summary>  Factorised databases are relational databases that use compact factorised
representations at the physical layer to reduce data redundancy and boost query
performance. This paper introduces FDB, an in-memory query engine for
select-project-join queries on factorised databases. Key components of FDB are
novel algorithms for query optimisation and evaluation that exploit the
succinctness brought by data factorisation. Experiments show that for data sets
with many-to-many relationships FDB can outperform relational engines by orders
of magnitude.
</summary>
    <author>
      <name>Nurzhan Bakibayev</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <author>
      <name>Jakub Závodný</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.2672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.6406v1</id>
    <updated>2012-03-29T00:07:19Z</updated>
    <published>2012-03-29T00:07:19Z</published>
    <title>An Analysis of Structured Data on the Web</title>
    <summary>  In this paper, we analyze the nature and distribution of structured data on
the Web. Web-scale information extraction, or the problem of creating
structured tables using extraction from the entire web, is gathering lots of
research interest. We perform a study to understand and quantify the value of
Web-scale extraction, and how structured information is distributed amongst top
aggregator websites and tail sites for various interesting domains. We believe
this is the first study of its kind, and gives us new insights for information
extraction over the Web.
</summary>
    <author>
      <name>Nilesh Dalvi</name>
    </author>
    <author>
      <name>Ashwin Machanavajjhala</name>
    </author>
    <author>
      <name>Bo Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  680-691 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.6406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.6406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1598v1</id>
    <updated>2012-04-07T06:24:53Z</updated>
    <published>2012-04-07T06:24:53Z</published>
    <title>Improving Seek Time for Column Store Using MMH Algorithm</title>
    <summary>  Hash based search has, proven excellence on large data warehouses stored in
column store. Data distribution has significant impact on hash based search. To
reduce impact of data distribution, we have proposed Memory Managed Hash (MMH)
algorithm that uses shift XOR group for Queries and Transactions in column
store. Our experiments show that MMH improves read and write throughput by 22%
for TPC-H distribution.
</summary>
    <author>
      <name>Tejaswini Apte</name>
    </author>
    <author>
      <name>Dr. Maya Ingle</name>
    </author>
    <author>
      <name>Dr. A. K. Goyal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJACSA) International Journal of Advanced Computer Science and
  Applications Vol. 3, No.2, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.1598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2610v1</id>
    <updated>2012-04-12T03:49:26Z</updated>
    <published>2012-04-12T03:49:26Z</published>
    <title>A Novel Framework using Elliptic Curve Cryptography for Extremely Secure
  Transmission in Distributed Privacy Preserving Data Mining</title>
    <summary>  Privacy Preserving Data Mining is a method which ensures privacy of
individual information during mining. Most important task involves retrieving
information from multiple data bases which is distributed. The data once in the
data warehouse can be used by mining algorithms to retrieve confidential
information. The proposed framework has two major tasks, secure transmission
and privacy of confidential information during mining. Secure transmission is
handled by using elliptic curve cryptography and data distortion for privacy
preservation ensuring highly secure environment.
</summary>
    <author>
      <name>P. Kiran</name>
    </author>
    <author>
      <name>S Sathish Kumar</name>
    </author>
    <author>
      <name>N. P. Kavya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advanced Computing: An International Journal ( ACIJ ), Vol.3,
  No.2, March 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.2610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2731v1</id>
    <updated>2012-04-12T14:05:37Z</updated>
    <published>2012-04-12T14:05:37Z</published>
    <title>How do Ontology Mappings Change in the Life Sciences?</title>
    <summary>  Mappings between related ontologies are increasingly used to support data
integration and analysis tasks. Changes in the ontologies also require the
adaptation of ontology mappings. So far the evolution of ontology mappings has
received little attention albeit ontologies change continuously especially in
the life sciences. We therefore analyze how mappings between popular life
science ontologies evolve for different match algorithms. We also evaluate
which semantic ontology changes primarily affect the mappings. We further
investigate alternatives to predict or estimate the degree of future mapping
changes based on previous ontology and mapping transitions.
</summary>
    <author>
      <name>Anika Gross</name>
    </author>
    <author>
      <name>Michael Hartung</name>
    </author>
    <author>
      <name>Andreas Thor</name>
    </author>
    <author>
      <name>Erhard Rahm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: mapping evolution, ontology matching, ontology evolution</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3223v1</id>
    <updated>2012-04-14T22:30:15Z</updated>
    <published>2012-04-14T22:30:15Z</published>
    <title>Intelligent Database Flexible Querying System by Approximate Query
  Processing</title>
    <summary>  Database flexible querying is an alternative to the classic one for users.
The use of Formal Concepts Analysis (FCA) makes it possible to make approximate
answers that those turned over by a classic DataBase Management System (DBMS).
Some applications do not need exact answers. However, flexible querying can be
expensive in response time. This time is more significant when the flexible
querying require the calculation of aggregate functions ("Sum", "Avg", "Count",
"Var" etc.). In this paper, we propose an approach which tries to solve this
problem by using Approximate Query Processing (AQP).
</summary>
    <author>
      <name>Oussama Tlili</name>
    </author>
    <author>
      <name>Minyar Sassi</name>
    </author>
    <author>
      <name>Habib Ounelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 9 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Third International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA 2011), January 23-28, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.3223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3432v3</id>
    <updated>2014-11-04T16:17:12Z</updated>
    <published>2012-04-16T10:13:43Z</published>
    <title>Converging to the Chase - a Tool for Finite Controllability</title>
    <summary>  We solve a problem, stated in [CGP10], showing that Sticky Datalog, defined
in the cited paper as an element of the Datalog\pm project, has the finite
controllability property. In order to do that, we develop a technique, which we
believe can have further applications, of approximating Chase(D, T), for a
database instance D and some sets of tuple generating dependencies T, by an
infinite sequence of finite structures, all of them being models of T.
</summary>
    <author>
      <name>T. Gogacz</name>
    </author>
    <author>
      <name>J. Marcinkowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LICS.2013.61</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LICS.2013.61" rel="related"/>
    <link href="http://arxiv.org/abs/1204.3432v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3432v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4948v2</id>
    <updated>2012-04-28T18:28:23Z</updated>
    <published>2012-04-22T23:47:28Z</published>
    <title>On Injective Embeddings of Tree Patterns</title>
    <summary>  We study three different kinds of embeddings of tree patterns:
weakly-injective, ancestor-preserving, and lca-preserving. While each of them
is often referred to as injective embedding, they form a proper hierarchy and
their computational properties vary (from P to NP-complete). We present a
thorough study of the complexity of the model checking problem i.e., is there
an embedding of a given tree pattern in a given tree, and we investigate the
impact of various restrictions imposed on the tree pattern: bound on the degree
of a node, bound on the height, and type of allowed labels and edges.
</summary>
    <author>
      <name>Jakub Michaliszyn</name>
    </author>
    <author>
      <name>Anca Muscholl</name>
    </author>
    <author>
      <name>Sławek Staworko</name>
    </author>
    <author>
      <name>Piotr Wieczorek</name>
    </author>
    <author>
      <name>Zhilin Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under conference submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6081v1</id>
    <updated>2012-04-26T23:25:50Z</updated>
    <published>2012-04-26T23:25:50Z</published>
    <title>Optimizing I/O for Big Array Analytics</title>
    <summary>  Big array analytics is becoming indispensable in answering important
scientific and business questions. Most analysis tasks consist of multiple
steps, each making one or multiple passes over the arrays to be analyzed and
generating intermediate results. In the big data setting, I/O optimization is a
key to efficient analytics. In this paper, we develop a framework and
techniques for capturing a broad range of analysis tasks expressible in
nested-loop forms, representing them in a declarative way, and optimizing their
I/O by identifying sharing opportunities. Experiment results show that our
optimizer is capable of finding execution plans that exploit nontrivial I/O
sharing opportunities with significant savings.
</summary>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Jun Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  764-775 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.6081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0084v1</id>
    <updated>2012-08-01T03:51:05Z</updated>
    <published>2012-08-01T03:51:05Z</published>
    <title>Fundamentals of Order Dependencies</title>
    <summary>  Dependencies have played a significant role in database design for many
years. They have also been shown to be useful in query optimization. In this
paper, we discuss dependencies between lexicographically ordered sets of
tuples. We introduce formally the concept of order dependency and present a set
of axioms (inference rules) for them. We show how query rewrites based on these
axioms can be used for query optimization. We present several interesting
theorems that can be derived using the inference rules. We prove that
functional dependencies are subsumed by order dependencies and that our set of
axioms for order dependencies is sound and complete.
</summary>
    <author>
      <name>Jaroslaw Szlichta</name>
    </author>
    <author>
      <name>Parke Godfrey</name>
    </author>
    <author>
      <name>Jarek Gryz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.
  1220-1231 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.0084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.1932v1</id>
    <updated>2012-08-09T14:52:19Z</updated>
    <published>2012-08-09T14:52:19Z</published>
    <title>Statistical Distortion: Consequences of Data Cleaning</title>
    <summary>  We introduce the notion of statistical distortion as an essential metric for
measuring the effectiveness of data cleaning strategies. We use this metric to
propose a widely applicable yet scalable experimental framework for evaluating
data cleaning strategies along three dimensions: glitch improvement,
statistical distortion and cost-related criteria. Existing metrics focus on
glitch improvement and cost, but not on the statistical impact of data cleaning
strategies. We illustrate our framework on real world data, with a
comprehensive suite of experiments and analyses.
</summary>
    <author>
      <name>Tamraparni Dasu</name>
    </author>
    <author>
      <name>Ji Meng Loh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.
  1674-1683 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.1932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.1932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3307v3</id>
    <updated>2013-04-21T22:21:40Z</updated>
    <published>2012-08-16T07:43:28Z</published>
    <title>Impedance mismatch is not an "Objects vs. Relations" problem</title>
    <summary>  A problem of impedance mismatch between applications written in OO languages
and relational DB is not a problem of discrepancy between object-oriented and
relational approaches themselves. Its real causes can be found in usual
implementation of the OO approach. Direct comparison of the two approaches
cannot be used as a base for the conclusion that they are discrepant or
mismatched. Experimental proof of absence of contradiction between
object-oriented paradigm and relational data model is also presented
</summary>
    <author>
      <name>Grigoriev Evgeny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3307v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3307v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.2; H.2.4; C.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4173v1</id>
    <updated>2012-08-21T02:53:52Z</updated>
    <published>2012-08-21T02:53:52Z</published>
    <title>The Vertica Analytic Database: C-Store 7 Years Later</title>
    <summary>  This paper describes the system architecture of the Vertica Analytic Database
(Vertica), a commercialization of the design of the C-Store research prototype.
Vertica demonstrates a modern commercial RDBMS system that presents a classical
relational interface while at the same time achieving the high performance
expected from modern "web scale" analytic systems by making appropriate
architectural choices. Vertica is also an instructive lesson in how academic
systems research can be directly commercialized into a successful product.
</summary>
    <author>
      <name>Andrew Lamb</name>
    </author>
    <author>
      <name>Matt Fuller</name>
    </author>
    <author>
      <name>Ramakrishna Varadarajan</name>
    </author>
    <author>
      <name>Nga Tran</name>
    </author>
    <author>
      <name>Ben Vandier</name>
    </author>
    <author>
      <name>Lyric Doshi</name>
    </author>
    <author>
      <name>Chuck Bear</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1790-1801 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.4173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1011v1</id>
    <updated>2012-09-05T15:18:35Z</updated>
    <published>2012-09-05T15:18:35Z</published>
    <title>Kleisli Database Instances</title>
    <summary>  We use monads to relax the atomicity requirement for data in a database.
Depending on the choice of monad, the database fields may contain generalized
values such as lists or sets of values, or they may contain exceptions such as
various types of nulls. The return operation for monads ensures that any
ordinary database instance will count as one of these generalized instances,
and the bind operation ensures that generalized values behave well under joins
of foreign key sequences. Different monads allow for vastly different types of
information to be stored in the database. For example, we show that classical
concepts like Markov chains, graphs, and finite state automata are each
perfectly captured by a different monad on the same schema.
</summary>
    <author>
      <name>David I. Spivak</name>
    </author>
    <link href="http://arxiv.org/abs/1209.1011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="18C20, 68P15, 68Q65" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; H.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2647v1</id>
    <updated>2012-09-12T15:48:33Z</updated>
    <published>2012-09-12T15:48:33Z</published>
    <title>Shadow Theory, data model design for data integration</title>
    <summary>  For data integration in information ecosystems, semantic heterogeneity is a
known difficulty. In this paper, we propose Shadow Theory as the philosophical
foundation to address this issue. It is based on the notion of shadows in
Plato's Allegory of the Cave. What we can observe are just shadows, and
meanings of shadows are mental entities that only exist in viewers' cognitive
structures. With enterprise customer data integration example, we proposed six
design principles and algebra to support required operations.
</summary>
    <author>
      <name>Jason T. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">85 pages, 31 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.2647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3944v1</id>
    <updated>2012-09-18T13:09:24Z</updated>
    <published>2012-09-18T13:09:24Z</published>
    <title>Cyclic Association Rules Mining under Constraints</title>
    <summary>  Several researchers have explored the temporal aspect of association rules
mining. In this paper, we focus on the cyclic association rules, in order to
discover correlations among items characterized by regular cyclic variation
overtime. The overview of the state of the art has revealed the drawbacks of
proposed algorithm literatures, namely the excessive number of generated rules
which are not meeting the expert's expectations. To overcome these
restrictions, we have introduced our approach dedicated to generate the cyclic
association rules under constraints through a new method called
Constraint-Based Cyclic Association Rules CBCAR. The carried out experiments
underline the usefulness and the performance of our new approach.
</summary>
    <author>
      <name>Wafa Tebourski Wahiba Ben Abdessalem Karaa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4169v1</id>
    <updated>2012-09-19T07:23:02Z</updated>
    <published>2012-09-19T07:23:02Z</published>
    <title>Hybrid Data Mining Technique for Knowledge Discovery from Engineering
  Materials' Data sets</title>
    <summary>  Studying materials informatics from a data mining perspective can be
beneficial for manufacturing and other industrial engineering applications.
Predictive data mining technique and machine learning algorithm are combined to
design a knowledge discovery system for the selection of engineering materials
that meet the design specifications. Predictive method-Naive Bayesian
classifier and Machine learning Algorithm - Pearson correlation coefficient
method were implemented respectively for materials classification and
selection. The knowledge extracted from the engineering materials data sets is
proposed for effective decision making in advanced engineering materials design
applications.
</summary>
    <author>
      <name> Doreswamy</name>
    </author>
    <author>
      <name>Hemanth K. S</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures; International Journal of Database Management
  Systems (IJDMS), Vol.3, No.1, February 2011. arXiv admin note: text overlap
  with arXiv:1206.3078 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.4891v1</id>
    <updated>2012-10-16T17:46:50Z</updated>
    <published>2012-10-16T17:46:50Z</published>
    <title>Hokusai - Sketching Streams in Real Time</title>
    <summary>  We describe Hokusai, a real time system which is able to capture frequency
information for streams of arbitrary sequences of symbols. The algorithm uses
the CountMin sketch as its basis and exploits the fact that sketching is
linear. It provides real time statistics of arbitrary events, e.g. streams of
queries as a function of time. We use a factorizing approximation to provide
point estimates at arbitrary (time, item) combinations. Queries can be answered
in constant time.
</summary>
    <author>
      <name>Sergiy Matusevych</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <author>
      <name>Amr Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.4891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.4891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6242v1</id>
    <updated>2012-10-23T14:22:02Z</updated>
    <published>2012-10-23T14:22:02Z</published>
    <title>Enhancing Algebraic Query Relaxation with Semantic Similarity</title>
    <summary>  Cooperative database systems support a database user by searching for answers
that are closely related to his query and hence are informative answers. Common
operators to relax the user query are Dropping Condition, Anti-Instantiation
and Goal Replacement. In this article, we provide an algebraic version of these
operators. Moreover we propose some heuristics to assign a degree of similarity
to each tuple of an answer table; this degree can help the user to determine
whether this answer is relevant for him or not.
</summary>
    <author>
      <name>Lena Wiese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in Proceedings of IADIS Information Systems 2012 (10-12
  March 2012, Berlin, Germany)</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0176v1</id>
    <updated>2012-11-01T13:45:04Z</updated>
    <published>2012-11-01T13:45:04Z</published>
    <title>Joining relations under discrete uncertainty</title>
    <summary>  In this paper we introduce and experimentally compare alternative algorithms
to join uncertain relations. Different algorithms are based on specific
principles, e.g., sorting, indexing, or building intermediate relational tables
to apply traditional approaches. As a consequence their performance is affected
by different features of the input data, and each algorithm is shown to be more
efficient than the others in specific cases. In this way statistics explicitly
representing the amount and kind of uncertainty in the input uncertain
relations can be used to choose the most efficient algorithm.
</summary>
    <author>
      <name>Matteo Magnani</name>
    </author>
    <author>
      <name>Danilo Montesi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">database join operator, uncertain relations with discrete
  uncertainty, algorithms and experimental evaluation (28 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4414v1</id>
    <updated>2012-11-19T13:43:39Z</updated>
    <published>2012-11-19T13:43:39Z</published>
    <title>Towards a Scalable Dynamic Spatial Database System</title>
    <summary>  With the rise of GPS-enabled smartphones and other similar mobile devices,
massive amounts of location data are available. However, no scalable solutions
for soft real-time spatial queries on large sets of moving objects have yet
emerged. In this paper we explore and measure the limits of actual algorithms
and implementations regarding different application scenarios. And finally we
propose a novel distributed architecture to solve the scalability issues.
</summary>
    <author>
      <name>Joaquín Keller</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Raluca Diaconu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Mathieu Valero</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.4414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3501v1</id>
    <updated>2012-12-14T15:28:45Z</updated>
    <published>2012-12-14T15:28:45Z</published>
    <title>On optimum left-to-right strategies for active context-free games</title>
    <summary>  Active context-free games are two-player games on strings over finite
alphabets with one player trying to rewrite the input string to match a target
specification. These games have been investigated in the context of exchanging
Active XML (AXML) data. While it was known that the rewriting problem is
undecidable in general, it is shown here that it is EXPSPACE-complete to decide
for a given context-free game, whether all safely rewritable strings can be
safely rewritten in a left-to-right manner, a problem that was previously
considered by Abiteboul et al. Furthermore, it is shown that the corresponding
problem for games with finite replacement languages is EXPTIME-complete.
</summary>
    <author>
      <name>Henrik Björklund</name>
    </author>
    <author>
      <name>Martin Schuster</name>
    </author>
    <author>
      <name>Thomas Schwentick</name>
    </author>
    <author>
      <name>Joscha Kulbatzki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICDT 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6051v1</id>
    <updated>2012-12-25T14:07:44Z</updated>
    <published>2012-12-25T14:07:44Z</published>
    <title>Automatic approach for generating ETL operators</title>
    <summary>  This article addresses the generation of the ETL
operators(Extract-Transform-Load) for supplying a Data Warehouse from a
relational data source. As a first step, we add new rules to those proposed by
the authors of [1], these rules deal with the combination of ETL operators. In
a second step, we propose an automatic approach based on model transformations
to generate the ETL operations needed for loading a data warehouse. This
approach offers the possibility to set some designer requirements for loading.
</summary>
    <author>
      <name>Wided Bakari</name>
    </author>
    <author>
      <name>Mouez Ali</name>
    </author>
    <author>
      <name>Hanene Ben-Abdallah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6640v1</id>
    <updated>2012-12-29T15:29:26Z</updated>
    <published>2012-12-29T15:29:26Z</published>
    <title>Exploring mutexes, the Oracle RDBMS retrial spinlocks</title>
    <summary>  Spinlocks are widely used in database engines for processes synchronization.
KGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions
for submicrosecond synchronization. The mutex contention is frequently observed
in highly concurrent OLTP environments.
  This work explores how Oracle mutexes operate, spin, and sleep. It develops
predictive mathematical model and discusses parameters and statistics related
to mutex performance tuning, as well as results of contention experiments.
</summary>
    <author>
      <name>Andrey Nikolaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Informatics MEDIAS2012.
  Cyprus, Limassol, May 7--14, 2012. ISBN 978-5-88835-023-2. 12 pages, 15
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4200v1</id>
    <updated>2013-01-17T19:42:55Z</updated>
    <published>2013-01-17T19:42:55Z</published>
    <title>Enabling Operator Reordering in Data Flow Programs Through Static Code
  Analysis</title>
    <summary>  In many massively parallel data management platforms, programs are
represented as small imperative pieces of code connected in a data flow. This
popular abstraction makes it hard to apply algebraic reordering techniques
employed by relational DBMSs and other systems that use an algebraic
programming abstraction. We present a code analysis technique based on reverse
data and control flow analysis that discovers a set of properties from user
code, which can be used to emulate algebraic optimizations in this setting.
</summary>
    <author>
      <name>Fabian Hueske</name>
    </author>
    <author>
      <name>Aljoscha Krettek</name>
    </author>
    <author>
      <name>Kostas Tzoumas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted and presented at the First International Workshop
  on Cross-model Language Design and Implementation (XLDI), affiliated with
  ICFP 2012, Copenhagen</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.4200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0418v1</id>
    <updated>2013-03-02T19:15:38Z</updated>
    <published>2013-03-02T19:15:38Z</published>
    <title>Transparent Data Encryption -- Solution for Security of Database
  Contents</title>
    <summary>  The present study deals with Transparent Data Encryption which is a
technology used to solve the problems of security of data. Transparent Data
Encryption means encrypting databases on hard disk and on any backup media.
Present day global business environment presents numerous security threats and
compliance challenges. To protect against data thefts and frauds we require
security solutions that are transparent by design.
</summary>
    <author>
      <name>Dr. Anwar Pasha Deshmukh</name>
    </author>
    <author>
      <name>Dr. Riyazuddin Qureshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications, Volume 2 No. 3, March 2011, pp 25-28. ISSN: 2156-5570(Online) &amp;
  ISSN: 2158-107X(Print)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.0418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5175v1</id>
    <updated>2013-03-21T06:14:05Z</updated>
    <published>2013-03-21T06:14:05Z</published>
    <title>Discovery of Convoys in Network Proximity Log</title>
    <summary>  This paper describes an algorithm for discovery of convoys in database with
proximity log. Traditionally, discovery of convoys covers trajectories
databases. This paper presents a model for context-aware browsing application
based on the network proximity. Our model uses mobile phone as proximity sensor
and proximity data replaces location information. As per our concept, any
existing or even especially created wireless network node could be used as
presence sensor that can discover access to some dynamic or user-generated
content. Content revelation in this model depends on rules based on the
proximity. Discovery of convoys in historical user's logs provides a new class
of rules for delivering local content to mobile subscribers.
</summary>
    <author>
      <name>Dmitry Namiot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0959v1</id>
    <updated>2013-04-03T14:11:04Z</updated>
    <published>2013-04-03T14:11:04Z</published>
    <title>Conditional Tables in practice</title>
    <summary>  Due to the ever increasing importance of the internet, interoperability of
heterogeneous data sources is as well of ever increasing importance.
Interoperability can be achieved e.g. through data integration and data
exchange. Common to both approaches is the need for the DBMS to be able to
store and query incomplete databases. In this report we present PossDB, a DBMS
capable of storing and querying incomplete databases. The system is wrapper
over PostgreSQL, and the query language is an extension of a subset of standard
SQL. Our experimental results show that our system scales well, actually better
than comparable systems.
</summary>
    <author>
      <name>Gosta Grahne</name>
    </author>
    <author>
      <name>Adrian Onet</name>
    </author>
    <author>
      <name>Nihat Tartal</name>
    </author>
    <link href="http://arxiv.org/abs/1304.0959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7285v1</id>
    <updated>2013-04-25T09:27:18Z</updated>
    <published>2013-04-25T09:27:18Z</published>
    <title>Traitement approximatif des requêtes flexibles avec groupement
  d'attributs et jointure</title>
    <summary>  This paper addresses the problem of approximate processing for flexible
queries in the form SELECT-FROM-WHERE-GROUP BY with join condition. It offers a
flexible framework for online aggregation while promoting response time at the
expense of result accuracy.
</summary>
    <author>
      <name>Minyar Sassi-Hidri</name>
    </author>
    <author>
      <name>Soukaina Ben Bdira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. The 13\`eme Conf\'erence Francophone sur l'Extraction et
  la Gestion des Connaissances (EGC), pp. 29-30, 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 3rd International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA), pp. 128-135, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3058v1</id>
    <updated>2013-05-14T08:31:10Z</updated>
    <published>2013-05-14T08:31:10Z</published>
    <title>Rule-Based Application Development using Webdamlog</title>
    <summary>  We present the WebdamLog system for managing distributed data on the Web in a
peer-to-peer manner. We demonstrate the main features of the system through an
application called Wepic for sharing pictures between attendees of the sigmod
conference. Using Wepic, the attendees will be able to share, download, rate
and annotate pictures in a highly decentralized manner. We show how WebdamLog
handles heterogeneity of the devices and services used to share data in such a
Web setting. We exhibit the simple rules that define the Wepic application and
show how to easily modify the Wepic application.
</summary>
    <author>
      <name>Serge Abiteboul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV</arxiv:affiliation>
    </author>
    <author>
      <name>Émilien Antoine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV</arxiv:affiliation>
    </author>
    <author>
      <name>Gerome Miklau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV, UMASS</arxiv:affiliation>
    </author>
    <author>
      <name>Julia Stoyanovich</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV, McGill</arxiv:affiliation>
    </author>
    <author>
      <name>Jules Testard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV, McGill</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD - Special Interest Group on Management Of Data (2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.3058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5653v1</id>
    <updated>2013-05-24T08:54:46Z</updated>
    <published>2013-05-24T08:54:46Z</published>
    <title>Geographica: A Benchmark for Geospatial RDF Stores</title>
    <summary>  Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently
been defined and corresponding geospatial RDF stores have been implemented.
However, there is no widely used benchmark for evaluating geospatial RDF stores
which takes into account recent advances to the state of the art in this area.
In this paper, we develop a benchmark, called Geographica, which uses both
real-world and synthetic data to test the offered functionality and the
performance of some prominent geospatial RDF stores.
</summary>
    <author>
      <name>George Garbis</name>
    </author>
    <author>
      <name>Kostis Kyzirakos</name>
    </author>
    <author>
      <name>Manolis Koubarakis</name>
    </author>
    <link href="http://arxiv.org/abs/1305.5653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6506v1</id>
    <updated>2013-05-28T14:16:11Z</updated>
    <published>2013-05-28T14:16:11Z</published>
    <title>Notes on Physical &amp; Logical Data Layouts</title>
    <summary>  In this short note I review and discuss fundamental options for physical and
logical data layouts as well as the impact of the choices on data processing. I
should say in advance that these notes offer no new insights, that is,
everything stated here has already been published elsewhere. In fact, it has
been published in so many different places, such as blog posts, in the
literature, etc. that the main contribution is to bring it all together in one
place.
</summary>
    <author>
      <name>Michael Hausenblas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.6506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0193v1</id>
    <updated>2013-06-30T10:07:22Z</updated>
    <published>2013-06-30T10:07:22Z</published>
    <title>A Sampling Algebra for Aggregate Estimation</title>
    <summary>  As of 2005, sampling has been incorporated in all major database systems.
While efficient sampling techniques are realizable, determining the accuracy of
an estimate obtained from the sample is still an unresolved problem. In this
paper, we present a theoretical framework that allows an elegant treatment of
the problem. We base our work on generalized uniform sampling (GUS), a class of
sampling methods that subsumes a wide variety of sampling techniques. We
introduce a key notion of equivalence that allows GUS sampling operators to
commute with selection and join, and derivation of confidence intervals. We
illustrate the theory through extensive examples and give indications on how to
use it to provide meaningful estimations in database systems.
</summary>
    <author>
      <name>Supriya Nirkhiwale</name>
    </author>
    <author>
      <name>Alin Dobra</name>
    </author>
    <author>
      <name>Chris Jermaine</name>
    </author>
    <link href="http://arxiv.org/abs/1307.0193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3448v1</id>
    <updated>2013-07-12T13:16:21Z</updated>
    <published>2013-07-12T13:16:21Z</published>
    <title>Evaluating a healthcare data warehouse for cancer diseases</title>
    <summary>  This paper presents the evaluation of the architecture of healthcare data
warehouse specific to cancer diseases. This data warehouse containing relevant
cancer medical information and patient data. The data warehouse provides the
source for all current and historical health data to help executive manager and
doctors to improve the decision making process for cancer patients. The
evaluation model based on Bill Inmon's definition of data warehouse is proposed
to evaluate the Cancer data warehouse.
</summary>
    <author>
      <name>Dr. Osama E. Sheta</name>
    </author>
    <author>
      <name>Ahmed Nour Eldeen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4519v1</id>
    <updated>2013-07-17T07:09:30Z</updated>
    <published>2013-07-17T07:09:30Z</published>
    <title>Extending the ER Model to relational Model novel transformation
  Algorithm: transforming relationship Types among Subtypes</title>
    <summary>  A novel approach for creating ER conceptual models and an algorithm for
transforming them to the relational model has been developed by modifying and
extending the existing methods. A part of the new algorithm has previously been
presented. This paper presents the rest of the algorithm. One of the objectives
of this paper is to use it as a supportive document for ongoing empirical
evaluations of the new approach being conducted using the cognitive engagement
method and with the participation of different segments of the field as
respondents.
</summary>
    <author>
      <name>Dhammika Pieris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7328v1</id>
    <updated>2013-07-28T03:50:16Z</updated>
    <published>2013-07-28T03:50:16Z</published>
    <title>Data Warehouse Success and Strategic Oriented Business Intelligence: A
  Theoretical Framework</title>
    <summary>  With the proliferation of the data warehouses as supportive decision making
tools, organizations are increasingly looking forward for a complete data
warehouse success model that would manage the enormous amounts of growing data.
It is therefore important to measure the success of these massive projects.
While general IS success models have received great deals of attention, few
research has been conducted to assess the success of data warehouses for
strategic business intelligence purposes. The framework developed in this study
consists of the following nine measures: Vendors and Consultants, Management
Actions, System Quality, Information Quality, Data Warehouse Usage, Perceived
utility, Individual Decision Making Impact, Organizational Decision Making
Impact, and Corporate Strategic Goals Attainment.
</summary>
    <author>
      <name>Eiad Basher Alhyasat</name>
    </author>
    <author>
      <name>Mahmoud Al-Dalahmeh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Management Research 5(3), 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.7328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8269v1</id>
    <updated>2013-07-31T10:21:33Z</updated>
    <published>2013-07-31T10:21:33Z</published>
    <title>Introducing Access Control in Webdamlog</title>
    <summary>  We survey recent work on the specification of an access control mechanism in
a collaborative environment. The work is presented in the context of the
WebdamLog language, an extension of datalog to a distributed context. We
discuss a fine-grained access control mechanism for intentional data based on
provenance as well as a control mechanism for delegation, i.e., for deploying
rules at remote peers.
</summary>
    <author>
      <name>Serge Abiteboul</name>
    </author>
    <author>
      <name>Émilien Antoine</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <author>
      <name>Julia Stoyanovich</name>
    </author>
    <author>
      <name>Vera Zaychik Moffitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1471v1</id>
    <updated>2013-08-07T04:12:50Z</updated>
    <published>2013-08-07T04:12:50Z</published>
    <title>Application of Inventory Management Principles for Efficient Data
  Placement in Storage Networks</title>
    <summary>  The principles and strategies found in material management are comparable and
analogue with the data management. This paper concentrates on the conversion of
product inventory management principles into data inventory management
principles. Efforts were made to enumerate various impacting parameters that
would be appropriate to consider if any data inventory model could be plotted.
</summary>
    <author>
      <name>R. Arokia Paul Rajan</name>
    </author>
    <author>
      <name>F. Sagayaraj Francis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  6, No 2, November 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.1471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2147v1</id>
    <updated>2013-08-09T15:04:09Z</updated>
    <published>2013-08-09T15:04:09Z</published>
    <title>Exploiting Locality in Lease-Based Replicated Transactional Memory via
  Task Migration</title>
    <summary>  We present Lilac-TM, the first locality-aware Distributed Software
Transactional Memory (DSTM) implementation. Lilac-TM is a fully decentralized
lease-based replicated DSTM. It employs a novel self- optimizing lease
circulation scheme based on the idea of dynamically determining whether to
migrate transactions to the nodes that own the leases required for their
validation, or to demand the acquisition of these leases by the node that
originated the transaction. Our experimental evaluation establishes that
Lilac-TM provides significant performance gains for distributed workloads
exhibiting data locality, while typically incurring no overhead for non-data
local workloads.
</summary>
    <author>
      <name>Danny Hendler</name>
    </author>
    <author>
      <name>Alex Naiman</name>
    </author>
    <author>
      <name>Sebastiano Peluso</name>
    </author>
    <author>
      <name>Francesco Quaglia</name>
    </author>
    <author>
      <name>Paolo Romano</name>
    </author>
    <author>
      <name>Adi Suissa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2310v1</id>
    <updated>2013-08-10T13:21:20Z</updated>
    <published>2013-08-10T13:21:20Z</published>
    <title>Mining Positive and Negative Association Rules Using CoherentApproach</title>
    <summary>  In the data mining field, association rules are discovered having domain
knowledge specified as a minimum support threshold. The accuracy in setting up
this threshold directly influences the number and the quality of association
rules discovered. Typically, before association rules are mined, a user needs
to determine a support threshold in order to obtain only the frequent item
sets. Having users to determine a support threshold attracts a number of
issues. We propose an association rule mining framework that does not require a
per-set support threshold. Often, the number of association rules, even though
large in number, misses some interesting rules and the rules quality
necessitates further analysis. As a result, decision making using these rules
could lead to risky actions.
</summary>
    <author>
      <name>Rakesh Duggirala</name>
    </author>
    <author>
      <name>P. Narayana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCTT-2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97Pxx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1334v2</id>
    <updated>2013-09-10T09:48:21Z</updated>
    <published>2013-09-05T12:48:03Z</published>
    <title>Proceedings of the 14th International Symposium on Database Programming
  Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento, Italy</title>
    <summary>  This volume contains the papers presented at the 14th Symposium on Database
Programming Languages (DBPL 2013) held on August 30th, 2013, in Riva del Garda,
co-located with the 39th International Conference on Very Large Databases (VLDB
2013). They cover a wide range of topics including the application of
programming language techniques to further the expressiveness of database
languages, schema management, and the practical use of XPath. To complement
this technical program, DBPL 2013 featured three invited talks by Serge
Abiteboul (Inria), J\'er\^ome Sim\'eon (IBM), and Soren Lassen (Facebook).
</summary>
    <author>
      <name>Todd J. Green</name>
    </author>
    <author>
      <name>Alan Schmitt</name>
    </author>
    <link href="http://arxiv.org/abs/1309.1334v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1334v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2371v1</id>
    <updated>2013-09-10T04:52:58Z</updated>
    <published>2013-09-10T04:52:58Z</published>
    <title>Performance analysis of modified algorithm for finding multilevel
  association rules</title>
    <summary>  Multilevel association rules explore the concept hierarchy at multiple levels
which provides more specific information. Apriori algorithm explores the single
level association rules. Many implementations are available of Apriori
algorithm. Fast Apriori implementation is modified to develop new algorithm for
finding multilevel association rules. In this study the performance of this new
algorithm is analyzed in terms of running time in seconds.
</summary>
    <author>
      <name>Arpna Shrivastava</name>
    </author>
    <author>
      <name>R. C. Jain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/cseij.2013.3401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/cseij.2013.3401" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 3, No. 4, August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2687v1</id>
    <updated>2013-09-10T23:06:57Z</updated>
    <published>2013-09-10T23:06:57Z</published>
    <title>CrowdPlanner: A Crowd-Based Route Recommendation System</title>
    <summary>  CrowdPlanner -- a novel crowd-based route recommendation system has been
developed, which requests human workers to evaluate candidates routes
recommended by different sources and methods, and determine the best route
based on the feedbacks of these workers. Our system addresses two critical
issues in its core components: a) task generation component generates a series
of informative and concise questions with optimized ordering for a given
candidate route set so that workers feel comfortable and easy to answer; and b)
worker selection component utilizes a set of selection criteria and an
efficient algorithm to find the most eligible workers to answer the questions
with high accuracy.
</summary>
    <author>
      <name>Han Su</name>
    </author>
    <link href="http://arxiv.org/abs/1309.2687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5821v1</id>
    <updated>2013-09-20T13:51:18Z</updated>
    <published>2013-09-20T13:51:18Z</published>
    <title>Undefined By Data: A Survey of Big Data Definitions</title>
    <summary>  The term big data has become ubiquitous. Owing to a shared origin between
academia, industry and the media there is no single unified definition, and
various stakeholders provide diverse and often contradictory definitions. The
lack of a consistent definition introduces ambiguity and hampers discourse
relating to big data. This short paper attempts to collate the various
definitions which have gained some degree of traction and to furnish a clear
and concise definition of an otherwise ambiguous term.
</summary>
    <author>
      <name>Jonathan Stuart Ward</name>
    </author>
    <author>
      <name>Adam Barker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Big data definition paper, 2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0141v4</id>
    <updated>2015-02-25T18:42:57Z</updated>
    <published>2013-10-01T05:02:14Z</published>
    <title>Hopping over Big Data: Accelerating Ad-hoc OLAP Queries with Grasshopper
  Algorithms</title>
    <summary>  This paper presents a family of algorithms for fast subset filtering within
ordered sets of integers representing composite keys. Applications include
significant acceleration of (ad-hoc) analytic queries against a data warehouse
without any additional indexing. The algorithms work for point, range and set
restrictions on multiple attributes, in any combination, and are inherently
multidimensional. The main idea consists in intelligent combination of
sequential crawling with jumps over large portions of irrelevant keys. The way
to combine them is adaptive to characteristics of the underlying data store.
</summary>
    <author>
      <name>Alexander Russakovsky</name>
    </author>
    <link href="http://arxiv.org/abs/1310.0141v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0141v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0229v2</id>
    <updated>2014-03-26T10:36:22Z</updated>
    <published>2013-10-01T10:31:31Z</published>
    <title>Evolutionary Algorithm for Graph Anonymization</title>
    <summary>  In recent years there has been a significant increase in the use of graphs as
a tool for representing information. It is very important to preserve the
privacy of users when one wants to publish this information, especially in the
case of social graphs. In this case, it is essential to implement an
anonymization process in the data in order to preserve users' privacy. In this
paper we present an algorithm for graph anonymization, called Evolutionary
Algorithm for Graph Anonymization (EAGA), based on edge modifications to
preserve the k-anonymity model.
</summary>
    <author>
      <name>Jordi Casas-Roma</name>
    </author>
    <author>
      <name>Jordi Herrera-Joancomartí</name>
    </author>
    <author>
      <name>Vicenç Torra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0229v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0229v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2375v1</id>
    <updated>2013-10-09T07:19:40Z</updated>
    <published>2013-10-09T07:19:40Z</published>
    <title>Web Usage Mining: Pattern Discovery and Forecasting</title>
    <summary>  Web usage mining: automatic discovery of patterns in clickstreams and
associated data collected or generated as a result of user interactions with
one or more Web sites. This paper describes web usage mining for our college
log files to analyze the behavioral patterns and profiles of users interacting
with a Web site. The discovered patterns are represented as clusters that are
frequently accessed by groups of visitors with common interests. In this paper,
the visitors and hits were forecasted to predict the further access statistics.
</summary>
    <author>
      <name>Dhanamma Jagli</name>
    </author>
    <author>
      <name>Sangeeta Oswal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IFRSA International Journal of Data Warehousing &amp; Mining |Vol
  2|issue4|November 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.2375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.4647v1</id>
    <updated>2013-10-17T10:21:58Z</updated>
    <published>2013-10-17T10:21:58Z</published>
    <title>Census Data Mining and Data Analysis using WEKA</title>
    <summary>  Data mining (also known as knowledge discovery from databases) is the process
of extraction of hidden, previously unknown and potentially useful information
from databases. The outcome of the extracted data can be analyzed for the
future planning and development perspectives. In this paper, we have made an
attempt to demonstrate how one can extract the local (district) level census,
socio-economic and population related other data for knowledge discovery and
their analysis using the powerful data mining tool Weka.
</summary>
    <author>
      <name>Sudhir B Jagtap</name>
    </author>
    <author>
      <name>Kodge B. G</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">06 pages, 03 figures. International Conference in Emerging Trends in
  Science, Technology and Management-2013, Singapore</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.4647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.4647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5254v1</id>
    <updated>2013-10-19T17:30:48Z</updated>
    <published>2013-10-19T17:30:48Z</published>
    <title>Real Time Data Warehouse</title>
    <summary>  Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged
as a fast growing reporting and analysis technique in early 1980s. Today, it
has almost replaced relational databases. However, with passage of time, static
and historic data of DWs could not produce Real Time reporting and analysis,
thus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).
Although, there are problems with RTDWs, but with advancement in technology and
researchers focus, RTDWs will be able to generate real time reports, analysis
and forecasting.
</summary>
    <author>
      <name>Syed Ijaz Ahmad Bukhari</name>
    </author>
    <link href="http://arxiv.org/abs/1310.5254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.7829v1</id>
    <updated>2013-10-29T15:17:39Z</updated>
    <published>2013-10-29T15:17:39Z</published>
    <title>About Summarization in Large Fuzzy Databases</title>
    <summary>  Moved by the need increased for modeling of the fuzzy data, the success of
the systems of exact generation of summary of data, we propose in this paper, a
new approach of generation of summary from fuzzy data called Fuzzy-SaintEtiQ.
This approach is an extension of the SaintEtiQ model to support the fuzzy data.
It presents the following optimizations such as 1) the minimization of the
expert risk; 2) the construction of a more detailed and more precise summaries
hierarchy, and 3) the co-operation with the user by giving him fuzzy summaries
in different hierarchical levels
</summary>
    <author>
      <name>Ines Benali Sougui</name>
    </author>
    <author>
      <name>Minyar Sassi Hidri</name>
    </author>
    <author>
      <name>Amel Grissa-Touzi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA), pp. 87-94, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.7829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.7829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0320v3</id>
    <updated>2018-07-03T15:27:41Z</updated>
    <published>2013-11-01T22:52:00Z</published>
    <title>An Improved Solution for Restricted and Uncertain TRQ</title>
    <summary>  CSPTRQ is an interesting problem and its has attracted much attention. The
CSPTRQ is a variant of the traditional PTRQ. As objects moving in a
constrained-space are common, clearly, it can also find many applications. At
the first sight, our problem can be easily tackled by extending existing
methods used to answer the PTRQ. Unfortunately, those classical techniques are
not well suitable for our problem, due to a set of new challenges. We develop
targeted solutions and demonstrate the efficiency and effectiveness of the
proposed methods through extensive experiments.
</summary>
    <author>
      <name>Jack Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0320v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0320v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; G.3; G.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0350v1</id>
    <updated>2013-11-02T06:55:10Z</updated>
    <published>2013-11-02T06:55:10Z</published>
    <title>Sequential Mining: Patterns and Algorithms Analysis</title>
    <summary>  This paper presents and analysis the common existing sequential pattern
mining algorithms. It presents a classifying study of sequential pattern-mining
algorithms into five extensive classes. First, on the basis of Apriori-based
algorithm, second on Breadth First Search-based strategy, third on Depth First
Search strategy, fourth on sequential closed-pattern algorithm and five on the
basis of incremental pattern mining algorithms. At the end, a comparative
analysis is done on the basis of important key features supported by various
algorithms. This study gives an enhancement in the understanding of the
approaches of sequential pattern mining.
</summary>
    <author>
      <name>Thabet Slimani</name>
    </author>
    <author>
      <name>Amor Lazzez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Electronics Research, Volume
  2, Issue 5, October 2013, pp 639-647</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.0350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.0156v1</id>
    <updated>2013-11-30T21:33:49Z</updated>
    <published>2013-11-30T21:33:49Z</published>
    <title>Datom: Towards modular data management</title>
    <summary>  Recent technology breakthroughs have enabled data collection of unprecedented
scale, rate, variety and complexity that has led to an explosion in data
management requirements. Existing theories and techniques are not adequate to
fulfil these requirements. We endeavour to rethink the way data management
research is being conducted and we propose to work towards modular data
management that will allow for unification of the expression of data management
problems and systematization of their solution. The core of such an approach is
the novel notion of a datom, i.e. a data management atom, which encapsulates
generic data management provision. The datom is the foundation for comparison,
customization and re-usage of data management problems and solutions. The
proposed approach can signal a revolution in data management research and a
long anticipated evolution in data management engineering.
</summary>
    <author>
      <name>Verena Kantere</name>
    </author>
    <link href="http://arxiv.org/abs/1312.0156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.0156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2353v1</id>
    <updated>2013-12-09T09:34:05Z</updated>
    <published>2013-12-09T09:34:05Z</published>
    <title>On the difference between checking integrity constraints before or after
  updates</title>
    <summary>  Integrity checking is a crucial issue, as databases change their instance all
the time and therefore need to be checked continuously and rapidly. Decades of
research have produced a plethora of methods for checking integrity constraints
of a database in an incremental manner. However, not much has been said about
when to check integrity. In this paper, we study the differences and
similarities between checking integrity before an update (a.k.a. pre-test) or
after (a.k.a. post-test) in order to assess the respective convenience and
properties.
</summary>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.2353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5148v2</id>
    <updated>2014-04-14T05:08:00Z</updated>
    <published>2013-12-18T14:17:09Z</published>
    <title>Object Selection under Team Context</title>
    <summary>  Context-aware database has drawn increasing attention from both industry and
academia recently by taking users' current situation and environment into
consideration. However, most of the literature focus on individual context,
overlooking the team users. In this paper, we investigate how to integrate team
context into database query process to help the users' get top-ranked database
tuples and make the team more competitive. We introduce naive and optimized
query algorithm to select the suitable records and show that they output the
same results while the latter is more computational efficient. Extensive
empirical studies are conducted to evaluate the query approaches and
demonstrate their effectiveness and efficiency.
</summary>
    <author>
      <name>Xiaolu Lu</name>
    </author>
    <author>
      <name>Dongxu Li</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Ling Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1312.5148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1327v1</id>
    <updated>2014-02-06T11:37:45Z</updated>
    <published>2014-02-06T11:37:45Z</published>
    <title>A Survey on Spatial Co-location Patterns Discovery from Spatial Datasets</title>
    <summary>  Spatial data mining or Knowledge discovery in spatial database is the
extraction of implicit knowledge, spatial relations and spatial patterns that
are not explicitly stored in databases. Co-location patterns discovery is the
process of finding the subsets of features that are frequently located together
in the same geographic area. In this paper, we discuss the different approaches
like Rule based approach, Join-less approach, Partial Join approach and
Constraint neighborhood based approach for finding co-location patterns.
</summary>
    <author>
      <name>Mr. Rushirajsinh L. Zala</name>
    </author>
    <author>
      <name>Mr. Brijesh B. Mehta</name>
    </author>
    <author>
      <name>Mr. Mahipalsinh R. Zala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V7P140</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V7P140" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCTT 7(3):137-142, January 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3461v1</id>
    <updated>2014-04-14T05:20:48Z</updated>
    <published>2014-04-14T05:20:48Z</published>
    <title>A 2D based Partition Strategy for Solving Ranking under Team Context
  (RTP)</title>
    <summary>  In this paper, we propose a 2D based partition method for solving the problem
of Ranking under Team Context(RTC) on datasets without a priori. We first map
the data into 2D space using its minimum and maximum value among all
dimensions. Then we construct window queries with consideration of current team
context. Besides, during the query mapping procedure, we can pre-prune some
tuples which are not top ranked ones. This pre-classified step will defer
processing those tuples and can save cost while providing solutions for the
problem. Experiments show that our algorithm performs well especially on large
datasets with correctness.
</summary>
    <author>
      <name>Xiaolu Lu</name>
    </author>
    <author>
      <name>Dongxu Li</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Ling Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1404.3461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6857v2</id>
    <updated>2014-06-28T23:32:35Z</updated>
    <published>2014-04-28T02:58:31Z</published>
    <title>Causality in Databases: The Diagnosis and Repair Connections</title>
    <summary>  In this work we establish and investigate the connections between causality
for query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. The vast body of research on database repairs can be applied
to the newer problem of determining actual causes for query answers. By
formulating a causality problem as a diagnosis problem, we manage to
characterize causes in terms of a system's diagnoses.
</summary>
    <author>
      <name>Babak Salimi</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 15th International Workshop on Non-Monotonic Reasoning (NMR
  2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6857v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6857v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1339v1</id>
    <updated>2014-05-06T16:22:38Z</updated>
    <published>2014-05-06T16:22:38Z</published>
    <title>General upper bounds for well-behaving goodness measures on dependency
  rules</title>
    <summary>  In the search for statistical dependency rules, a crucial task is to restrict
the search space by estimating upper bounds for the goodness of yet
undiscovered rules. In this paper, we show that all well-behaving goodness
measures achieve their maximal values in the same points. Therefore, the same
generic search strategy can be applied with any of these measures. The notion
of well-behaving measures is based on the classical axioms for any proper
goodness measures, and extended to negative dependencies, as well. As an
example, we show that several commonly used goodness measures are
well-behaving.
</summary>
    <author>
      <name>Wilhelmiina Hämäläinen</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1360v1</id>
    <updated>2014-05-06T16:54:20Z</updated>
    <published>2014-05-06T16:54:20Z</published>
    <title>Assessing the statistical significance of association rules</title>
    <summary>  An association rule is statistically significant, if it has a small
probability to occur by chance. It is well-known that the traditional
frequency-confidence framework does not produce statistically significant
rules. It can both accept spurious rules (type 1 error) and reject significant
rules (type 2 error). The same problem concerns other commonly used
interestingness measures and pruning heuristics.
  In this paper, we inspect the most common measure functions - frequency,
confidence, degree of dependence, $\chi^2$, correlation coefficient, and
$J$-measure - and redundancy reduction techniques. For each technique, we
analyze whether it can make type 1 or type 2 error and the conditions under
which the error occurs. In addition, we give new theoretical results which can
be use to guide the search for statistically significant association rules.
</summary>
    <author>
      <name>Wilhelmiina Hämäläinen</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1851v1</id>
    <updated>2014-05-08T09:26:52Z</updated>
    <published>2014-05-08T09:26:52Z</published>
    <title>Output Privacy Protection With Pattern-Based Heuristic Algorithm</title>
    <summary>  Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at
bridging the gap between the collaborative data mining and data confidentiality
There are many different approaches which have been adopted for PPDM, of them
the rule hiding approach is used in this article. This approach ensures output
privacy that prevent the mined patterns(itemsets) from malicious inference
problems. An efficient algorithm named as Pattern-based Maxcover Algorithm is
proposed with experimental results. This algorithm minimizes the dissimilarity
between the source and the released database; Moreover the patterns protected
cannot be retrieved from the released database by an adversary or counterpart
even with an arbitrarily low support threshold.
</summary>
    <author>
      <name>P. Cynthia Selvi</name>
    </author>
    <author>
      <name>A. R. Mohammed Shanavas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2014.6210</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2014.6210" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1912v1</id>
    <updated>2014-05-08T13:06:34Z</updated>
    <published>2014-05-08T13:06:34Z</published>
    <title>The Efficiency Examination of Teaching of Different Normalization
  Methods</title>
    <summary>  Normalization is an important database design method, in the course of the
teaching of data modeling the understanding and applying of this method cause
problems for students the most. For improving the efficiency of learning
normalization we looked for alternative normalization methods and introduced
them into education. We made a survey among engineer students how efficient
could they execute the normalization with different methods. We executed
statistical and data mining examinations to decide whether any of the methods
resulted significantly better solutions.
</summary>
    <author>
      <name>Márta Czenky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2014.6201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2014.6201" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems ( IJDMS )
  Vol.6, No.2, April 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.1912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4607v1</id>
    <updated>2014-05-19T05:09:50Z</updated>
    <published>2014-05-19T05:09:50Z</published>
    <title>$Υ$-DB: Managing scientific hypotheses as uncertain data</title>
    <summary>  In view of the paradigm shift that makes science ever more data-driven, we
consider deterministic scientific hypotheses as uncertain data. This vision
comprises a probabilistic database (p-DB) design methodology for the systematic
construction and management of U-relational hypothesis DBs, viz.,
$\Upsilon$-DBs. It introduces hypothesis management as a promising new class of
applications for p-DBs. We illustrate the potential of $\Upsilon$-DB as a tool
for deep predictive analytics.
</summary>
    <author>
      <name>Bernardo Gonçalves</name>
    </author>
    <author>
      <name>Fabio Porto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in PVLDB 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PVLDB 7(11):959-62, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.4607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5671v1</id>
    <updated>2014-05-22T08:56:01Z</updated>
    <published>2014-05-22T08:56:01Z</published>
    <title>A Logical Formalization of a Secure XML Database</title>
    <summary>  In this paper, we first define a logical theory representing an XML database
supporting XPath as query language and XUpdate as modification language. We
then extend our theory with predicates allowing us to specify the security
policy protecting the database. The security policy includes rules addressing
the read and write privileges. We propose axioms to derive the database view
each user is permitted to see. We also propose axioms to derive the new
database content after an update.
</summary>
    <author>
      <name>Alban Gabillon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GePaSUD</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1405.5671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5905v2</id>
    <updated>2015-05-17T23:01:31Z</updated>
    <published>2014-05-22T20:43:51Z</published>
    <title>Managing large-scale scientific hypotheses as uncertain and
  probabilistic data with support for predictive analytics</title>
    <summary>  The sheer scale of high-resolution raw data generated by simulation has
motivated non-conventional approaches for data exploration referred as
`immersive' and `in situ' query processing of the raw simulation data. Another
step towards supporting scientific progress is to enable data-driven hypothesis
management and predictive analytics out of simulation results. We present a
synthesis method and tool for encoding and managing competing hypotheses as
uncertain data in a probabilistic database that can be conditioned in the
presence of observations.
</summary>
    <author>
      <name>Bernardo Gonçalves</name>
    </author>
    <author>
      <name>Fabio Porto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCSE.2015.102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCSE.2015.102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computing in Science and Eng. 17(5):35-43, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.5905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2125v1</id>
    <updated>2014-06-09T10:33:41Z</updated>
    <published>2014-06-09T10:33:41Z</published>
    <title>From XML Schema to JSON Schema: Translation with CHR</title>
    <summary>  Despite its rising popularity as data format especially for web services, the
software ecosystem around the JavaScript Object Notation (JSON) is not as
widely distributed as that of XML. For both data formats there exist schema
languages to specify the structure of instance documents, but there is
currently no opportunity to translate already existing XML Schema documents
into equivalent JSON Schemas.
  In this paper we introduce an implementation of a language translator. It
takes an XML Schema and creates its equivalent JSON Schema document. Our
approach is based on Prolog and CHR. By unfolding the XML Schema document into
CHR constraints, it is possible to specify the concrete translation rules in a
declarative way.
</summary>
    <author>
      <name>Falco Nogatz</name>
    </author>
    <author>
      <name>Thom Frühwirth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of CHR 2014 proceedings (arXiv:1406.1510)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5917v1</id>
    <updated>2014-06-23T14:21:13Z</updated>
    <published>2014-06-23T14:21:13Z</published>
    <title>BSTree: an Incremental Indexing Structure for Similarity Search and Real
  Time Monitoring of Data Streams</title>
    <summary>  In this work, a new indexing technique of data streams called BSTree is
proposed. This technique uses the method of data discretization, SAX [4], to
reduce online the dimensionality of data streams. It draws on Btree to build
the index and finally uses an LRV (least Recently visited) pruning technique to
rid the index structure from data whose last visit time exceeds a threshold
value and thus minimizes response time for similarity search queries.
</summary>
    <author>
      <name>Abdelwaheb Ferchichi</name>
    </author>
    <author>
      <name>Mohamed Salah Gouider</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Future Information Technology Lecture Notes in Electrical
  Engineering Volume 276, 2014, pp 185-190</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.5917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7371v1</id>
    <updated>2014-06-28T08:18:57Z</updated>
    <published>2014-06-28T08:18:57Z</published>
    <title>Using Apriori with WEKA for Frequent Pattern Mining</title>
    <summary>  Knowledge exploration from the large set of data,generated as a result of the
various data processing activities due to data mining only. Frequent Pattern
Mining is a very important undertaking in data mining. Apriori approach applied
to generate frequent item set generally espouse candidate generation and
pruning techniques for the satisfaction of the desired objective. This paper
shows how the different approaches achieve the objective of frequent mining
along with the complexities required to perform the job. This paper
demonstrates the use of WEKA tool for association rule mining using Apriori
algorithm.
</summary>
    <author>
      <name>Paresh Tanna</name>
    </author>
    <author>
      <name>Yogesh Ghodasara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V12P223</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V12P223" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 4 Figures, "Published with International Journal of
  Engineering Trends and Technology (IJETT)"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Trends and Technology
  (IJETT), V12(3), 127-131, June 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.7371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0116v1</id>
    <updated>2014-07-01T07:03:22Z</updated>
    <published>2014-07-01T07:03:22Z</published>
    <title>Differential privacy for counting queries: can Bayes estimation help
  uncover the true value?</title>
    <summary>  Differential privacy is achieved by the introduction of Laplacian noise in
the response to a query, establishing a precise trade-off between the level of
differential privacy and the accuracy of the database response (via the amount
of noise introduced). Multiple queries may improve the accuracy but erode the
privacy budget. We examine the case where we submit just a single counting
query. We show that even in that case a Bayesian approach may be used to
improve the accuracy for the same amount of noise injected, if we know the size
of the database and the probability of a positive response to the query.
</summary>
    <author>
      <name>Maurizio Naldi</name>
    </author>
    <author>
      <name>Giuseppe D'Acquisto</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.2.4; K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3191v1</id>
    <updated>2014-07-11T15:06:03Z</updated>
    <published>2014-07-11T15:06:03Z</published>
    <title>A Comparison of Blocking Methods for Record Linkage</title>
    <summary>  Record linkage seeks to merge databases and to remove duplicates when unique
identifiers are not available. Most approaches use blocking techniques to
reduce the computational complexity associated with record linkage. We review
traditional blocking techniques, which typically partition the records
according to a set of field attributes, and consider two variants of a method
known as locality sensitive hashing, sometimes referred to as "private
blocking." We compare these approaches in terms of their recall, reduction
ratio, and computational complexity. We evaluate these methods using different
synthetic datafiles and conclude with a discussion of privacy-related issues.
</summary>
    <author>
      <name>Rebecca C. Steorts</name>
    </author>
    <author>
      <name>Samuel L. Ventura</name>
    </author>
    <author>
      <name>Mauricio Sadinle</name>
    </author>
    <author>
      <name>Stephen E. Fienberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 tables, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6812v1</id>
    <updated>2014-07-25T08:33:12Z</updated>
    <published>2014-07-25T08:33:12Z</published>
    <title>Aber-OWL: a framework for ontology-based data access in biology</title>
    <summary>  Many ontologies have been developed in biology and these ontologies
increasingly contain large volumes of formalized knowledge commonly expressed
in the Web Ontology Language (OWL). Computational access to the knowledge
contained within these ontologies relies on the use of automated reasoning. We
have developed the Aber-OWL infrastructure that provides reasoning services for
bio-ontologies. Aber-OWL consists of an ontology repository, a set of web
services and web interfaces that enable ontology-based semantic access to
biological data and literature. Aber-OWL is freely available at
http://aber-owl.net.
</summary>
    <author>
      <name>Robert Hoehndorf</name>
    </author>
    <author>
      <name>Luke Slater</name>
    </author>
    <author>
      <name>Paul N. Schofield</name>
    </author>
    <author>
      <name>Georgios V. Gkoutos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s12859-015-0456-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s12859-015-0456-9" rel="related"/>
    <link href="http://arxiv.org/abs/1407.6812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2800v2</id>
    <updated>2014-08-27T12:33:33Z</updated>
    <published>2014-08-12T18:53:23Z</published>
    <title>Supporting SPARQL Update Queries in RDF-XML Integration</title>
    <summary>  The Web of Data encourages organizations and companies to publish their data
according to the Linked Data practices and offer SPARQL endpoints. On the other
hand, the dominant standard for information exchange is XML. The SPARQL2XQuery
Framework focuses on the automatic translation of SPARQL queries in XQuery
expressions in order to access XML data across the Web. In this paper, we
outline our ongoing work on supporting update queries in the RDF-XML
integration scenario.
</summary>
    <author>
      <name>Nikos Bikakis</name>
    </author>
    <author>
      <name>Chrisa Tsinaraki</name>
    </author>
    <author>
      <name>Ioannis Stavrakantonakis</name>
    </author>
    <author>
      <name>Stavros Christodoulakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Semantic Web Conference (ISWC '14)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.2800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4793v1</id>
    <updated>2014-08-19T22:57:41Z</updated>
    <published>2014-08-19T22:57:41Z</published>
    <title>Restpark: Minimal RESTful API for Retrieving RDF Triples</title>
    <summary>  How do RDF datasets currently get published on the Web? They are either
available as large RDF files, which need to be downloaded and processed
locally, or they exist behind complex SPARQL endpoints. By providing a RESTful
API that can access triple data, we allow users to query a dataset through a
simple interface based on just a couple of HTTP parameters. If RDF resources
were published this way we could quickly build applications that depend on
these datasets, without having to download and process them locally. This is
what Restpark is: a set of HTTP GET parameters that servers need to handle, and
respond with JSON-LD.
</summary>
    <author>
      <name>Luca Matteis</name>
    </author>
    <link href="http://arxiv.org/abs/1408.4793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6395v2</id>
    <updated>2014-09-03T19:09:38Z</updated>
    <published>2014-08-27T12:31:17Z</published>
    <title>Bridging the Semantic Gap between RDF and SPARQL using Completeness
  Statements [Extended Version]</title>
    <summary>  RDF data is often treated as incomplete, following the Open-World Assumption.
On the other hand, SPARQL, the standard query language over RDF, usually
follows the Closed-World Assumption, assuming RDF data to be complete. This
gives rise to a semantic gap between RDF and SPARQL. In this paper, we address
how to close the semantic gap between RDF and SPARQL in terms of certain
answers and possible answers using completeness statements.
</summary>
    <author>
      <name>Fariz Darari</name>
    </author>
    <author>
      <name>Simon Razniewski</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is an extended version with proofs of a poster paper at
  ISWC 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.6395v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6395v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05546v1</id>
    <updated>2015-01-22T15:58:42Z</updated>
    <published>2015-01-22T15:58:42Z</published>
    <title>Using a Big Data Database to Identify Pathogens in Protein Data Space</title>
    <summary>  Current metagenomic analysis algorithms require significant computing
resources, can report excessive false positives (type I errors), may miss
organisms (type II errors / false negatives), or scale poorly on large
datasets. This paper explores using big data database technologies to
characterize very large metagenomic DNA sequences in protein space, with the
ultimate goal of rapid pathogen identification in patient samples. Our approach
uses the abilities of a big data databases to hold large sparse associative
array representations of genetic data to extract statistical patterns about the
data that can be used in a variety of ways to improve identification
algorithms.
</summary>
    <author>
      <name>Ashley Mae Conard</name>
    </author>
    <author>
      <name>Stephanie Dodson</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Darrell Ricke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.05546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06964v1</id>
    <updated>2015-01-27T11:03:12Z</updated>
    <published>2015-01-27T11:03:12Z</published>
    <title>Learning Analytics: A Survey</title>
    <summary>  Learning analytics is a research topic that is gaining increasing popularity
in recent time. It analyzes the learning data available in order to make aware
or improvise the process itself and/or the outcome such as student performance.
In this survey paper, we look at the recent research work that has been
conducted around learning analytics, framework and integrated models, and
application of various models and data mining techniques to identify students
at risk and to predict student performance.
</summary>
    <author>
      <name>Usha Keshavamurthy</name>
    </author>
    <author>
      <name>H. S. Guruprasad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V18P155</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V18P155" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT)
  Volume 18 Number 6 Dec 2014 Page 260 - 264</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.06964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03247v1</id>
    <updated>2015-04-13T16:29:10Z</updated>
    <published>2015-04-13T16:29:10Z</published>
    <title>Handling Skew in Multiway Joins in Parallel Processing</title>
    <summary>  Handling skew is one of the major challenges in query processing. In
distributed computational environments such as MapReduce, uneven distribution
of the data to the servers is not desired. One of the dominant measures that we
want to optimize in distributed environments is communication cost. In a
MapReduce job this is the amount of data that is transferred from the mappers
to the reducers. In this paper we will introduce a novel technique for handling
skew when we want to compute a multiway join in one MapReduce round with
minimum communication cost. This technique is actually an adaptation of the
Shares algorithm [Afrati et. al, TKDE 2011].
</summary>
    <author>
      <name>Foto N. Afrati</name>
    </author>
    <author>
      <name>Jeffrey D. Ullman</name>
    </author>
    <author>
      <name>Angelos Vasilakopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03386v1</id>
    <updated>2015-04-13T23:01:58Z</updated>
    <published>2015-04-13T23:01:58Z</published>
    <title>Tractable Query Answering and Optimization for Extensions of
  Weakly-Sticky Datalog+-</title>
    <summary>  We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic
subclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that
of weakly-sticky (WS) programs, which appear in our applications to data
quality. For WChS programs we propose a practical, polynomial-time query
answering algorithm (QAA). We establish that the two classes are closed under
magic-sets rewritings. As a consequence, QAA can be applied to the optimized
programs. QAA takes as inputs the program (including the query) and semantic
information about the "finiteness" of predicate positions. For the syntactic
subclasses JWS and WS of WChS, this additional information is computable.
</summary>
    <author>
      <name>Mostafa Milani</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proc. Alberto Mendelzon WS on Foundations of Data
  Management (AMW15)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04031v1</id>
    <updated>2015-04-15T20:06:33Z</updated>
    <published>2015-04-15T20:06:33Z</published>
    <title>Mining Semi-structured Data</title>
    <summary>  The need for discovering knowledge from XML documents according to both
structure and content features has become challenging, due to the increase in
application contexts for which handling both structure and content information
in XML data is essential. So, the challenge is to find an hierarchical
structure which ensure a combination of data levels and their representative
structures. In this work, we will be based on the Formal Concept Analysis-based
views to index and query both content and structure. We evaluate given
structure in a querying process which allows the searching of user query
answers.
</summary>
    <author>
      <name>Olfa Arfaoui</name>
    </author>
    <author>
      <name>Minyar Sassi Hidri</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Conference on Web and Information
  Technologies (ICWIT), pp. 51-60, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.04031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07597v1</id>
    <updated>2015-04-27T11:53:01Z</updated>
    <published>2015-04-27T11:53:01Z</published>
    <title>Duplicate Detection with Efficient Language Models for Automatic
  Bibliographic Heterogeneous Data Integration</title>
    <summary>  We present a new method to detect duplicates used to merge different
bibliographic record corpora with the help of lexical and social information.
As we show, a trivial key is not available to delete useless documents. Merging
heteregeneous document databases to get a maximum of information can be of
interest. In our case we try to build a document corpus about the TOR molecule
so as to extract relationships with other gene components from PubMed and
WebOfScience document databases. Our approach makes key fingerprints based on
n-grams. We made two documents gold standards using this corpus to make an
evaluation. Comparison with other well-known methods in deduplication gives
best scores of recall (95\%) and precision (100\%).
</summary>
    <author>
      <name>Nicolas Turenne</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04180v1</id>
    <updated>2015-07-15T11:59:07Z</updated>
    <published>2015-07-15T11:59:07Z</published>
    <title>Wikidata through the Eyes of DBpedia</title>
    <summary>  DBpedia is one of the first and most prominent nodes of the Linked Open Data
cloud. It provides structured data for more than 100 Wikipedia language
editions as well as Wikimedia Commons, has a mature ontology and a stable and
thorough Linked Data publishing lifecycle. Wikidata, on the other hand, has
recently emerged as a user curated source for structured information which is
included in Wikipedia. In this paper, we present how Wikidata is incorporated
in the DBpedia ecosystem. Enriching DBpedia with structured information from
Wikidata provides added value for a number of usage scenarios. We outline those
scenarios and describe the structure and conversion process of the
DBpediaWikidata dataset.
</summary>
    <author>
      <name>Ali Ismayilov</name>
    </author>
    <author>
      <name>Dimitris Kontokostas</name>
    </author>
    <author>
      <name>Sören Auer</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <author>
      <name>Sebastian Hellmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04955v1</id>
    <updated>2015-07-17T13:04:32Z</updated>
    <published>2015-07-17T13:04:32Z</published>
    <title>Structurally Tractable Uncertain Data</title>
    <summary>  Many data management applications must deal with data which is uncertain,
incomplete, or noisy. However, on existing uncertain data representations, we
cannot tractably perform the important query evaluation tasks of determining
query possibility, certainty, or probability: these problems are hard on
arbitrary uncertain input instances. We thus ask whether we could restrict the
structure of uncertain data so as to guarantee the tractability of exact query
evaluation. We present our tractability results for tree and tree-like
uncertain data, and a vision for probabilistic rule reasoning. We also study
uncertainty about order, proposing a suitable representation, and study
uncertain data conditioned by additional observations.
</summary>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2744680.2744690</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2744680.2744690" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure, 1 table. To appear in SIGMOD/PODS PhD Symposium
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00703v1</id>
    <updated>2015-08-04T08:42:41Z</updated>
    <published>2015-08-04T08:42:41Z</published>
    <title>Parameter Database : Data-centric Synchronization for Scalable Machine
  Learning</title>
    <summary>  We propose a new data-centric synchronization framework for carrying out of
machine learning (ML) tasks in a distributed environment. Our framework
exploits the iterative nature of ML algorithms and relaxes the application
agnostic bulk synchronization parallel (BSP) paradigm that has previously been
used for distributed machine learning. Data-centric synchronization complements
function-centric synchronization based on using stale updates to increase the
throughput of distributed ML computations. Experiments to validate our
framework suggest that we can attain substantial improvement over BSP while
guaranteeing sequential correctness of ML tasks.
</summary>
    <author>
      <name>Naman Goel</name>
    </author>
    <author>
      <name>Divyakant Agrawal</name>
    </author>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <author>
      <name>Ahmed Elmagarmid</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00524v2</id>
    <updated>2017-12-12T18:24:33Z</updated>
    <published>2015-12-30T22:45:56Z</published>
    <title>Ideal Databases</title>
    <summary>  From algebraic geometry perspective database relations are succinctly defined
as Finite Varieties. After establishing basic framework, we give analytic proof
of Heath theorem from Database Dependency theory. Next, we leverage
Algebra/Geometry dictionary and focus on algebraic counterparts of finite
varieties, polynomial ideals. It is well known that intersection and sum of
ideals are lattice operations. We generalize this fact to ideals from different
rings, therefore establishing that algebra of ideals is Relational Lattice. The
final stop is casting the framework into Linear Algebra, and traversing to
Quantum Theory.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Amended the introduction; added CoCoA section</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03240v2</id>
    <updated>2016-04-20T13:42:55Z</updated>
    <published>2016-01-13T13:40:55Z</published>
    <title>Counting Answers to Existential Positive Queries: A Complexity
  Classification</title>
    <summary>  Existential positive formulas form a fragment of first-order logic that
includes and is semantically equivalent to unions of conjunctive queries, one
of the most important and well-studied classes of queries in database theory.
We consider the complexity of counting the number of answers to existential
positive formulas on finite structures and give a trichotomy theorem on query
classes, in the setting of bounded arity. This theorem generalizes and unifies
several known results on the complexity of conjunctive queries and unions of
conjunctive queries.
</summary>
    <author>
      <name>Hubie Chen</name>
    </author>
    <author>
      <name>Stefan Mengel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1501.07195</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04980v1</id>
    <updated>2016-01-19T16:24:43Z</updated>
    <published>2016-01-19T16:24:43Z</published>
    <title>Integrity Constraints for General-Purpose Knowledge Bases</title>
    <summary>  Integrity constraints in databases have been studied extensively since the
1980s, and they are considered essential to guarantee database integrity. In
recent years, several authors have studied how the same notion can be adapted
to reasoning frameworks, in such a way that they achieve the purpose of
guaranteeing a system's consistency, but are kept separate from the reasoning
mechanisms.
  In this paper we focus on multi-context systems, a general-purpose framework
for combining heterogeneous reasoning systems, enhancing them with a notion of
integrity constraints that generalizes the corresponding concept in the
database world.
</summary>
    <author>
      <name>Luís Cruz-Filipe</name>
    </author>
    <author>
      <name>Isabel Nunes</name>
    </author>
    <author>
      <name>Peter Schneider-Kamp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-30024-5_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-30024-5_13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">FoIKS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00503v1</id>
    <updated>2016-02-01T12:49:25Z</updated>
    <published>2016-02-01T12:49:25Z</published>
    <title>GRAD: On Graph Database Modeling</title>
    <summary>  Graph databases have emerged as the fundamental technology underpinning
trendy application domains where traditional databases are not well-equipped to
handle complex graph data. However, current graph databases support basic graph
structures and integrity constraints with no standard algebra. In this paper,
we introduce GRAD, a native and generic graph database model. GRAD goes beyond
traditional graph database models, which support simple graph structures and
constraints. Instead, GRAD presents a complete graph database model supporting
advanced graph structures, a set of well-defined constraints over these
structures and a powerful graph analysis-oriented algebra.
</summary>
    <author>
      <name>Amine Ghrab</name>
    </author>
    <author>
      <name>Oscar Romero</name>
    </author>
    <author>
      <name>Sabri Skhiri</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <author>
      <name>Esteban Zimányi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06458v1</id>
    <updated>2016-02-20T20:57:59Z</updated>
    <published>2016-02-20T20:57:59Z</published>
    <title>Causes for Query Answers from Databases, Datalog Abduction and
  View-Updates: The Presence of Integrity Constraints</title>
    <summary>  Causality has been recently introduced in databases, to model, characterize
and possibly compute causes for query results (answers). Connections between
queryanswer causality, consistency-based diagnosis, database repairs (wrt.
integrity constraint violations), abductive diagnosis and the view-update
problem have been established. In this work we further investigate connections
between query-answer causality and abductive diagnosis and the view-update
problem. In this context, we also define and investigate the notion of
query-answer causality in the presence of integrity constraints.
</summary>
    <author>
      <name>Babak Salimi</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings Flairs, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07064v1</id>
    <updated>2016-02-23T07:33:02Z</updated>
    <published>2016-02-23T07:33:02Z</published>
    <title>SIFT: An Algorithm for Extracting Structural Information From Taxonomies</title>
    <summary>  In this work we present SIFT, a 3-step algorithm for the analysis of the
structural information represented by means of a taxonomy. The major advantage
of this algorithm is the capability to leverage the information inherent to the
hierarchical structures of taxonomies to infer correspondences which can allow
to merge them in a later step. This method is particular relevant in scenarios
where taxonomy alignment techniques exploiting textual information from
taxonomy nodes cannot operate successfully.
</summary>
    <author>
      <name>Jorge Martinez-Gil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08791v1</id>
    <updated>2016-02-29T00:49:11Z</updated>
    <published>2016-02-29T00:49:11Z</published>
    <title>The BigDAWG Architecture</title>
    <summary>  BigDAWG is a polystore system designed to work on complex problems that
naturally span across different processing or storage engines. BigDAWG provides
an architecture that supports diverse database systems working with different
data models, support for the competing notions of location transparency and
semantic completeness via islands of information and a middleware that provides
a uniform multi-island interface. In this article, we describe the current
architecture of BigDAWG, its application on the MIMIC II medical dataset, and
our plans for the mechanics of cross-system queries. During the presentation,
we will also deliver a brief demonstration of the current version of BigDAWG.
</summary>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jennie Duggan</name>
    </author>
    <author>
      <name>Aaron Elmore</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Tim Mattson</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <link href="http://arxiv.org/abs/1602.08791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00117v1</id>
    <updated>2016-09-01T05:55:02Z</updated>
    <published>2016-09-01T05:55:02Z</published>
    <title>Group Rotation Type Crowdsourcing</title>
    <summary>  A common workflow to perform a continuous human task stream is to divide
workers into groups, have one group perform the newly-arrived task, and rotate
the groups. We call this type of workflow the group rotation. This paper
addresses the problem of how to manage Group Rotation Type Crowdsourcing, the
group rotation in a crowdsourcing setting. In the group-rotation type
crowdsourcing, we must change the group structure dynamically because workers
come in and leave frequently. This paper proposes an approach to explore a
design space of methods for group restructuring in the group rotation type
crowdsourcing.
</summary>
    <author>
      <name>Katsumi Kumai</name>
    </author>
    <author>
      <name>Yuhki Shiraishi</name>
    </author>
    <author>
      <name>Jianwei Zhang</name>
    </author>
    <author>
      <name>Hiroyuki Kitagawa</name>
    </author>
    <author>
      <name>Atsuyuki Morishima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 non-reference pages + reference-only page, HCOMP2016, WiP paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03346v1</id>
    <updated>2016-09-12T11:16:35Z</updated>
    <published>2016-09-12T11:16:35Z</published>
    <title>A Meaning-oriented Approach to Semantic Data Modeling</title>
    <summary>  Semantic information is often represented as the entities and the
relationships among them with conventional semantic models. This approach is
straightforward but is not suitable for many posteriori requests in semantic
data modeling. In this paper, we propose a meaning-oriented approach to
modeling semantic data and establish a graph-based semantic data model. In this
approach we use the meanings, i.e., the subjective views of the entities and
relationships, to describe the semantic information, and use the semantic
graphs containing the meaning nodes and the meta-meaning relations to specify
the taxonomy and the compound construction of the semantic concepts. We
demonstrate how this meaning-oriented approach can address many important
semantic representation issues, including dynamic specialization and natural
join.
</summary>
    <author>
      <name>Xuhui Li</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06019v1</id>
    <updated>2016-09-20T05:03:58Z</updated>
    <published>2016-09-20T05:03:58Z</published>
    <title>Active Integrity Constraints for Multi-Context Systems</title>
    <summary>  We introduce a formalism to couple integrity constraints over general-purpose
knowledge bases with actions that can be executed to restore consistency. This
formalism generalizes active integrity constraints over databases. In the more
general setting of multi-context systems, adding repair suggestions to
integrity constraints allows defining simple iterative algorithms to find all
possible grounded repairs - repairs for the global system that follow the
suggestions given by the actions in the individual rules. We apply our
methodology to ontologies, and show that it can express most relevant types of
integrity constraints in this domain.
</summary>
    <author>
      <name>Luís Cruz-Filipe</name>
    </author>
    <author>
      <name>Graça Gaspar</name>
    </author>
    <author>
      <name>Isabel Nunes</name>
    </author>
    <author>
      <name>Peter Schneider-Kamp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-49004-5_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-49004-5_7" rel="related"/>
    <link href="http://arxiv.org/abs/1609.06019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03680v1</id>
    <updated>2016-11-11T12:39:41Z</updated>
    <published>2016-11-11T12:39:41Z</published>
    <title>DB-Nets: on The Marriage of Colored Petri Nets and Relational Databases</title>
    <summary>  The integrated management of business processes and mas- ter data is being
increasingly considered as a fundamental problem, by both the academia and the
industry. In this position paper, we focus on the foundations of the problem,
arguing that contemporary approaches struggle to find a suitable equilibrium
between data- and process-related aspects. We then propose db-nets, a new
formal model that balances such two pillars through the marriage of colored
Petri nets and relational databases. We invite the research community to build
on this model, discussing its potential in modeling, formal verification, and
simulation.
</summary>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Andrey Rivkin</name>
    </author>
    <link href="http://arxiv.org/abs/1611.03680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04977v1</id>
    <updated>2016-10-16T12:29:38Z</updated>
    <published>2016-10-16T12:29:38Z</published>
    <title>Query Data With Fuzzy Information In Object-Oriented Databases An
  Approach Interval Values</title>
    <summary>  In this paper, we propose methods of handling attributive values of object
classes in object oriented database with fuzzy information and uncertainty
based on quantitatively semantics based hedge algebraic. In this approach we
consider to attributive values (as well as methods) object class is interval
values and the interval values are converted into sub interval in [0, 1]
respectively. That its the fuzziness of the elements in the hedge algebra is
also sub interval in [0,1]. So, we present an algorithm allows the comparison
of two sub interval [0,1] helping the requirements of the query data
</summary>
    <author>
      <name>Doan Van Thang</name>
    </author>
    <author>
      <name>Doan Van Ban</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06417v1</id>
    <updated>2016-11-19T19:37:41Z</updated>
    <published>2016-11-19T19:37:41Z</published>
    <title>Discover Aggregates Exceptions over Hidden Web Databases</title>
    <summary>  Nowadays, many web databases "hidden" behind their restrictive search
interfaces (e.g., Amazon, eBay) contain rich and valuable information that is
of significant interests to various third parties. Recent studies have
demonstrated the possibility of estimating/tracking certain aggregate queries
over dynamic hidden web databases. Nonetheless, tracking all possible aggregate
query answers to report interesting findings (i.e., exceptions), while still
adhering to the stringent query-count limitations enforced by many hidden web
databases providers, is very challenging. In this paper, we develop a novel
technique for tracking and discovering exceptions (in terms of sudden changes
of aggregates) over dynamic hidden web databases. Extensive real-world
experiments demonstrate the superiority of our proposed algorithms over
baseline solutions.
</summary>
    <author>
      <name>Saad Bin Suhaim</name>
    </author>
    <author>
      <name>Weimo Liu</name>
    </author>
    <author>
      <name>Nan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08269v1</id>
    <updated>2016-11-24T17:43:48Z</updated>
    <published>2016-11-24T17:43:48Z</published>
    <title>On measuring performances of C-SPARQL and CQELS</title>
    <summary>  To cope with the massive growth of semantic data streams, several RDF Stream
Processing (RSP) engines have been implemented. The efficiency of their
throughput, latency and memory consumption can be evaluated using available
benchmarks such as LSBench and City- Bench. Nevertheless, these benchmarks lack
an in-depth performance evaluation as some measurement metrics have not been
considered. The main goal of this paper is to analyze the performance of two
popular RSP engines, namely C-SPARQL and CQELS, when varying a set of
performance metrics. More precisely, we evaluate the impact of stream rate,
number of streams and window size on execution time as well as on memory
consumption.
</summary>
    <author>
      <name>Xiangnan Ren</name>
    </author>
    <author>
      <name>Houda Khrouf</name>
    </author>
    <author>
      <name>Zakia Kazi-Aoul</name>
    </author>
    <author>
      <name>Yousra Chabchoub</name>
    </author>
    <author>
      <name>Olivier Curé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09170v1</id>
    <updated>2016-11-28T15:16:42Z</updated>
    <published>2016-11-28T15:16:42Z</published>
    <title>DESP-C++: A Discrete-Event Simulation Package for C++</title>
    <summary>  DESP-C++ is a C++ discrete-event random simulation engine that has been
designed to be fast, very easy to use and expand, and valid. DESP-C++ is based
on the resource view. Its complete architecture is presented in detail, as well
as a short " user manual ". The validity of DESP-C++ is demonstrated by the
simulation of three significant models. In each case, the simulation results
obtained with DESP-C++ match those obtained with a validated simulation
software: QNAP2. The versatility of DESP-C++ is also illustrated this way,
since the modelled systems are very different from each other: a simple
production system, the dining philosopher classical deadlock problem, and a
complex object-oriented database management system.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Software: Practice and Experience, Wiley, 2000, 30 (1), pp.37-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09691v1</id>
    <updated>2016-11-29T16:05:56Z</updated>
    <published>2016-11-29T16:05:56Z</published>
    <title>Data Partitioning View of Mining Big Data</title>
    <summary>  There are two main approximations of mining big data in memory. One is to
partition a big dataset to several subsets, so as to mine each subset in
memory. By this way, global patterns can be obtained by synthesizing all local
patterns discovered from these subsets. Another is the statistical sampling
method. This indicates that data partitioning should be an important strategy
for mining big data. This paper recalls our work on mining big data with a data
partitioning and shows some interesting findings among the local patterns
discovered from subsets of a dataset.
</summary>
    <author>
      <name>Shichao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02221v1</id>
    <updated>2017-01-09T15:53:41Z</updated>
    <published>2017-01-09T15:53:41Z</published>
    <title>JSON: data model, query languages and schema specification</title>
    <summary>  Despite the fact that JSON is currently one of the most popular formats for
exchanging data on the Web, there are very few studies on this topic and there
are no agreement upon theoretical framework for dealing with JSON. There- fore
in this paper we propose a formal data model for JSON documents and, based on
the common features present in available systems using JSON, we define a
lightweight query language allowing us to navigate through JSON documents. We
also introduce a logic capturing the schema proposal for JSON and study the
complexity of basic computational tasks associated with these two formalisms.
</summary>
    <author>
      <name>Pierre Bourhis</name>
    </author>
    <author>
      <name>Juan L. Reutter</name>
    </author>
    <author>
      <name>Fernando Suárez</name>
    </author>
    <author>
      <name>Domagoj Vrgoč</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05799v1</id>
    <updated>2017-01-19T00:29:31Z</updated>
    <published>2017-01-19T00:29:31Z</published>
    <title>BigDAWG Polystore Release and Demonstration</title>
    <summary>  The Intel Science and Technology Center for Big Data is developing a
reference implementation of a Polystore database. The BigDAWG (Big Data Working
Group) system supports "many sizes" of database engines, multiple programming
languages and complex analytics for a variety of workloads. Our recent efforts
include application of BigDAWG to an ocean metagenomics problem and
containerization of BigDAWG. We intend to release an open source BigDAWG v1.0
in the Spring of 2017. In this article, we will demonstrate a number of
polystore applications developed with oceanographic researchers at MIT and
describe our forthcoming open source release of the BigDAWG system.
</summary>
    <author>
      <name>Kyle OBrien</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jennie Duggan</name>
    </author>
    <author>
      <name>Adam Dziedzic</name>
    </author>
    <author>
      <name>Aaron Elmore</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Tim Mattson</name>
    </author>
    <author>
      <name>Zuohao She</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08029v1</id>
    <updated>2017-01-27T12:30:49Z</updated>
    <published>2017-01-27T12:30:49Z</published>
    <title>Index and Materialized View Selection in Data Warehouses</title>
    <summary>  The aim of this article is to present an overview of the major families of
state-of-the-art index and materialized view selection methods, and to discuss
the issues and future trends in data warehouse performance optimization. We
particularly focus on data mining-based heuristics we developed to reduce the
selection problem complexity and target the most pertinent candidate indexes
and materialized views.
</summary>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Centre LICEF - TÉLUQ</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Handbook of Research on Innovations in Database Technologies and
  Applications, II, pp.693-700, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08054v1</id>
    <updated>2017-01-27T14:00:53Z</updated>
    <published>2017-01-27T14:00:53Z</published>
    <title>Indices in XML Databases</title>
    <summary>  With XML becoming a standard for business information representation and
exchange, stor-ing, indexing, and querying XML documents have rapidly become
major issues in database research. In this context, query processing and
optimization are primordial, native-XML data-bases not being mature yet. Data
structures such as indices, which help enhance performances substantially, are
extensively researched, especially since XML data bear numerous specifici-ties
with respect to relational data. In this paper, we survey state-of-the-art XML
indices and discuss the main issues, tradeoffs and future trends in XML
indexing. We also present an in-dex that we specifically designed for the
particular architecture of XML data warehouses.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4018/978-1-60566-242-8.ch072</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4018/978-1-60566-242-8.ch072" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Handbook of Research on Innovations in Database Technologies and
  Applications, II, IGI Global, pp.674-681, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08612v1</id>
    <updated>2017-01-30T14:27:07Z</updated>
    <published>2017-01-30T14:27:07Z</published>
    <title>XML Warehousing and OLAP</title>
    <summary>  The aim of this article is to present an overview of the major XML
warehousing approaches from the literature, as well as the existing approaches
for performing OLAP analyses over XML data (which is termed XML-OLAP or XOLAP;
Wang et al., 2005). We also discuss the issues and future trends in this area
and illustrate this topic by presenting the design of a unified, XML data
warehouse architecture and a set of XOLAP operators expressed in an XML
algebra.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Marouane Hachicha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1701.08033</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Data Warehousing and Mining, Second Edition, IV,
  IGI Publishing, pp.2109-2116, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.09049v1</id>
    <updated>2017-01-31T14:19:18Z</updated>
    <published>2017-01-31T14:19:18Z</published>
    <title>Batch Incremental Shared Nearest Neighbor Density Based Clustering
  Algorithm for Dynamic Datasets</title>
    <summary>  Incremental data mining algorithms process frequent updates to dynamic
datasets efficiently by avoiding redundant computation. Existing incremental
extension to shared nearest neighbor density based clustering (SNND) algorithm
cannot handle deletions to dataset and handles insertions only one point at a
time. We present an incremental algorithm to overcome both these bottlenecks by
efficiently identifying affected parts of clusters while processing updates to
dataset in batch mode. We show effectiveness of our algorithm by performing
experiments on large synthetic as well as real world datasets. Our algorithm is
up to four orders of magnitude faster than SNND and requires up to 60% extra
memory than SNND while providing output identical to SNND.
</summary>
    <author>
      <name>Panthadeep Bhattacharjee</name>
    </author>
    <author>
      <name>Amit Awekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Accepted at ECIR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.09049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.09049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04302v1</id>
    <updated>2017-04-13T23:34:43Z</updated>
    <published>2017-04-13T23:34:43Z</published>
    <title>On a Distributed Approach for Density-based Clustering</title>
    <summary>  Efficient extraction of useful knowledge from these data is still a
challenge, mainly when the data is distributed, heterogeneous and of different
quality depending on its corresponding local infrastructure. To reduce the
overhead cost, most of the existing distributed clustering approaches generate
global models by aggregating local results obtained on each individual node.
The complexity and quality of solutions depend highly on the quality of the
aggregation. In this respect, we proposed for distributed density-based
clustering that both reduces the communication overheads due to the data
exchange and improves the quality of the global models by considering the
shapes of local clusters. From preliminary results we show that this algorithm
is very promising.
</summary>
    <author>
      <name>Nhien-An Le-Khac</name>
    </author>
    <author>
      <name>M-Tahar Kechadi</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05730v2</id>
    <updated>2017-10-03T07:52:26Z</updated>
    <published>2017-04-19T13:41:50Z</published>
    <title>On Measuring Bias in Online Information</title>
    <summary>  Bias in online information has recently become a pressing issue, with search
engines, social networks and recommendation services being accused of
exhibiting some form of bias. In this vision paper, we make the case for a
systematic approach towards measuring bias. To this end, we discuss formal
measures for quantifying the various types of bias, we outline the system
components necessary for realizing them, and we highlight the related research
challenges and open problems.
</summary>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <author>
      <name>Panayiotis Tsaparas</name>
    </author>
    <author>
      <name>Giorgos Flouris</name>
    </author>
    <author>
      <name>Irini Fundulaki</name>
    </author>
    <author>
      <name>Panagiotis Papadakos</name>
    </author>
    <author>
      <name>Serge Abiteboul</name>
    </author>
    <author>
      <name>Gerhard Weikum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05730v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05730v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00721v1</id>
    <updated>2017-07-03T18:24:24Z</updated>
    <published>2017-07-03T18:24:24Z</published>
    <title>Version 0.1 of the BigDAWG Polystore System</title>
    <summary>  A polystore system is a database management system (DBMS) composed of
integrated heterogeneous database engines and multiple programming languages.
By matching data to the storage engine best suited to its needs, complex
analytics run faster and flexible storage choices helps improve data
organization. BigDAWG (Big Data Working Group) is our reference implementation
of a polystore system. In this paper, we describe the current BigDAWG software
release which supports PostgreSQL, Accumulo and SciDB. We describe the overall
architecture, API and initial results of applying BigDAWG to the MIMIC II
medical dataset.
</summary>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Kyle OBrien</name>
    </author>
    <author>
      <name>Adam Dziedzic</name>
    </author>
    <author>
      <name>Aaron Elmore</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Tim Mattson</name>
    </author>
    <author>
      <name>Jennie Rogers</name>
    </author>
    <author>
      <name>Zuohao She</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2017.8091077</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2017.8091077" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE HPEC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09140v1</id>
    <updated>2017-08-30T07:03:58Z</updated>
    <published>2017-08-30T07:03:58Z</published>
    <title>The Complexity of Computing a Cardinality Repair for Functional
  Dependencies</title>
    <summary>  For a relation that violates a set of functional dependencies, we consider
the task of finding a maximum number of pairwise-consistent tuples, or what is
known as a "cardinality repair." We present a polynomial-time algorithm that,
for certain fixed relation schemas (with functional dependencies), computes a
cardinality repair. Moreover, we prove that on any of the schemas not covered
by the algorithm, finding a cardinality repair is, in fact, an NP-hard problem.
In particular, we establish a dichotomy in the complexity of computing a
cardinality repair, and we present an efficient algorithm to determine whether
a given schema belongs to the positive side or the negative side of the
dichotomy.
</summary>
    <author>
      <name>Ester Livshits</name>
    </author>
    <author>
      <name>Benny Kimelfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09171v1</id>
    <updated>2017-08-30T08:47:20Z</updated>
    <published>2017-08-30T08:47:20Z</published>
    <title>Enforcing Privacy in Cloud Databases</title>
    <summary>  Outsourcing databases, i.e., resorting to Database-as-a-Service (DBaaS), is
nowadays a popular choice due to the elasticity, availability, scalability and
pay-as-you-go features of cloud computing. However, most data are sensitive to
some extent, and data privacy remains one of the top concerns to DBaaS users,
for obvious legal and competitive reasons.In this paper, we survey the
mechanisms that aim at making databases secure in a cloud environment, and
discuss current pitfalls and related research challenges.
</summary>
    <author>
      <name>Somayeh Sobati Moghadam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Gérald Gavin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Conference on Big Data Analytics and Knowledge
  Discovery (DaWaK 2017), Aug 2017, Lyon, France. Springer, Lecture Notes in
  Computer Science, 10440, pp.53-73, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.09171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00813v1</id>
    <updated>2017-09-28T20:38:47Z</updated>
    <published>2017-09-28T20:38:47Z</published>
    <title>A Practical Python API for Querying AFLOWLIB</title>
    <summary>  Large databases such as aflowlib.org provide valuable data sources for
discovering material trends through machine learning. Although a REST API and
query language are available, there is a learning curve associated with the
AFLUX language that acts as a barrier for new users. Additionally, the data is
stored using non-standard serialization formats. Here we present a high-level
API that allows immediate access to the aflowlib data using standard python
operators and language features. It provides an easy way to integrate aflowlib
data with other python materials packages such as ase and quippy, and provides
automatic deserialization into numpy arrays and python objects. This package is
available via "pip install aflow".
</summary>
    <author>
      <name>Conred W. Rosenbrock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 code listings</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04419v1</id>
    <updated>2017-10-12T09:30:34Z</updated>
    <published>2017-10-12T09:30:34Z</published>
    <title>Querying Best Paths in Graph Databases</title>
    <summary>  Querying graph databases has recently received much attention. We propose a
new approach to this problem, which balances competing goals of expressive
power, language clarity and computational complexity. A distinctive feature of
our approach is the ability to express properties of minimal (e.g. shortest)
and maximal (e.g. most valuable) paths satisfying given criteria. To express
complex properties in a modular way, we introduce labelling-generating
ontologies. The resulting formalism is computationally attractive -- queries
can be answered in non-deterministic logarithmic space in the size of the
database.
</summary>
    <author>
      <name>Jakub Michaliszyn</name>
    </author>
    <author>
      <name>Jan Otop</name>
    </author>
    <author>
      <name>Piotr Wieczorek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A conference version fo this paper has been accepted to FSTTCS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02952v1</id>
    <updated>2017-11-08T14:15:53Z</updated>
    <published>2017-11-08T14:15:53Z</published>
    <title>Marginal Release Under Local Differential Privacy</title>
    <summary>  Many analysis and machine learning tasks require the availability of marginal
statistics on multidimensional datasets while providing strong privacy
guarantees for the data subjects. Applications for these statistics range from
finding correlations in the data to fitting sophisticated prediction models. In
this paper, we provide a set of algorithms for materializing marginal
statistics under the strong model of local differential privacy. We prove the
first tight theoretical bounds on the accuracy of marginals compiled under each
approach, perform empirical evaluation to confirm these bounds, and evaluate
them for tasks such as modeling and correlation testing. Our results show that
releasing information based on (local) Fourier transformations of the input is
preferable to alternatives based directly on (local) marginals.
</summary>
    <author>
      <name>Tejas Kulkarni</name>
    </author>
    <author>
      <name>Graham Cormode</name>
    </author>
    <author>
      <name>Divesh Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01620v1</id>
    <updated>2018-08-05T14:10:14Z</updated>
    <published>2018-08-05T14:10:14Z</published>
    <title>Schema Integration on Massive Data Sources</title>
    <summary>  As the fundamental phrase of collecting and analyzing data, data integration
is used in many applications, such as data cleaning, bioinformatics and pattern
recognition. In big data era, one of the major problems of data integration is
to obtain the global schema of data sources since the global schema could be
hardly derived from massive data sources directly. In this paper, we attempt to
solve such schema integration problem. For different scenarios, we develop
batch and incremental schema integration algorithms. We consider the
representation difference of attribute names in various data sources and
propose ED Join and Semantic Join algorithms to integrate attributes with
different representations. Extensive experimental results demonstrate that the
proposed algorithms could integrate schemas efficiently and effectively.
</summary>
    <author>
      <name>Tianbao Lia</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <author>
      <name>Jianzhong Li</name>
    </author>
    <author>
      <name>Hong Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01621v1</id>
    <updated>2018-08-05T14:11:14Z</updated>
    <published>2018-08-05T14:11:14Z</published>
    <title>Mining CFD Rules on Big Data</title>
    <summary>  Current conditional functional dependencies (CFDs) discovery algorithms
always need a well-prepared training data set. This makes them difficult to be
applied on large datasets which are always in low-quality. To handle the volume
issue of big data, we develop the sampling algorithms to obtain a small
representative training set. For the low-quality issue of big data, we then
design the fault-tolerant rule discovery algorithm and the conflict resolution
algorithm. We also propose parameter selection strategy for CFD discovery
algorithm to ensure its effectiveness. Experimental results demonstrate that
our method could discover effective CFD rules on billion-tuple data within
reasonable time.
</summary>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <author>
      <name>Mingda Li</name>
    </author>
    <author>
      <name>Jiawei Zhao</name>
    </author>
    <author>
      <name>Jianzhong Li</name>
    </author>
    <author>
      <name>Hong Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05138v1</id>
    <updated>2018-08-13T20:19:56Z</updated>
    <published>2018-08-13T20:19:56Z</published>
    <title>Database Operations in D4M.jl</title>
    <summary>  Each step in the data analytics pipeline is important, including database
ingest and query. The D4M-Accumulo database connector has allowed analysts to
quickly and easily ingest to and query from Apache Accumulo using MATLAB(R)/GNU
Octave syntax. D4M.jl, a Julia implementation of D4M, provides much of the
functionality of the original D4M implementation to the Julia community. In
this work, we extend D4M.jl to include many of the same database capabilities
that the MATLAB(R)/GNU Octave implementation provides. Here we will describe
the D4M.jl database connector, demonstrate how it can be used, and show that it
has comparable or better performance to the original implementation in
MATLAB(R)/GNU Octave.
</summary>
    <author>
      <name>Lauren Milechin</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2018.8547567</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2018.8547567" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HPEC 2018. arXiv admin note: text overlap with arXiv:1708.02934</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05448v1</id>
    <updated>2018-08-16T12:42:29Z</updated>
    <published>2018-08-16T12:42:29Z</published>
    <title>Automatic Generation of a Hybrid Query Execution Engine</title>
    <summary>  The ever-increasing need for fast data processing demands new methods for
efficient query execution. Just-in-time query compilation techniques have been
demonstrated to improve performance in a set of analytical tasks significantly.
In this work, we investigate the possibility of adding this approach to
existing database solutions and the benefits it provides. To that end, we
create a set of automated tools to create a runtime code generation engine and
integrate such an engine into SQLite which is one of the most popular
relational databases in the world and is used in a large variety of contexts.
Speedups of up to 1.7x were observed in microbenchmarks with queries involving
a large number of operations.
</summary>
    <author>
      <name>Aleksei Kashuba</name>
    </author>
    <author>
      <name>Hannes Mühleisen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07767v2</id>
    <updated>2019-01-25T20:11:34Z</updated>
    <published>2018-08-22T15:12:34Z</published>
    <title>The First Order Truth behind Undecidability of Regular Path Queries
  Determinacy</title>
    <summary>  In our paper [G{\l}uch, Marcinkowski, Ostropolski-Nalewaja, LICS ACM, 2018]
we have solved an old problem stated in [Calvanese, De Giacomo, Lenzerini,
Vardi, SPDS ACM, 2000] showing that query determinacy is undecidable for
Regular Path Queries. Here a strong generalisation of this result is shown, and
-- we think -- a very unexpected one. We prove that no regularity is needed:
determinacy remains undecidable even for finite unions of conjunctive path
queries.
</summary>
    <author>
      <name>Grzegorz Głuch</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <author>
      <name>Piotr Ostropolski-Nalewaja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1802.01554</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07767v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07767v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00326v1</id>
    <updated>2018-09-30T06:42:18Z</updated>
    <published>2018-09-30T06:42:18Z</published>
    <title>Using Graph-Pattern Association Rules On Yago Knowledge Base</title>
    <summary>  We propose the use of Graph-Pattern Association Rules (GPARs) on the Yago
knowledge base. Extending association rules for itemsets, GPARS can help to
discover regularities between entities in knowledge bases. A rule-generated
graph pattern (RGGP) algorithm was used for extracting rules from the Yago
knowledge base and a graph-pattern association rules algorithm for creating
association rules. Our research resulted in 1114 association rules, where the
value of standard confidence at 50.18% was better than partial completeness
assumption (PCA) confidence at 49.82%. Besides that the computation time for
standard confidence was also better than for PCA confidence
</summary>
    <author>
      <name> Wahyudi</name>
    </author>
    <author>
      <name>Masayu Leylia Khodra</name>
    </author>
    <author>
      <name>Ary Setijadi Prihatmanto</name>
    </author>
    <author>
      <name>Carmadi Machbub</name>
    </author>
    <link href="http://arxiv.org/abs/1810.00326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04604v1</id>
    <updated>2018-10-10T15:50:03Z</updated>
    <published>2018-10-10T15:50:03Z</published>
    <title>A Similarity Measure for Weaving Patterns in Textiles</title>
    <summary>  We propose a novel approach for measuring the similarity between weaving
patterns that can provide similarity-based search functionality for textile
archives. We represent textile structures using hypergraphs and extract
multisets of k-neighborhoods from these graphs. The resulting multisets are
then compared using Jaccard coefficients, Hamming distances, and cosine
measures. We evaluate the different variants of our similarity measure
experimentally, showing that it can be implemented efficiently and illustrating
its quality using it to cluster and query a data set containing more than a
thousand textile samples.
</summary>
    <author>
      <name>Sven Helmer</name>
    </author>
    <author>
      <name>Vuong M. Ngo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 papes, will be published in SIGIR 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.04604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.09227v1</id>
    <updated>2018-10-22T12:59:51Z</updated>
    <published>2018-10-22T12:59:51Z</published>
    <title>Knowledge Graph Completion to Predict Polypharmacy Side Effects</title>
    <summary>  The polypharmacy side effect prediction problem considers cases in which two
drugs taken individually do not result in a particular side effect; however,
when the two drugs are taken in combination, the side effect manifests. In this
work, we demonstrate that multi-relational knowledge graph completion achieves
state-of-the-art results on the polypharmacy side effect prediction problem.
Empirical results show that our approach is particularly effective when the
protein targets of the drugs are well-characterized. In contrast to prior work,
our approach provides more interpretable predictions and hypotheses for wet lab
validation.
</summary>
    <author>
      <name>Brandon Malone</name>
    </author>
    <author>
      <name>Alberto García-Durán</name>
    </author>
    <author>
      <name>Mathias Niepert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Data Integration in the Life
  Sciences (DILS2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.09227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.09227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03291v1</id>
    <updated>2019-09-07T15:42:22Z</updated>
    <published>2019-09-07T15:42:22Z</published>
    <title>Compiling PL/SQL Away</title>
    <summary>  "PL/SQL functions are slow," is common developer wisdom that derives from the
tension between set-oriented SQL evaluation and statement-by-statement PL/SQL
interpretation. We pursue the radical approach of compiling PL/SQL away,
turning interpreted functions into regular subqueries that can then be
efficiently evaluated together with their embracing SQL query, avoiding any
PL/SQL to SQL context switches. Input PL/SQL functions may exhibit arbitrary
control flow. Iteration, in particular, is compiled into SQL-level recursion.
RDBMSs across the board reward this compilation effort with significant run
time savings that render established developer lore questionable.
</summary>
    <author>
      <name>Christian Duta</name>
    </author>
    <author>
      <name>Denis Hirn</name>
    </author>
    <author>
      <name>Torsten Grust</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05859v1</id>
    <updated>2019-09-12T16:23:56Z</updated>
    <published>2019-09-12T16:23:56Z</published>
    <title>Simple-ML: Towards a Framework for Semantic Data Analytics Workflows</title>
    <summary>  In this paper we present the Simple-ML framework that we develop to support
efficient configuration, robustness and reusability of data analytics workflows
through the adoption of semantic technologies. We present semantic data models
that lay the foundation for the framework development and discuss the data
analytics workflows based on these models. Furthermore, we present an example
instantiation of the Simple-ML data models for a real-world use case in the
mobility domain.
</summary>
    <author>
      <name>Simon Gottschalk</name>
    </author>
    <author>
      <name>Nicolas Tempelmeier</name>
    </author>
    <author>
      <name>Günter Kniesel</name>
    </author>
    <author>
      <name>Vasileios Iosifidis</name>
    </author>
    <author>
      <name>Besnik Fetahu</name>
    </author>
    <author>
      <name>Elena Demidova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SEMANTiCS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.05859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05134v1</id>
    <updated>2020-06-09T09:21:18Z</updated>
    <published>2020-06-09T09:21:18Z</published>
    <title>Dynamic Interleaving of Content and Structure for Robust Indexing of
  Semi-Structured Hierarchical Data (Extended Version)</title>
    <summary>  We propose a robust index for semi-structured hierarchical data that supports
content-and-structure (CAS) queries specified by path and value predicates. At
the heart of our approach is a novel dynamic interleaving scheme that merges
the path and value dimensions of composite keys in a balanced way. We store
these keys in our trie-based Robust Content-And-Structure index, which
efficiently supports a wide range of CAS queries, including queries with
wildcards and descendant axes. Additionally, we show important properties of
our scheme, such as robustness against varying selectivities, and demonstrate
improvements of up to two orders of magnitude over existing approaches in our
experimental evaluation.
</summary>
    <author>
      <name>Kevin Wellenzohn</name>
    </author>
    <author>
      <name>Michael H. Böhlen</name>
    </author>
    <author>
      <name>Sven Helmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14778/3401960.3401963</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14778/3401960.3401963" rel="related"/>
    <link href="http://arxiv.org/abs/2006.05134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08224v1</id>
    <updated>2020-06-15T08:54:22Z</updated>
    <published>2020-06-15T08:54:22Z</published>
    <title>Needles in the 'Sheet'stack: Augmented Analytics to get Insights from
  Spreadsheets</title>
    <summary>  Business intelligence (BI) tools for database analytics have come a long way
and nowadays also provide ready insights or visual query explorations, e.g.
QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In
this demo, we focus on providing insights by examining periodic spreadsheets of
different reports (aka views), without prior knowledge of the schema of the
database or reports, or data information. Such a solution is targeted at users
without the familiarity with the database schema or resources to conduct
analytics in the contemporary way.
</summary>
    <author>
      <name>Medha Atre</name>
    </author>
    <author>
      <name>Anand Deshpande</name>
    </author>
    <author>
      <name>Reshma Godse</name>
    </author>
    <author>
      <name>Pooja Deokar</name>
    </author>
    <author>
      <name>Sandip Moharir</name>
    </author>
    <author>
      <name>Dhruva Ray</name>
    </author>
    <author>
      <name>Akshay Chitlangia</name>
    </author>
    <author>
      <name>Trupti Phadnis</name>
    </author>
    <author>
      <name>Yugansh Goyal</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08842v1</id>
    <updated>2020-06-16T00:40:50Z</updated>
    <published>2020-06-16T00:40:50Z</published>
    <title>Index Selection for NoSQL Database with Deep Reinforcement Learning</title>
    <summary>  We propose a new approach of NoSQL database index selection. For different
workloads, we select different indexes and their different parameters to
optimize the database performance. The approach builds a deep reinforcement
learning model to select an optimal index for a given fixed workload and adapts
to a changing workload. Experimental results show that, Deep Reinforcement
Learning Index Selection Approach (DRLISA) has improved performance to varying
degrees according to traditional single index structures.
</summary>
    <author>
      <name>Shun Yao</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <author>
      <name>Yu Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01762v1</id>
    <updated>2017-06-06T13:43:08Z</updated>
    <published>2017-06-06T13:43:08Z</published>
    <title>Specifying Transaction Control to Serialize Concurrent Program
  Executions</title>
    <summary>  We define a programming language independent transaction controller and an
operator which when applied to concurrent programs with shared locations turns
their behavior with respect to some abstract termination criterion into a
transactional behavior. We prove the correctness property that concurrent runs
under the transaction controller are serialisable. We specify the transaction
controller TaCtl and the operator TA in terms of Abstract State Machines. This
makes TaCtl applicable to a wide range of programs and in particular provides
the possibility to use it as a plug-in when specifying concurrent system
components in terms of Abstract State Machines.
</summary>
    <author>
      <name>Egon Börger</name>
    </author>
    <author>
      <name>Klaus-Dieter Schewe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-662-43652-3_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-662-43652-3_13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, vol. 8477, Springer 2014, pp. 142-157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.01762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15, 68Q60, 68Q85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07493v1</id>
    <updated>2017-09-21T18:50:32Z</updated>
    <published>2017-09-21T18:50:32Z</published>
    <title>Big Data Systems Meet Machine Learning Challenges: Towards Big Data
  Science as a Service</title>
    <summary>  Recently, we have been witnessing huge advancements in the scale of data we
routinely generate and collect in pretty much everything we do, as well as our
ability to exploit modern technologies to process, analyze and understand this
data. The intersection of these trends is what is called, nowadays, as Big Data
Science. Cloud computing represents a practical and cost-effective solution for
supporting Big Data storage, processing and for sophisticated analytics
applications. We analyze in details the building blocks of the software stack
for supporting big data science as a commodity service for data scientists. We
provide various insights about the latest ongoing developments and open
challenges in this domain.
</summary>
    <author>
      <name>Radwa Elshawi</name>
    </author>
    <author>
      <name>Sherif Sakr</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08880v1</id>
    <updated>2017-09-26T08:18:15Z</updated>
    <published>2017-09-26T08:18:15Z</published>
    <title>An enhanced method to compute the similarity between concepts of
  ontology</title>
    <summary>  With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.
</summary>
    <author>
      <name>Noreddine Gherabi</name>
    </author>
    <author>
      <name>Abdelhadi Daoui</name>
    </author>
    <author>
      <name>Abderrahim Marzouk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-64719-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-64719-7" rel="related"/>
    <link href="http://arxiv.org/abs/1709.08880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.07388v1</id>
    <updated>2019-01-18T13:34:26Z</updated>
    <published>2019-01-18T13:34:26Z</published>
    <title>Improving the data quality in the research information systems</title>
    <summary>  In order to introduce an integrated research information system, this will
provide scientific institutions with the necessary information on research
activities and research results in assured quality. Since data collection,
duplication, missing values, incorrect formatting, inconsistencies, etc. can
arise in the collection of research data in different research information
systems, which can have a wide range of negative effects on data quality, the
subject of data quality should be treated with better results. This paper
examines the data quality problems in research information systems and presents
the new techniques that enable organizations to improve their quality of
research information.
</summary>
    <author>
      <name>Otmane Azeroual</name>
    </author>
    <author>
      <name>Mohammad Abuosba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15(11), pp. 82-86. arXiv admin note: substantial text overlap with
  arXiv:1901.06208</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.07388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08248v1</id>
    <updated>2019-01-24T06:34:15Z</updated>
    <published>2019-01-24T06:34:15Z</published>
    <title>TigerGraph: A Native MPP Graph Database</title>
    <summary>  We present TigerGraph, a graph database system built from the ground up to
support massively parallel computation of queries and analytics.
  TigerGraph's high-level query language, GSQL, is designed for compatibility
with SQL, while simultaneously allowing NoSQL programmers to continue thinking
in Bulk-Synchronous Processing (BSP) terms and reap the benefits of high-level
specification.
  GSQL is sufficiently high-level to allow declarative SQL-style programming,
yet sufficiently expressive to concisely specify the sophisticated iterative
algorithms required by modern graph analytics and traditionally coded in
general-purpose programming languages like C++ and Java.
  We report very strong scale-up and scale-out performance over a benchmark we
published on GitHub for full reproducibility.
</summary>
    <author>
      <name>Alin Deutsch</name>
    </author>
    <author>
      <name>Yu Xu</name>
    </author>
    <author>
      <name>Mingxi Wu</name>
    </author>
    <author>
      <name>Victor Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1901.08248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11543v1</id>
    <updated>2019-11-24T02:40:09Z</updated>
    <published>2019-11-24T02:40:09Z</published>
    <title>Schema Matching using Machine Learning</title>
    <summary>  Schema Matching is a method of finding attributes that are either similar to
each other linguistically or represent the same information. In this project,
we take a hybrid approach at solving this problem by making use of both the
provided data and the schema name to perform one to one schema matching and
introduce the creation of a global dictionary to achieve one to many schema
matching. We experiment with two methods of one to one matching and compare
both based on their F-scores, precision, and recall. We also compare our method
with the ones previously suggested and highlight differences between them.
</summary>
    <author>
      <name>Tanvi Sahay</name>
    </author>
    <author>
      <name>Ankita Mehta</name>
    </author>
    <author>
      <name>Shruti Jadon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SPIN48934.2020.9071272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SPIN48934.2020.9071272" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0686v1</id>
    <updated>2011-03-03T14:09:24Z</updated>
    <published>2011-03-03T14:09:24Z</published>
    <title>Querying and Manipulating Temporal Databases</title>
    <summary>  Many works have focused, for over twenty five years, on the integration of
the time dimension in databases (DB). However, the standard SQL3 does not yet
allow easy definition, manipulation and querying of temporal DBs. In this
paper, we study how we can simplify querying and manipulating temporal facts in
SQL3, using a model that integrates time in a native manner. To do this, we
propose new keywords and syntax to define different temporal versions for many
relational operators and functions used in SQL. It then becomes possible to
perform various queries and updates appropriate to temporal facts. We
illustrate the use of these proposals on many examples from a real application.
</summary>
    <author>
      <name>Mohamed Mkaouar</name>
    </author>
    <author>
      <name>Rafik Bouaziz</name>
    </author>
    <author>
      <name>Mohamed Moalla</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems (IJDMS),
  February 2011, Volume 3, Number 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.0686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.1367v1</id>
    <updated>2011-03-07T20:22:38Z</updated>
    <published>2011-03-07T20:22:38Z</published>
    <title>Efficient Batch Query Answering Under Differential Privacy</title>
    <summary>  Differential privacy is a rigorous privacy condition achieved by randomizing
query answers. This paper develops efficient algorithms for answering multiple
queries under differential privacy with low error. We pursue this goal by
advancing a recent approach called the matrix mechanism, which generalizes
standard differentially private mechanisms. This new mechanism works by first
answering a different set of queries (a strategy) and then inferring the
answers to the desired workload of queries. Although a few strategies are known
to work well on specific workloads, finding the strategy which minimizes error
on an arbitrary workload is intractable. We prove a new lower bound on the
optimal error of this mechanism, and we propose an efficient algorithm that
approaches this bound for a wide range of workloads.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figues, 22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.1367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.1367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2406v1</id>
    <updated>2011-03-12T01:05:38Z</updated>
    <published>2011-03-12T01:05:38Z</published>
    <title>Automatic Wrappers for Large Scale Web Extraction</title>
    <summary>  We present a generic framework to make wrapper induction algorithms tolerant
to noise in the training data. This enables us to learn wrappers in a
completely unsupervised manner from automatically and cheaply obtained noisy
training data, e.g., using dictionaries and regular expressions. By removing
the site-level supervision that wrapper-based techniques require, we are able
to perform information extraction at web-scale, with accuracy unattained with
existing unsupervised extraction techniques. Our system is used in production
at Yahoo! and powers live applications.
</summary>
    <author>
      <name>Nilesh Dalvi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Ravi Kumar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Mohamed Soliman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U. of Waterloo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.
  219-230 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.2406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2410v1</id>
    <updated>2011-03-12T01:09:30Z</updated>
    <published>2011-03-12T01:09:30Z</published>
    <title>Large-Scale Collective Entity Matching</title>
    <summary>  There have been several recent advancements in Machine Learning community on
the Entity Matching (EM) problem. However, their lack of scalability has
prevented them from being applied in practical settings on large real-life
datasets. Towards this end, we propose a principled framework to scale any
generic EM algorithm. Our technique consists of running multiple instances of
the EM algorithm on small neighborhoods of the data and passing messages across
neighborhoods to construct a global solution. We prove formal properties of our
framework and experimentally demonstrate the effectiveness of our approach in
scaling EM algorithms.
</summary>
    <author>
      <name>Vibhor Rastogi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Nilesh Dalvi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Minos Garofalakis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technical University of Crete</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.
  208-218 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.2410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3857v2</id>
    <updated>2011-04-26T20:17:21Z</updated>
    <published>2011-03-20T15:35:03Z</published>
    <title>Difference Sequence Compression of Multidimensional Databases</title>
    <summary>  The multidimensional databases often use compression techniques in order to
decrease the size of the database. This paper introduces a new method called
difference sequence compression. Under some conditions, this new technique is
able to create a smaller size multidimensional database than others like single
count header compression, logical position compression or base-offset
compression. Keywords: compression, multidimensional database, On-line
Analytical Processing, OLAP.
</summary>
    <author>
      <name>István Szépkúti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 4 figures, 5 tables. Paper presented at the Third
  Conference of PhD Students in Computer Science, Szeged, Hungary, 1 - 4 July
  2002. For further details, please refer to
  http://www.inf.u-szeged.hu/~szepkuti/papers.html#differencesequence</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Periodica Polytechnica Electrical Engineering, Vol. 48, Number
  3-4, pp. 197-218, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.3857v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3857v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4979v3</id>
    <updated>2016-03-01T04:55:06Z</updated>
    <published>2011-03-25T14:24:18Z</published>
    <title>An Introduction to Functional dependency in Relational Databases</title>
    <summary>  This write-up is the suggested lecture notes for a second level course on
advanced topics in database systems for master's students of Computer Science
with a theoretical focus. A prerequisite in algorithms and an exposure to
database systems are required. Additional reading may require exposure to
mathematical logic. The starting point for these notes are from M.Y.Vardi's
survey listed herein as a reference - some of the proofs are presented as such
. This select rewrite on functional dependency is intended to provide a few
clarifications even though radically new design approaches are now being
proposed.
</summary>
    <author>
      <name>K. Viswanathan Iyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised 2nd version</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.4979v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4979v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3495v1</id>
    <updated>2014-03-14T07:26:33Z</updated>
    <published>2014-03-14T07:26:33Z</published>
    <title>Analyzing Large Biological Datasets with an Improved Algorithm for MIC</title>
    <summary>  A computational framework utilizes the traditional similarity measures for
mining the significant relationships in biological annotations is recently
proposed by Tatiana V. Karpinets et al. [2]. In this paper, an improved
approximation algorithm for MIC (maximal information coefficient) named IAMIC
is suggested to perfect this framework for discovering the hidden regularities
between biological annotations. Further, IAMIC is the enhanced algorithm for
approximating a novel similarity coefficient MIC with generality and
equitability, which makes it more appropriate for data exploration. Here it is
shown that IAMIC is also applicable for identify the associations between
biological annotations.
</summary>
    <author>
      <name>Shuliang Wang</name>
    </author>
    <author>
      <name>Yiping Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1403.3495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3622v1</id>
    <updated>2014-11-13T17:30:03Z</updated>
    <published>2014-11-13T17:30:03Z</published>
    <title>Handling owl:sameAs via Rewriting</title>
    <summary>  Rewriting is widely used to optimise owl:sameAs reasoning in materialisation
based OWL 2 RL systems. We investigate issues related to both the correctness
and efficiency of rewriting, and present an algorithm that guarantees
correctness, improves efficiency, and can be effectively parallelised. Our
evaluation shows that our approach can reduce reasoning times on practical data
sets by orders of magnitude.
</summary>
    <author>
      <name>Boris Motik</name>
    </author>
    <author>
      <name>Yavor Nenov</name>
    </author>
    <author>
      <name>Robert Piro</name>
    </author>
    <author>
      <name>Ian Horrocks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the technical report supporting the AAAI 2015 Conference
  submission with the same title</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5014v1</id>
    <updated>2014-11-18T14:19:28Z</updated>
    <published>2014-11-18T14:19:28Z</published>
    <title>Music Data Analysis: A State-of-the-art Survey</title>
    <summary>  Music accounts for a significant chunk of interest among various online
activities. This is reflected by wide array of alternatives offered in music
related web/mobile apps, information portals, featuring millions of artists,
songs and events attracting user activity at similar scale. Availability of
large scale structured and unstructured data has attracted similar level of
attention by data science community. This paper attempts to offer current
state-of-the-art in music related analysis. Various approaches involving
machine learning, information theory, social network analysis, semantic web and
linked open data are represented in the form of taxonomy along with data
sources and use cases addressed by the research community.
</summary>
    <author>
      <name>Shubhanshu Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.5014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97M80" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6562v1</id>
    <updated>2014-11-12T23:50:43Z</updated>
    <published>2014-11-12T23:50:43Z</published>
    <title>Evaluating the Crowd with Confidence</title>
    <summary>  Worker quality control is a crucial aspect of crowdsourcing systems;
typically occupying a large fraction of the time and money invested on
crowdsourcing. In this work, we devise techniques to generate confidence
intervals for worker error rate estimates, thereby enabling a better evaluation
of worker quality. We show that our techniques generate correct confidence
intervals on a range of real-world datasets, and demonstrate wide applicability
by using them to evict poorly performing workers, and provide confidence
intervals on the accuracy of the answers.
</summary>
    <author>
      <name>Manas Joglekar</name>
    </author>
    <author>
      <name>Hector Garcia-Molina</name>
    </author>
    <author>
      <name>Aditya Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05947v2</id>
    <updated>2015-05-12T22:47:57Z</updated>
    <published>2015-02-20T17:41:40Z</published>
    <title>Functorial Data Migration: From Theory to Practice</title>
    <summary>  In this paper we describe a functorial data migration scenario about the
manufacturing service capability of a distributed supply chain. The scenario is
a category-theoretic analog of an OWL ontology-based semantic enrichment
scenario developed at the National Institute of Standards and Technology
(NIST). The scenario is presented using, and is included with, the open-source
FQL tool, available for download at categoricaldata.net/fql.html.
</summary>
    <author>
      <name>Ryan Wisnesky</name>
    </author>
    <author>
      <name>David I. Spivak</name>
    </author>
    <author>
      <name>Patrick Schultz</name>
    </author>
    <author>
      <name>Eswaran Subrahmanian</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05947v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05947v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00307v1</id>
    <updated>2015-05-31T23:37:58Z</updated>
    <published>2015-05-31T23:37:58Z</published>
    <title>Efficient Iterative Processing in the SciDB Parallel Array Engine</title>
    <summary>  Many scientific data-intensive applications perform iterative computations on
array data. There exist multiple engines specialized for array processing.
These engines efficiently support various types of operations, but none
includes native support for iterative processing. In this paper, we develop a
model for iterative array computations and a series of optimizations. We
evaluate the benefits of an optimized, native support for iterative array
processing on the SciDB engine and real workloads from the astronomy domain.
</summary>
    <author>
      <name>Emad Soroush</name>
    </author>
    <author>
      <name>Magdalena Balazinska</name>
    </author>
    <author>
      <name>Simon Krughoff</name>
    </author>
    <author>
      <name>Andrew Connolly</name>
    </author>
    <link href="http://arxiv.org/abs/1506.00307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05158v1</id>
    <updated>2015-06-16T21:54:12Z</updated>
    <published>2015-06-16T21:54:12Z</published>
    <title>An Entropy Maximizing Geohash for Distributed Spatiotemporal Database
  Indexing</title>
    <summary>  We present a modification of the standard geohash algorithm based on maximum
entropy encoding in which the data volume is approximately constant for a given
hash prefix length. Distributed spatiotemporal databases, which typically
require interleaving spatial and temporal elements into a single key, reap
large benefits from a balanced geohash by creating a consistent ratio between
spatial and temporal precision even across areas of varying data density. This
property is also useful for indexing purely spatial datasets, where the load
distribution of large range scans is an important aspect of query performance.
We apply our algorithm to data generated proportional to population as given by
census block population counts provided from the US Census Bureau.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.05158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00196v2</id>
    <updated>2015-12-03T12:06:50Z</updated>
    <published>2015-12-01T09:44:05Z</published>
    <title>SQL Queries for Declarative Process Mining on Event Logs of Relational
  Databases</title>
    <summary>  Flexible business processes can often be modelled more easily using a
declarative rather than a procedural modelling approach. Process mining aims at
automating the discovery of business process models. Existing declarative
process mining approaches either suffer performance issues with real-life event
logs or limit their expressiveness to a specific set of constaint types.
Lately, with RelationalXES a relational database architecture for storing event
log data has been introduced. In this technical report, we introduce a mining
approach that directly works on relational event data by querying the log with
conventional SQL. We provide a list of SQL queries for discovering a set of
commonly used and mined process constraints.
</summary>
    <author>
      <name>Stefan Schönig</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00196v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00196v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01681v1</id>
    <updated>2015-12-05T15:54:19Z</updated>
    <published>2015-12-05T15:54:19Z</published>
    <title>Red Spider Meets a Rainworm: Conjunctive Query Finite Determinacy Is
  Undecidable</title>
    <summary>  We solve a well known and long-standing open problem in database theory,
proving that Conjunctive Query Finite Determinacy Problem is undecidable. The
technique we use builds on the top of our Red Spider method which we developed
in our paper [GM15] to show undecidability of the same problem in the
"unrestricted case" -- when database instances are allowed to be infinite. We
also show a specific instance $Q_0$, ${\cal Q}= \{Q_1, Q_2, \ldots Q_k\}$ such
that the set $\cal Q$ of CQs does not determine CQ $Q_0$ but finitely
determines it. Finally, we claim that while $Q_0$ is finitely determined by
$\cal Q$, there is no FO-rewriting of $Q_0$, with respect to $\cal Q$, and we
outline a proof of this claim
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01808v1</id>
    <updated>2015-12-06T17:15:38Z</updated>
    <published>2015-12-06T17:15:38Z</published>
    <title>Entropy bounds for conjunctive queries with functional dependencies</title>
    <summary>  We study the problem of finding the worst-case bound for the size of the
result $Q(\mathbb{ D})$ of a fixed conjunctive query $Q$ applied to a database
$\mathbb{ D}$ satisfying given functional dependencies. We provide a precise
characterization of this bound in terms of entropy vectors, and in terms of
finite groups. In particular, we show that an upper bound provided by Gottlob,
Lee, Valiant and Valiant is tight, answering a question from their paper. Our
result generalizes the bound due to Atserias, Grohe and Marx, who consider the
case without functional dependencies. Our result shows that the problem of
computing the worst-case size bound, in the general case, is closely related to
difficult problems from information theory.
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Szymon Toruńczyk</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06246v1</id>
    <updated>2015-12-19T13:18:24Z</updated>
    <published>2015-12-19T13:18:24Z</published>
    <title>Parallel-Correctness and Containment for Conjunctive Queries with Union
  and Negation</title>
    <summary>  Single-round multiway join algorithms first reshuffle data over many servers
and then evaluate the query at hand in a parallel and communication-free way. A
key question is whether a given distribution policy for the reshuffle is
adequate for computing a given query, also referred to as parallel-correctness.
This paper extends the study of the complexity of parallel-correctness and its
constituents, parallel-soundness and parallel-completeness, to unions of
conjunctive queries with and without negation. As a by-product it is shown that
the containment problem for conjunctive queries with negation is
coNEXPTIME-complete.
</summary>
    <author>
      <name>Gaetano Geck</name>
    </author>
    <author>
      <name>Bas Ketsman</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Thomas Schwentick</name>
    </author>
    <link href="http://arxiv.org/abs/1512.06246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08568v2</id>
    <updated>2016-05-02T13:13:22Z</updated>
    <published>2016-04-28T19:34:43Z</published>
    <title>Towards Temporal Graph Databases</title>
    <summary>  In spite of the extensive literature on graph databases (GDBs), temporal GDBs
have not received too much attention so far. Temporal GBDs can capture, for
example, the evolution of social networks across time, a relevant topic in data
analysis nowadays. In this paper we propose a data model and query language
(denoted TEG-QL) for temporal GDBs, based on the notion of attribute graphs.
This allows a straightforward translation to Neo4J, a well-known GBD. We
present extensive examples of the use of TEG-QL, and comment our
implementation.
</summary>
    <author>
      <name>Alexander Campos</name>
    </author>
    <author>
      <name>Jorge Mozzino</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08568v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08568v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01742v1</id>
    <updated>2016-06-06T13:48:34Z</updated>
    <published>2016-06-06T13:48:34Z</published>
    <title>Adaptive Distributed Top-k Query Processing</title>
    <summary>  ADiT is an adaptive approach for processing distributed top-$k$ queries over
peer-to-peer networks optimizing both system load and query response time. This
approach considers the size of the peer to peer network, the amount $k$ of
searched objects, the network capabilities of a connected peer, i.e. the
transmission rate, the amount of objects stored on each peer, and the speed of
a peer in processing a local top-$k$ query. In extensive experiments with a
variety of scenarios we could show that ADiT outperforms state of the art
distributed query processing techniques.
</summary>
    <author>
      <name>Claus Dabringer</name>
    </author>
    <author>
      <name>Johann Eder</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02208v1</id>
    <updated>2016-06-07T16:46:53Z</updated>
    <published>2016-06-07T16:46:53Z</published>
    <title>Initialization Errors in Quantum Data Base Recall</title>
    <summary>  This paper analyzes the relationship between initialization error and recall
of a specific memory in the Grover algorithm for quantum database search. It is
shown that the correct memory is obtained with high probability even when the
initial state is far removed from the correct one. The analysis is done by
relating the variance of error in the initial state to the recovery of the
correct memory and the surprising result is obtained that the relationship
between the two is essentially linear.
</summary>
    <author>
      <name>Kalyani Natu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02237v1</id>
    <updated>2016-06-07T18:05:50Z</updated>
    <published>2016-06-07T18:05:50Z</published>
    <title>Concept-Oriented Model: the Functional View</title>
    <summary>  The plethora of existing data models and specific data modeling techniques is
not only confusing but leads to complex, eclectic and inefficient designs of
systems for data management and analytics. The main goal of this paper is to
describe a unified approach to data modeling, called the concept-oriented model
(COM), by using functions as a basis for its formalization. COM tries to answer
the question what is data and to rethink basic assumptions underlying this and
related notions. Its main goal is to unify major existing views on data
(generality), using only a few main notions (simplicity) which are very close
to how data is used in real life (naturalness).
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08657v1</id>
    <updated>2016-06-28T11:33:55Z</updated>
    <published>2016-06-28T11:33:55Z</published>
    <title>RDF Graph Alignment with Bisimulation</title>
    <summary>  We investigate the problem of aligning two RDF databases, an essential
problem in understanding the evolution of ontologies. Our approaches address
three fundamental challenges: 1) the use of "blank" (null) names, 2) ontology
changes in which different names are used to identify the same entity, and 3)
small changes in the data values as well as small changes in the graph
structure of the RDF database. We propose approaches inspired by the classical
notion of graph bisimulation and extend them to capture the natural metrics of
edit distance on the data values and the graph structure. We evaluate our
methods on three evolving curated data sets. Overall, our results show that the
proposed methods perform well and are scalable.
</summary>
    <author>
      <name>Peter Buneman</name>
    </author>
    <author>
      <name>Sławek Staworko</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00623v1</id>
    <updated>2016-12-02T10:38:49Z</updated>
    <published>2016-12-02T10:38:49Z</published>
    <title>Density Based Algorithm With Automatic Parameters Generation</title>
    <summary>  The traditional algorithms do not meet the latest multiple requirements
simultaneously for objects. Density-based method is one of the methodologies,
which can detect arbitrary shaped clusters where clusters are defined as dense
regions separated by low density regions. In this paper, we present a new
clustering algorithm to enhance the density-based algorithm DBSCAN. This
enables an automatic parameter generation strategy to create clusters with
different densities and enables noises recognition, and generates arbitrary
shaped clusters. The kdtree is used for increasing the memory efficiency.
Experimental result shows that proposed algorithm is capable of handling
complex objects with good memory efficiency and accuracy.
</summary>
    <author>
      <name>Singh Vijendra</name>
    </author>
    <author>
      <name>Priyanka Trikha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2011 IEEE 3rd International Conference on Machine Learning and
  Computing (ICMLC 2011), Singapore</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05786v1</id>
    <updated>2016-12-17T16:08:04Z</updated>
    <published>2016-12-17T16:08:04Z</published>
    <title>Predicting Completeness in Knowledge Bases</title>
    <summary>  Knowledge bases such as Wikidata, DBpedia, or YAGO contain millions of
entities and facts. In some knowledge bases, the correctness of these facts has
been evaluated. However, much less is known about their completeness, i.e., the
proportion of real facts that the knowledge bases cover. In this work, we
investigate different signals to identify the areas where a knowledge base is
complete. We show that we can combine these signals in a rule mining approach,
which allows us to predict where facts may be missing. We also show that
completeness predictions can help other applications such as fact prediction.
</summary>
    <author>
      <name>Luis Galárraga</name>
    </author>
    <author>
      <name>Simon Razniewski</name>
    </author>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <author>
      <name>Fabian M. Suchanek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3018661.3018739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3018661.3018739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 19 references, 1 figure, 5 tables. Complete version of the
  article accepted at WSDM'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03524v2</id>
    <updated>2017-05-04T01:01:48Z</updated>
    <published>2017-03-10T02:48:29Z</published>
    <title>The Ontological Multidimensional Data Model</title>
    <summary>  In this extended abstract we describe, mainly by examples, the main elements
of the Ontological Multidimensional Data Model, which considerably extends a
relational reconstruction of the multidimensional data model proposed by
Hurtado and Mendelzon by means of tuple-generating dependencies,
equality-generating dependencies, and negative constraints as found in
Datalog+-. We briefly mention some good computational properties of the model.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Mostafa Milani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract. This version with minor revisions and slightly
  extended. To appear in Proc. AMW'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04206v1</id>
    <updated>2017-03-13T00:32:14Z</updated>
    <published>2017-03-13T00:32:14Z</published>
    <title>Application of Bitcoin Data-Structures &amp; Design Principles to Supply
  Chain Management</title>
    <summary>  Heretofore the concept of "blockchain" has not been precisely defined.
Accordingly the potential useful applications of this technology have been
largely inflated. This work sidesteps the question of what constitutes a
blockchain as such and focuses on the architectural components of the Bitcoin
cryptocurrency, insofar as possible, in isolation. We consider common problems
inherent in the design of effective supply chain management systems. With each
identified problem we propose a solution that utilizes one or more component
aspects of Bitcoin. This culminates in five design principles for increased
efficiency in supply chain management systems through the application of
incentive mechanisms and data structures native to the Bitcoin cryptocurrency
protocol.
</summary>
    <author>
      <name>S. Matthew English</name>
    </author>
    <author>
      <name>Ehsan Nezhadian</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06348v1</id>
    <updated>2017-03-18T20:31:37Z</updated>
    <published>2017-03-18T20:31:37Z</published>
    <title>Application of Information Centric Networking to NoSQL Databases: the
  Spatio-Temporal use case</title>
    <summary>  This paper explores methodologies, advantages and challenges related to the
use of the Information Centric Network technology for developing NoSQL
distributed databases, which are expected to play a central role in the
forthcoming IoT and BigData era. ICN services make possible to simplify the
development of the database software, improve performance, and provide
data-level access control. We use our findings for devising a NoSQL
spatio-temporal database, named OpenGeoBase, and evaluate its performance with
a real data set related to Intelligent Transport System applications.
</summary>
    <author>
      <name>Andrea Detti</name>
    </author>
    <author>
      <name>Michele Orru</name>
    </author>
    <author>
      <name>Riccardo Paolillo</name>
    </author>
    <author>
      <name>Giulio Rossi</name>
    </author>
    <author>
      <name>Pierpaolo Loreti</name>
    </author>
    <author>
      <name>Lorenzo Bracciale</name>
    </author>
    <author>
      <name>Nicola Blefari Melazzi</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07371v1</id>
    <updated>2017-03-21T18:06:29Z</updated>
    <published>2017-03-21T18:06:29Z</published>
    <title>Multi Agent Driven Data Mining For Knowledge Discovery in Cloud
  Computing</title>
    <summary>  Today, huge amount of data is available on the web. Now there is a need to
convert that data in knowledge which can be useful for different purposes. This
paper depicts the use of data mining process, OLAP with the combination of
multi agent system to find the knowledge from data in cloud computing. For
this, I am also trying to explain one case study of online shopping of one
Bakery Shop. May be we can increase the sale of items by using the model, which
I am trying to represent.
</summary>
    <author>
      <name>Vishal Jain</name>
    </author>
    <author>
      <name>Mahesh Kumar Madan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  Research Excellence Vol. 2, Issue 1, Jan-Feb 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.07371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08425v1</id>
    <updated>2017-03-24T14:35:08Z</updated>
    <published>2017-03-24T14:35:08Z</published>
    <title>Redynis: Traffic-aware dynamic repartitioning for a distributed
  key-value store</title>
    <summary>  Most modern data stores tend to be distributed, to enable the scaling of the
data across multiple instances of commodity hardware. Although this ensures a
near unlimited potential for storage, the data itself is not always ideally
partitioned, and the cost of a network round-trip may cause a degradation of
end-user experience with respect to response latency. The problem being solved
is bringing the data objects closer to the frequent sources of requests using a
dynamic repartitioning algorithm. This is important if the objective is to
mitigate the overhead of network latency, and especially so if the partitions
are widely geo-distributed. The intention is to bring these features to an
existing distributed key-value store product, Redis.
</summary>
    <author>
      <name>Vineet John</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.12252.39048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.12252.39048" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09574v1</id>
    <updated>2017-03-28T13:52:03Z</updated>
    <published>2017-03-28T13:52:03Z</published>
    <title>Stored and Inherited Relations</title>
    <summary>  The universally applied Codd's relational model has two constructs: a stored
relation, with stored attributes only and a view, only with the inherited ones.
In 1992, we have proposed third construct, mixing both types of attributes.
Examples showed the idea attractive. No one followed however. We now revisit
our proposal. We show that a relational database scheme using also our
construct may be more faithful to reality. It may spare the logical navigation
or complex value expressions to queries. It may also avoid auxiliary views,
often necessary in practice at present. Better late than never, existing DBSs
should easily accommodate our proposal, with almost no storage and processing
overhead.
</summary>
    <author>
      <name>Witold Litwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01248v1</id>
    <updated>2018-03-03T21:40:33Z</updated>
    <published>2018-03-03T21:40:33Z</published>
    <title>Imprecise temporal associations and decision support systems</title>
    <summary>  The quick and pervasive infiltration of decision support systems, artificial
intelligence, and data mining in consumer electronics and everyday life in
general has been significant in recent years. Fields such as UX have been
facilitating the integration of such technologies into software and hardware,
but the back-end processing is still based on binary foundations. This article
describes an approach to mining for imprecise temporal associations among
events in data streams, taking into account the very natural concept of
approximation. This type of association analysis is likely to lead to more
meaningful and actionable decision support systems.
</summary>
    <author>
      <name>Giovanni Vincenti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.procs.2018.04.096</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.procs.2018.04.096" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06071v2</id>
    <updated>2021-04-26T07:48:11Z</updated>
    <published>2018-03-16T04:23:00Z</published>
    <title>Impacts of Dirty Data: and Experimental Evaluation</title>
    <summary>  Data quality issues have attracted widespread attention due to the negative
impacts of dirty data on data mining and machine learning results. The
relationship between data quality and the accuracy of results could be applied
on the selection of the appropriate algorithm with the consideration of data
quality and the determination of the data share to clean. However, rare
research has focused on exploring such relationship. Motivated by this, this
paper conducts an experimental comparison for the effects of missing,
inconsistent and conflicting data on classification and clustering algorithms.
Based on the experimental findings, we provide guidelines for algorithm
selection and data cleaning.
</summary>
    <author>
      <name>Zhixin Qi</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <author>
      <name>Jianzhong Li</name>
    </author>
    <author>
      <name>Hong Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 192 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.06071v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06071v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00680v1</id>
    <updated>2018-05-02T08:53:35Z</updated>
    <published>2018-05-02T08:53:35Z</published>
    <title>BUDAMAF: Data Management in Cloud Federations</title>
    <summary>  Data management has always been a multi-domain problem even in the simplest
cases. It involves, quality of service, security, resource management, cost
management, incident identification, disaster avoidance and/or recovery, as
well as many other concerns. In our case, this situation gets ever more
complicated because of the divergent nature of a cloud federation like BASMATI.
In this federation, the BASMATI Unified Data Management Framework (BUDaMaF),
tries to create an automated uniform way of managing all the data transactions,
as well as the data stores themselves, in a polyglot multi-cloud, consisting of
a plethora of different machines and data store systems.
</summary>
    <author>
      <name>Evangelos Psomakelis</name>
    </author>
    <author>
      <name>Konstantinos Tserpes</name>
    </author>
    <author>
      <name>Dimosthenis Anagnostopoulos</name>
    </author>
    <author>
      <name>Theodora Varvarigou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05235v2</id>
    <updated>2018-08-11T14:06:40Z</updated>
    <published>2018-05-14T15:37:02Z</published>
    <title>Decomposition of quantitative Gaifman graphs as a data analysis tool</title>
    <summary>  We argue the usefulness of Gaifman graphs of first-order relational
structures as an exploratory data analysis tool. We illustrate our approach
with cases where the modular decompositions of these graphs reveal interesting
facts about the data. Then, we introduce generalized notions of Gaifman graphs,
enhanced with quantitative information, to which we can apply more general,
existing decomposition notions via 2-structures; thus enlarging the analytical
capabilities of the scheme. The very essence of Gaifman graphs makes this
approach immediately appropriate for the multirelational data framework.
</summary>
    <author>
      <name>José Luis Balcázar</name>
    </author>
    <author>
      <name>Marie Ely Piceno</name>
    </author>
    <author>
      <name>Laura Rodríguez-Navas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at: Intelligent Data Analysis 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05235v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05235v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09157v1</id>
    <updated>2018-05-21T23:29:09Z</updated>
    <published>2018-05-21T23:29:09Z</published>
    <title>A New Finitely Controllable Class of Tuple Generating Dependencies: The
  Triangularly-Guarded Class</title>
    <summary>  In this paper we introduce a new class of tuple-generating dependencies
(TGDs) called triangularly-guarded (TG) TGDs. We show that conjunctive query
answering under this new class of TGDs is decidable since this new class of
TGDs also satisfies the finite controllability (FC) property. We further show
that this new class strictly contains some other decidable classes such as
weak-acyclic, guarded, sticky and shy. In this sense, the class TG provides a
unified representation of all these aforementioned classes of TGDs.
</summary>
    <author>
      <name>Vernon Asuncion</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for review. arXiv admin note: substantial text overlap with
  arXiv:1804.05997</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06151v2</id>
    <updated>2018-06-19T09:59:49Z</updated>
    <published>2018-06-15T23:51:52Z</published>
    <title>Efficient Data Perturbation for Privacy Preserving and Accurate Data
  Stream Mining</title>
    <summary>  The widespread use of the Internet of Things (IoT) has raised many concerns,
including the protection of private information. Existing privacy preservation
methods cannot provide a good balance between data utility and privacy, and
also have problems with efficiency and scalability. This paper proposes an
efficient data stream perturbation method (named as $P^2RoCAl$). $P^2RoCAl$
offers better data utility than similar methods: classification accuracies of
$P^2RoCAl$ perturbed data streams are very close to those of the original data
streams. $P^2RoCAl$ also provides higher resilience against data reconstruction
attacks.
</summary>
    <author>
      <name>M. A. P. Chamikara</name>
    </author>
    <author>
      <name>P. Bertok</name>
    </author>
    <author>
      <name>D. Liu</name>
    </author>
    <author>
      <name>S. Camtepe</name>
    </author>
    <author>
      <name>I. Khalil</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.pmcj.2018.05.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.pmcj.2018.05.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pervasive and Mobile Computing 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06151v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06151v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07728v1</id>
    <updated>2018-06-20T13:45:24Z</updated>
    <published>2018-06-20T13:45:24Z</published>
    <title>Parallelization of XPath Queries using Modern XQuery Processors</title>
    <summary>  A practical and promising approach to parallelizing XPath queries was
proposed by Bordawekar et al. in 2009, which enables parallelization on top of
existing XML database engines. Although they experimentally demonstrated the
speedup by their approach, their practice has already been out of date because
the software environment has largely changed with the capability of XQuery
processing. In this work, we implement their approach in two ways on top of a
state-of-the-art XML database engine and experimentally demonstrate that our
implementations can bring significant speedup on a commodity server.
</summary>
    <author>
      <name>Shigeyuki Sato</name>
    </author>
    <author>
      <name>Wei Hao</name>
    </author>
    <author>
      <name>Kiminori Matsuzaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the full version of our publication to appear at ADBIS 2018
  as a short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10078v2</id>
    <updated>2018-07-03T09:46:41Z</updated>
    <published>2018-06-26T15:55:37Z</published>
    <title>A General Framework for Anytime Approximation in Probabilistic Databases</title>
    <summary>  Anytime approximation algorithms that compute the probabilities of queries
over probabilistic databases can be of great use to statistical learning tasks.
Those approaches have been based so far on either (i) sampling or (ii)
branch-and-bound with model-based bounds. We present here a more general
branch-and-bound framework that extends the possible bounds by using
'dissociation', which yields tighter bounds.
</summary>
    <author>
      <name>Maarten Van den Heuvel</name>
    </author>
    <author>
      <name>Floris Geerts</name>
    </author>
    <author>
      <name>Wolfgang Gatterbauer</name>
    </author>
    <author>
      <name>Martin Theobald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, submitted to StarAI 2018 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01946v1</id>
    <updated>2020-07-03T21:50:35Z</updated>
    <published>2020-07-03T21:50:35Z</published>
    <title>CICLAD: A Fast and Memory-efficient Closed Itemset Miner for Streams</title>
    <summary>  Mining association rules from data streams is a challenging task due to the
(typically) limited resources available vs. the large size of the result.
Frequent closed itemsets (FCI) enable an efficient first step, yet current FCI
stream miners are not optimal on resource consumption, e.g. they store a large
number of extra itemsets at an additional cost. In a search for a better
storage-efficiency trade-off, we designed Ciclad,an intersection-based
sliding-window FCI miner. Leveraging in-depth insights into FCI evolution, it
combines minimal storage with quick access. Experimental results indicate
Ciclad's memory imprint is much lower and its performances globally better than
competitor methods.
</summary>
    <author>
      <name>Tomas Martin</name>
    </author>
    <author>
      <name>Guy Francoeur</name>
    </author>
    <author>
      <name>Petko Valtchev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3394486.3403232</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3394486.3403232" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD20</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08876v2</id>
    <updated>2021-06-16T18:53:32Z</updated>
    <published>2020-07-17T10:10:04Z</published>
    <title>Tractability Beyond $β$-Acyclicity for Conjunctive Queries with
  Negation</title>
    <summary>  Numerous fundamental database and reasoning problems are known to be NP-hard
in general but tractable on instances where the underlying hypergraph structure
is $\beta$-acyclic. Despite the importance of many of these problems, there has
been little success in generalizing these results beyond acyclicity.
  In this paper, we take on this challenge and propose nest-set width, a novel
generalization of hypergraph $\beta$-acyclicity. We demonstrate that nest-set
width has desirable properties and algorithmic significance. In particular,
evaluation of boolean conjunctive queries with negation is tractable for
classes with bounded nest-set width. Furthermore, propositional satisfiability
is fixed-parameter tractable when parameterized by nest-set width.
</summary>
    <author>
      <name>Matthias Lanzinger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3452021.3458308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3452021.3458308" rel="related"/>
    <link href="http://arxiv.org/abs/2007.08876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12799v2</id>
    <updated>2020-08-19T01:53:53Z</updated>
    <published>2020-07-24T23:13:27Z</published>
    <title>Score-Based Explanations in Data Management and Machine Learning</title>
    <summary>  We describe some approaches to explanations for observed outcomes in data
management and machine learning. They are based on the assignment of numerical
scores to predefined and potentially relevant inputs. More specifically, we
consider explanations for query answers in databases, and for results from
classification models. The described approaches are mostly of a causal and
counterfactual nature. We argue for the need to bring domain and semantic
knowledge into score computations; and suggest some ways to do this.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Companion paper for a tutorial at the Scalable Uncertainty Management
  Conference (SUM'20). To appear in Proc. SUM'20. Minor fixes made</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.12799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05032v1</id>
    <updated>2020-09-10T17:53:19Z</updated>
    <published>2020-09-10T17:53:19Z</published>
    <title>GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of
  Graph, Raster and Vector Data -- Technical Report</title>
    <summary>  We introduce an approach to semantically represent and query raster data in a
Semantic Web graph. We extend the GeoSPARQL vocabulary and query language to
support raster data as a new type of geospatial data. We define new filter
functions and illustrate our approach using several use cases on real-world
data sets. Finally, we describe a prototypical implementation and validate the
feasibility of our approach.
</summary>
    <author>
      <name>Timo Homburg</name>
    </author>
    <author>
      <name>Steffen Staab</name>
    </author>
    <author>
      <name>Daniel Janke</name>
    </author>
    <link href="http://arxiv.org/abs/2009.05032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05798v1</id>
    <updated>2020-09-12T14:06:39Z</updated>
    <published>2020-09-12T14:06:39Z</published>
    <title>A Simple and Efficient Framework for Identifying Relation-gaps in
  Ontologies</title>
    <summary>  Though many ontologies have huge number of classes, one cannot find a good
number of object properties connecting the classes in most of the cases. Adding
object properties makes an ontology richer and more applicable for tasks such
as Question Answering. In this context, the question of which two classes
should be considered for discovering object properties becomes very important.
We address the above question in this paper. We propose a simple machine
learning framework which exhibits low time complexity and yet gives promising
results with respect to both precision as well as number of class-pairs
retrieved.
</summary>
    <author>
      <name>Subhashree S</name>
    </author>
    <author>
      <name>P Sreenivasa Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2009.05798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09895v2</id>
    <updated>2020-09-22T13:29:34Z</updated>
    <published>2020-09-02T16:24:01Z</published>
    <title>Data mining and time series segmentation via extrema: preliminary
  investigations</title>
    <summary>  Time series segmentation is one of the many data mining tools. This paper, in
French, takes local extrema as perceptually interesting points (PIPs). The
blurring of those PIPs by the quick fluctuations around any time series is
treated via an additive decomposition theorem, due to Cartier and Perrin, and
algebraic estimation techniques, which are already useful in automatic control
and signal processing. Our approach is validated by several computer
illustrations. They underline the importance of the choice of a threshold for
the extrema detection.
</summary>
    <author>
      <name>Michel Fliess</name>
    </author>
    <author>
      <name>Cédric Join</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Modeling, Optimization and
  Simulation (MOSIM 2020), Agadir (Morocco), 12-14 November 2020, in French</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09895v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09895v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11543v1</id>
    <updated>2020-09-24T08:21:10Z</updated>
    <published>2020-09-24T08:21:10Z</published>
    <title>Compressed Key Sort and Fast Index Reconstruction</title>
    <summary>  In this paper we propose an index key compression scheme based on the notion
of distinction bits by proving that the distinction bits of index keys are
sufficient information to determine the sorted order of the index keys
correctly. While the actual compression ratio may vary depending on the
characteristics of datasets (an average of 2.76 to one compression ratio was
observed in our experiments), the index key compression scheme leads to
significant performance improvements during the reconstruction of large-scale
indexes. Our index key compression can be effectively used in database
replication and index recovery of modern main-memory database systems.
</summary>
    <author>
      <name>Yongsik Kwon</name>
    </author>
    <author>
      <name>Cheol Ryu</name>
    </author>
    <author>
      <name>Sang Kyun Cha</name>
    </author>
    <author>
      <name>Arthur H. Lee</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <author>
      <name>Bongki Moon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages and 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14821v2</id>
    <updated>2020-10-01T00:58:37Z</updated>
    <published>2020-09-30T17:51:37Z</published>
    <title>A Systematic Method for On-The-Fly Denormalization of Relational
  Databases</title>
    <summary>  Normalized relational databases are a common method for storing data, but
pulling out usable denormalized data for consumption generally requires either
direct access to the source data or creation of an appropriate view or table by
a database administrator. End-users are thus limited in their ability to
explore and use data that is stored in this manner. Presented here is a method
for performing automated denormalization of relational databases at run-time,
without requiring access to source data or ongoing intervention by a database
administrator. Furthermore, this method does not require a restructure of the
database itself and so it can be flexibly applied as a layer on top of already
existing databases.
</summary>
    <author>
      <name>Sareen Shah</name>
    </author>
    <link href="http://arxiv.org/abs/2009.14821v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14821v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02145v1</id>
    <updated>2021-03-03T02:56:46Z</updated>
    <published>2021-03-03T02:56:46Z</published>
    <title>Enhancing the Interactivity of Dataframe Queries by Leveraging Think
  Time</title>
    <summary>  We propose opportunistic evaluation, a framework for accelerating
interactions with dataframes. Interactive latency is critical for iterative,
human-in-the-loop dataframe workloads for supporting exploratory data analysis.
Opportunistic evaluation significantly reduces interactive latency by 1)
prioritizing computation directly relevant to the interactions and 2)
leveraging think time for asynchronous background computation for non-critical
operators that might be relevant to future interactions. We show, through
empirical analysis, that current user behavior presents ample opportunities for
optimization, and the solutions we propose effectively harness such
opportunities.
</summary>
    <author>
      <name>Doris Xin</name>
    </author>
    <author>
      <name>Devin Petersohn</name>
    </author>
    <author>
      <name>Dixin Tang</name>
    </author>
    <author>
      <name>Yifan Wu</name>
    </author>
    <author>
      <name>Joseph E. Gonzalez</name>
    </author>
    <author>
      <name>Joseph M. Hellerstein</name>
    </author>
    <author>
      <name>Anthony D. Joseph</name>
    </author>
    <author>
      <name>Aditya G. Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.03314v3</id>
    <updated>2021-11-12T06:48:25Z</updated>
    <published>2021-03-04T20:35:53Z</published>
    <title>Consistent Answers of Aggregation Queries using SAT Solvers</title>
    <summary>  The framework of database repairs and consistent answers to queries is a
principled approach to managing inconsistent databases. We describe the first
system able to compute the consistent answers of general aggregation queries
with the COUNT(A), COUNT(*), SUM(A), MIN(A), and MAX(A) operators, and with or
without grouping constructs. Our system uses reductions to optimization
versions of Boolean satisfiability (SAT) and then leverages powerful SAT
solvers. We carry out an extensive set of experiments on both synthetic and
real-world data that demonstrate the usefulness and scalability of this
approach.
</summary>
    <author>
      <name>Akhil A. Dixit</name>
    </author>
    <author>
      <name>Phokion G. Kolaitis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 10 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.03314v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03314v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.07978v1</id>
    <updated>2021-03-14T17:21:26Z</updated>
    <published>2021-03-14T17:21:26Z</published>
    <title>Putting Data Science Pipelines on the Edge</title>
    <summary>  This paper proposes a composable "Just in Time Architecture" for Data Science
(DS) Pipelines named JITA-4DS and associated resource management techniques for
configuring disaggregated data centers (DCs). DCs under our approach are
composable based on vertical integration of the application,
middleware/operating system, and hardware layers customized dynamically to meet
application Service Level Objectives (SLO - application-aware management).
Thereby, pipelines utilize a set of flexible building blocks that can be
dynamically and automatically assembled and re-assembled to meet the dynamic
changes in the workload's SLOs. To assess disaggregated DC's, we study how to
model and validate their performance in large-scale settings.
</summary>
    <author>
      <name>Ali Akoglu</name>
    </author>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <link href="http://arxiv.org/abs/2103.07978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.07978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15816v1</id>
    <updated>2021-03-27T08:26:45Z</updated>
    <published>2021-03-27T08:26:45Z</published>
    <title>Some Results of Experimental Check of The Model of the Object
  Innovativeness Quantitative Evaluation</title>
    <summary>  The paper presents the results of the experiments that were conducted to
confirm the main ideas of the proposed approach to determining the objects
innovativeness. This approach assumed that the product life cycle of whose
descriptions are placed in different data warehouses is adequate. The proposed
formal model allows us to calculate the quantitative value of the additive
evaluation criterion of objects innovativeness. The obtained experimental data
make it possible to evaluate the adopted approach correctness.
</summary>
    <author>
      <name>V. K. Ivanov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.31432/1994-2443-2020-15-3-27-36</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.31432/1994-2443-2020-15-3-27-36" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in Russian</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information and Innovations. 2020. Vol. 15, N 3</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.15816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05130v1</id>
    <updated>2021-05-11T15:39:55Z</updated>
    <published>2021-05-11T15:39:55Z</published>
    <title>Towards a Model for LSH</title>
    <summary>  As data volumes continue to grow, clustering and outlier detection algorithms
are becoming increasingly time-consuming. Classical index structures for
neighbor search are no longer sustainable due to the "curse of dimensionality".
Instead, approximated index structures offer a good opportunity to
significantly accelerate the neighbor search for clustering and outlier
detection and to have the lowest possible error rate in the results of the
algorithms. Locality-sensitive hashing is one of those. We indicate directions
to model the properties of LSH.
</summary>
    <author>
      <name>Li Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2103.01888</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.05130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05443v3</id>
    <updated>2021-12-27T08:56:03Z</updated>
    <published>2021-05-12T05:47:07Z</published>
    <title>A Nearly Instance-optimal Differentially Private Mechanism for
  Conjunctive Queries</title>
    <summary>  Releasing the result size of conjunctive queries and graph pattern queries
under differential privacy (DP) has received considerable attention in the
literature, but existing solutions do not offer any optimality guarantees. We
provide the first DP mechanism for this problem with a fairly strong notion of
optimality, which can be considered as a natural relaxation of
instance-optimality to a constant.
</summary>
    <author>
      <name>Wei Dong</name>
    </author>
    <author>
      <name>Ke Yi</name>
    </author>
    <link href="http://arxiv.org/abs/2105.05443v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05443v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00100v1</id>
    <updated>2015-09-01T00:15:42Z</updated>
    <published>2015-09-01T00:15:42Z</published>
    <title>Decidability of Equivalence of Aggregate Count-Distinct Queries</title>
    <summary>  We address the problem of equivalence of count-distinct aggregate queries,
prove that the problem is decidable, and can be decided in the third level of
Polynomial hierarchy. We introduce the notion of core for conjunctive queries
with comparisons as an extension of the classical notion for relational
queries, and prove that the existence of isomorphism among cores of queries is
a sufficient and necessary condition for equivalence of conjunctive queries
with comparisons similar to the classical relational setting. However, it is
not a necessary condition for equivalence of count-distinct queries. We
introduce a relaxation of this condition based on a new notion, which is a
potentially new query equivalent to the initial query, introduced to capture
the behavior of count-distinct operator.
</summary>
    <author>
      <name>Babak Bagheri Hariri</name>
    </author>
    <author>
      <name>Val Tannen</name>
    </author>
    <link href="http://arxiv.org/abs/1509.00100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04238v1</id>
    <updated>2015-09-14T18:57:02Z</updated>
    <published>2015-09-14T18:57:02Z</published>
    <title>A Practioner's Guide to Evaluating Entity Resolution Results</title>
    <summary>  Entity resolution (ER) is the task of identifying records belonging to the
same entity (e.g. individual, group) across one or multiple databases.
Ironically, it has multiple names: deduplication and record linkage, among
others. In this paper we survey metrics used to evaluate ER results in order to
iteratively improve performance and guarantee sufficient quality prior to
deployment. Some of these metrics are borrowed from multi-class classification
and clustering domains, though some key differences exist differentiating
entity resolution from general clustering. Menestrina et al. empirically showed
rankings from these metrics often conflict with each other, thus our primary
motivation for studying them. This paper provides practitioners the basic
knowledge to begin evaluating their entity resolution results.
</summary>
    <author>
      <name>Matt Barnes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.04238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06053v4</id>
    <updated>2016-06-06T11:38:47Z</updated>
    <published>2016-03-19T06:37:28Z</published>
    <title>Negation in SPARQL</title>
    <summary>  This paper presents a thorough study of negation in SPARQL. The types of
negation supported in SPARQL are identified and their main features discussed.
Then, we study the expressive power of the corresponding negation operators. At
this point, we identify a core SPARQL algebra which could be used instead of
the W3C SPARQL algebra. Finally, we analyze the negation operators in terms of
their compliance with elementary axioms of set theory.
</summary>
    <author>
      <name>Renzo Angles</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the Alberto Mendelzon International Workshop on Foundations
  of Data Management (AMW'2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06053v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06053v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07865v2</id>
    <updated>2016-11-05T20:34:28Z</updated>
    <published>2016-05-25T12:59:37Z</published>
    <title>Constructing Data Graphs for Keyword Search</title>
    <summary>  A data graph is a convenient paradigm for supporting keyword search that
takes into account available semantic structure and not just textual relevance.
However, the problem of constructing data graphs that facilitate both
efficiency and effectiveness of the underlying system has hardly been
addressed. A conceptual model for this task is proposed. Principles for
constructing good data graphs are explained. Transformations for generating
data graphs from RDB and XML are developed. The results obtained from these
transformations are analyzed. It is shown that XML is a better starting point
for getting a good data graph.
</summary>
    <author>
      <name>Konstantin Golenberg</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-44406-2_33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-44406-2_33" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of DEXA'16 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09753v1</id>
    <updated>2016-05-31T18:20:36Z</updated>
    <published>2016-05-31T18:20:36Z</published>
    <title>PerfEnforce: A Dynamic Scaling Engine for Analytics with Performance
  Guarantees</title>
    <summary>  In this paper, we present PerfEnforce, a scaling engine designed to enable
cloud providers to sell performance levels for data analytics cloud services.
PerfEnforce scales a cluster of virtual machines allocated to a user in a way
that minimizes cost while probabilistically meeting the query runtime
guarantees offered by a service level agreement. With PerfEnforce, we show how
to scale a cluster in a way that minimally disrupts a user's query session. We
further show when to scale the cluster using one of three methods: feedback
control, reinforcement learning, or perceptron learning. We find that
perceptron learning outperforms the other two methods when making cluster
scaling decisions.
</summary>
    <author>
      <name>Jennifer Ortiz</name>
    </author>
    <author>
      <name>Brendan Lee</name>
    </author>
    <author>
      <name>Magdalena Balazinska</name>
    </author>
    <author>
      <name>Joseph L. Hellerstein</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05538v1</id>
    <updated>2016-07-19T12:15:37Z</updated>
    <published>2016-07-19T12:15:37Z</published>
    <title>Challenges for Efficient Query Evaluation on Structured Probabilistic
  Data</title>
    <summary>  Query answering over probabilistic data is an important task but is generally
intractable. However, a new approach for this problem has recently been
proposed, based on structural decompositions of input databases, following,
e.g., tree decompositions. This paper presents a vision for a database
management system for probabilistic data built following this structural
approach. We review our existing and ongoing work on this topic and highlight
many theoretical and practical challenges that remain to be addressed.
</summary>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <author>
      <name>Silviu Maniu</name>
    </author>
    <author>
      <name>Mikaël Monet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-45856-4_22</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-45856-4_22" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, 23 references. Accepted for publication at SUM
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05702v1</id>
    <updated>2016-07-19T19:18:08Z</updated>
    <published>2016-07-19T19:18:08Z</published>
    <title>Integration of Probabilistic Uncertain Information</title>
    <summary>  We study the problem of data integration from sources that contain
probabilistic uncertain information. Data is modeled by possible-worlds with
probability distribution, compactly represented in the probabilistic relation
model. Integration is achieved efficiently using the extended probabilistic
relation model. We study the problem of determining the probability
distribution of the integration result. It has been shown that, in general,
only probability ranges can be determined for the result of integration. In
this paper we concentrate on a subclass of extended probabilistic relations,
those that are obtainable through integration. We show that under intuitive and
reasonable assumptions we can determine the exact probability distribution of
the result of integration.
</summary>
    <author>
      <name>Fereidoon Sadri</name>
    </author>
    <author>
      <name>Gayatri Tallur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06063v1</id>
    <updated>2016-07-20T19:10:48Z</updated>
    <published>2016-07-20T19:10:48Z</published>
    <title>Fragment Allocation Configuration in Distributed Database Systems</title>
    <summary>  In distributed database (DDB) management systems, fragment allocation is one
of the most important components that can directly affect the performance of
DDB. In this research work, we will show that declarative programming
languages, e.g. logic programming languages, can be used to represent different
data fragment allocation techniques. Results indicate that, using declarative
programming language significantly simplifies the representation of fragment
allocation algorithm, thus opens door for any further developments and
optimizations. The under consideration case study also show that our approach
can be extended to be used in different areas of distributed systems.
</summary>
    <author>
      <name>Mohammad Reza Abbasifard</name>
    </author>
    <author>
      <name>Omid Isfahani Alamdari</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04915v1</id>
    <updated>2017-05-14T04:03:15Z</updated>
    <published>2017-05-14T04:03:15Z</published>
    <title>Discovering Multiple Truths with a Hybrid Model</title>
    <summary>  Many data management applications require integrating information from
multiple sources. The sources may not be accurate and provide erroneous values.
We thus have to identify the true values from conflicting observations made by
the sources. The problem is further complicated when there may exist multiple
truths (e.g., a book written by several authors). In this paper we propose a
model called Hybrid that jointly makes two decisions: how many truths there
are, and what they are. It considers the conflicts between values as important
evidence for ruling out wrong values, while keeps the flexibility of allowing
multiple truths. In this way, Hybrid is able to achieve both high precision and
high recall.
</summary>
    <author>
      <name>Furong Li</name>
    </author>
    <author>
      <name>Xin Luna Dong</name>
    </author>
    <author>
      <name>Anno Langen</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04928v1</id>
    <updated>2017-05-14T08:44:28Z</updated>
    <published>2017-05-14T08:44:28Z</published>
    <title>Big Data: Challenges, Opportunities and Realities</title>
    <summary>  With the advent of Internet of Things (IoT) and Web 2.0 technologies, there
has been a tremendous growth in the amount of data generated. This chapter
emphasizes on the need for big data, technological advancements, tools and
techniques being used to process big data are discussed. Technological
improvements and limitations of existing storage techniques are also presented.
Since, the traditional technologies like Relational Database Management System
(RDBMS) have their own limitations to handle big data, new technologies have
been developed to handle them and to derive useful insights. This chapter
presents an overview of big data analytics, its application, advantages, and
limitations. Few research issues and future directions are presented in this
chapter.
</summary>
    <author>
      <name>Abhay Bhadani</name>
    </author>
    <author>
      <name>Dhanya Jothimani</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08618v1</id>
    <updated>2017-12-11T21:02:21Z</updated>
    <published>2017-12-11T21:02:21Z</published>
    <title>Feature Extraction and Feature Selection: Reducing Data Complexity with
  Apache Spark</title>
    <summary>  Feature extraction and feature selection are the first tasks in
pre-processing of input logs in order to detect cyber security threats and
attacks while utilizing machine learning. When it comes to the analysis of
heterogeneous data derived from different sources, these tasks are found to be
time-consuming and difficult to be managed efficiently. In this paper, we
present an approach for handling feature extraction and feature selection for
security analytics of heterogeneous data derived from different network
sensors. The approach is implemented in Apache Spark, using its python API,
named pyspark.
</summary>
    <author>
      <name>Dimitrios Sisiaridis</name>
    </author>
    <author>
      <name>Olivier Markowitch</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.9, No.6, November 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.08618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08809v3</id>
    <updated>2018-03-27T09:56:52Z</updated>
    <published>2017-12-23T17:23:08Z</published>
    <title>The tractability frontier of well-designed SPARQL queries</title>
    <summary>  We study the complexity of query evaluation of SPARQL queries. We focus on
the fundamental fragment of well-designed SPARQL restricted to the AND,
OPTIONAL and UNION operators. Our main result is a structural characterisation
of the classes of well-designed queries that can be evaluated in polynomial
time. In particular, we introduce a new notion of width called domination
width, which relies on the well-known notion of treewidth. We show that, under
some complexity theoretic assumptions, the classes of well-designed queries
that can be evaluated in polynomial time are precisely those of bounded
domination width.
</summary>
    <author>
      <name>Miguel Romero</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08809v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08809v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0; H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01554v1</id>
    <updated>2018-02-05T18:33:16Z</updated>
    <published>2018-02-05T18:33:16Z</published>
    <title>Can One Escape Red Chains? Regular Path Queries Determinacy is
  Undecidable</title>
    <summary>  For a given set of queries (which are expressions in some query language)
$\mathcal{Q}=\{Q_1$, $Q_2, \ldots Q_k\}$ and for another query $Q_0$ we say
that $\mathcal{Q}$ determines $Q_0$ if -- informally speaking -- for every
database $\mathbb D$, the information contained in the views
$\mathcal{Q}({\mathbb D})$ is sufficient to compute $Q_0({\mathbb D})$. Query
Determinacy Problem is the problem of deciding, for given $\mathcal{Q}$ and
$Q_0$, whether $\mathcal{Q}$ determines $Q_0$. Many versions of this problem,
for different query languages, were studied in database theory. In this paper
we solve a problem stated in [CGLV02] and show that Query Determinacy Problem
is undecidable for the Regular Path Queries -- the paradigmatic query language
of graph databases.
</summary>
    <author>
      <name>Grzegorz Głuch</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <author>
      <name>Piotr Ostropolski-Nalewaja</name>
    </author>
    <link href="http://arxiv.org/abs/1802.01554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08586v1</id>
    <updated>2018-02-23T15:15:53Z</updated>
    <published>2018-02-23T15:15:53Z</published>
    <title>Database Aggregation</title>
    <summary>  Knowledge can be represented compactly in a multitude ways, from a set of
propositional formulas, to a Kripke model, to a database. In this paper we
study the aggregation of information coming from multiple sources, each source
submitting a database modelled as a first-order relational structure. In the
presence of an integrity constraint, we identify classes of aggregators that
respect it in the aggregated database, provided all individual databases
satisfy it. We also characterise languages for first-order queries on which the
answer to queries on the aggregated database coincides with the aggregation of
the answers to the query obtained on each individual database. This
contribution is meant to be a first step on the application of techniques from
rational choice theory to knowledge representation in databases.
</summary>
    <author>
      <name>Francesco Belardinelli</name>
    </author>
    <author>
      <name>Umberto Grandi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09984v2</id>
    <updated>2018-03-20T18:27:52Z</updated>
    <published>2018-02-27T16:01:36Z</published>
    <title>Formal Semantics of the Language Cypher</title>
    <summary>  Cypher is a query language for property graphs. It was originally designed
and implemented as part of the Neo4j graph database, and it is currently used
in a growing number of commercial systems, industrial applications and research
projects. In this work, we provide denotational semantics of the core fragment
of the read-only part of Cypher, which features in particular pattern matching,
filtering, and most relational operations on tables.
</summary>
    <author>
      <name>Nadime Francis</name>
    </author>
    <author>
      <name>Alastair Green</name>
    </author>
    <author>
      <name>Paolo Guagliardo</name>
    </author>
    <author>
      <name>Leonid Libkin</name>
    </author>
    <author>
      <name>Tobias Lindaaker</name>
    </author>
    <author>
      <name>Victor Marsault</name>
    </author>
    <author>
      <name>Stefan Plantikow</name>
    </author>
    <author>
      <name>Mats Rydberg</name>
    </author>
    <author>
      <name>Martin Schuster</name>
    </author>
    <author>
      <name>Petra Selmer</name>
    </author>
    <author>
      <name>Andrés Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00465v1</id>
    <updated>2018-04-02T12:08:01Z</updated>
    <published>2018-04-02T12:08:01Z</published>
    <title>Database as a Service - Current Issues and Its Future</title>
    <summary>  With the prevalence of applications in cloud, Database as a Service (DBaaS)
becomes a promising method to provide cloud applications with reliable and
flexible data storage services. It provides a number of interesting features to
cloud developers, however, it suffers a few drawbacks: long learning curve and
development cycle, lacking of in-depth support for NoSQL, lacking of flexible
configuration for security and privacy, and high cost models. In this paper, we
investigate these issues among current DBaaS providers and propose a novel
Trinity Model that can significantly reduce the learning curves, improve the
security and privacy, and accelerate database design and development. We
further elaborate our ongoing and future work on developing large real-world
SaaS projects using this new DBaaS model.
</summary>
    <author>
      <name>Xi Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1804.00465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05892v1</id>
    <updated>2018-04-16T18:54:11Z</updated>
    <published>2018-04-16T18:54:11Z</published>
    <title>Accelerating Human-in-the-loop Machine Learning: Challenges and
  Opportunities</title>
    <summary>  Development of machine learning (ML) workflows is a tedious process of
iterative experimentation: developers repeatedly make changes to workflows
until the desired accuracy is attained. We describe our vision for a
"human-in-the-loop" ML system that accelerates this process: by intelligently
tracking changes and intermediate results over time, such a system can enable
rapid iteration, quick responsive feedback, introspection and debugging, and
background execution and automation. We finally describe Helix, our preliminary
attempt at such a system that has already led to speedups of up to 10x on
typical iterative workflows against competing systems.
</summary>
    <author>
      <name>Doris Xin</name>
    </author>
    <author>
      <name>Litian Ma</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Stephen Macke</name>
    </author>
    <author>
      <name>Shuchen Song</name>
    </author>
    <author>
      <name>Aditya Parameswaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be published in SIGMOD '18 DEEM Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08834v3</id>
    <updated>2018-07-12T20:42:04Z</updated>
    <published>2018-04-24T04:04:27Z</published>
    <title>Measuring and Computing Database Inconsistency via Repairs</title>
    <summary>  We propose a generic numerical measure of inconsistency of a database with
respect to a set of integrity constraints. It is based on an abstract repair
semantics. A particular inconsistency measure associated to cardinality-repairs
is investigated; and we show that it can be computed via answer-set programs.
  Keywords: Integrity constraints in databases, inconsistent databases,
database repairs, inconsistency measure.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission as short paper; to appear in Proc. Scalable Uncertainty
  Management, SUM 2018. Abstract and keywords added</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08834v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08834v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00602v1</id>
    <updated>2018-07-02T11:27:51Z</updated>
    <published>2018-07-02T11:27:51Z</published>
    <title>Semantic Query Language for Temporal Genealogical Trees</title>
    <summary>  Computers play a crucial role in modern ancestry management, they are used to
collect, store, analyze, sort and display genealogical data. However, current
applications do not take into account the kinship structure of a natural
language.
  In this paper we propose a new domain-specific language KISP which is based
on a formalization of English kinship system, for accessing and querying
traditional genealogical trees. KISP is a dynamically typed LISP-like
programming language with a rich set of features, such as kinship term
reduction and temporal information expression.
  Our solution provides a user with a coherent genealogical framework that
allows for a natural navigation over any traditional family tree.
</summary>
    <author>
      <name>Evgeniy Gryaznov</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05258v1</id>
    <updated>2018-07-13T19:31:08Z</updated>
    <published>2018-07-13T19:31:08Z</published>
    <title>QR2: A Third-party Query Reranking Service Over Web Databases</title>
    <summary>  The ranked retrieval model has rapidly become the de-facto way for search
query processing in web databases. Despite the extensive efforts on designing
better ranking mechanisms, in practice, many such databases fail to address the
diverse and sometimes contradicting preferences of users. In this paper, we
present QR2, a third-party service that uses nothing but the public search
interface of a web database and enables the on-the-fly processing of queries
with any user-specified ranking functions, no matter if the ranking function is
supported by the database or not.
</summary>
    <author>
      <name>Yeshwanth D. Gunasekaran</name>
    </author>
    <author>
      <name>Abolfazl Asudeh</name>
    </author>
    <author>
      <name>Sona Hasani</name>
    </author>
    <author>
      <name>Nan Zhang</name>
    </author>
    <author>
      <name>Ali Jaoua</name>
    </author>
    <author>
      <name>Gautam Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34th IEEE International Conference on Data Engineering (ICDE Demo),
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05667v1</id>
    <updated>2020-01-16T06:22:51Z</updated>
    <published>2020-01-16T06:22:51Z</published>
    <title>Hardware-Conscious Stream Processing: A Survey</title>
    <summary>  Data stream processing systems (DSPSs) enable users to express and run stream
applications to continuously process data streams. To achieve real-time data
analytics, recent researches keep focusing on optimizing the system latency and
throughput. Witnessing the recent great achievements in the computer
architecture community, researchers and practitioners have investigated the
potential of adoption hardware-conscious stream processing by better utilizing
modern hardware capacity in DSPSs. In this paper, we conduct a systematic
survey of recent work in the field, particularly along with the following three
directions: 1) computation optimization, 2) stream I/O optimization, and 3)
query deployment. Finally, we advise on potential future research directions.
</summary>
    <author>
      <name>Shuhao Zhang</name>
    </author>
    <author>
      <name>Feng Zhang</name>
    </author>
    <author>
      <name>Yingjun Wu</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <author>
      <name>Paul Johns</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3385658.3385662</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3385658.3385662" rel="related"/>
    <link href="http://arxiv.org/abs/2001.05667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08329v1</id>
    <updated>2020-01-23T01:27:41Z</updated>
    <published>2020-01-23T01:27:41Z</published>
    <title>Leveraging Neighborhood Summaries for Efficient RDF Queries on RDBMS</title>
    <summary>  Using structural informations to summarize graph-structured RDF data is
helpful in tackling query performance issues. However, leveraging structural
indexes needs to revise or even redesign the internal of RDF systems. Given an
RDF dataset that have already been bulk loaded into a relational RDF system, we
aim at improving the query performance on such systems. We do so by summarizing
neighborhood structures and encoding them into triples which can be managed
along side the exist instance data. At query time, we optimally select the
effective structural patterns, and adding these patterns to the existing
queries to gain an improved query performance. Empirical evaluations shown the
effectiveness of our method.
</summary>
    <author>
      <name>Lei Gai</name>
    </author>
    <link href="http://arxiv.org/abs/2001.08329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11324v1</id>
    <updated>2020-01-29T07:30:00Z</updated>
    <published>2020-01-29T07:30:00Z</published>
    <title>Proceedings of Symposium on Data Mining Applications 2014</title>
    <summary>  The Symposium on Data Mining and Applications (SDMA 2014) is aimed to gather
researchers and application developers from a wide range of data mining related
areas such as statistics, computational intelligence, pattern recognition,
databases, Big Data Mining and visualization. SDMA is organized by MEGDAM to
advance the state of the art in data mining research field and its various real
world applications. The symposium will provide opportunities for technical
collaboration among data mining and machine learning researchers around the
Saudi Arabia, GCC countries and Middle-East region. Acceptance will be based
primarily on originality, significance and quality of contribution.
</summary>
    <author>
      <name>Basit Qureshi</name>
    </author>
    <author>
      <name>Yasir Javed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Symposium on Data Mining Applications 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.11324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.7; B.8; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11506v1</id>
    <updated>2020-01-30T07:45:18Z</updated>
    <published>2020-01-30T07:45:18Z</published>
    <title>Theoretical Model and Practical Considerations for Data Lineage
  Reconstruction</title>
    <summary>  We live in a world driven by data. The amount of it outgrows anyone's ability
to oversee it or even observe its scope. Along with all the advances in the
space of data management, there is still a significant lack of formalism and
standardization around defining data ecosystems and processes occurring within
those. In order to address the issue we propose a notation for data flow
modeling and evaluate some of the most common applications of it based on
real-world use cases. To facilitate future work, we provide detailed reference
of the data model we defined and consider potential programming paradigms.
</summary>
    <author>
      <name>Egor Pushkin</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00054v1</id>
    <updated>2020-02-28T20:34:09Z</updated>
    <published>2020-02-28T20:34:09Z</published>
    <title>An Empirical Study on the Design and Evolution of NoSQL Database Schemas</title>
    <summary>  We study how software engineers design and evolve their domain model when
building applications against NoSQL data stores. Specifically, we target Java
projects that use object-NoSQL mappers to interface with schema-free NoSQL data
stores. Given the source code of ten real-world database applications, we
extract the implicit NoSQL database schema. We capture the sizes of the
schemas, and investigate whether the schema is denormalized, as is recommended
practice in data modeling for NoSQL data stores. Further, we analyze the entire
project history, and with it, the evolution history of the NoSQL database
schema. In doing so, we conduct the so far largest empirical study on NoSQL
schema design and evolution.
</summary>
    <author>
      <name>Stefanie Scherzinger</name>
    </author>
    <author>
      <name>Sebastian Sidortschuck</name>
    </author>
    <link href="http://arxiv.org/abs/2003.00054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00965v1</id>
    <updated>2020-03-02T15:25:15Z</updated>
    <published>2020-03-02T15:25:15Z</published>
    <title>Distribution Constraints: The Chase for Distributed Data</title>
    <summary>  This paper introduces a declarative framework to specify and reason about
distributions of data over computing nodes in a distributed setting. More
specifically, it proposes distribution constraints which are tuple and equality
generating dependencies (tgds and egds) extended with node variables ranging
over computing nodes. In particular, they can express co-partitioning
constraints and constraints about range-based data distributions by using
comparison atoms. The main technical contribution is the study of the
implication problem of distribution constraints. While implication is
undecidable in general, relevant fragments of so-called data-full constraints
are exhibited for which the corresponding implication problems are complete for
EXPTIME, PSPACE and NP. These results yield bounds on deciding
parallel-correctness for conjunctive queries in the presence of distribution
constraints.
</summary>
    <author>
      <name>Gaetano Geck</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Thomas Schwentick</name>
    </author>
    <link href="http://arxiv.org/abs/2003.00965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01331v1</id>
    <updated>2020-03-03T04:48:40Z</updated>
    <published>2020-03-03T04:48:40Z</published>
    <title>Data Migration using Datalog Program Synthesis</title>
    <summary>  This paper presents a new technique for migrating data between different
schemas. Our method expresses the schema mapping as a Datalog program and
automatically synthesizes a Datalog program from simple input-output examples
to perform data migration. This approach can transform data between different
types of schemas (e.g., relational-to-graph, document-to-relational) and
performs synthesis efficiently by leveraging the semantics of Datalog. We
implement the proposed technique as a tool called Dynamite and show its
effectiveness by evaluating Dynamite on 28 realistic data migration scenarios.
</summary>
    <author>
      <name>Yuepeng Wang</name>
    </author>
    <author>
      <name>Rushi Shah</name>
    </author>
    <author>
      <name>Abby Criswell</name>
    </author>
    <author>
      <name>Rong Pan</name>
    </author>
    <author>
      <name>Isil Dillig</name>
    </author>
    <link href="http://arxiv.org/abs/2003.01331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02446v1</id>
    <updated>2020-03-05T06:08:25Z</updated>
    <published>2020-03-05T06:08:25Z</published>
    <title>LAQP: Learning-based Approximate Query Processing</title>
    <summary>  Querying on big data is a challenging task due to the rapid growth of data
amount. Approximate query processing (AQP) is a way to meet the requirement of
fast response. In this paper, we propose a learning-based AQP method called the
LAQP. The LAQP builds an error model learned from the historical queries to
predict the sampling-based estimation error of each new query. It makes a
combination of the sampling-based AQP, the pre-computed aggregations and the
learned error model to provide high-accurate query estimations with a small
off-line sample. The experimental results indicate that our LAQP outperforms
the sampling-based AQP, the pre-aggregation-based AQP and the most recent
learning-based AQP method.
</summary>
    <author>
      <name>Meifan Zhang</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03505v1</id>
    <updated>2020-03-07T03:34:14Z</updated>
    <published>2020-03-07T03:34:14Z</published>
    <title>Data Management for Context-Aware Computing</title>
    <summary>  We envisage future context-aware applications will dynamically adapt their
behaviors to various context data from sources in wide-area networks, such as
the Internet. Facing the changing context and the sheer number of context
sources, a data management system that supports effective source organization
and efficient data lookup becomes crucial to the easy development of
context-aware applications. In this paper, we propose the design of a new
context data management system that is equipped with query processing
capabilities. We encapsulate the context sources into physical spaces belonging
to different context spaces and organize them as peers in semantic overlay
networks. Initial evaluation results of an experimental system prototype
demonstrate the effectiveness of our design
</summary>
    <author>
      <name>Wenwei Xue</name>
    </author>
    <author>
      <name>Hungkeng Pung</name>
    </author>
    <author>
      <name>Wenlong Ng</name>
    </author>
    <author>
      <name>Tao Gu</name>
    </author>
    <link href="http://arxiv.org/abs/2003.03505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08215v1</id>
    <updated>2020-03-17T02:50:13Z</updated>
    <published>2020-03-17T02:50:13Z</published>
    <title>Multi-dimensional Skyline Query to Find Best Shopping Mall for Customers</title>
    <summary>  This paper presents a new application for multi-dimensional Skyline query.
The idea presented in this paper can be used to find best shopping malls based
on users requirements. A web-based application was used to simulate the problem
and proposed solution. Also, a mathematical definition was developed to define
the problem and show how multi-dimensional Skyline query can be used to solve
complex problems, such as, finding shopping malls using multiple different
criteria. The idea of this paper can be used in other fields, where different
criteria should be considered.
</summary>
    <author>
      <name>Md Amiruzzaman</name>
    </author>
    <author>
      <name>Suphanut Jamonnak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CDMA47397.2020.00018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CDMA47397.2020.00018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted for publication in IEEE CDMA 2020 conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 6th Conference on Data Science and Machine Learning
  Applications (CDMA)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.08215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11124v1</id>
    <updated>2020-03-24T21:38:55Z</updated>
    <published>2020-03-24T21:38:55Z</published>
    <title>Implementing Suffix Array Algorithm Using Apache Big Table Data
  Implementation</title>
    <summary>  In this paper we will describe a new approach on the well-known suffix-array
algorithm using Big Table Data Technology. We will demonstrate how it is
possible to refactor a well-known algorithm coupled by taking advantage of an
high-performance distributed datastore, to illustrate the advantages of using
datastore cloud related technology for storing large text sequences and
retrieving them. A case study using DNA strings, considered one of the most
difficult pattern matching problem, will be described and evaluated to
demonstrate the potentiality of this implementation. Further discussion on
performances and other big data related issues will be described as well as new
possible lines of research in big data technology for precise medical
applications.
</summary>
    <author>
      <name>Piero Giacomelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper prepared for a conference but never submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.01833v1</id>
    <updated>2020-04-04T01:47:35Z</updated>
    <published>2020-04-04T01:47:35Z</published>
    <title>On the Efficient Design of LSM Stores</title>
    <summary>  In the last decade, key-value data storage systems have gained significantly
more interest from academia and industry. These systems face numerous
challenges concerning storage space- and read optimization. There exists a
large potential for improving current solutions by introducing new management
techniques and algorithms.
  In this paper we give an overview of the basic concept of key-value data
storage systems and provide an explanation for bottlenecks. Further we
introduce two new memory management algorithms and a improved index structure.
Finally, these solutions are compared to each other and discussed.
</summary>
    <author>
      <name>Martin Weise</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, course material</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10247v3</id>
    <updated>2020-06-15T13:48:46Z</updated>
    <published>2020-04-21T19:20:02Z</published>
    <title>GGDs: Graph Generating Dependencies</title>
    <summary>  We propose Graph Generating Dependencies (GGDs), a new class of dependencies
for property graphs. Extending the expressivity of state of the art constraint
languages, GGDs can express both tuple- and equality-generating dependencies on
property graphs, both of which find broad application in graph data management.
We provide the formal definition of GGDs, analyze the validation problem for
GGDs, and demonstrate the practical utility of GGDs.
</summary>
    <author>
      <name>Larissa C. Shimomura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eindhoven University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>George Fletcher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eindhoven University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Nikolay Yakovets</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eindhoven University of Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10247v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10247v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00993v1</id>
    <updated>2020-05-03T05:41:36Z</updated>
    <published>2020-05-03T05:41:36Z</published>
    <title>An Algebraic Approach for High-level Text Analytics</title>
    <summary>  Text analytical tasks like word embedding, phrase mining, and topic modeling,
are placing increasing demands as well as challenges to existing database
management systems.
  In this paper, we provide a novel algebraic approach based on associative
arrays. Our data model and algebra can bring together relational operators and
text operators, which enables interesting optimization opportunities for hybrid
data sources that have both relational and textual data. We demonstrate its
expressive power in text analytics using several real-world tasks.
</summary>
    <author>
      <name>Xiuwen Zheng</name>
    </author>
    <author>
      <name>Amarnath Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/2005.00993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.01389v1</id>
    <updated>2020-05-04T11:09:11Z</updated>
    <published>2020-05-04T11:09:11Z</published>
    <title>Knowledge Graph Validation</title>
    <summary>  Knowledge graphs (KGs) have shown to be an important asset of large companies
like Google and Microsoft. KGs play an important role in providing structured
and semantically rich information, making them available to people and
machines, and supplying accurate, correct and reliable knowledge. To do so a
critical task is knowledge validation, which measures whether statements from
KGs are semantically correct and correspond to the so-called "real" world. In
this paper, we provide an overview and review of the state-of-the-art
approaches, methods and tools on knowledge validation for KGs, as well as an
evaluation of them. As a result, we demonstrate a lack of reproducibility of
tools results, give insights, and state our future research direction.
</summary>
    <author>
      <name>Elwin Huaman</name>
    </author>
    <author>
      <name>Elias Kärle</name>
    </author>
    <author>
      <name>Dieter Fensel</name>
    </author>
    <link href="http://arxiv.org/abs/2005.01389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08483v2</id>
    <updated>2022-10-05T07:41:12Z</updated>
    <published>2020-05-18T06:48:45Z</published>
    <title>Improving Reverse k Nearest Neighbors Queries</title>
    <summary>  The reverse $k$ nearest neighbor query finds all points that have the query
point as one of their $k$ nearest neighbors, where the $k$NN query finds the
$k$ closest points to its query point. Based on conics, we propose an efficent
R$k$NN verification method. By using the proposed verification method, we
implement an efficient R$k$NN algorithm on VoR-tree, which has a computational
complexity of $O(k^{1.5}\cdot log\,k)$. The comparative experiments are
conducted between our algorithm and other two state-of-the-art R$k$NN
algorithms. The experimental results indicate that the efficiency of our
algorithm is significantly higher than its competitors.
</summary>
    <author>
      <name>Lixin Ye</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/13658816.2023.2249521</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/13658816.2023.2249521" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1911.02788 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08483v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08483v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.04838v1</id>
    <updated>2020-11-10T00:10:43Z</updated>
    <published>2020-11-10T00:10:43Z</published>
    <title>Answer Graph: Factorization Matters in Large Graphs</title>
    <summary>  Our answer-graph method to evaluate SPARQL conjunctive queries (CQs) finds a
factorized answer set first, an answer graph, and then finds the embedding
tuples from this. This approach can reduce greatly the cost to evaluate CQs.
This affords a second advantage: we can construct a cost-based planner. We
present the answer-graph approach, and overview our prototype system,
Wireframe. We then offer proof of concept via a micro-benchmark over the YAGO2s
dataset with two prevalent shapes of queries, snowflake and diamond. We compare
Wireframe's performance over these against PostgreSQL, Virtuoso, MonetDB, and
Neo4J to illustrate the performance advantages of our answer-graph approach.
</summary>
    <author>
      <name>Zahid Abul-Basher</name>
    </author>
    <author>
      <name>Nikolay Yakovets</name>
    </author>
    <author>
      <name>Parke Godfrey</name>
    </author>
    <author>
      <name>Stanley Clark</name>
    </author>
    <author>
      <name>Mark Chignell</name>
    </author>
    <link href="http://arxiv.org/abs/2011.04838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.04838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09314v1</id>
    <updated>2020-11-18T14:31:17Z</updated>
    <published>2020-11-18T14:31:17Z</published>
    <title>First-Order Rewritability of Frontier-Guarded Ontology-Mediated Queries</title>
    <summary>  We focus on ontology-mediated queries (OMQs) based on (frontier-)guarded
existential rules and (unions of) conjunctive queries, and we investigate the
problem of FO-rewritability, i.e., whether an OMQ can be rewritten as a
first-order query. We adopt two different approaches. The first approach
employs standard two-way alternating parity tree automata. Although it does not
lead to a tight complexity bound, it provides a transparent solution based on
widely known tools. The second approach relies on a sophisticated automata
model, known as cost automata. This allows us to show that our problem is
2ExpTime-complete. In both approaches, we provide semantic characterizations of
FO-rewritability that are of independent interest.
</summary>
    <author>
      <name>Pablo Barcelo</name>
    </author>
    <author>
      <name>Gerald Berger</name>
    </author>
    <author>
      <name>Carsten Lutz</name>
    </author>
    <author>
      <name>Andreas Pieris</name>
    </author>
    <link href="http://arxiv.org/abs/2011.09314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00331v2</id>
    <updated>2022-03-03T10:02:37Z</updated>
    <published>2022-03-01T10:12:39Z</published>
    <title>Counting stars: a survey on flexible Skyline Query approaches</title>
    <summary>  Nowadays, as the quantity of data to process began to rise, so did the need
for a method to discern what pieces of information could be useful for the
user; in response, researchers focused their efforts on improving the already
existing ranking methods or creating new ones starting from them. This survey
will be presented a small list of some of the most known and/or most recent
solutions proposed, with some possible applications for them, concerning a
state of the art restricted to around the last ten years, comparing their
performance with the traditional one top-k and skyline queries.
</summary>
    <author>
      <name>Alessandro Del Giudice</name>
    </author>
    <link href="http://arxiv.org/abs/2203.00331v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00331v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.09236v1</id>
    <updated>2022-03-17T10:56:30Z</updated>
    <published>2022-03-17T10:56:30Z</published>
    <title>Weighing the techniques for data optimization in a database</title>
    <summary>  A set of preferred records can be obtained from a large database in a
multi-criteria setting using various computational methods which either depend
on the concept of dominance or on the concept of utility or scoring function
based on the attributes of the database record. A skyline approach relies on
the dominance relationship between different data points to discover
interesting data from a huge database. On the other hand, ranking queries make
use of specific scoring functions to rank tuples in a database. An experimental
evaluation of datasets can provides us with information on the effectiveness of
each of these methods.
</summary>
    <author>
      <name>Anagha Radhakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/2203.09236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.09236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.09271v2</id>
    <updated>2022-03-20T16:36:40Z</updated>
    <published>2022-03-17T11:53:21Z</published>
    <title>A flexible solution to embrace Ranking and Skyline queries approaches</title>
    <summary>  The multi-objective optimization problem has always been the main objective
of the principal traditional approaches, such as Ranking queries and Skyline
queries. The conventional idea was to either use one or the other, trying to
exploit both ranking queries advantages when it comes to taking into account
user preferences, and skyline queries points of strength when the main
objective was to obtain interesting results from a dataset in a simple, yet
effective fashion, both of them showing limitations when entering specific
fields of interest.
</summary>
    <author>
      <name>Simone Censuales</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.09271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.09271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.0724v1</id>
    <updated>2011-12-14T11:46:13Z</updated>
    <published>2011-12-14T11:46:13Z</published>
    <title>Using Data Warehouse to Support Building Strategy or Forecast Business
  Tend</title>
    <summary>  The data warehousing is becoming increasingly important in terms of strategic
decision making through their capacity to integrate heterogeneous data from
multiple information sources in a common storage space, for querying and
analysis. So it can evolve into a multi-tier structure where parts of the
organization take information from the main data warehouse into their own
systems. These may include analysis databases or dependent data marts. As the
data warehouse evolves and the organization gets better at capturing
information on all interactions with the customer. Data warehouse can track
customer interactions over the whole of the customer's lifetime.
</summary>
    <author>
      <name>Phuc V. Nguyen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hung Vuong Univesity, Ho Chi Minh City</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.0724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1126v1</id>
    <updated>2012-05-05T12:29:35Z</updated>
    <published>2012-05-05T12:29:35Z</published>
    <title>A Comprehensive Study of CRM through Data Mining Techniques</title>
    <summary>  In today's competitive scenario in corporate world, "Customer Retention"
strategy in Customer Relationship Management (CRM) is an increasingly pressed
issue. Data mining techniques play a vital role in better CRM. This paper
attempts to bring a new perspective by focusing the issue of data mining
applications, opportunities and challenges in CRM. It covers the topic such as
customer retention, customer services, risk assessment, fraud detection and
some of the data mining tools which are widely used in CRM.
</summary>
    <author>
      <name>Md. Rashid Farooqi</name>
    </author>
    <author>
      <name>Khalid Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the National Conference; NCCIST-2011, September 09,
  2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1796v1</id>
    <updated>2012-05-08T19:58:28Z</updated>
    <published>2012-05-08T19:58:28Z</published>
    <title>Moving Object Trajectories Meta-Model And Spatio-Temporal Queries</title>
    <summary>  In this paper, a general moving object trajectories framework is put forward
to allow independent applications processing trajectories data benefit from a
high level of interoperability, information sharing as well as an efficient
answer for a wide range of complex trajectory queries. Our proposed meta-model
is based on ontology and event approach, incorporates existing presentations of
trajectory and integrates new patterns like space-time path to describe
activities in geographical space-time. We introduce recursive Region of
Interest concepts and deal mobile objects trajectories with diverse
spatio-temporal sampling protocols and different sensors available that
traditional data model alone are incapable for this purpose.
</summary>
    <author>
      <name>Azedine Boulmakoul</name>
    </author>
    <author>
      <name>Lamia Karim</name>
    </author>
    <author>
      <name>Ahmed Lbath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2012.4203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2012.4203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems (IJDMS) Vol.4,
  No.2, April 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2320v1</id>
    <updated>2012-05-10T17:29:05Z</updated>
    <published>2012-05-10T17:29:05Z</published>
    <title>Publishing Life Science Data as Linked Open Data: the Case Study of
  miRBase</title>
    <summary>  This paper presents our Linked Open Data (LOD) infrastructures for genomic
and experimental data related to microRNA biomolecules. Legacy data from two
well-known microRNA databases with experimental data and observations, as well
as change and version information about microRNA entities, are fused and
exported as LOD. Our LOD server assists biologists to explore biological
entities and their evolution, and provides a SPARQL endpoint for applications
and services to query historical miRNA data and track changes, their causes and
effects.
</summary>
    <author>
      <name>Theodore Dalamagas</name>
    </author>
    <author>
      <name>Nikos Bikakis</name>
    </author>
    <author>
      <name>George Papastefanatos</name>
    </author>
    <author>
      <name>Yannis Stavrakas</name>
    </author>
    <author>
      <name>Artemis G. Hatzigeorgiou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the First International Workshop On Open Data, WOD-2012
  (arXiv:1204.3726)</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.2320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5353v1</id>
    <updated>2012-05-24T07:37:28Z</updated>
    <published>2012-05-24T07:37:28Z</published>
    <title>A hybrid clustering algorithm for data mining</title>
    <summary>  Data clustering is a process of arranging similar data into groups. A
clustering algorithm partitions a data set into several groups such that the
similarity within a group is better than among groups. In this paper a hybrid
clustering algorithm based on K-mean and K-harmonic mean (KHM) is described.
The proposed algorithm is tested on five different datasets. The research is
focused on fast and accurate clustering. Its performance is compared with the
traditional K-means &amp; KHM algorithm. The result obtained from proposed hybrid
algorithm is much better than the traditional K-mean &amp; KHM algorithm.
</summary>
    <author>
      <name>Ravindra Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5922v2</id>
    <updated>2021-09-30T10:38:28Z</updated>
    <published>2012-05-26T22:41:44Z</published>
    <title>Discovering new technique for mapping relational database based on
  semantic web technology</title>
    <summary>  Most of data on the Web are still stored in relational databases. Therefore,
it is more important to make the correspondence between relational databases
(RDB) and ontologies for storing the Web data. In this paper, we present an new
approach to map the data stored in relational databases into the Semantic Web,
we exploit simple mappings based on some specifications of the database schema,
and we explain how relational databases can be used to define a mapping
mechanism between relational database and OWL ontology. A framework has been
developed, which migrates successfully RDB into OWL document. The experimental
results were very important, demonstrating that the proposed method is feasible
and efficient.
</summary>
    <author>
      <name>Noreddine Gherabi</name>
    </author>
    <author>
      <name>Khaoula Addakiri</name>
    </author>
    <author>
      <name>Mohamed Bahaj</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5922v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5922v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.6698v1</id>
    <updated>2012-05-30T14:33:10Z</updated>
    <published>2012-05-30T14:33:10Z</published>
    <title>Type-Based Detection of XML Query-Update Independence</title>
    <summary>  This paper presents a novel static analysis technique to detect XML
query-update independence, in the presence of a schema. Rather than types, our
system infers chains of types. Each chain represents a path that can be
traversed on a valid document during query/update evaluation. The resulting
independence analysis is precise, although it raises a challenging issue:
recursive schemas may lead to infer infinitely many chains. A sound and
complete approximation technique ensuring a finite analysis in any case is
presented, together with an efficient implementation performing the chain-based
analysis in polynomial space and time.
</summary>
    <author>
      <name>Nicole Bidoit-Tollu</name>
    </author>
    <author>
      <name>Dario Colazzo</name>
    </author>
    <author>
      <name>Federico Ulliana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  872-883 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.6698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.6698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0651v1</id>
    <updated>2014-09-02T10:07:27Z</updated>
    <published>2014-09-02T10:07:27Z</published>
    <title>An LSH Index for Computing Kendall's Tau over Top-k Lists</title>
    <summary>  We consider the problem of similarity search within a set of top-k lists
under the Kendall's Tau distance function. This distance describes how related
two rankings are in terms of concordantly and discordantly ordered items. As
top-k lists are usually very short compared to the global domain of possible
items to be ranked, creating an inverted index to look up overlapping lists is
possible but does not capture tight enough the similarity measure. In this
work, we investigate locality sensitive hashing schemes for the Kendall's Tau
distance and evaluate the proposed methods using two real-world datasets.
</summary>
    <author>
      <name>Koninika Pal</name>
    </author>
    <author>
      <name>Sebastian Michel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 subfigures, presented in Seventeenth International
  Workshop on the Web and Databases (WebDB 2014) co-located with ACM SIGMOD2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0798v1</id>
    <updated>2014-09-02T17:16:47Z</updated>
    <published>2014-09-02T17:16:47Z</published>
    <title>DataHub: Collaborative Data Science &amp; Dataset Version Management at
  Scale</title>
    <summary>  Relational databases have limited support for data collaboration, where teams
collaboratively curate and analyze large datasets. Inspired by software version
control systems like git, we propose (a) a dataset version control system,
giving users the ability to create, branch, merge, difference and search large,
divergent collections of datasets, and (b) a platform, DataHub, that gives
users the ability to perform collaborative data analysis building on this
version control system. We outline the challenges in providing dataset version
control at scale.
</summary>
    <author>
      <name>Anant Bhardwaj</name>
    </author>
    <author>
      <name>Souvik Bhattacherjee</name>
    </author>
    <author>
      <name>Amit Chavan</name>
    </author>
    <author>
      <name>Amol Deshpande</name>
    </author>
    <author>
      <name>Aaron J. Elmore</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Aditya G. Parameswaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0980v2</id>
    <updated>2015-07-03T20:17:42Z</updated>
    <published>2014-09-03T08:06:22Z</published>
    <title>Monoidal functional dependencies</title>
    <summary>  We present a complete logic for reasoning with functional dependencies (FDs)
with semantics defined over classes of commutative integral partially ordered
monoids and complete residuated lattices. The dependencies allow us to express
stronger relationships between attribute values than the ordinary FDs. In our
setting, the dependencies not only express that certain values are determined
by others but also express that similar values of attributes imply similar
values of other attributes. We show complete axiomatization using a system of
Armstrong-like rules, comment on related computational issues, and the
relational vs. propositional semantics of the dependencies.
</summary>
    <author>
      <name>Vilem Vychodil</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jcss.2015.03.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jcss.2015.03.006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer and System Sciences 81(7) (2015) 1357-1372</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.0980v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0980v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15, 03B52, 03G10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3682v1</id>
    <updated>2014-09-12T08:57:24Z</updated>
    <published>2014-09-12T08:57:24Z</published>
    <title>A novel recovery mechanism enabling fine-granularity locking and fast,
  REDO-only recovery</title>
    <summary>  We present a series of novel techniques and algorithms for transaction
commit, logging, recovery, and propagation control. In combination, they
provide a recovery component that maintains the persistent state of the
database (both log and data pages) always in a committed state. Recovery from
system and media failures only requires only REDO operations, which can happen
concurrently with the processing of new transactions. The mechanism supports
fine-granularity locking, partial rollbacks, and snapshot isolation for reader
transactions. Our design does not assume a specific hardware configuration such
as non-volatile RAM or flash---it is designed for traditional disk
environments. Nevertheless, it can exploit modern I/O devices for higher
transaction throughput and reduced recovery time with a high degree of
flexibility.
</summary>
    <author>
      <name>Caetano Sauer</name>
    </author>
    <author>
      <name>Theo Härder</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.6848v1</id>
    <updated>2014-09-24T07:20:20Z</updated>
    <published>2014-09-24T07:20:20Z</published>
    <title>A New Clustering Algorithm Based on Near Neighbor Influence</title>
    <summary>  This paper presents Clustering based on Near Neighbor Influence (CNNI), a new
clustering algorithm which is inspired by the idea of near neighbor and the
superposition principle of influence. In order to clearly describe this
algorithm, it introduces some important concepts, such as near neighbor point
set, near neighbor influence, and similarity measure. By simulated experiments
of some artificial data sets and seven real data sets, we observe that this
algorithm can often get good clustering quality when making proper value of
some parameters. At last, it gives some research expectations to popularize
this algorithm.
</summary>
    <author>
      <name>Xinquan Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 9 figures, and 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.6848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7311v2</id>
    <updated>2014-09-30T15:22:51Z</updated>
    <published>2014-09-25T16:08:14Z</published>
    <title>Estimating the pattern frequency spectrum inside the browser</title>
    <summary>  We present a browser application for estimating the number of frequent
patterns, in particular itemsets, as well as the pattern frequency spectrum.
The pattern frequency spectrum is defined as the function that shows for every
value of the frequency threshold $\sigma$ the number of patterns that are
frequent in a given dataset. Our demo implements a recent algorithm proposed by
the authors for finding the spectrum. The demo is 100% JavaScript, and runs in
all modern browsers. We observe that modern JavaScript engines can deliver
performance that makes it viable to run non-trivial data analysis algorithms in
browser applications.
</summary>
    <author>
      <name>Matthijs van Leeuwen</name>
    </author>
    <author>
      <name>Antti Ukkonen</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7311v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7311v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03484v1</id>
    <updated>2017-02-12T03:06:25Z</updated>
    <published>2017-02-12T03:06:25Z</published>
    <title>MapSQ: A MapReduce-based Framework for SPARQL Queries on GPU</title>
    <summary>  In this paper, we present a MapReduce-based framework for evaluating SPARQL
queries on GPU (named MapSQ) to large-scale RDF datesets efficiently by
applying both high performance. Firstly, we develop a MapReduce-based Join
algorithm to handle SPARQL queries in a parallel way. Secondly, we present a
coprocessing strategy to manage the process of evaluating queries where CPU is
used to assigns subqueries and GPU is used to compute the join of subqueries.
Finally, we implement our proposed framework and evaluate our proposal by
comparing with two popular and latest SPARQL query engines gStore and gStoreD
on the LUBM benchmark. The experiments demonstrate that our proposal MapSQ is
highly efficient and effective (up to 50% speedup).
</summary>
    <author>
      <name>Jiaying Feng</name>
    </author>
    <author>
      <name>Xiaowang Zhang</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06379v1</id>
    <updated>2017-02-21T13:41:35Z</updated>
    <published>2017-02-21T13:41:35Z</published>
    <title>Probabilistic Complex Event Recognition: A Survey</title>
    <summary>  Complex Event Recognition applications exhibit various types of uncertainty,
ranging from incomplete and erroneous data streams to imperfect complex event
patterns. We review Complex Event Recognition techniques that handle, to some
extent, uncertainty. We examine techniques based on automata, probabilistic
graphical models and first-order logic, which are the most common ones, and
approaches based on Petri Nets and Grammars, which are less frequently used. A
number of limitations are identified with respect to the employed languages,
their probabilistic models and their performance, as compared to the purely
deterministic cases. Based on those limitations, we highlight promising
directions for future work.
</summary>
    <author>
      <name>Elias Alevizos</name>
    </author>
    <author>
      <name>Anastasios Skarlatidis</name>
    </author>
    <author>
      <name>Alexander Artikis</name>
    </author>
    <author>
      <name>George Paliouras</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01135v1</id>
    <updated>2019-05-03T12:10:02Z</updated>
    <published>2019-05-03T12:10:02Z</published>
    <title>On the Impact of Memory Allocation on High-Performance Query Processing</title>
    <summary>  Somewhat surprisingly, the behavior of analytical query engines is crucially
affected by the dynamic memory allocator used. Memory allocators highly
influence performance, scalability, memory efficiency and memory fairness to
other processes. In this work, we provide the first comprehensive experimental
analysis on the impact of memory allocation for high-performance query engines.
We test five state-of-the-art dynamic memory allocators and discuss their
strengths and weaknesses within our DBMS. The right allocator can increase the
performance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.
</summary>
    <author>
      <name>Dominik Durner</name>
    </author>
    <author>
      <name>Viktor Leis</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3329785.3329918</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3329785.3329918" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DaMoN 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.01135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01294v1</id>
    <updated>2019-05-01T23:39:42Z</updated>
    <published>2019-05-01T23:39:42Z</published>
    <title>RedisGraph GraphBLAS Enabled Graph Database</title>
    <summary>  RedisGraph is a Redis module developed by Redis Labs to add graph database
functionality to the Redis database. RedisGraph represents connected data as
adjacency matrices. By representing the data as sparse matrices and employing
the power of GraphBLAS (a highly optimized library for sparse matrix
operations), RedisGraph delivers a fast and efficient way to store, manage and
process graphs. Initial benchmarks indicate that RedisGraph is significantly
faster than comparable graph databases.
</summary>
    <author>
      <name>Pieter Cailliau</name>
    </author>
    <author>
      <name>Tim Davis</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Roi Lipman</name>
    </author>
    <author>
      <name>Jeffrey Lovitz</name>
    </author>
    <author>
      <name>Keren Ouaknine</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IPDPSW.2019.00054</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IPDPSW.2019.00054" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE IPDPS 2019 GrAPL workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01306v1</id>
    <updated>2019-05-03T16:13:52Z</updated>
    <published>2019-05-03T16:13:52Z</published>
    <title>Big Data Model "Entity and Features"</title>
    <summary>  The article deals with the problem which led to Big Data. Big Data
information technology is the set of methods and means of processing different
types of structured and unstructured dynamic large amounts of data for their
analysis and use of decision support. Features of NoSQL databases and
categories are described. The developed Big Data Model "Entity and Features"
allows determining the distance between the sources of data on the availability
of information about a particular entity. The information structure of Big Data
has been devised. It became a basis for further research and for concentrating
on a problem of development of diverse data without their preliminary
integration.
</summary>
    <author>
      <name>Nataliya Shakhovska</name>
    </author>
    <author>
      <name>Uyrii Bolubash</name>
    </author>
    <author>
      <name>Oleh Veres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04767v1</id>
    <updated>2019-05-12T18:27:44Z</updated>
    <published>2019-05-12T18:27:44Z</published>
    <title>Moving Processing to Data: On the Influence of Processing in Memory on
  Data Management</title>
    <summary>  Near-Data Processing refers to an architectural hardware and software
paradigm, based on the co-location of storage and compute units. Ideally, it
will allow to execute application-defined data- or compute-intensive operations
in-situ, i.e. within (or close to) the physical data storage. Thus, Near-Data
Processing seeks to minimize expensive data movement, improving performance,
scalability, and resource-efficiency. Processing-in-Memory is a sub-class of
Near-Data processing that targets data processing directly within memory (DRAM)
chips. The effective use of Near-Data Processing mandates new architectures,
algorithms, interfaces, and development toolchains.
</summary>
    <author>
      <name>Tobias Vincon</name>
    </author>
    <author>
      <name>Andreas Koch</name>
    </author>
    <author>
      <name>Ilia Petrov</name>
    </author>
    <link href="http://arxiv.org/abs/1905.04767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07924v3</id>
    <updated>2019-10-01T02:07:32Z</updated>
    <published>2019-08-20T17:23:00Z</published>
    <title>Data Management for Causal Algorithmic Fairness</title>
    <summary>  Fairness is increasingly recognized as a critical component of machine
learning systems. However, it is the underlying data on which these systems are
trained that often reflects discrimination, suggesting a data management
problem. In this paper, we first make a distinction between associational and
causal definitions of fairness in the literature and argue that the concept of
fairness requires causal reasoning. We then review existing works and identify
future opportunities for applying data management techniques to causal
algorithmic fairness.
</summary>
    <author>
      <name>Babak Salimi</name>
    </author>
    <author>
      <name>Bill Howe</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1902.08283</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07924v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07924v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00084v2</id>
    <updated>2020-04-27T16:29:09Z</updated>
    <published>2020-01-31T22:47:43Z</published>
    <title>Approximate Summaries for Why and Why-not Provenance (Extended Version)</title>
    <summary>  Why and why-not provenance have been studied extensively in recent years.
However, why-not provenance, and to a lesser degree why provenance, can be very
large resulting in severe scalability and usability challenges. In this paper,
we introduce a novel approximate summarization technique for provenance which
overcomes these challenges. Our approach uses patterns to encode (why-not)
provenance concisely. We develop techniques for efficiently computing
provenance summaries balancing informativeness, conciseness, and completeness.
To achieve scalability, we integrate sampling techniques into provenance
capture and summarization. Our approach is the first to scale to large datasets
and to generate comprehensive and meaningful summaries.
</summary>
    <author>
      <name>Seokki Lee</name>
    </author>
    <author>
      <name>Bertram Ludaescher</name>
    </author>
    <author>
      <name>Boris Glavic</name>
    </author>
    <link href="http://arxiv.org/abs/2002.00084v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00084v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05589v1</id>
    <updated>2020-02-13T16:06:39Z</updated>
    <published>2020-02-13T16:06:39Z</published>
    <title>Explainable Queries over Event Logs</title>
    <summary>  Added value can be extracted from event logs generated by business processes
in various ways. However, although complex computations can be performed over
event logs, the result of such computations is often difficult to explain; in
particular, it is hard to determine what parts of an input log actually matters
in the production of that result. This paper describes how an existing log
processing library, called BeepBeep, can be extended in order to provide a form
of provenance: individual output events produced by a query can be precisely
traced back to the data elements of the log that contribute to (i.e. "explain")
the result.
</summary>
    <author>
      <name>Sylvain Hallé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to IJCNN 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06039v1</id>
    <updated>2020-02-14T14:02:29Z</updated>
    <published>2020-02-14T14:02:29Z</published>
    <title>Benchmarking Knowledge Graphs on the Web</title>
    <summary>  The growing interest in making use of Knowledge Graphs for developing
explainable artificial intelligence, there is an increasing need for a
comparable and repeatable comparison of the performance of Knowledge
Graph-based systems. History in computer science has shown that a main driver
to scientific advances, and in fact a core element of the scientific method as
a whole, is the provision of benchmarks to make progress measurable. This paper
gives an overview of benchmarks used to evaluate systems that process Knowledge
Graphs.
</summary>
    <author>
      <name>Michael Röder</name>
    </author>
    <author>
      <name>Mohamed Ahmed Sherif</name>
    </author>
    <author>
      <name>Muhammad Saleem</name>
    </author>
    <author>
      <name>Felix Conrads</name>
    </author>
    <author>
      <name>Axel-Cyrille Ngonga Ngomo</name>
    </author>
    <link href="http://arxiv.org/abs/2002.06039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10283v1</id>
    <updated>2020-02-24T14:35:02Z</updated>
    <published>2020-02-24T14:35:02Z</published>
    <title>The Knowledge Graph Track at OAEI -- Gold Standards, Baselines, and the
  Golden Hammer Bias</title>
    <summary>  The Ontology Alignment Evaluation Initiative (OAEI) is an annual evaluation
of ontology matching tools. In 2018, we have started the Knowledge Graph track,
whose goal is to evaluate the simultaneous matching of entities and schemas of
large-scale knowledge graphs. In this paper, we discuss the design of the track
and two different strategies of gold standard creation. We analyze results and
experiences obtained in first editions of the track, and, by revealing a hidden
task, we show that all tools submitted to the track (and probably also to other
tracks) suffer from a bias which we name the golden hammer bias.
</summary>
    <author>
      <name>Sven Hertling</name>
    </author>
    <author>
      <name>Heiko Paulheim</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11771v1</id>
    <updated>2020-02-26T19:57:59Z</updated>
    <published>2020-02-26T19:57:59Z</published>
    <title>Distributed Cross-Blockchain Transactions</title>
    <summary>  The interoperability across multiple or many blockchains would play a
critical role in the forthcoming blockchain-based data management paradigm. In
particular, how to ensure the ACID properties of those transactions across an
arbitrary number of blockchains remains an open problem in both academic and
industry: Existing solutions either work for only two blockchains or requires a
centralized component, neither of which would meet the scalability requirement
in practice. This short paper shares our vision and some early results toward
scalable cross-blockchain transactions. Specifically, we design two distributed
commit protocols and, both analytically and experimentally, demonstrate their
effectiveness.
</summary>
    <author>
      <name>Dongfang Zhao</name>
    </author>
    <author>
      <name>Tonglin Li</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03090v4</id>
    <updated>2021-12-15T18:22:05Z</updated>
    <published>2020-10-06T23:40:03Z</published>
    <title>Validating UTF-8 In Less Than One Instruction Per Byte</title>
    <summary>  The majority of text is stored in UTF-8, which must be validated on
ingestion. We present the lookup algorithm, which outperforms UTF-8 validation
routines used in many libraries and languages by more than 10 times using
commonly available SIMD instructions. To ensure reproducibility, our work is
freely available as open source software.
</summary>
    <author>
      <name>John Keiser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/spe.2920</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/spe.2920" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Software: Practice and Experience 51 (5), 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.03090v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03090v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08807v1</id>
    <updated>2020-10-17T15:07:45Z</updated>
    <published>2020-10-17T15:07:45Z</published>
    <title>MithraDetective: A System for Cherry-picked Trendlines Detection</title>
    <summary>  Given a data set, misleading conclusions can be drawn from it by
cherry-picking selected samples. One important class of conclusions is a trend
derived from a data set of values over time. Our goal is to evaluate whether
the 'trends' described by the extracted samples are representative of the true
situation represented in the data. We demonstrate MithraDetective, a system to
compute a support score to indicate how cherry-picked a statement is; that is,
whether the reported trend is well-supported by the data. The system can also
be used to discover more supported alternatives. MithraDetective provides an
interactive visual interface for both tasks.
</summary>
    <author>
      <name>Yoko Nagafuchi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Will</arxiv:affiliation>
    </author>
    <author>
      <name>Yin Lin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Will</arxiv:affiliation>
    </author>
    <author>
      <name>Kaushal Mamgain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Will</arxiv:affiliation>
    </author>
    <author>
      <name>Abolfazl Asudeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Will</arxiv:affiliation>
    </author>
    <author>
      <name>H. V. Jagadish</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Will</arxiv:affiliation>
    </author>
    <author>
      <name> You</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Will</arxiv:affiliation>
    </author>
    <author>
      <name> Wu</name>
    </author>
    <author>
      <name>Cong Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2010.08807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12243v3</id>
    <updated>2022-03-24T19:51:30Z</updated>
    <published>2020-10-23T09:04:23Z</published>
    <title>An analysis of the SIGMOD 2014 Programming Contest: Complex queries on
  the LDBC social network graph</title>
    <summary>  This report contains an analysis of the queries defined in the SIGMOD 2014
Programming Contest. We first describe the data set, then present the queries,
providing graphical illustrations for them and pointing out their caveats. Our
intention is to document our lessons learnt and simplify the work of those who
will attempt to create a solution to this contest. We also demonstrate the
influence of this contest by listing followup works which used these queries as
inspiration to design better algorithms or to define interesting graph queries.
</summary>
    <author>
      <name>Márton Elekes</name>
    </author>
    <author>
      <name>János Benjamin Antal</name>
    </author>
    <author>
      <name>Gábor Szárnyas</name>
    </author>
    <link href="http://arxiv.org/abs/2010.12243v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12243v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13605v1</id>
    <updated>2021-04-28T07:31:20Z</updated>
    <published>2021-04-28T07:31:20Z</published>
    <title>A Linked Data Application Framework to Enable Rapid Prototyping</title>
    <summary>  Application developers, in our experience, tend to hesitate when dealing with
linked data technologies. To reduce their initial hurdle and enable rapid
prototyping, we propose in this paper a framework for building linked data
applications. Our approach especially considers the participation of web
developers and non-technical users without much prior knowledge about linked
data concepts. Web developers are supported with bidirectional RDF to JSON
conversions and suitable CRUD endpoints. Non-technical users can browse
websites generated from JSON data by means of a template language. A
prototypical open source implementation demonstrates its capabilities.
</summary>
    <author>
      <name>Markus Schröder</name>
    </author>
    <author>
      <name>Christian Jilek</name>
    </author>
    <author>
      <name>Andreas Dengel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, demo</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.13605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.00412v1</id>
    <updated>2021-06-01T11:52:59Z</updated>
    <published>2021-06-01T11:52:59Z</published>
    <title>Curating Covid-19 data in Links</title>
    <summary>  Curated scientific databases play an important role in the scientific
endeavour and support is needed for the significant effort that goes into their
creation and maintenance. This demonstration and case study illustrate how
curation support has been developed in the Links cross-tier programming
language, a functional, strongly typed language with language-integrated query
and support for temporal databases. The chosen case study uses weekly released
Covid-19 fatality figures from the Scottish government which exhibit updates to
previously released data. This data allows the capture and query of update
provenance in our prototype. This demonstration will highlight the potential
for language-integrated support for curation to simplify and streamline
prototyping of web-applications in support of scientific databases
</summary>
    <author>
      <name>Vashti Galpin</name>
    </author>
    <author>
      <name>James Cheney</name>
    </author>
    <link href="http://arxiv.org/abs/2106.00412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.00412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07837v1</id>
    <updated>2021-06-15T02:06:34Z</updated>
    <published>2021-06-15T02:06:34Z</published>
    <title>A Survey on Mining and Analysis of Uncertain Graphs</title>
    <summary>  \emph{Uncertain Graph} (also known as \emph{Probabilistic Graph}) is a
generic model to represent many real\mbox{-}world networks from social to
biological. In recent times analysis and mining of uncertain graphs have drawn
significant attention from the researchers of the data management community.
Several noble problems have been introduced and efficient methodologies have
been developed to solve those problems. Hence, there is a need to summarize the
existing results on this topic in a self\mbox{-}organized way. In this paper,
we present a comprehensive survey on uncertain graph mining focusing on mainly
three aspects: (i) different problems studied, (ii) computational challenges
for solving those problems, and (iii) proposed methodologies. Finally, we list
out important future research directions.
</summary>
    <author>
      <name>Suman Banerjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 Pages, 2 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.07837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11456v2</id>
    <updated>2021-06-25T03:18:36Z</updated>
    <published>2021-06-22T00:17:06Z</published>
    <title>Querying in the Age of Graph Databases and Knowledge Graphs</title>
    <summary>  Graphs have become the best way we know of representing knowledge. The
computing community has investigated and developed the support for managing
graphs by means of digital technology. Graph databases and knowledge graphs
surface as the most successful solutions to this program. The goal of this
document is to provide a conceptual map of the data management tasks underlying
these developments, paying particular attention to data models and query
languages for graphs.
</summary>
    <author>
      <name>Marcelo Arenas</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <author>
      <name>Juan F. Sequeda</name>
    </author>
    <link href="http://arxiv.org/abs/2106.11456v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11456v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15664v1</id>
    <updated>2021-06-29T18:14:26Z</updated>
    <published>2021-06-29T18:14:26Z</published>
    <title>Is 2NF a Stable Normal Form?</title>
    <summary>  Traditionally, it was accepted that a relational database can be normalized
step-by-step, from a set of un-normalized tables to tables in $1NF$, then to
$2NF$, then to $3NF$, then (possibly) to $BCNF$. The rule applied to a table in
$1NF$ in order to transform it to a set of tables in $2NF$ seems to be too
straightforward to pose any difficulty.
  However, we show that, depending on the set of functional dependencies, it is
impossible to reach $2NF$ and stop there; one must, in these cases, perform the
normalization from $1NF$ to $3NF$ as an indecomposable move. The minimal setup
to exhibit the phenomena requires a single composite key, and two partially
overlapping chains of transitive dependencies.
</summary>
    <author>
      <name>Amir Sapir</name>
    </author>
    <author>
      <name>Ariel Sapir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.15664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01778v1</id>
    <updated>2021-10-05T01:38:24Z</updated>
    <published>2021-10-05T01:38:24Z</published>
    <title>Version Reconciliation for Collaborative Databases</title>
    <summary>  We propose MindPalace, a prototype of a versioned database for efficient
collaborative data management. MindPalace supports offline collaboration, where
users work independently without real-time correspondence. The core of
MindPalace is a critical step of offline collaboration: reconciling divergent
branches made by simultaneous data manipulation. We formalize the concept of
auto-mergeability, a condition under which branches may be reconciled without
human intervention, and propose an efficient framework for determining whether
two branches are auto-mergeable and identifying particular records for manual
reconciliation.
</summary>
    <author>
      <name>Nalin Ranjan</name>
    </author>
    <author>
      <name>Zechao Shang</name>
    </author>
    <author>
      <name>Aaron J. Elmore</name>
    </author>
    <author>
      <name>Sanjay Krishnan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3472883.3486980</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3472883.3486980" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper to appear in SoCC '21</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.01778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.03028v1</id>
    <updated>2021-10-06T19:21:51Z</updated>
    <published>2021-10-06T19:21:51Z</published>
    <title>Reconsidering Optimistic Algorithms for Relational DBMS</title>
    <summary>  At DBKDA 2019, we demonstrated that StrongDBMS with simple but rigorous
optimistic algorithms, provides better performance in situations of high
concurrency than major commercial database management systems (DBMS). The
demonstration was convincing but the reasons for its success were not fully
analysed. There is a brief account of the results below. In this short
contribution, we wish to discuss the reasons for the results. The analysis
leads to a strong criticism of all DBMS algorithms based on locking, and based
on these results, it is not fanciful to suggest that it is time to re-engineer
existing DBMS.
</summary>
    <author>
      <name>Malcolm Crowe</name>
    </author>
    <author>
      <name>Fritz Laux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.03028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03643v1</id>
    <updated>2022-01-10T20:55:51Z</updated>
    <published>2022-01-10T20:55:51Z</published>
    <title>Designing a Visual Tool for Property Graph Schema Extraction and
  Refinement: An Expert Study</title>
    <summary>  The design space of visual tools that aim to help people create schemas for
property graphs is explored. Interviews are conducted with experts in the
domain of property graphs and data management in general. Through this
collaboration, we determine how a schema extraction tool can provide value.
These insights are used to establish design requirements and design a UI
prototype, which are then relayed back to the experts. Positive reactions were
received, which encourage future work in the property graph schema space.
</summary>
    <author>
      <name>Nimo Beeren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.03643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03832v1</id>
    <updated>2022-01-11T08:25:06Z</updated>
    <published>2022-01-11T08:25:06Z</published>
    <title>Parallel Acyclic Joins with Canonical Edge Covers</title>
    <summary>  In PODS'21, Hu presented an algorithm in the massively parallel computation
(MPC) model that processes any acyclic join with an asymptotically optimal
load. In this paper, we present an alternative analysis of her algorithm. The
novelty of our analysis is in the revelation of a new mathematical structure --
which we name "canonical edge cover" -- for acyclic hypergraphs. We prove
non-trivial properties for canonical edge covers that offer us a
graph-theoretic perspective about why Hu's algorithm works.
</summary>
    <author>
      <name>Yufei Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICDT'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.03832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04233v1</id>
    <updated>2022-01-11T23:00:18Z</updated>
    <published>2022-01-11T23:00:18Z</published>
    <title>Finding Your Way Through the Jungle of Big Data Architectures</title>
    <summary>  This paper presents a systematic review of common analytical data
architectures based on DAMA-DMBOK and ArchiMate. The paper is work in progress
and provides a first view on Gartner's Logical Data Warehouse paradigm, Data
Fabric and Dehghani's Data Mesh proposal as well as their interdependencies. It
furthermore sketches the way forward how this work can be extended by covering
more architecture paradigms (incl. classic Data Warehouse, Data Vault, Data
Lake, Lambda and Kappa architectures) and introducing a template with among
others "context", "problem" and "solution" descriptions, leading ultimately to
a pattern system providing guidance for choosing the right architecture
paradigm for the right situation.
</summary>
    <author>
      <name>Torsten Priebe</name>
    </author>
    <author>
      <name>Sebastian Neumaier</name>
    </author>
    <author>
      <name>Stefan Markus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BigData52589.2021.9671862</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BigData52589.2021.9671862" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE International Conference on Big Data (IEEE BigData 2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.04233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04899v1</id>
    <updated>2022-01-13T11:41:28Z</updated>
    <published>2022-01-13T11:41:28Z</published>
    <title>An outline of multi objective optimization in databases with focus on
  flexible skyline queries</title>
    <summary>  The problem of optimizing across different, conceivably conflicting, criteria
is called multi-objective optimization and it is widely spread across many
fields. This is a recurring problem in database queries when there is the need
of obtaining the best objects from a very large data set. In this article, I
included a complete review of the main approaches typically used to achieve
multi-criteria optimization. Starting from ranking queries and skylines and
then proceeding to more advanced methods, this paper aims to define a clear
outline of multi-objective optimization in databases. In particular, the
flexible skyline paradigm is considered and thoroughly discussed as it
overcomes many of the critical issues that arise with other methods.
</summary>
    <author>
      <name>Matteo Savino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, 1 table, 5 examples</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.04899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05096v2</id>
    <updated>2022-01-14T11:53:55Z</updated>
    <published>2022-01-11T13:15:08Z</published>
    <title>Flexible Skyline: one query to rule them all</title>
    <summary>  The most common archetypes to identify relevant information in large datasets
and find the bestoptions according to some preferences or user criteria, are
the top-k queries (ranking method based ona score function defined over the
records attributes) and skyline queries (based on Pareto dominance oftuples).
Despite their large diffusion, both approaches have their pros and cons. In
this survey paper, a comparison is made between these methods and the Flexible
Skylines, which is a framework that combines the ranking and skyline approaches
using the novel concept ofF-dominanceto a set of monotone scoring function F.
</summary>
    <author>
      <name>Giacomo Vinati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.05096v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05096v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09018v1</id>
    <updated>2022-01-22T10:50:17Z</updated>
    <published>2022-01-22T10:50:17Z</published>
    <title>Comparison of 6 different approaches to outclass Top-k queries and
  Skyline queries</title>
    <summary>  Topk queries and skyline queries have well explored limitations which recent
research have tried to complete through new techniques. In this survey, after
resuming such limitations, we consider Restricted Skyline Queries, ORD and ORU
approach, Krepresentative minimization queries, Skyline ordering queries, UTK
queries approach and Skyrank that aim to overcome them. After introducing and
comparing their main concepts, pros and cons, we briefly report the algorithms
and confront some of the experimental data collected from the bibliography. To
conclude the paper, we summarize the results presented with a short guide on
how to select the best approach according to specific needs.
</summary>
    <author>
      <name>Martino Manzolini</name>
    </author>
    <link href="http://arxiv.org/abs/2201.09018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10217v2</id>
    <updated>2022-01-30T17:24:39Z</updated>
    <published>2022-01-25T10:18:42Z</published>
    <title>Poisson's CDF applied to Flexible Skylines</title>
    <summary>  The evolution of skyline and ranking queries has created new archetypes like
flexible skylines, which have proven to be an efficient method to select
relevant data from large datasets using multi objective optimization. This
paper aims to study the possible applications of Poisson distribution mass
function as a monotonic scoring function in flexible skyline processes,
especially those featuring schemas whose attributes can be translated to
constant mean rates. Moreover, a method to express users's requirement by means
of the F-dominant set of tuples will be proposed using parametrical variations
in F[1], simultaneously, algorithm construction and potential applications will
be studied.
</summary>
    <author>
      <name>Jaime Pons Garrido</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.10217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.12038v4</id>
    <updated>2022-02-09T22:08:39Z</updated>
    <published>2022-01-28T11:06:42Z</published>
    <title>A survey on flexible/restricted skyline and their applicability</title>
    <summary>  Skyline and Top-k are two of the most important methods to extract
information from datasets, but both come with their drawbacks, that's why
lately some new technics that try to mix the features of the two have been
studied. In this survey three new operators are analysed, F-Skyline, ORU/ORD,
and ${\epsilon}$-Skyline. After giving the main ideas behind those and their
properties, they are compered on 3 fundamental features such as
personalization, cardinality control, and generalization to guide the user to
choose the best one for any task.
</summary>
    <author>
      <name>Davide Canali</name>
    </author>
    <link href="http://arxiv.org/abs/2201.12038v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.12038v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.01546v1</id>
    <updated>2022-02-03T12:12:14Z</updated>
    <published>2022-02-03T12:12:14Z</published>
    <title>QueryER: A Framework for Fast Analysis-Aware Deduplication over Dirty
  Data</title>
    <summary>  In this work, we explore the problem of correctly and efficiently answering
complex SPJ queries issued directly on top of dirty data. We introduce QueryER,
a framework that seamlessly integrates Entity Resolution into Query Processing.
QueryER executes analysis-aware deduplication by weaving ER operators into the
query plan. The experimental evaluation of our approach exhibits that it adapts
to the workload and scales on both real and synthetic datasets.
</summary>
    <author>
      <name>Giorgos Alexiou</name>
    </author>
    <author>
      <name>George Papastefanatos</name>
    </author>
    <author>
      <name>Vassilis Stamatopoulos</name>
    </author>
    <author>
      <name>Georgia Koutrika</name>
    </author>
    <author>
      <name>Nectarios Koziris</name>
    </author>
    <link href="http://arxiv.org/abs/2202.01546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.01546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.01550v1</id>
    <updated>2022-02-03T12:22:19Z</updated>
    <published>2022-02-03T12:22:19Z</published>
    <title>Multi-Objective Optimization, different approach to query a database</title>
    <summary>  The datasets available nowadays are very rich and complex, but how do we
reach the information we are looking for? In this survey, two different
approaches to query a dataset are analyzed and algorithms for each type are
explained. Specifically, the TA and NRA have been analyzed for the Top-K query
and the Basic Block Nested Loops has been examined for the skyline query.
Moreover, it's explained the core idea behind the Prioritized and Flexible
skyline. In the end, the pros and cons of each type of analyzed query have been
evaluated based on different criteria.
</summary>
    <author>
      <name>Matteo Cordioli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.01550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.01550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02619v1</id>
    <updated>2022-02-05T19:40:04Z</updated>
    <published>2022-02-05T19:40:04Z</published>
    <title>A bird's eye view on Multi-Objective Optimization techniques in
  Relational Databases</title>
    <summary>  Multi-objective optimization is the problem of optimizing simultaneously
multiple objective functions and several techniques exist to deal with this
problem. This paper aims to present the main methods that can be used to solve
this issue in the context of relational databases. In particular, this work
examines Top-k query to get the k best result from a dataset and Skyline query
that provides a more general overview of the best results. We also discuss
Flexible-skyline, a new method designed to improve upon the previous
techniques, mitigating their shortcomings. For each method, we describe the
main characteristics and present an overview of the algorithms implementing
such thecniques, while comparing advantages and disadvantages.
</summary>
    <author>
      <name>Giuseppe Tortorelli</name>
    </author>
    <link href="http://arxiv.org/abs/2202.02619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05689v1</id>
    <updated>2022-02-11T15:21:24Z</updated>
    <published>2022-02-11T15:21:24Z</published>
    <title>Conservative Extensions for Existential Rules</title>
    <summary>  We study the problem to decide, given sets T1,T2 of tuple-generating
dependencies (TGDs), also called existential rules, whether T2 is a
conservative extension of T1. We consider two natural notions of conservative
extension, one pertaining to answers to conjunctive queries over databases and
one to homomorphisms between chased databases. Our main results are that these
problems are undecidable for linear TGDs, undecidable for guarded TGDs even
when T1 is empty, and decidable for frontier-one TGDs.
</summary>
    <author>
      <name>Jean Christoph Jung</name>
    </author>
    <author>
      <name>Carsten Lutz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/2202.05689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.06430v1</id>
    <updated>2022-02-13T22:36:16Z</updated>
    <published>2022-02-13T22:36:16Z</published>
    <title>Giving the Right Answer: a Brief Overview on How to Extend Ranking and
  Skyline Queries</title>
    <summary>  To retrieve the best results in a database we use Top-K queries and Skyline
queries but some problems arise. The formers rely too much on user preferences,
which are difficult to quantify and may skew the fetching of the data, while
the latters tend to output too much data. In this paper, we explore three
different branches of research that seek to overcome such limitations:
Flexible/Restricted Skylines, Skyline Ordering/Ranking, and Regret
Minimization. We analyze how they work and we make comparisons among them to
guide the reader to choose the approach that best fits their use cases.
</summary>
    <author>
      <name>Sergio Cuzzucoli</name>
    </author>
    <link href="http://arxiv.org/abs/2202.06430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.06430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09552v1</id>
    <updated>2022-02-19T08:52:11Z</updated>
    <published>2022-02-19T08:52:11Z</published>
    <title>A survey on making skylines more flexible</title>
    <summary>  Top-$k$ queries and skylines are the two most common approaches to finding
the most interesting entries in a homogeneous multi-dimensional dataset.
However, both of these strategies have some shortcomings. Top-$k$ queries are
very challenging to specify precisely and skylines are not customizable to
specific scenarios, on top of having unpredictable output cardinalities. We
describe some alternative methods aimed at addressing the shortcomings of
top-$k$ queries and skylines and compare all approaches to illustrate which of
the desired properties each of them possesses.
</summary>
    <author>
      <name>Cem Cebeci</name>
    </author>
    <link href="http://arxiv.org/abs/2202.09552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10502v1</id>
    <updated>2022-02-21T19:26:52Z</updated>
    <published>2022-02-21T19:26:52Z</published>
    <title>Flexible Skylines: Customizing Skyline Queries Catching Desired
  Preferences</title>
    <summary>  The techniques most extensively used to retrieve interesting data from
data-sets are the Skyline and the Top-k queries. Sadly, they are not enough for
facing modern problems, so the needing of something more usable and reliable
has come. In this survey we are going to explore Flexible Skylines which are
proposed to overcame the old fashion techniques' problems by extending the
concept of dominance. After, we are going to compare this approach with the old
and new ones evaluating pros and cons. Finally, we will see some interesting
applications.
</summary>
    <author>
      <name>Giuseppe Montanaro</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10785v1</id>
    <updated>2022-02-22T10:15:29Z</updated>
    <published>2022-02-22T10:15:29Z</published>
    <title>Comparing the latest ranking techniques: pros and cons of flexible
  skylines, regret minimization and skyline ranking queries</title>
    <summary>  Long-established ranking approaches, such as top-k and skyline queries, have
been thoroughly discussed and their drawbacks are well acknowledged. New
techniques have been developed in recent years that try to combine traditional
ones to overcome their limitations. In this paper we focus our attention on
some of them: flexible skylines, regret minimization and skyline ranking
queries, because, while these new methods are promising and have shown
interesting results, a comparison between them is still not available. After a
short introduction of each approach, we discuss analogies and differences
between them with the advantages and disadvantages of every technique debated.
</summary>
    <author>
      <name>Davide Foini</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12618v1</id>
    <updated>2022-02-25T11:10:50Z</updated>
    <published>2022-02-25T11:10:50Z</published>
    <title>Getting the best from skylines and top-k queries</title>
    <summary>  Top-k and skylines are two important techniques that can be used to extract
the best objects from a set. Both the approaches have well-known pros and cons:
a quite big limitation of skyline queries is the impossibility to control the
cardinality of the output and the difficulty in specifying a trade-off among
attributes, whereas the ranking queries allow so. On the other hand, the usage
of ranking implies that ranking functions need to be specified by users and
renouncing the simplicity of skylines. Flexible/ restricted skylines present a
new approach to tackle this problem, combining the best characteristics of both
techniques making use of a new flexible relation of dominance.
</summary>
    <author>
      <name>Marco Costanzo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.12618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0337v1</id>
    <updated>2013-02-02T03:36:37Z</updated>
    <published>2013-02-02T03:36:37Z</published>
    <title>Perancangan basisdata sistem informasi penggajian</title>
    <summary>  The purpose of this research is to design database scheme of information
system at XYZ University. By using database design methods (conceptual scheme,
logical scheme, &amp; physical scheme) the writer designs payroll information
system. The physical scheme is compatible with Borland Delphi Database Engine
Scheme to support the implementation of the I.S. After 3 (three) steps we get 7
(seven) tables, dan 6 (six) forms. By using this shemce, the system can produce
several reports quickly, accurately, efficiently, and effectively.
</summary>
    <author>
      <name>Leon Andretti Abdillah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MATRIK. 8 (2006) 135-152</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.0337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1923v1</id>
    <updated>2013-02-08T01:22:54Z</updated>
    <published>2013-02-08T01:22:54Z</published>
    <title>Update XML Views</title>
    <summary>  View update is the problem of translating an update to a view to some updates
to the source data of the view. In this paper, we show the factors determining
XML view update translation, propose a translation procedure, and propose
translated updates to the source document for different types of views. We
further show that the translated updates are precise. The proposed solution
makes it possible for users who do not have access privileges to the source
data to update the source data via a view.
</summary>
    <author>
      <name>Jixue Liu</name>
    </author>
    <author>
      <name>Chengfei Liu</name>
    </author>
    <author>
      <name>Theo Haerder</name>
    </author>
    <author>
      <name>Jeffery Xu Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1302.1923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.3860v1</id>
    <updated>2013-02-15T20:04:10Z</updated>
    <published>2013-02-15T20:04:10Z</published>
    <title>ScalienDB: Designing and Implementing a Distributed Database using Paxos</title>
    <summary>  ScalienDB is a scalable, replicated database built on top of the Paxos
algorithm. It was developed from 2010 to 2012, when the startup backing it
failed. This paper discusses the design decisions of the distributed database,
describes interesting parts of the C++ codebase and enumerates lessons learned
putting ScalienDB into production at a handful of clients. The source code is
available on Github under the AGPL license, but it is no longer developed or
maintained.
</summary>
    <author>
      <name>Márton Trencséni</name>
    </author>
    <author>
      <name>Attila Gazsó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5549v1</id>
    <updated>2013-02-22T11:01:11Z</updated>
    <published>2013-02-22T11:01:11Z</published>
    <title>On Graph Deltas for Historical Queries</title>
    <summary>  In this paper, we address the problem of evaluating historical queries on
graphs. To this end, we investigate the use of graph deltas, i.e., a log of
time-annotated graph operations. Our storage model maintains the current graph
snapshot and the delta. We reconstruct past snapshots by applying appropriate
parts of the graph delta on the current snapshot. Query evaluation proceeds on
the reconstructed snapshots but we also propose algorithms based mostly on
deltas for efficiency. We introduce various techniques for improving
performance, including materializing intermediate snapshots, partial
reconstruction and indexing deltas.
</summary>
    <author>
      <name>Georgia Koloniari</name>
    </author>
    <author>
      <name>Dimitris Souravlias</name>
    </author>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, WOSS 2012, Istanbul, Turkey</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 1st Workshop on Online Social Systems (WOSS) 2012,
  in conjunction with VLDB 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.5549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1689v1</id>
    <updated>2013-06-07T11:17:41Z</updated>
    <published>2013-06-07T11:17:41Z</published>
    <title>Verification of Query Completeness over Processes [Extended Version]</title>
    <summary>  Data completeness is an essential aspect of data quality, and has in turn a
huge impact on the effective management of companies. For example, statistics
are computed and audits are conducted in companies by implicitly placing the
strong assumption that the analysed data are complete. In this work, we are
interested in studying the problem of completeness of data produced by business
processes, to the aim of automatically assessing whether a given database query
can be answered with complete information in a certain state of the process. We
formalize so-called quality-aware processes that create data in the real world
and store it in the company's information system possibly at a later point.
</summary>
    <author>
      <name>Simon Razniewski</name>
    </author>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper that was submitted to BPM 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.1689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.2460v1</id>
    <updated>2013-06-11T09:24:06Z</updated>
    <published>2013-06-11T09:24:06Z</published>
    <title>StreamWorks - A system for Dynamic Graph Search</title>
    <summary>  Acting on time-critical events by processing ever growing social media, news
or cyber data streams is a major technical challenge. Many of these data
sources can be modeled as multi-relational graphs. Mining and searching for
subgraph patterns in a continuous setting requires an efficient approach to
incremental graph search. The goal of our work is to enable real-time search
capabilities for graph databases. This demonstration will present a dynamic
graph query system that leverages the structural and semantic characteristics
of the underlying multi-relational graph.
</summary>
    <author>
      <name>Sutanay Choudhury</name>
    </author>
    <author>
      <name>Lawrence Holder</name>
    </author>
    <author>
      <name>George Chin</name>
    </author>
    <author>
      <name>Abhik Ray</name>
    </author>
    <author>
      <name>Sherman Beus</name>
    </author>
    <author>
      <name>John Feo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD 2013: International Conference on Management of Data</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.2460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.2460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6734v1</id>
    <updated>2013-06-28T07:12:23Z</updated>
    <published>2013-06-28T07:12:23Z</published>
    <title>A novel ER model to relational model transformation algorithm for
  semantically clear high quality database design</title>
    <summary>  Conceptual modelling using the entity relationship (ER) model has been widely
used for database design for a long period of time. However, studies indicate
that creating a satisfactory relational model representation from an ER model
is uncertain due to the insufficiencies both in the transformation methods used
and in the relational model itself. In an effort to solve the issue the
original ER notation has been modified, and accordingly, a new transformation
algorithm has been developed. This paper presents the proposed transformation
algorithm. Using a real world example it shows how the algorithm can be applied
in practice. The paper also discusses how to validate the resulted database and
reclaim the information that it represents.
</summary>
    <author>
      <name>Dhammika Pieris</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0494v1</id>
    <updated>2014-01-02T18:14:33Z</updated>
    <published>2014-01-02T18:14:33Z</published>
    <title>Flexible SQLf query based on fuzzy linguistic summaries</title>
    <summary>  Data is often partially known, vague or ambiguous in many real world
applications. To deal with such imprecise information, fuzziness is introduced
in the classical model. SQLf is one of the practical language to deal with
flexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount
of fuzzy data, the necessity to work with synthetic views became a challenge
for many DB community researchers. The present work deals with Flexible SQLf
query based on fuzzy linguistic summaries. We use the fuzzy summaries produced
by our Fuzzy-SaintEtiq approach. It provides a description of objects depending
on the fuzzy linguistic labels specified as selection criteria.
</summary>
    <author>
      <name>Ines Benali-Sougui</name>
    </author>
    <author>
      <name>Minyar Sassi-Hidri</name>
    </author>
    <author>
      <name>Amel Grissa-Touzi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Control, Engineering &amp; Information
  Technology (CEIT), Proceedings Engineering &amp; Technology, Vol. 1, pp. 175-180,
  2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.0494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.7733v1</id>
    <updated>2014-01-30T04:43:22Z</updated>
    <published>2014-01-30T04:43:22Z</published>
    <title>Security Implications of Distributed Database Management System Models</title>
    <summary>  Security features must be addressed when escalating a distributed database.
The choice between the object oriented and the relational data model, several
factors should be considered. The most important of these factors are single
and multilevel access controls (MAC), protection and integrity maintenance.
While determining which distributed database replica will be more secure for a
particular function, the choice should not be made exclusively on the basis of
available security features. One should also query the effectiveness and
efficiency of the delivery of these characteristics. In this paper, the
security strengths and weaknesses of both database models and the thorough
problems initiate in the distributed environment are conversed.
</summary>
    <author>
      <name>C. Sunil Kumar</name>
    </author>
    <author>
      <name>J. Seetha</name>
    </author>
    <author>
      <name>S. R. Vinotha</name>
    </author>
    <link href="http://arxiv.org/abs/1401.7733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1343v1</id>
    <updated>2014-10-06T12:28:22Z</updated>
    <published>2014-10-06T12:28:22Z</published>
    <title>Combined Algorithm for Data Mining using Association rules</title>
    <summary>  Association Rule mining is one of the most important fields in data mining
and knowledge discovery. This paper proposes an algorithm that combines the
simple association rules derived from basic Apriori Algorithm with the multiple
minimum support using maximum constraints. The algorithm is implemented, and is
compared to its predecessor algorithms using a novel proposed comparison
algorithm. Results of applying the proposed algorithm show faster performance
than other algorithms without scarifying the accuracy.
</summary>
    <author>
      <name>Walaa Medhat</name>
    </author>
    <author>
      <name>Ahmed Hassan Yousef</name>
    </author>
    <author>
      <name>Hoda Korashy Mohamed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ain Shams Journal of Electrical Engineering, 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7990v1</id>
    <updated>2014-10-27T22:17:19Z</updated>
    <published>2014-10-27T22:17:19Z</published>
    <title>Linked Data Integration with Conflicts</title>
    <summary>  Linked Data have emerged as a successful publication format and one of its
main strengths is its fitness for integration of data from multiple sources.
This gives them a great potential both for semantic applications and the
enterprise environment where data integration is crucial. Linked Data
integration poses new challenges, however, and new algorithms and tools
covering all steps of the integration process need to be developed. This paper
explores Linked Data integration and its specifics. We focus on data fusion and
conflict resolution: two novel algorithms for Linked Data fusion with
provenance tracking and quality assessment of fused data are proposed. The
algorithms are implemented as part of the ODCleanStore framework and evaluated
on real Linked Open Data.
</summary>
    <author>
      <name>Jan Michelfeit</name>
    </author>
    <author>
      <name>Tomáš Knap</name>
    </author>
    <author>
      <name>Martin Nečaský</name>
    </author>
    <link href="http://arxiv.org/abs/1410.7990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00301v1</id>
    <updated>2015-03-01T16:40:56Z</updated>
    <published>2015-03-01T16:40:56Z</published>
    <title>On Defining SPARQL with Boolean Tensor Algebra</title>
    <summary>  The Resource Description Framework (RDF) represents information as
subject-predicate-object triples. These triples are commonly interpreted as a
directed labelled graph. We propose an alternative approach, interpreting the
data as a 3-way Boolean tensor. We show how SPARQL queries - the standard
queries for RDF - can be expressed as elementary operations in Boolean algebra,
giving us a complete re-interpretation of RDF and SPARQL. We show how the
Boolean tensor interpretation allows for new optimizations and analyses of the
complexity of SPARQL queries. For example, estimating the size of the results
for different join queries becomes much simpler.
</summary>
    <author>
      <name>Saskia Metzler</name>
    </author>
    <author>
      <name>Pauli Miettinen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2740908.2742738</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2740908.2742738" rel="related"/>
    <link href="http://arxiv.org/abs/1503.00301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00650v1</id>
    <updated>2015-03-02T18:14:20Z</updated>
    <published>2015-03-02T18:14:20Z</published>
    <title>Consistent Answers of Conjunctive Queries on Graphs</title>
    <summary>  During the past decade, there has been an extensive investigation of the
computational complexity of the consistent answers of Boolean conjunctive
queries under primary key constraints. Much of this investigation has focused
on self-join-free Boolean conjunctive queries. In this paper, we study the
consistent answers of Boolean conjunctive queries involving a single binary
relation, i.e., we consider arbitrary Boolean conjunctive queries on directed
graphs. In the presence of a single key constraint, we show that for each such
Boolean conjunctive query, either the problem of computing its consistent
answers is expressible in first-order logic, or it is polynomial-time solvable,
but not expressible in first-order logic.
</summary>
    <author>
      <name>Foto N. Afrati</name>
    </author>
    <author>
      <name>Phokion G. Kolaitis</name>
    </author>
    <author>
      <name>Angelos Vasilakopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1503.00650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02940v1</id>
    <updated>2015-03-10T14:57:26Z</updated>
    <published>2015-03-10T14:57:26Z</published>
    <title>Efficient Query Processing for SPARQL Federations with Replicated
  Fragments</title>
    <summary>  Low reliability and availability of public SPARQL endpoints prevent
real-world applications from exploiting all the potential of these querying
infras-tructures. Fragmenting data on servers can improve data availability but
degrades performance. Replicating fragments can offer new tradeoff between
performance and availability. We propose FEDRA, a framework for querying Linked
Data that takes advantage of client-side data replication, and performs a
source selection algorithm that aims to reduce the number of selected public
SPARQL endpoints, execution time, and intermediate results. FEDRA has been
implemented on the state-of-the-art query engines ANAPSID and FedX, and
empirically evaluated on a variety of real-world datasets.
</summary>
    <author>
      <name>Gabriela Montoya</name>
    </author>
    <author>
      <name>Hala Skaf-Molli</name>
    </author>
    <author>
      <name>Pascal Molli</name>
    </author>
    <author>
      <name>Maria-Esther Vidal</name>
    </author>
    <link href="http://arxiv.org/abs/1503.02940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03208v1</id>
    <updated>2015-03-11T08:13:51Z</updated>
    <published>2015-03-11T08:13:51Z</published>
    <title>Fraudulent Electronic transaction detection using KDA Model</title>
    <summary>  Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System &amp; Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.
</summary>
    <author>
      <name>M. Vadoodparast</name>
    </author>
    <author>
      <name>A. Razak Hamdan</name>
    </author>
    <author>
      <name> Hafiz</name>
    </author>
    <link href="http://arxiv.org/abs/1503.03208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04385v1</id>
    <updated>2015-03-15T04:09:26Z</updated>
    <published>2015-03-15T04:09:26Z</published>
    <title>Design and Implementation of Database Independent Auto Sequence Numbers</title>
    <summary>  Developers across the world use autonumber or auto sequences field of the
backend databases for developing both the desktop and web based data centric
applications which is easier to use at the development and deployment purpose
but can create a lot of problems under varied situations. This paper examines
how a database independent autonumber could be developed and reused solving all
the problems as well as providing the same degree of easy to use features of
autonumber offered by modern Relational Database Systems.
</summary>
    <author>
      <name>Kisor Ray</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V20P111</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V20P111" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">03 pages, 02 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology,Volume-20
  Number-2,2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.04385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05157v1</id>
    <updated>2015-03-17T18:39:22Z</updated>
    <published>2015-03-17T18:39:22Z</published>
    <title>Quality Assessment of Linked Datasets using Probabilistic Approximation</title>
    <summary>  With the increasing application of Linked Open Data, assessing the quality of
datasets by computing quality metrics becomes an issue of crucial importance.
For large and evolving datasets, an exact, deterministic computation of the
quality metrics is too time consuming or expensive. We employ probabilistic
techniques such as Reservoir Sampling, Bloom Filters and Clustering Coefficient
estimation for implementing a broad set of data quality metrics in an
approximate but sufficiently accurate way. Our implementation is integrated in
the comprehensive data quality assessment framework Luzzu. We evaluated its
performance and accuracy on Linked Open Datasets of broad relevance.
</summary>
    <author>
      <name>Jeremy Debattista</name>
    </author>
    <author>
      <name>Santiago Londoño</name>
    </author>
    <author>
      <name>Christoph Lange</name>
    </author>
    <author>
      <name>Sören Auer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, To appear in ESWC 2015 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.05157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06548v1</id>
    <updated>2015-03-23T08:15:46Z</updated>
    <published>2015-03-23T08:15:46Z</published>
    <title>Using MongoDB for Social Networking Website</title>
    <summary>  Social media is a biggest successful buzzword used in the recent time. Its
success opened various opportunities for the developers. Developing any
application requires storage of large data into databases. Many databases are
available for the developers, Choosing the right one make development easier.
MongoDB is a cross platform document oriented, schema-less database eschewed
the traditional table based relational database structure in favor of JSON like
documents. This article discusses various pros and cons encountered with the
use of the MongoDB so that developers would be helped while choosing it wisely.
</summary>
    <author>
      <name>Sumitkumar Kanoje</name>
    </author>
    <author>
      <name>Varsha Powar</name>
    </author>
    <author>
      <name>Debajyoti Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08636v1</id>
    <updated>2015-03-30T11:00:35Z</updated>
    <published>2015-03-30T11:00:35Z</published>
    <title>Design &amp; Implementation Approach for Error Free Clinical Data Repository
  for the Medical Practitioners</title>
    <summary>  The modern treatment of any disease is heavily dependent on the medical
diagnosis. Clinical data obtained through the diagnostics tests need to be
collected and entered into the computer database in order to make a clinical
data repository. In most of the cases, manual entry is an absolute necessity.
However, manual entry can cause errors also, leading to wrong diagnosis. This
paper explains how data could be entered free of error to reduce the chances of
wrong diagnosis by designing and implementation of a simple database driven
application.
</summary>
    <author>
      <name>Kisor Ray</name>
    </author>
    <author>
      <name>Santanu Ghosh</name>
    </author>
    <author>
      <name>Mridul Das</name>
    </author>
    <author>
      <name>Bhaswati Ray</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V21P113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V21P113" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">04 pages, 04 Figures, International Journal of Computer Trends and
  Technology, Volume-21 Number-2,2015, ISSN 2231-2803</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.08636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03935v1</id>
    <updated>2015-11-12T15:54:49Z</updated>
    <published>2015-11-12T15:54:49Z</published>
    <title>Fast Data Management with Distributed Streaming SQL</title>
    <summary>  To stay competitive in today's data driven economy, enterprises large and
small are turning to stream processing platforms to process high volume, high
velocity, and diverse streams of data (fast data) as they arrive. Low-level
programming models provided by the popular systems of today suffer from lack of
responsiveness to change: enhancements require code changes with attendant
large turn-around times. Even though distributed SQL query engines have been
available for Big Data, we still lack support for SQL-based stream querying
capabilities in distributed stream processing systems. In this white paper, we
identify a set of requirements and propose a standard SQL based streaming query
model for management of what has been referred to as Fast Data.
</summary>
    <author>
      <name>Milinda Pathirage</name>
    </author>
    <author>
      <name>Beth Plale</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08915v2</id>
    <updated>2016-02-11T16:12:55Z</updated>
    <published>2015-11-28T17:16:55Z</published>
    <title>Column-Oriented Datalog Materialization for Large Knowledge Graphs
  (Extended Technical Report)</title>
    <summary>  The evaluation of Datalog rules over large Knowledge Graphs (KGs) is
essential for many applications. In this paper, we present a new method of
materializing Datalog inferences, which combines a column-based memory layout
with novel optimization methods that avoid redundant inferences at runtime. The
pro-active caching of certain subqueries further increases efficiency. Our
empirical evaluation shows that this approach can often match or even surpass
the performance of state-of-the-art systems, especially under restricted
resources.
</summary>
    <author>
      <name>Jacopo Urbani</name>
    </author>
    <author>
      <name>Ceriel Jacobs</name>
    </author>
    <author>
      <name>Markus Krötzsch</name>
    </author>
    <link href="http://arxiv.org/abs/1511.08915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.3; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.09059v1</id>
    <updated>2015-11-29T18:46:06Z</updated>
    <published>2015-11-29T18:46:06Z</published>
    <title>Analysis Traceability and Provenance for HEP</title>
    <summary>  This paper presents the use of the CRISTAL software in the N4U project.
CRISTAL was used to create a set of provenance aware analysis tools for the
Neuroscience domain. This paper advocates that the approach taken in N4U to
build the analysis suite is sufficiently generic to be able to be applied to
the HEP domain. A mapping to the PROV model for provenance interoperability is
also presented and how this can be applied to the HEP domain for the
interoperability of HEP analyses.
</summary>
    <author>
      <name>Jetendr Shamdasani</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Andrew Branson</name>
    </author>
    <author>
      <name>Zsolt Kovacs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/664/3/032028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/664/3/032028" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pagesd, 4 figures. Presented at 21st Int Conf on Computing in High
  Energy and Nuclear Physics (CHEP15). Okinawa, Japan. April 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.09059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.09059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06084v1</id>
    <updated>2016-10-19T16:07:30Z</updated>
    <published>2016-10-19T16:07:30Z</published>
    <title>Portable Ontological Expressions in NoSQL Queries</title>
    <summary>  A significant barrier to the portability of queries across di- verse physical
implementations of large data stores, espe- cially NoSQL data stores, is that
the queries reference the physical storage attributes, such as the table and
column names. In this paper, we describe a technique for embed- ding
ontological expressions called Address Expressions, or A-Expressions, in NoSQL
queries to improve their portability across diverse physical implementations.
We discuss an implementation of such queries over a MongoDB data store of the
Enron email corpus with examples, and conduct a preliminary performance
assessment.
</summary>
    <author>
      <name>Suresh K. Damodaran</name>
    </author>
    <author>
      <name>Pedro A. Colon-Hernandez</name>
    </author>
    <link href="http://arxiv.org/abs/1610.06084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07649v1</id>
    <updated>2016-10-24T20:52:57Z</updated>
    <published>2016-10-24T20:52:57Z</published>
    <title>ALPINE: Anytime Mining with Definite Guarantees</title>
    <summary>  ALPINE is to our knowledge the first anytime algorithm to mine frequent
itemsets and closed frequent itemsets. It guarantees that all itemsets with
support exceeding the current checkpoint's support have been found before it
proceeds further. Thus, it is very attractive for extremely long mining tasks
with very high dimensional data (for example in genetics) because it can offer
intermediate meaningful and complete results. This ANYTIME feature is the most
important contribution of ALPINE, which is also fast but not necessarily the
fastest algorithm around. Another critical advantage of ALPINE is that it does
not require the apriori decided minimum support value.
</summary>
    <author>
      <name>Qiong Hu</name>
    </author>
    <author>
      <name>Tomasz Imielinski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06258v1</id>
    <updated>2018-01-19T00:02:34Z</updated>
    <published>2018-01-19T00:02:34Z</published>
    <title>Towards a Theory of Data-Diff: Optimal Synthesis of Succinct Data
  Modification Scripts</title>
    <summary>  This paper addresses the Data-Diff problem: given a dataset and a subsequent
version of the dataset, find the shortest sequence of operations that
transforms the dataset to the subsequent version, under a restricted family of
operations. We consider operations similar to SQL UPDATE, each with a condition
(WHERE) that matches a subset of tuples and a modifier (SET) that makes changes
to those matched tuples. We characterize the problem based on different
constraints on the attributes and the allowed conditions and modifiers,
providing complexity classification and algorithms in each case.
</summary>
    <author>
      <name>Tana Wattanawaroon</name>
    </author>
    <author>
      <name>Stephen Macke</name>
    </author>
    <author>
      <name>Aditya Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08052v1</id>
    <updated>2018-01-24T16:07:39Z</updated>
    <published>2018-01-24T16:07:39Z</published>
    <title>The Historic Development of the Zooarchaeological Database OssoBook and
  the xBook Framework for Scientific Databases</title>
    <summary>  In this technical report, we describe the historic development of the
zooarchaeological database OssoBook and the resulting framework xBook, a
generic infrastructure for distributed, relational data management that is
mainly designed for the needs of scientific data. We describe the concepts of
the architecture and its most important features. We especially point out the
Server-Client architecture, the synchronization process, the Launcher
application, and the structure and features of the application.
</summary>
    <author>
      <name>Daniel Kaltenthaler</name>
    </author>
    <author>
      <name>Johannes-Y. Lohrer</name>
    </author>
    <link href="http://arxiv.org/abs/1801.08052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00677v2</id>
    <updated>2018-12-18T11:16:34Z</updated>
    <published>2018-09-03T18:05:12Z</published>
    <title>Learned Cardinalities: Estimating Correlated Joins with Deep Learning</title>
    <summary>  We describe a new deep learning approach to cardinality estimation. MSCN is a
multi-set convolutional network, tailored to representing relational query
plans, that employs set semantics to capture query features and true
cardinalities. MSCN builds on sampling-based estimation, addressing its
weaknesses when no sampled tuples qualify a predicate, and in capturing
join-crossing correlations. Our evaluation of MSCN using a real-world dataset
shows that deep learning significantly enhances the quality of cardinality
estimation, which is the core problem in query optimization.
</summary>
    <author>
      <name>Andreas Kipf</name>
    </author>
    <author>
      <name>Thomas Kipf</name>
    </author>
    <author>
      <name>Bernhard Radke</name>
    </author>
    <author>
      <name>Viktor Leis</name>
    </author>
    <author>
      <name>Peter Boncz</name>
    </author>
    <author>
      <name>Alfons Kemper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2019. https://github.com/andreaskipf/learnedcardinalities</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00677v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00677v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03261v1</id>
    <updated>2018-09-10T12:11:29Z</updated>
    <published>2018-09-10T12:11:29Z</published>
    <title>The Skiplist-Based LSM Tree</title>
    <summary>  Log-Structured Merge (LSM) Trees provide a tiered data storage and retrieval
paradigm that is attractive for write-optimized data systems. Maintaining an
efficient buffer in memory and deferring updates past their initial write-time,
the structure provides quick operations over hot data. Because each layer of
the structure is logically separate from the others, the structure is also
conducive to opportunistic and granular optimization. In this paper, we
introduce the Skiplist-Based LSM Tree (sLSM), a novel system in which the
memory buffer of the LSM is composed of a sequence of skiplists. We develop
theoretical and experimental results that demonstrate that the breadth of
tuning parameters inherent to the sLSM allows it broad flexibility for
excellent performance across a wide variety of workloads.
</summary>
    <author>
      <name>Aron Szanto</name>
    </author>
    <link href="http://arxiv.org/abs/1809.03261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03822v1</id>
    <updated>2018-09-11T12:39:17Z</updated>
    <published>2018-09-11T12:39:17Z</published>
    <title>Integration of Relational and Graph Databases Functionally</title>
    <summary>  A significant category of NoSQL approaches is known as graph da-tabases. They
are usually represented by one property graph. We introduce a functional
approach to modelling relations and property graphs. Single-valued and
multivalued functions will be sufficient in this case. Then, a typed
{\lambda}-calculus, i.e., the language of lambda terms, will be used as a data
manipulation lan-guage. Some integration options at the query language level
are discussed.
</summary>
    <author>
      <name>Jaroslav Pokorny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the Pre-proceedings of the Semantics in Big Data Management
  workshop, IFIP W.G. 2.6 on Database, Tuesday 18th September 2018 at IFIP
  World Computer Congress 2018 - Poznan, Poland</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.03822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10286v3</id>
    <updated>2019-01-22T17:14:34Z</updated>
    <published>2018-09-27T00:57:33Z</published>
    <title>Repair-Based Degrees of Database Inconsistency: Computation and
  Complexity</title>
    <summary>  We propose a generic numerical measure of the inconsistency of a database
with respect to a set of integrity constraints. It is based on an abstract
repair semantics. In particular, an inconsistency measure associated to
cardinality-repairs is investigated in detail. More specifically, it is shown
that it can be computed via answer-set programs, but sometimes its computation
can be intractable in data complexity. However, polynomial-time deterministic
and randomized approximations are exhibited. The behavior of this measure under
small updates is analyzed, obtaining fixed-parameter tractability results.
Furthermore, alternative inconsistency measures are proposed and discussed.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some editing made and some new paragraphs added</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.10286v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10286v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10856v2</id>
    <updated>2018-10-15T03:15:27Z</updated>
    <published>2018-09-28T04:47:54Z</published>
    <title>Answering Analytical Queries on Text Data with Temporal Term Histograms</title>
    <summary>  Temporal text, i.e., time-stamped text data are found abundantly in a variety
of data sources like newspapers, blogs and social media posts. While today's
data management systems provide facilities for searching full-text data, they
do not provide any simple primitives for performing analytical operations with
text. This paper proposes the temporal term histograms (TTH) as an intermediate
primitive that can be used for analytical tasks. We propose an algebra, with
operators and equivalence rules for TTH and present a reference implementation
on a relational database system.
</summary>
    <author>
      <name>Kai Lin</name>
    </author>
    <author>
      <name>Subhasis Dasgupta</name>
    </author>
    <author>
      <name>Amarnath Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1809.10856v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10856v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10855v1</id>
    <updated>2018-11-27T07:54:43Z</updated>
    <published>2018-11-27T07:54:43Z</published>
    <title>Data Management in Time-Domain Astronomy: Requirements and Challenges</title>
    <summary>  In time-domain astronomy, we need to use the relational database to manage
star catalog data. With the development of sky survey technology, the size of
star catalog data is larger, and the speed of data generation is faster. So, in
this paper, we make a systematic and comprehensive introduction to process the
data in time-domain astronomy, and valuable research questions are detailed.
Then, we list candidate systems usually used in astronomy and point out the
advantages and disadvantages of these systems. In addition, we present the key
techniques needed to deal with astronomical data. Finally, we summarize the
challenges faced by the design of our database prototype.
</summary>
    <author>
      <name>Chen Yang</name>
    </author>
    <author>
      <name>Xiaofeng Meng</name>
    </author>
    <author>
      <name>Zhihui Du</name>
    </author>
    <author>
      <name>Zhiqiang Duan</name>
    </author>
    <author>
      <name>Yongjie Du</name>
    </author>
    <link href="http://arxiv.org/abs/1811.10855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04379v2</id>
    <updated>2020-02-03T14:54:35Z</updated>
    <published>2018-12-11T13:13:38Z</published>
    <title>On the expressive power of linear algebra on graphs</title>
    <summary>  Most graph query languages are rooted in logic. By contrast, in this paper we
consider graph query languages rooted in linear algebra. More specifically, we
consider MATLANG, a matrix query language recently introduced, in which some
basic linear algebra functionality is supported. We investigate the problem of
characterising equivalence of graphs, represented by their adjacency matrices,
for various fragments of MATLANG. A complete picture is painted of the impact
of the linear algebra operations in MATLANG on their ability to distinguish
graphs.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">51 pages, revised extended version of conference paper (International
  Conference on Database Theory 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.04379v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04379v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01304v1</id>
    <updated>2019-02-04T16:52:40Z</updated>
    <published>2019-02-04T16:52:40Z</published>
    <title>Declarative Data Analytics: a Survey</title>
    <summary>  The area of declarative data analytics explores the application of the
declarative paradigm on data science and machine learning. It proposes
declarative languages for expressing data analysis tasks and develops systems
which optimize programs written in those languages. The execution engine can be
either centralized or distributed, as the declarative paradigm advocates
independence from particular physical implementations. The survey explores a
wide range of declarative data analysis frameworks by examining both the
programming model and the optimization techniques used, in order to provide
conclusions on the current state of the art in the area and identify open
challenges.
</summary>
    <author>
      <name>Nantia Makrynioti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Athens University of Economics and Business</arxiv:affiliation>
    </author>
    <author>
      <name>Vasilis Vassalos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Athens University of Economics and Business</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.01304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02013v1</id>
    <updated>2019-02-06T03:23:51Z</updated>
    <published>2019-02-06T03:23:51Z</published>
    <title>Finding the Transitive Closure of Functional Dependencies using
  Strategic Port Graph Rewriting</title>
    <summary>  We present a new approach to the logical design of relational databases,
based on strategic port graph rewriting. We show how to model relational
schemata as attributed port graphs and provide port graph rewriting rules to
perform computations on functional dependencies. Using these rules we present a
strategic graph program to find the transitive closure of a set of functional
dependencies. This program is sound, complete and terminating, assuming that
there are no cyclical dependencies in the schema.
</summary>
    <author>
      <name>János Varga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.288.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.288.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TERMGRAPH 2018, arXiv:1902.01510</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 288, 2019, pp. 50-62</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.02013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03338v2</id>
    <updated>2019-02-12T19:07:20Z</updated>
    <published>2019-02-09T00:23:56Z</published>
    <title>WarpFlow: Exploring Petabytes of Space-Time Data</title>
    <summary>  WarpFlow is a fast, interactive data querying and processing system with a
focus on petabyte-scale spatiotemporal datasets and Tesseract queries. With the
rapid growth in smartphones and mobile navigation services, we now have an
opportunity to radically improve urban mobility and reduce friction in how
people and packages move globally every minute-mile, with data. WarpFlow speeds
up three key metrics for data engineers working on such datasets --
time-to-first-result, time-to-full-scale-result, and time-to-trained-model for
machine learning.
</summary>
    <author>
      <name>Catalin Popescu</name>
    </author>
    <author>
      <name>Deepak Merugu</name>
    </author>
    <author>
      <name>Giao Nguyen</name>
    </author>
    <author>
      <name>Shiva Shivakumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03948v1</id>
    <updated>2019-02-11T15:46:54Z</updated>
    <published>2019-02-11T15:46:54Z</published>
    <title>Scaling Big Data Platform for Big Data Pipeline</title>
    <summary>  Monitoring and Managing High Performance Computing (HPC) systems and
environments generate an ever growing amount of data. Making sense of this data
and generating a platform where the data can be visualized for system
administrators and management to proactively identify system failures or
understand the state of the system requires the platform to be as efficient and
scalable as the underlying database tools used to store and analyze the data.
In this paper we will show how we leverage Accumulo, d4m, and Unity to generate
a 3D visualization platform to monitor and manage the Lincoln Laboratory
Supercomputer systems and how we have had to retool our approach to scale with
our systems.
</summary>
    <author>
      <name>Rebecca Wild</name>
    </author>
    <author>
      <name>Matthew Hubbell</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to MIT Northeast Database Day 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04938v1</id>
    <updated>2019-02-13T14:54:40Z</updated>
    <published>2019-02-13T14:54:40Z</published>
    <title>Snapshot Semantics for Temporal Multiset Relations (Extended Version)</title>
    <summary>  Snapshot semantics is widely used for evaluating queries over temporal data:
temporal relations are seen as sequences of snapshot relations, and queries are
evaluated at each snapshot. In this work, we demonstrate that current
approaches for snapshot semantics over interval-timestamped multiset relations
are subject to two bugs regarding snapshot aggregation and bag difference. We
introduce a novel temporal data model based on K-relations that overcomes these
bugs and prove it to correctly encode snapshot semantics. Furthermore, we
present an efficient implementation of our model as a database middleware and
demonstrate experimentally that our approach is competitive with native
implementations and significantly outperforms such implementations on queries
that involve aggregation.
</summary>
    <author>
      <name>Anton Dignös</name>
    </author>
    <author>
      <name>Boris Glavic</name>
    </author>
    <author>
      <name>Xing Niu</name>
    </author>
    <author>
      <name>Michael Böhlen</name>
    </author>
    <author>
      <name>Johann Gamper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of PVLDB paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02949v2</id>
    <updated>2019-03-15T01:33:33Z</updated>
    <published>2019-03-07T14:46:04Z</published>
    <title>SAVIME: A Multidimensional System for the Analysis and Visualization of
  Simulation Data</title>
    <summary>  Scientific applications produce a huge amount of data, which imposes serious
management and analysis challenges. In particular, limitations in current
database management systems prevent their adoption in simulation applications,
in which in-situ analysis libraries, in-transit I/O interfaces and scientific
format files are preferred over DBMSs. In order to make simulation applications
benefit from DBMS support, the author proposes the development of a system
called SAVIME in the context of his PhD thesis. SAVIME is an array database
system designed to manage numerical simulation data. In this document, the
author presents all work conducted so far and the current state of development.
</summary>
    <author>
      <name>Hermano Lustosa</name>
    </author>
    <author>
      <name>Fabio Porto</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02949v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02949v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.05228v1</id>
    <updated>2019-03-12T21:29:08Z</updated>
    <published>2019-03-12T21:29:08Z</published>
    <title>Distributed Dependency Discovery</title>
    <summary>  We analyze the problem of discovering dependencies from distributed big data.
Existing (non-distributed) algorithms focus on minimizing computation by
pruning the search space of possible dependencies. However, distributed
algorithms must also optimize communication costs, especially in shared-nothing
settings, leading to a more complex optimization space. To understand this
space, we introduce six primitives shared by existing dependency discovery
algorithms, corresponding to data processing steps separated by communication
barriers. Through case studies, we show how the primitives allow us to analyze
the design space and develop communication-optimized implementations. Finally,
we support our analysis with an experimental evaluation on real datasets.
</summary>
    <author>
      <name>Hemant Saxena</name>
    </author>
    <author>
      <name>Lukasz Golab</name>
    </author>
    <author>
      <name>Ihab F. Ilyas</name>
    </author>
    <link href="http://arxiv.org/abs/1903.05228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10579v1</id>
    <updated>2019-03-25T20:08:22Z</updated>
    <published>2019-03-25T20:08:22Z</published>
    <title>Categorical Data Integration for Computational Science</title>
    <summary>  Categorical Query Language is an open-source query and data integration
scripting language that can be applied to common challenges in the field of
computational science. We discuss how the structure-preserving nature of CQL
data migrations protect those who publicly share data from the
misinterpretation of their data. Likewise, this feature of CQL migrations
allows those who draw from public data sources to be sure only data which meets
their specification will actually be transferred. We argue some open problems
in the field of data sharing in computational science are addressable by
working within this paradigm of functorial data migration. We demonstrate these
tools by integrating data from the Open Quantum Materials Database with some
alternative materials databases.
</summary>
    <author>
      <name>Kristopher Brown</name>
    </author>
    <author>
      <name>David I. Spivak</name>
    </author>
    <author>
      <name>Ryan Wisnesky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00079v4</id>
    <updated>2021-01-27T22:47:22Z</updated>
    <published>2019-03-29T20:17:48Z</published>
    <title>Query the model: precomputations for efficient inference with Bayesian
  Networks</title>
    <summary>  Variable Elimination is a fundamental algorithm for probabilistic inference
over Bayesian networks. In this paper, we propose a novel materialization
method for Variable Elimination, which can lead to significant efficiency gains
when answering inference queries. We evaluate our technique using real-world
Bayesian networks. Our results show that a modest amount of materialization can
lead to significant improvements in the running time of queries. Furthermore,
in comparison with junction tree methods that also rely on materialization, our
approach achieves comparable efficiency during inference using significantly
lighter materialization.
</summary>
    <author>
      <name>Cigdem Aslay</name>
    </author>
    <author>
      <name>Martino Ciaperoni</name>
    </author>
    <author>
      <name>Aristides Gionis</name>
    </author>
    <author>
      <name>Michael Mathioudakis</name>
    </author>
    <link href="http://arxiv.org/abs/1904.00079v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00079v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.01614v3</id>
    <updated>2019-06-06T08:38:41Z</updated>
    <published>2019-04-02T18:39:16Z</published>
    <title>Persistent Memory I/O Primitives</title>
    <summary>  I/O latency and throughput is one of the major performance bottlenecks for
disk-based database systems. Upcoming persistent memory (PMem) technologies,
like Intel's Optane DC Persistent Memory Modules, promise to bridge the gap
between NAND-based flash (SSD) and DRAM, and thus eliminate the I/O bottleneck.
In this paper, we provide one of the first performance evaluations of PMem in
terms of bandwidth and latency. Based on the results, we develop guidelines for
efficient PMem usage and two essential I/O primitives tuned for PMem: log
writing and block flushing.
</summary>
    <author>
      <name>Alexander van Renen</name>
    </author>
    <author>
      <name>Lukas Vogel</name>
    </author>
    <author>
      <name>Viktor Leis</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <author>
      <name>Alfons Kemper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, DaMoN 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.01614v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.01614v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03403v1</id>
    <updated>2019-04-06T09:48:57Z</updated>
    <published>2019-04-06T09:48:57Z</published>
    <title>Inconsistency Measures for Relational Databases</title>
    <summary>  In this paper, building on work done on measuring inconsistency in knowledge
bases, we introduce inconsistency measures for databases. In particular,
focusing on databases with denial constraints, we first consider the natural
approach of virtually transforming a database into a propositional knowledge
base and then applying well-known measures. However, using this method, tuples
and constraints are equally considered in charge of inconsistencies. Then, we
introduce a version of inconsistency measures blaming database tuples only,
i.e., treating integrity constraints as irrefutable statements.
  We analyze the compliance of database inconsistency measures with standard
rationality postulates and find interesting relationships between measures.
Finally, we investigate the complexity of the inconsistency measurement problem
as well as of the problems of deciding whether the inconsistency is lower than,
greater than, or equal to a given threshold.
</summary>
    <author>
      <name>Francesco Parisi</name>
    </author>
    <author>
      <name>John Grant</name>
    </author>
    <link href="http://arxiv.org/abs/1904.03403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03604v1</id>
    <updated>2019-04-07T08:22:53Z</updated>
    <published>2019-04-07T08:22:53Z</published>
    <title>BriskStream: Scaling Data Stream Processing on Shared-Memory Multicore
  Architectures</title>
    <summary>  We introduce BriskStream, an in-memory data stream processing system (DSPSs)
specifically designed for modern shared-memory multicore architectures.
BriskStream's key contribution is an execution plan optimization paradigm,
namely RLAS, which takes relative-location (i.e., NUMA distance) of each pair
of producer-consumer operators into consideration. We propose a branch and
bound based approach with three heuristics to resolve the resulting nontrivial
optimization problem. The experimental evaluations demonstrate that BriskStream
yields much higher throughput and better scalability than existing DSPSs on
multi-core architectures when processing different types of workloads.
</summary>
    <author>
      <name>Shuhao Zhang</name>
    </author>
    <author>
      <name>Jiong He</name>
    </author>
    <author>
      <name>Amelie Chi Zhou</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3299869.3300067</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3299869.3300067" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SIGMOD'19</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGMOD/PODS International Conference on Management of Data
  2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.03604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03934v1</id>
    <updated>2019-04-08T10:26:35Z</updated>
    <published>2019-04-08T10:26:35Z</published>
    <title>On matrices and $K$-relations</title>
    <summary>  We show that the matrix query language $\mathsf{MATLANG}$ corresponds to a
natural fragment of the positive relational algebra on $K$-relations. The
fragment is defined by introducing a composition operator and restricting
$K$-relation arities to two. We then proceed to show that $\mathsf{MATLANG}$
can express all matrix queries expressible in the positive relational algebra
on $K$-relations, when intermediate arities are restricted to three. Thus we
offer an analogue, in a model with numerical data, to the situation in
classical logic, where the algebra of binary relations is equivalent to
first-order logic with three variables.
</summary>
    <author>
      <name>Robert Brijder</name>
    </author>
    <author>
      <name>Marc Gyssens</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08223v1</id>
    <updated>2019-04-17T12:29:28Z</updated>
    <published>2019-04-17T12:29:28Z</published>
    <title>Estimating Cardinalities with Deep Sketches</title>
    <summary>  We introduce Deep Sketches, which are compact models of databases that allow
us to estimate the result sizes of SQL queries. Deep Sketches are powered by a
new deep learning approach to cardinality estimation that can capture
correlations between columns, even across tables. Our demonstration allows
users to define such sketches on the TPC-H and IMDb datasets, monitor the
training process, and run ad-hoc queries against trained sketches. We also
estimate query cardinalities with HyPer and PostgreSQL to visualize the gains
over traditional cardinality estimators.
</summary>
    <author>
      <name>Andreas Kipf</name>
    </author>
    <author>
      <name>Dimitri Vorona</name>
    </author>
    <author>
      <name>Jonas Müller</name>
    </author>
    <author>
      <name>Thomas Kipf</name>
    </author>
    <author>
      <name>Bernhard Radke</name>
    </author>
    <author>
      <name>Viktor Leis</name>
    </author>
    <author>
      <name>Peter Boncz</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <author>
      <name>Alfons Kemper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SIGMOD'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.08223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.12626v1</id>
    <updated>2019-04-18T05:27:09Z</updated>
    <published>2019-04-18T05:27:09Z</published>
    <title>tsmp: An R Package for Time Series with Matrix Profile</title>
    <summary>  This article describes tsmp, an R package that implements the matrix profile
concept for time series. The tsmp package is a toolkit that allows all-pairs
similarity joins, motif, discords and chains discovery, semantic segmentation,
etc. Here we describe how the tsmp package may be used by showing some of the
use-cases from the original articles and evaluate the algorithm speed in the R
environment. This package can be downloaded at
https://CRAN.R-project.org/package=tsmp.
</summary>
    <author>
      <name>Francisco Bischoff</name>
    </author>
    <author>
      <name>Pedro Pereira Rodrigues</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.32614/RJ-2020-021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.32614/RJ-2020-021" rel="related"/>
    <link href="http://arxiv.org/abs/1904.12626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.13164v1</id>
    <updated>2019-04-30T11:29:00Z</updated>
    <published>2019-04-30T11:29:00Z</published>
    <title>Learning Restricted Regular Expressions with Interleaving</title>
    <summary>  The advantages for the presence of an XML schema for XML documents are
numerous. However, many XML documents in practice are not accompanied by a
schema or by a valid schema. Relax NG is a popular and powerful schema
language, which supports the unconstrained interleaving operator. Focusing on
the inference of Relax NG, we propose a new subclass of regular expressions
with interleaving and design a polynomial inference algorithm. Then we
conducted a series of experiments based on large-scale real data and on three
XML data corpora, and experimental results show that our subclass has a better
practicality than previous ones, and the regular expressions inferred by our
algorithm are more precise.
</summary>
    <author>
      <name>Chunmei Dong</name>
    </author>
    <author>
      <name>Yeting Li</name>
    </author>
    <author>
      <name>Haiming Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1904.13164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.13164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00781v1</id>
    <updated>2019-05-30T20:10:14Z</updated>
    <published>2019-05-30T20:10:14Z</published>
    <title>Learning Semantic Annotations for Tabular Data</title>
    <summary>  The usefulness of tabular data such as web tables critically depends on
understanding their semantics. This study focuses on column type prediction for
tables without any meta data. Unlike traditional lexical matching-based
methods, we propose a deep prediction model that can fully exploit a table's
contextual semantics, including table locality features learned by a Hybrid
Neural Network (HNN), and inter-column semantics features learned by a
knowledge base (KB) lookup and query answering algorithm.It exhibits good
performance not only on individual table sets, but also when transferring from
one table set to another.
</summary>
    <author>
      <name>Jiaoyan Chen</name>
    </author>
    <author>
      <name>Ernesto Jimenez-Ruiz</name>
    </author>
    <author>
      <name>Ian Horrocks</name>
    </author>
    <author>
      <name>Charles Sutton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.00781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05366v2</id>
    <updated>2020-09-03T17:19:13Z</updated>
    <published>2019-06-12T20:10:29Z</published>
    <title>Geo-L: Linking Geospatial Data Made Easy</title>
    <summary>  Geospatial Linked Data is an emerging domain with growing interest in
research and industry. There is an increasing number of publicly available
geospatial Linked Data resources and they need to be interlinked and easily
integrated with private and industrial Linked Data on the Web. The present
paper introduces Geo-L, a system for discovery of RDF spatial links based on
topological relations. Experiments show that the proposed system improves
state-of-the-art spatial linking processes in terms of mapping-time and
-accuracy, as well as concerning resources retrieval efficiency and robustness.
</summary>
    <author>
      <name>Christian Zinke-Wehlmann</name>
    </author>
    <author>
      <name>Amit Kirschenbaum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 10 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05366v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05366v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06085v1</id>
    <updated>2019-06-14T09:16:16Z</updated>
    <published>2019-06-14T09:16:16Z</published>
    <title>DeepSPACE: Approximate Geospatial Query Processing with Deep Learning</title>
    <summary>  The amount of the available geospatial data grows at an ever faster pace.
This leads to the constantly increasing demand for processing power and storage
in order to provide data analysis in a timely manner. At the same time, a lot
of geospatial processing is visual and exploratory in nature, thus having
bounded precision requirements. We present DeepSPACE, a deep learning-based
approximate geospatial query processing engine which combines modest hardware
requirements with the ability to answer flexible aggregation queries while
keeping the required state to a few hundred KiBs.
</summary>
    <author>
      <name>Dimitri Vorona</name>
    </author>
    <author>
      <name>Andreas Kipf</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <author>
      <name>Alfons Kemper</name>
    </author>
    <link href="http://arxiv.org/abs/1906.06085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08574v1</id>
    <updated>2019-06-20T12:22:00Z</updated>
    <published>2019-06-20T12:22:00Z</published>
    <title>Extracting Basic Graph Patterns from Triple Pattern Fragment Logs</title>
    <summary>  The Triple Pattern Fragment (TPF) approach is de-facto a new way to publish
Linked Data at low cost and with high server availability. However, data
providers hosting TPF servers are not able to analyze the SPARQL queries they
execute because they only receive and evaluate queries with one triple pattern.
In this paper, we propose LIFT: an algorithm to extract Basic Graph Patterns
(BGPs) of executed queries from TPF server logs. Experiments show that LIFT
extracts BGPs with good precision and good recall generating limited noise.
</summary>
    <author>
      <name>Nassopoulos Georges</name>
    </author>
    <author>
      <name>Serrano-Alvarado Patricia</name>
    </author>
    <author>
      <name>Molli Pascal</name>
    </author>
    <author>
      <name>Desmontils Emmanuel</name>
    </author>
    <link href="http://arxiv.org/abs/1906.08574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00146v1</id>
    <updated>2019-06-29T04:45:37Z</updated>
    <published>2019-06-29T04:45:37Z</published>
    <title>DataPop: Knowledge Base Population using Distributed Voice Enabled
  Devices</title>
    <summary>  Data scientists are constantly creating methods to efficiently and accurately
populate big data sets for use in large-scale applications. Many recent efforts
utilize crowd-sourcing and textual interfaces. In this paper, we propose a new
method of curating data; namely, creating a multi-device Amazon Alexa Skill in
the form of a research trivia game. Users experience a synchronized gaming
experience with other Amazon Echo users, competing against one another while
filling in gaps of a connected knowledge base. This allows for full
exploitation of the speed improvement offered by voice interface technology in
a game-based format.
</summary>
    <author>
      <name>Elena Montes</name>
    </author>
    <author>
      <name>Monique Shotande</name>
    </author>
    <author>
      <name>Daniel Helm</name>
    </author>
    <author>
      <name>Christan Grant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 references, unsubmitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05618v1</id>
    <updated>2019-07-12T08:35:23Z</updated>
    <published>2019-07-12T08:35:23Z</published>
    <title>Detecting coherent explorations in SQL workloads</title>
    <summary>  This paper presents a proposal aiming at better understanding a workload of
SQL queries and detecting coherent explorations hidden within the workload. In
particular, our work investigates SQLShare [11], a database-as-a-service
platform targeting scientists and data scientists with minimal database
experience, whose workload was made available to the research community.
According to the authors of [11], this workload is the only one containing
primarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed
this workload by extracting features that characterize SQL queries and we show
how to use these features to separate sequences of SQL queries into meaningful
explorations. We ran several tests over various query workloads to validate
empirically our approach.
</summary>
    <author>
      <name>Veronika Peralta</name>
    </author>
    <author>
      <name>Patrick Marcel</name>
    </author>
    <author>
      <name>Willeme Verdeaux</name>
    </author>
    <author>
      <name>Aboubakar Sidikhy Diakhaby</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.09535v1</id>
    <updated>2019-07-22T19:11:49Z</updated>
    <published>2019-07-22T19:11:49Z</published>
    <title>Association rule mining and itemset-correlation based variants</title>
    <summary>  Association rules express implication formed relations among attributes in
databases of itemsets. The apriori algorithm is presented, the basis for most
association rule mining algorithms. It works by pruning away rules that need
not be evaluated based on the user specified minimum support confidence.
Additionally, variations of the algorithm are presented that enable it to
handle quantitative attributes and to extract rules about generalizations of
items, but preserve the downward closure property that enables pruning.
Intertransformation of the extensions is proposed for special cases.
</summary>
    <author>
      <name>Niels Mündler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE format, 6 pages, 4 figures, seminar paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.09535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05146v2</id>
    <updated>2020-11-01T11:10:20Z</updated>
    <published>2019-10-11T12:57:17Z</published>
    <title>Analysis of Co-Occurrence Patterns in Data through Modular and Clan
  Decompositions of Gaifman Graphs</title>
    <summary>  We argue that the existing knowledge about modular decomposition of graphs
and clan decomposition of 2-structures can be put to use advantageously in a
context of data analysis. We show how to obtain visual descriptions of
co-occurrence patterns by employing these decompositions on possibly
generalized Gaifman graphs associated to datasets. We provide both theoretical
advances that connect the proposed process to other data mining aspects
(namely, closed set mining), as well as implemented algorithmics leading to an
open-source tool that demonstrates our approach.
</summary>
    <author>
      <name>Marie Ely Piceno</name>
    </author>
    <author>
      <name>José Luis Balcázar</name>
    </author>
    <link href="http://arxiv.org/abs/1910.05146v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05146v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10350v1</id>
    <updated>2019-10-23T04:53:16Z</updated>
    <published>2019-10-23T04:53:16Z</published>
    <title>A Queue-oriented Transaction Processing Paradigm</title>
    <summary>  Transaction processing has been an active area of research for several
decades. A fundamental characteristic of classical transaction processing
protocols is non-determinism, which causes them to suffer from performance
issues on modern computing environments such as main-memory databases using
many-core, and multi-socket CPUs and distributed environments. Recent proposals
of deterministic transaction processing techniques have shown great potential
in addressing these performance issues. In this position paper, I argue for a
queue-oriented transaction processing paradigm that leads to better design and
implementation of deterministic transaction processing protocols. I support my
approach with extensive experimental evaluations and demonstrate significant
performance gains.
</summary>
    <author>
      <name>Thamir M. Qadah</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00580v1</id>
    <updated>2019-12-02T05:05:39Z</updated>
    <published>2019-12-02T05:05:39Z</published>
    <title>Multi-version Indexing in Flash-based Key-Value Stores</title>
    <summary>  Maintaining multiple versions of data is popular in key-value stores since it
increases concurrency and improves performance. However, designing a
multi-version key-value store entails several challenges, such as additional
capacity for storing extra versions and an indexing mechanism for mapping
versions of a key to their values. We present SkimpyFTL, a FTL-integrated
multi-version key-value store that exploits the remap-on-write property of
flash-based SSDs for multi-versioning and provides a tradeoff between memory
capacity and lookup latency for indexing.
</summary>
    <author>
      <name>Pulkit A. Misra</name>
    </author>
    <author>
      <name>Jeffrey S. Chase</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Alvin R. Lebeck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02454v1</id>
    <updated>2020-12-04T08:18:48Z</updated>
    <published>2020-12-04T08:18:48Z</published>
    <title>Data Lakes for Digital Humanities</title>
    <summary>  Traditional data in Digital Humanities projects bear various formats
(structured, semi-structured, textual) and need substantial transformations
(encoding and tagging, stemming, lemmatization, etc.) to be managed and
analyzed. To fully master this process, we propose the use of data lakes as a
solution to data siloing and big data variety problems. We describe data lake
projects we currently run in close collaboration with researchers in humanities
and social sciences and discuss the lessons learned running these projects.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Cécile Favre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Sabine Loudcher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Camille Noûs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3423603.3424004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3423603.3424004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Data and Digital Humanities Track</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2nd International Digital Tools &amp; Uses Congress (DTUC 2020), Oct
  2020, Hammamet, Tunisia. pp.38-41</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.02454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02619v3</id>
    <updated>2020-12-08T11:17:14Z</updated>
    <published>2020-12-04T14:26:21Z</published>
    <title>Computational Complexity of Three Central Problems in Itemset Mining</title>
    <summary>  Itemset mining is one of the most studied tasks in knowledge discovery. In
this paper we analyze the computational complexity of three central itemset
mining problems. We prove that mining confident rules with a given item in the
head is NP-hard. We prove that mining high utility itemsets is NP-hard. We
finally prove that mining maximal or closed itemsets is coNP-hard as soon as
the users can specify constraints on the kind of itemsets they are interested
in.
</summary>
    <author>
      <name>Christian Bessiere</name>
    </author>
    <author>
      <name>Mohamed-Bachir Belaid</name>
    </author>
    <author>
      <name>Nadjib Lazaar</name>
    </author>
    <link href="http://arxiv.org/abs/2012.02619v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02619v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07108v1</id>
    <updated>2020-12-13T17:25:26Z</updated>
    <published>2020-12-13T17:25:26Z</published>
    <title>Knowledge Graph Management on the Edge</title>
    <summary>  Edge computing emerges as an innovative platform for services requiring low
latency decision making. Its success partly depends on the existence of
efficient data management systems. We consider that knowledge graph management
systems have a key role to play in this context due to their data integration
and reasoning features. In this paper, we present SuccinctEdge, a compact,
decompression-free, self-index, in-memory RDF store that can answer SPARQL
queries, including those requiring reasoning services associated to some
ontology. We provide details on its design and implementation before
demonstrating its efficiency on real-world and synthetic datasets.
</summary>
    <author>
      <name>Weiqin Xu</name>
    </author>
    <author>
      <name>Olivier Curé</name>
    </author>
    <author>
      <name>Philippe Calvez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 14 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07309v1</id>
    <updated>2020-12-14T07:49:04Z</updated>
    <published>2020-12-14T07:49:04Z</published>
    <title>Event Data Quality: A Survey</title>
    <summary>  Event data are prevalent in diverse domains such as financial trading,
business workflows and industrial IoT nowadays. An event is often characterized
by several attributes denoting the meaning associated with the corresponding
occurrence time/duration. From traditional operational systems in enterprises
to online systems for Web services, event data is generated from physical world
uninterruptedly. However, due to the variety and veracity features of Big data,
event data generated from heterogeneous and dirty sources could have very
different event representations and data quality issues. In this work, we
summarize several typical works on studying data quality issues of event data,
including: (1) event matching, (2) event error detection, (3) event data
repair, and (4) approximate pattern matching.
</summary>
    <author>
      <name>Ruihong Huang</name>
    </author>
    <author>
      <name>Jianmin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2012.07309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.11269v3</id>
    <updated>2021-05-03T18:52:50Z</updated>
    <published>2020-12-21T11:54:35Z</published>
    <title>A Journey to the Frontiers of Query Rewritability</title>
    <summary>  This paper is about (first order) query rewritability in the context of
theory-mediated query answering. The starting point of our journey is the
FUS/FES conjecture, saying that if a theory is core-terminating (FES) and
admits query rewriting (BDD, FUS) then it is uniformly bounded. We show that
this conjecture is true for a wide class of "local" BDD theories. Then we ask
how non-local can a BDD theory actually be and we discover phenomena which we
think are quite counter-intuitive.
</summary>
    <author>
      <name>Piotr Ostropolski-Nalewaja</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <author>
      <name>David Carral</name>
    </author>
    <author>
      <name>Sebastian Rudolph</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removed faulty observation, fixed everything that depended on it</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.11269v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.11269v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13677v1</id>
    <updated>2020-12-26T04:45:40Z</updated>
    <published>2020-12-26T04:45:40Z</published>
    <title>Toward Compact Data from Big Data</title>
    <summary>  Bigdata is a dataset of which size is beyond the ability of handling a
valuable raw material that can be refined and distilled into valuable specific
insights. Compact data is a method that optimizes the big dataset that gives
best assets without handling complex bigdata. The compact dataset contains the
maximum knowledge patterns at fine grained level for effective and personalized
utilization of bigdata systems without bigdata. The compact data method is a
tailor-made design which depends on problem situations. Various compact data
techniques have been demonstrated into various data-driven research area in the
paper.
</summary>
    <author>
      <name> Song-Kyoo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amang</arxiv:affiliation>
    </author>
    <author>
      <name> Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted in the 2020 IEEE-ICITIS Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.13677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00170v3</id>
    <updated>2022-05-02T04:50:28Z</updated>
    <published>2021-01-01T05:41:44Z</published>
    <title>Visualization Techniques with Data Cubes: Utilizing Concurrency for
  Complex Data</title>
    <summary>  With web and mobile platforms becoming more prominent devices utilized in
data analysis, there are currently few systems which are not without flaw. In
order to increase the performance of these systems and decrease errors of data
oversimplification, we seek to understand how other programming languages can
be used across these platforms which provide data and type safety, as well as
utilizing concurrency to perform complex data manipulation tasks.
</summary>
    <author>
      <name>Daniel Szelogowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures Update: Revised format to align closer to IEEE
  standards</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00170v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00170v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; E.5; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00171v2</id>
    <updated>2021-04-19T22:34:35Z</updated>
    <published>2021-01-01T05:42:38Z</published>
    <title>Optimizing Data Cube Visualization for Web Applications: Performance and
  User-Friendly Data Aggregation</title>
    <summary>  Current open source applications which allow for cross-platform data
visualization of OLAP cubes feature issues of high overhead and inconsistency
due to data oversimplification. To improve upon this issue, there is a need to
cut down the number of pipelines that the data must travel between for these
aggregation operations and create a single, unified application which performs
efficiently without sacrificing data, and allows for ease of usability and
extension.
</summary>
    <author>
      <name>Daniel Szelogowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 3 tables Update: Revised format to align closer
  to IEEE standards</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00171v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00171v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; E.5; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02174v1</id>
    <updated>2021-01-06T18:22:52Z</updated>
    <published>2021-01-06T18:22:52Z</published>
    <title>Efficient Discovery of Approximate Order Dependencies</title>
    <summary>  Order dependencies (ODs) capture relationships between ordered domains of
attributes. Approximate ODs (AODs) capture such relationships even when there
exist exceptions in the data. During automated discovery of ODs, validation is
the process of verifying whether an OD holds. We present an algorithm for
validating approximate ODs with significantly improved runtime performance over
existing methods for AODs, and prove that it is correct and has optimal
runtime. By replacing the validation step in a leading algorithm for
approximate OD discovery with ours, we achieve orders-of-magnitude improvements
in performance.
</summary>
    <author>
      <name>Reza Karegar</name>
    </author>
    <author>
      <name>Parke Godfrey</name>
    </author>
    <author>
      <name>Lukasz Golab</name>
    </author>
    <author>
      <name>Mehdi Kargar</name>
    </author>
    <author>
      <name>Divesh Srivastava</name>
    </author>
    <author>
      <name>Jaroslaw Szlichta</name>
    </author>
    <link href="http://arxiv.org/abs/2101.02174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06637v1</id>
    <updated>2021-01-17T10:17:06Z</updated>
    <published>2021-01-17T10:17:06Z</published>
    <title>AMALGAM: A Matching Approach to fairfy tabuLar data with knowledGe grAph
  Model</title>
    <summary>  In this paper we present AMALGAM, a matching approach to fairify tabular data
with the use of a knowledge graph. The ultimate goal is to provide fast and
efficient approach to annotate tabular data with entities from a background
knowledge. The approach combines lookup and filtering services combined with
text pre-processing techniques. Experiments conducted in the context of the
2020 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching with
both Column Type Annotation and Cell Type Annotation tasks showed promising
results.
</summary>
    <author>
      <name>Rabia Azzi</name>
    </author>
    <author>
      <name>Gayo Diallo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06139v1</id>
    <updated>2021-02-11T17:28:52Z</updated>
    <published>2021-02-11T17:28:52Z</published>
    <title>A GeoSPARQL Compliance Benchmark</title>
    <summary>  We propose a series of tests that check for the compliance of RDF
triplestores with the GeoSPARQL standard. The purpose of the benchmark is to
test how many of the requirements outlined in the standard a tested system
supports and to push triplestores forward in achieving a full GeoSPARQL
compliance. This topic is of concern because the support of GeoSPARQL varies
greatly between different triplestore implementations, and such support is of
great importance for the domain of geospatial RDF data. Additionally, we
present a comprehensive comparison of triplestores, providing an insight into
their current GeoSPARQL support.
</summary>
    <author>
      <name>Milos Jovanovik</name>
    </author>
    <author>
      <name>Timo Homburg</name>
    </author>
    <author>
      <name>Mirko Spasić</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/ijgi10070487</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/ijgi10070487" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ISPRS International Journal of Geo-Information. 2021; 10(7):487</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.06139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06563v3</id>
    <updated>2021-08-30T21:34:25Z</updated>
    <published>2021-02-12T15:00:51Z</published>
    <title>Querying collections of tree-structured records in the presence of
  within-record referential constraints</title>
    <summary>  In this paper, we consider a tree-structured data model used in many
commercial databases like Dremel, F1, JSON stores. We define identity and
referential constraints within each tree-structured record. The query language
is a variant of SQL and flattening is used as an evaluation mechanism. We
investigate querying in the presence of these constraints, and point out the
challenges that arise from taking them into account during query evaluation.
</summary>
    <author>
      <name>Foto N. Afrati</name>
    </author>
    <author>
      <name>Matthew Damigos</name>
    </author>
    <link href="http://arxiv.org/abs/2102.06563v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06563v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08228v1</id>
    <updated>2021-02-16T15:44:27Z</updated>
    <published>2021-02-16T15:44:27Z</published>
    <title>Data provenance, curation and quality in metrology</title>
    <summary>  Data metrology -- the assessment of the quality of data -- particularly in
scientific and industrial settings, has emerged as an important requirement for
the UK National Physical Laboratory (NPL) and other national metrology
institutes. Data provenance and data curation are key components for emerging
understanding of data metrology. However, to date provenance research has had
limited visibility to or uptake in metrology. In this work, we summarize a
scoping study carried out with NPL staff and industrial participants to
understand their current and future needs for provenance, curation and data
quality. We then survey provenance technology and standards that are relevant
to metrology. We analyse the gaps between requirements and the current state of
the art.
</summary>
    <author>
      <name>James Cheney</name>
    </author>
    <author>
      <name>Adriane Chapman</name>
    </author>
    <author>
      <name>Joy Davidson</name>
    </author>
    <author>
      <name>Alistair Forbes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/9789811242380_0009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/9789811242380_0009" rel="related"/>
    <link href="http://arxiv.org/abs/2102.08228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08942v1</id>
    <updated>2021-02-17T18:56:03Z</updated>
    <published>2021-02-17T18:56:03Z</published>
    <title>A Survey on Locality Sensitive Hashing Algorithms and their Applications</title>
    <summary>  Finding nearest neighbors in high-dimensional spaces is a fundamental
operation in many diverse application domains. Locality Sensitive Hashing (LSH)
is one of the most popular techniques for finding approximate nearest neighbor
searches in high-dimensional spaces. The main benefits of LSH are its
sub-linear query performance and theoretical guarantees on the query accuracy.
In this survey paper, we provide a review of state-of-the-art LSH and
Distributed LSH techniques. Most importantly, unlike any other prior survey, we
present how Locality Sensitive Hashing is utilized in different application
domains.
</summary>
    <author>
      <name>Omid Jafari</name>
    </author>
    <author>
      <name>Preeti Maurya</name>
    </author>
    <author>
      <name>Parth Nagarkar</name>
    </author>
    <author>
      <name>Khandker Mushfiqul Islam</name>
    </author>
    <author>
      <name>Chidambaram Crushev</name>
    </author>
    <link href="http://arxiv.org/abs/2102.08942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.13370v1</id>
    <updated>2021-02-26T09:41:04Z</updated>
    <published>2021-02-26T09:41:04Z</published>
    <title>Fast Distributed Complex Join Processing</title>
    <summary>  In this work, we study the problem of co-optimize communication,
pre-computing, and computation cost in one-round multi-way join evaluation. We
propose a multi-way join approach ADJ (Adaptive Distributed Join) for complex
join which finds one optimal query plan to process by exploring cost-effective
partial results in terms of the trade-off between pre-computing, communication,
and computation.We analyze the input relations for a given join query and find
one optimal over a set of query plans in some specific form, with high-quality
cost estimation by sampling. Our extensive experiments confirm that ADJ
outperforms the existing multi-way join methods by up to orders of magnitude.
</summary>
    <author>
      <name>Hao Zhang</name>
    </author>
    <author>
      <name>Miao Qiao</name>
    </author>
    <author>
      <name>Jeffrey Xu Yu</name>
    </author>
    <author>
      <name>Hong Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long Version</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.13370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.13370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.00729v3</id>
    <updated>2021-10-20T09:47:46Z</updated>
    <published>2021-07-01T20:09:56Z</published>
    <title>Essence of Factual Knowledge</title>
    <summary>  Knowledge bases are collections of domain-specific and commonsense facts.
Recently, the sizes of KBs are rocketing due to automatic extraction for
knowledge and facts. For example, the number of facts in WikiData is up to 974
million! According to our observation, current KBs, especially domain KBs, show
strong relevance in relations according to some topics. These patterns can be
used to conclude and infer for part of facts in the KBs. Therefore, the
original KBs can be minimzed by extracting patterns and essential facts.
</summary>
    <author>
      <name>Ruoyu Wang</name>
    </author>
    <author>
      <name>Daniel Sun</name>
    </author>
    <author>
      <name>Guoqiang Li</name>
    </author>
    <author>
      <name>Raymond Wong</name>
    </author>
    <author>
      <name>Shiping Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.00729v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.00729v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.02885v1</id>
    <updated>2021-07-05T14:03:53Z</updated>
    <published>2021-07-05T14:03:53Z</published>
    <title>Data Lake Ingestion Management</title>
    <summary>  Data Lake (DL) is a Big Data analysis solution which ingests raw data in
their native format and allows users to process these data upon usage. Data
ingestion is not a simple copy and paste of data, it is a complicated and
important phase to ensure that ingested data are findable, accessible,
interoperable and reusable at all times. Our solution is threefold. Firstly, we
propose a metadata model that includes information about external data sources,
data ingestion processes, ingested data, dataset veracity and dataset security.
Secondly, we present the algorithms that ensure the ingestion phase (data
storage and metadata instanciation). Thirdly, we introduce a developed metadata
management system whereby users can easily consult different elements stored in
DL.
</summary>
    <author>
      <name>Yan Zhao</name>
    </author>
    <author>
      <name>Imen Megdiche</name>
    </author>
    <author>
      <name>Franck Ravat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.02885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03997v1</id>
    <updated>2021-07-08T17:42:57Z</updated>
    <published>2021-07-08T17:42:57Z</published>
    <title>Probabilistic Trace Alignment</title>
    <summary>  Alignments provide sophisticated diagnostics that pinpoint deviations in a
trace with respect to a process model and their severity. However, approaches
based on trace alignments use crisp process models as reference and recent
probabilistic conformance checking approaches check the degree of conformance
of an event log with respect to a stochastic process model instead of finding
trace alignments. In this paper, for the first time, we provide a conformance
checking approach based on trace alignments using stochastic Workflow nets.
Conceptually, this requires to handle the two possibly contrasting forces of
the cost of the alignment on the one hand and the likelihood of the model trace
with respect to which the alignment is computed on the other.
</summary>
    <author>
      <name>Giacomo Bergami</name>
    </author>
    <author>
      <name>Fabrizio Maria Maggi</name>
    </author>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Rafael Peñaloza</name>
    </author>
    <link href="http://arxiv.org/abs/2107.03997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04027v1</id>
    <updated>2021-07-05T07:56:27Z</updated>
    <published>2021-07-05T07:56:27Z</published>
    <title>goldMEDAL : une nouvelle contribution {à} la mod{é}lisation
  g{é}n{é}rique des m{é}tadonn{é}es des lacs de donn{é}es</title>
    <summary>  We summarize here a paper published in 2021 in the DOLAP international
workshop DOLAP associated with the EDBT and ICDT conferences. We propose
goldMEDAL, a generic metadata model for data lakes based on four concepts and a
three-level modeling: conceptual, logical and physical.
</summary>
    <author>
      <name>Etienne Scholly</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Pegdwendé Sawadogo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Pengfei Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Javier Espinosa-Oviedo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Cécile Favre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Sabine Loudcher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Camille Noûs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. 17e journ{\'e}es Business Intelligence et Big Data (EDA
  2021), Jul 2021, Toulouse, France</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.04027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08297v2</id>
    <updated>2021-09-27T16:43:11Z</updated>
    <published>2021-07-17T18:08:51Z</published>
    <title>Spatial Data Generators</title>
    <summary>  This gem describes a standard method for generating synthetic spatial data
that can be used in benchmarking and scalability tests. The goal is to improve
the reproducibility and increase the trust in experiments on synthetic data by
using standard widely acceptable dataset distributions. In addition, this
article describes how to assign a unique identifier to each synthetic dataset
that can be shared in papers for reproducibility of results. Finally, this gem
provides a supplementary material that gives a reference implementation for all
the provided distributions.
</summary>
    <author>
      <name>Tin Vu</name>
    </author>
    <author>
      <name>Sara Migliorini</name>
    </author>
    <author>
      <name>Ahmed Eldawy</name>
    </author>
    <author>
      <name>Alberto Belussi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1st ACM SIGSPATIAL International Workshop on Spatial Gems
  (SpatialGems 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.08297v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08297v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08814v1</id>
    <updated>2021-07-14T06:28:42Z</updated>
    <published>2021-07-14T06:28:42Z</published>
    <title>MARC: Mining Association Rules from datasets by using Clustering models</title>
    <summary>  Association rules are useful to discover relationships, which are mostly
hidden, between the different items in large datasets. Symbolic models are the
principal tools to extract association rules. This basic technique is
time-consuming, and it generates a big number of associated rules. To overcome
this drawback, we suggest a new method, called MARC, to extract the more
important association rules of two important levels: Type I, and Type II. This
approach relies on a multi-topographic unsupervised neural network model as
well as clustering quality measures that evaluate the success of a given
numerical classification model to behave as a natural symbolic model.
</summary>
    <author>
      <name>Shadi Al Shehabi</name>
    </author>
    <author>
      <name>Abdullatif Baba</name>
    </author>
    <link href="http://arxiv.org/abs/2107.08814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.11592v2</id>
    <updated>2021-08-04T16:08:19Z</updated>
    <published>2021-07-24T12:20:36Z</published>
    <title>Blockchain Transaction Processing</title>
    <summary>  A blockchain is an append-only linked-list of blocks, which is maintained at
each participating node. Each block records a set of transactions and their
associated metadata. Blockchain transactions act on the identical ledger data
stored at each node. Blockchain was first perceived by Satoshi Nakamoto as a
peer-to-peer digital-commodity (also known as crypto-currency) exchange system.
Blockchains received traction due to their inherent property of
immutability-once a block is accepted, it cannot be reverted.
</summary>
    <author>
      <name>Suyash Gupta</name>
    </author>
    <author>
      <name>Mohammad Sadoghi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-77525-8_333</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-77525-8_333" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Big Data Technologies 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.11592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.11592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12373v1</id>
    <updated>2021-07-25T20:29:28Z</updated>
    <published>2021-07-25T20:29:28Z</published>
    <title>Relational Boosted Regression Trees</title>
    <summary>  Many tasks use data housed in relational databases to train boosted
regression tree models. In this paper, we give a relational adaptation of the
greedy algorithm for training boosted regression trees. For the subproblem of
calculating the sum of squared residuals of the dataset, which dominates the
runtime of the boosting algorithm, we provide a $(1 + \epsilon)$-approximation
using the tensor sketch technique. Employing this approximation within the
relational boosted regression trees algorithm leads to learning similar model
parameters, but with asymptotically better runtime.
</summary>
    <author>
      <name>Sonia Cromp</name>
    </author>
    <author>
      <name>Alireza Samadian</name>
    </author>
    <author>
      <name>Kirk Pruhs</name>
    </author>
    <link href="http://arxiv.org/abs/2107.12373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13923v1</id>
    <updated>2021-07-29T12:00:11Z</updated>
    <published>2021-07-29T12:00:11Z</published>
    <title>Machine Learning over Static and Dynamic Relational Data</title>
    <summary>  This tutorial overviews principles behind recent works on training and
maintaining machine learning models over relational data, with an emphasis on
the exploitation of the relational data structure to improve the runtime
performance of the learning task.
  The tutorial has the following parts:
  1) Database research for data science
  2) Three main ideas to achieve performance improvements
  2.1) Turn the ML problem into a DB problem
  2.2) Exploit structure of the data and problem
  2.3) Exploit engineering tools of a DB researcher
  3) Avenues for future research
</summary>
    <author>
      <name>Ahmet Kara</name>
    </author>
    <author>
      <name>Milos Nikolic</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <author>
      <name>Haozhe Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2008.07864</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.13923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.14274v1</id>
    <updated>2021-07-29T18:42:47Z</updated>
    <published>2021-07-29T18:42:47Z</published>
    <title>Interactive Region-of-Interest Discovery using Exploratory Feedback</title>
    <summary>  In this paper, we propose a geospatial data management framework called
IRIDEF which captures and analyzes user's exploratory feedback for an enriched
guidance mechanism in the context of interactive analysis. We discuss that
exploratory feedback can be a proxy for decision-making feedback when the
latter is scarce or unavailable. IRIDEF identifies regions of interest (ROIs)
via exploratory feedback and highlights a few interesting and out-of-sight POIs
in each ROI. These highlights enable the user to shape up his/her future
interactions with the system. We detail the components of our proposed
framework in the form of a data analysis pipeline and present the aspects of
efficiency and effectiveness for each component. We also discuss evaluation
plans and future directions for IRIDEF.
</summary>
    <author>
      <name>Behrooz Omidvar-Tehrani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.14274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04727v1</id>
    <updated>2021-08-10T14:44:59Z</updated>
    <published>2021-08-10T14:44:59Z</published>
    <title>Crowdsourced Databases and Sui Generis Rights</title>
    <summary>  In this study we propose a new concept of databases (crowdsourced databases),
adding a new conceptual approach to the debate on legal protection of databases
in Europe. We also summarise the current legal framework and current indexing
and web scraping practices - it would not be prudent to suggest a new theory
without contextualising it in the legal and practical context in which it is
developed.
</summary>
    <author>
      <name>Gonçalo Simões de Almeida</name>
    </author>
    <author>
      <name>Gonçalo Faria Abreu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04556v1</id>
    <updated>2021-11-08T15:12:33Z</updated>
    <published>2021-11-08T15:12:33Z</published>
    <title>Time- and Space-Efficient Regular Path Queries on Graphs</title>
    <summary>  We introduce a time- and space-efficient technique to solve regularpath
queries over labeled graphs. We combine a bit-parallel simula-tion of the
Glushkov automaton of the regular expression with thering index introduced by
Arroyuelo et al., exploiting its wavelettree representation of the triples in
order to efficiently reach thestates of the product graph that are relevant for
the query. Ourquery algorithm is able to simultaneously process several
automa-ton states, as well as several graph nodes/labels. Our
experimentalresults show that our representation uses 3-5 times less space
thanthe alternatives in the literature, while generally outperformingthem in
query times (1.67 times faster than the next best).
</summary>
    <author>
      <name>Diego Arroyuelo</name>
    </author>
    <author>
      <name>Aidan Hogan</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Javiel Rojas-Ledesma</name>
    </author>
    <link href="http://arxiv.org/abs/2111.04556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.12487v1</id>
    <updated>2021-11-24T13:31:03Z</updated>
    <published>2021-11-24T13:31:03Z</published>
    <title>Distributed Evaluation of Graph Queries using Recursive Relational
  Algebra</title>
    <summary>  We present a system called Dist-$\mu$-RA for the distributed evaluation of
recursive graph queries. Dist-$\mu$-RA builds on the recursive relational
algebra and extends it with evaluation plans suited for the distributed
setting. The goal is to offer expressivity for high-level queries while
providing efficiency at scale and reducing communication costs. Experimental
results on both real and synthetic graphs show the effectiveness of the
proposed approach compared to existing systems.
</summary>
    <author>
      <name>Sarah Chlyah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TYREX</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Genevès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TYREX</arxiv:affiliation>
    </author>
    <author>
      <name>Nabil Layaïda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TYREX</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2111.12487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.12835v1</id>
    <updated>2021-11-24T23:06:02Z</updated>
    <published>2021-11-24T23:06:02Z</published>
    <title>SchemaDB: Structures in Relational Datasets</title>
    <summary>  In this paper we introduce the SchemaDB data-set; a collection of relational
database schemata in both sql and graph formats. Databases are not commonly
shared publicly for reasons of privacy and security, so schemata are not
available for study. Consequently, an understanding of database structures in
the wild is lacking, and most examples found publicly belong to common
development frameworks or are derived from textbooks or engine benchmark
designs. SchemaDB contains 2,500 samples of relational schemata found in public
repositories which we have standardised to MySQL syntax. We provide our
gathering and transformation methodology, summary statistics, and structural
analysis, and discuss potential downstream research tasks in several domains.
</summary>
    <author>
      <name>Cody James Christopher</name>
    </author>
    <author>
      <name>Kristen Moore</name>
    </author>
    <author>
      <name>David Liebowitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.12835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00288v1</id>
    <updated>2021-12-01T05:47:39Z</updated>
    <published>2021-12-01T05:47:39Z</published>
    <title>Operation-based Collaborative Data Sharing for Distributed Systems</title>
    <summary>  Collaborative Data Sharing raises a fundamental issue in distributed systems.
Several strategies have been proposed for making shared data consistent between
peers in such a way that the shared part of their local data become equal. Most
of the proposals rely on state-based semantics. But this suffers from a lack of
descriptiveness in conflict-free features of synchronization required for
flexible network connections. Recent applications tend to use non-permanent
connection with mobile devices or allow temporary breakaways from the system,
for example. To settle ourselves in conflict-free data sharing, we propose a
novel scheme "Operation-based Collaborative Data Sharing" that enables
conflict-free strategies for synchronization based on operational semantics.
</summary>
    <author>
      <name>Masato Takeichi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01132v1</id>
    <updated>2021-12-02T11:12:23Z</updated>
    <published>2021-12-02T11:12:23Z</published>
    <title>A Practical Dynamic Programming Approach to Datalog Provenance
  Computation</title>
    <summary>  We establish a translation between a formalism for dynamic programming over
hypergraphs and the computation of semiring-based provenance for Datalog
programs. The benefit of this translation is a new method for computing
provenance for a specific class of semirings. Theoretical and practical
optimizations lead to an efficient implementation using \textsc{Souffl\'e}, a
state-of-the-art Datalog interpreter. Experimental results on real-world data
suggest this approach to be efficient in practical contexts, even competing
with our previous dedicated solutions for computing provenance in annotated
graph databases. The cost overhead compared to plain Datalog evaluation is
fairly moderate in many cases of interest.
</summary>
    <author>
      <name>Yann Ramusat</name>
    </author>
    <author>
      <name>Silviu Maniu</name>
    </author>
    <author>
      <name>Pierre Senellart</name>
    </author>
    <link href="http://arxiv.org/abs/2112.01132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04628v2</id>
    <updated>2023-04-25T06:55:05Z</updated>
    <published>2022-04-10T08:07:33Z</published>
    <title>A Skyline and ranking query odyssey: a journey from skyline and ranking
  queries up to f-skyline queries</title>
    <summary>  Skyline and ranking queries are two of the most used tools to manage large
data sets. The former is based on non-dominance, while the latter on a scoring
function. Despite their effectiveness, they have some drawbacks like the result
size or the need for a utility function that must be taken into account. To do
this, in the last years, new kinds of queries, called flexible skyline queries,
have been developed. In the present article, a description of skyline and
ranking queries, f-skyline queries and a comparison among them are provided to
highlight the improvements achieved and how some limitations have been
overcome.
</summary>
    <author>
      <name>Giuseppe Sorrentino</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04628v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04628v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04898v1</id>
    <updated>2022-04-11T06:53:36Z</updated>
    <published>2022-04-11T06:53:36Z</published>
    <title>PM4Py-GPU: a High-Performance General-Purpose Library for Process Mining</title>
    <summary>  Open-source process mining provides many algorithms for the analysis of event
data which could be used to analyze mainstream processes (e.g., O2C, P2P, CRM).
However, compared to commercial tools, they lack the performance and struggle
to analyze large amounts of data. This paper presents PM4Py-GPU, a Python
process mining library based on the NVIDIA RAPIDS framework. Thanks to the
dataframe columnar storage and the high level of parallelism, a significant
speed-up is achieved on classic process mining computations and processing
activities.
</summary>
    <author>
      <name>Alessandro Berti</name>
    </author>
    <author>
      <name>Minh Phan Nghia</name>
    </author>
    <author>
      <name>Wil M. P. van der Aalst</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06078v1</id>
    <updated>2022-04-12T20:42:28Z</updated>
    <published>2022-04-12T20:42:28Z</published>
    <title>Understanding the compromise between skyline and ranking queries</title>
    <summary>  Skyline and Ranking queries have gained great popularity in the recent years.
These two techniques are crucial for multi-criteria decision support
applications, which are now more popular than ever before. Skyline and Ranking
queries are, however, affected by well-known limitations. In the past recent
years, the database community provided numerous studies in this field with the
aim to overcome the weaknesses of these two approaches. This survey introduces
the reader to Skyline and Ranking queries, explaining the concepts on which
they are based, with the intent to present the compromise between the two
techniques: flexible skylines.
</summary>
    <author>
      <name>Marco Tonnarelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.08941v1</id>
    <updated>2022-04-19T15:19:35Z</updated>
    <published>2022-04-19T15:19:35Z</published>
    <title>CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex</title>
    <summary>  CodexDB is an SQL processing engine whose internals can be customized via
natural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model
which translates text into code. It is a framework on top of GPT-3 Codex that
decomposes complex SQL queries into a series of simple processing steps,
described in natural language. Processing steps are enriched with user-provided
instructions and descriptions of database properties. Codex translates the
resulting text into query processing code. An early prototype of CodexDB is
able to generate correct code for a majority of queries of the WikiSQL
benchmark and can be customized in various ways.
</summary>
    <author>
      <name>Immanuel Trummer</name>
    </author>
    <link href="http://arxiv.org/abs/2204.08941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.08941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10655v1</id>
    <updated>2022-04-22T11:54:38Z</updated>
    <published>2022-04-22T11:54:38Z</published>
    <title>Use of Context in Data Quality Management: a Systematic Literature
  Review</title>
    <summary>  The importance of context in data quality (DQ) was shown many years ago and
nowadays is widely accepted. Early approaches and surveys defined DQ as
\textit{fitness for use} and showed the influence of context on DQ. This paper
presents a Systematic Literature Review (SLR) for investigating how context is
taken into account in recent proposals for DQ management. We specifically
present the planning and execution of the SLR, the analysis criteria and our
results reflecting the relationship between context and DQ in the state of the
art and, particularly, how that context is defined and used for DQ management.
</summary>
    <author>
      <name>Flavia Serra</name>
    </author>
    <author>
      <name>Veronika Peralta</name>
    </author>
    <author>
      <name>Adriana Marotta</name>
    </author>
    <author>
      <name>Patrick Marcel</name>
    </author>
    <link href="http://arxiv.org/abs/2204.10655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.11137v3</id>
    <updated>2023-02-02T09:29:41Z</updated>
    <published>2022-04-23T20:05:35Z</published>
    <title>Evaluating regular path queries under the all-shortest paths semantics</title>
    <summary>  The purpose of this report is to explain how the textbook breadth-first
search algorithm (BFS) can be modified in order to also create a compact
representation of all shortest paths connecting a single source node to all the
nodes reachable from it. From this representation, all these paths can also be
efficiently enumerated. We then apply this algorithm to solve a similar problem
in edge labelled graphs, where paths also have an additional restriction that
their edge labels form a word belonging to a regular language. Namely, we solve
the problem of evaluating regular path queries (RPQs) under the all-shortest
paths semantics.
</summary>
    <author>
      <name>Domagoj Vrgoč</name>
    </author>
    <link href="http://arxiv.org/abs/2204.11137v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11137v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00285v1</id>
    <updated>2022-04-30T14:55:33Z</updated>
    <published>2022-04-30T14:55:33Z</published>
    <title>Flexible skyline: overview and applicability</title>
    <summary>  Ranking (or top-k) and skyline queries are the most popular approaches used
to extract interesting data from large datasets. The first one is based on a
scoring function to evaluate and rank tuples. Its computation is fast, but it
is sensitive to the choice of the evaluating function. Skyline queries are
based on the idea of dominance and the result is the set of all non-dominated
tuples. This is a very interesting approach, but it can't allow to control the
cardinality of the output. Recent researches discovered more techniques to
compensate for these drawbacks. In particular, this paper will focus on the
flexible skyline approach.
</summary>
    <author>
      <name>Carlo Bellacoscia</name>
    </author>
    <link href="http://arxiv.org/abs/2205.00285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01428v1</id>
    <updated>2022-05-03T11:33:50Z</updated>
    <published>2022-05-03T11:33:50Z</published>
    <title>Filtering and Sampling Object-Centric Event Logs</title>
    <summary>  The scalability of process mining techniques is one of the main challenges to
tackling the massive amount of event data produced every day in enterprise
information systems. To this purpose, filtering and sampling techniques are
proposed to keep a subset of the behavior of the original log and make the
application of process mining techniques feasible. While techniques for
filtering/sampling traditional event logs have been already proposed,
filtering/sampling object-centric event logs is more challenging as the number
of factors (events, objects, object types) to consider is significantly higher.
This paper provides some techniques to filter/sample object-centric event logs.
</summary>
    <author>
      <name>Alessandro Berti</name>
    </author>
    <link href="http://arxiv.org/abs/2205.01428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.02190v2</id>
    <updated>2022-09-13T20:01:43Z</updated>
    <published>2022-05-04T17:13:08Z</published>
    <title>Ontology-Mediated Querying on Databases of Bounded Cliquewidth</title>
    <summary>  We study the evaluation of ontology-mediated queries (OMQs) on databases of
bounded cliquewidth from the viewpoint of parameterized complexity theory. As
the ontology language, we consider the description logics $\mathcal{ALC}$ and
$\mathcal{ALCI}$ as well as the guarded two-variable fragment GF$_2$ of
first-order logic. Queries are atomic queries (AQs), conjunctive queries (CQs),
and unions of CQs. All studied OMQ problems are fixed-parameter linear (FPL)
when the parameter is the size of the OMQ plus the cliquewidth. Our main
contribution is a detailed analysis of the dependence of the running time on
the parameter, exhibiting several interesting effects.
</summary>
    <author>
      <name>Carsten Lutz</name>
    </author>
    <author>
      <name>Leif Sabellek</name>
    </author>
    <author>
      <name>Lukas Schulze</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.24963/kr.2022/25</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.24963/kr.2022/25" rel="related"/>
    <link href="http://arxiv.org/abs/2205.02190v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02190v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.15547v1</id>
    <updated>2022-05-31T05:40:28Z</updated>
    <published>2022-05-31T05:40:28Z</published>
    <title>Discovery of Keys for Graphs [Extended Version]</title>
    <summary>  Keys for graphs uses the topology and value constraints needed to uniquely
identify entities in a graph database. They have been studied to support object
identification, knowledge fusion, data deduplication, and social network
reconciliation. In this paper, we present our algorithm to mine keys over
graphs. Our algorithm discovers keys in a graph via frequent subgraph
expansion. We present two properties that define a meaningful key, including
minimality and support. Lastly, using real-world graphs, we experimentally
verify the efficiency of our algorithm on real world graphs.
</summary>
    <author>
      <name>Morteza Alipourlangouri</name>
    </author>
    <author>
      <name>Fei Chiang</name>
    </author>
    <link href="http://arxiv.org/abs/2205.15547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00623v1</id>
    <updated>2022-06-01T16:48:47Z</updated>
    <published>2022-06-01T16:48:47Z</published>
    <title>P4DB -- The Case for In-Network OLTP (Extended Technical Report)</title>
    <summary>  In this paper we present a new approach for distributed DBMSs called P4DB,
that uses a programmable switch to accelerate OLTP workloads. The main idea of
P4DB is that it implements a transaction processing engine on top of a
P4-programmable switch. The switch can thus act as an accelerator in the
network, especially when it is used to store and process hot (contended) tuples
on the switch. In our experiments, we show that P4DB hence provides significant
benefits compared to traditional DBMS architectures and can achieve a speedup
of up to 8x.
</summary>
    <author>
      <name>Matthias Jasny</name>
    </author>
    <author>
      <name>Lasse Thostrup</name>
    </author>
    <author>
      <name>Tobias Ziegler</name>
    </author>
    <author>
      <name>Carsten Binnig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Technical Report for: P4DB - The Case for In-Network OLTP</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.00623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.01643v1</id>
    <updated>2022-06-03T15:38:33Z</updated>
    <published>2022-06-03T15:38:33Z</published>
    <title>ChaTEAU: A Universal Toolkit for Applying the Chase</title>
    <summary>  What do applications like semantic optimization, data exchange and
integration, answering queries under dependencies, query reformulation with
constraints, and data cleaning have in common? All these applications can be
processed by the Chase, a family of algorithms for reasoning with constraints.
While the theory of the Chase is well understood, existing implementations are
confined to specific use cases and application scenarios, making it difficult
to reuse them in other settings. ChaTEAU overcomes this limitation: It takes
the logical core of the Chase, generalizes it, and provides a software library
for different Chase applications in a single toolkit.
</summary>
    <author>
      <name>Tanja Auge</name>
    </author>
    <author>
      <name>Nic Scharlau</name>
    </author>
    <author>
      <name>Andreas Görres</name>
    </author>
    <author>
      <name>Jakob Zimmer</name>
    </author>
    <author>
      <name>Andreas Heuer</name>
    </author>
    <link href="http://arxiv.org/abs/2206.01643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.02736v1</id>
    <updated>2022-06-06T16:46:04Z</updated>
    <published>2022-06-06T16:46:04Z</published>
    <title>Comparing modern techniques for querying data starting from top-k and
  skyline queries</title>
    <summary>  To make intelligent decisions over complex data by discovering a set of
interesting options is something that has become very important for users of
modern applications. Consequently, researchers are studying new techniques to
overcome limitations of traditional ways of querying data from databases as
top-k queries and skyline queries. Over the past few years new methods have
been developed as Flexible Skylines, Regret Minimization and Skyline
ordering/ranking. The aim of this survey is to describe these techniques and
some their possible variants comparing them and explaining how they improve
traditional methods.
</summary>
    <author>
      <name>Fabio Patella</name>
    </author>
    <link href="http://arxiv.org/abs/2206.02736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.02736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12753v1</id>
    <updated>2022-06-26T00:08:06Z</updated>
    <published>2022-06-26T00:08:06Z</published>
    <title>Spatiotemporal Data Mining: A Survey</title>
    <summary>  Spatiotemporal data mining aims to discover interesting, useful but
non-trivial patterns in big spatial and spatiotemporal data. They are used in
various application domains such as public safety, ecology, epidemiology, earth
science, etc. This problem is challenging because of the high societal cost of
spurious patterns and exorbitant computational cost. Recent surveys of
spatiotemporal data mining need update due to rapid growth. In addition, they
did not adequately survey parallel techniques for spatiotemporal data mining.
This paper provides a more up-to-date survey of spatiotemporal data mining
methods. Furthermore, it has a detailed survey of parallel formulations of
spatiotemporal data mining.
</summary>
    <author>
      <name>Arun Sharma</name>
    </author>
    <author>
      <name>Zhe Jiang</name>
    </author>
    <author>
      <name>Shashi Shekhar</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.14163v1</id>
    <updated>2022-07-21T05:35:02Z</updated>
    <published>2022-07-21T05:35:02Z</published>
    <title>Towards Specificationless Monitoring of Provenance-Emitting Systems</title>
    <summary>  Monitoring often requires insight into the monitored system as well as
concrete specifications of expected behavior. More and more systems, however,
provide information about their inner procedures by emitting provenance
information in a W3C-standardized graph format.
  In this work, we present an approach to monitor such provenance data for
anomalous behavior by performing spectral graph analysis on slices of the
constructed provenance graph and by comparing the characteristics of each slice
with those of a sliding window over recently seen slices. We argue that this
approach not only simplifies the monitoring of heterogeneous distributed
systems, but also enables applying a host of well-studied techniques to monitor
such systems.
</summary>
    <author>
      <name>Martin Stoffers</name>
    </author>
    <author>
      <name>Alexander Weinert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication as a short paper at Runtime Verification
  2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.14163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.14163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.02697v1</id>
    <updated>2022-08-04T14:51:35Z</updated>
    <published>2022-08-04T14:51:35Z</published>
    <title>WShEx: A language to describe and validate Wikibase entities</title>
    <summary>  Wikidata is one of the most successful Semantic Web projects. Its underlying
Wikibase data model departs from RDF with the inclusion of several features
like qualifiers and references, built-in datatypes, etc. Those features are
serialized to RDF for content negotiation, RDF dumps and in the SPARQL
endpoint. Wikidata adopted the entity schemas namespace using the ShEx language
to describe and validate the RDF serialization of Wikidata entities. In this
paper we propose WShEx, a language inspired by ShEx that directly supports the
Wikibase data model and can be used to describe and validate Wikibase entities.
The paper presents a the abstract syntax and semantic of the WShEx language.
</summary>
    <author>
      <name>Jose Emilio Labra Gayo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2110.11709</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.02697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.02697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.03823v1</id>
    <updated>2022-08-07T21:25:07Z</updated>
    <published>2022-08-07T21:25:07Z</published>
    <title>Automatically Finding Optimal Index Structure</title>
    <summary>  Existing learned indexes (e.g., RMI, ALEX, PGM) optimize the internal
regressor of each node, not the overall structure such as index height, the
size of each layer, etc. In this paper, we share our recent findings that we
can achieve significantly faster lookup speed by optimizing the structure as
well as internal regressors. Specifically, our approach (called AirIndex)
expresses the end-to-end lookup time as a novel objective function, and
searches for optimal design decisions using a purpose-built optimizer. In our
experiments with state-of-the-art methods, AirIndex achieves 3.3x-7.7x faster
lookup for the data stored on local SSD, and 1.4x-3.0x faster lookup for the
data on Azure Cloud Storage.
</summary>
    <author>
      <name>Supawit Chockchowwat</name>
    </author>
    <author>
      <name>Wenjie Liu</name>
    </author>
    <author>
      <name>Yongjoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, to be published in AIDB at VLDB 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.03823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09672v1</id>
    <updated>2022-08-20T12:34:18Z</updated>
    <published>2022-08-20T12:34:18Z</published>
    <title>Comparing graph data science libraries for querying and analysing
  datasets: towards data science queries on graphs</title>
    <summary>  This paper presents an experimental study to compare analysis tools with
management systems for querying and analysing graphs. Our experiment compares
classic graph navigational operations queries where analytics tools and
management systems adopt different execution strategies. Then, our experiment
addresses data science pipelines with clustering and prediction models applied
to graphs. In this kind of experiment, we underline the interest in combining
both approaches and the interest of relying on a parallel execution platform
for executing queries.
</summary>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <author>
      <name>Pierre Marrec</name>
    </author>
    <author>
      <name>Mirian Halfeld Ferrari Alves</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09673v1</id>
    <updated>2022-08-20T12:40:25Z</updated>
    <published>2022-08-20T12:40:25Z</published>
    <title>Graph analytics workflows enactment on just in time data centres,
  Position Paper</title>
    <summary>  This paper discusses our vision of multirole-capable decision-making systems
across a broad range of Data Science (DS) workflows working on graphs through
disaggregated data centres. Our vision is that an alternative is possible to
work on a disaggregated solution for the provision of computational services
under the notion of a disaggregated data centre. We define this alternative as
a virtual entity that dynamically provides resources crosscutting the layers of
edge, fog and data centre according to the workloads submitted by the workflows
and their Service Level Objectives.
</summary>
    <author>
      <name>Ali Akoglu</name>
    </author>
    <author>
      <name>José-Luis Zechinelli-Martini</name>
    </author>
    <author>
      <name>Hamamache Kheddouci</name>
    </author>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2103.07978</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.09673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10415v1</id>
    <updated>2022-08-22T15:53:39Z</updated>
    <published>2022-08-22T15:53:39Z</published>
    <title>NLDS-QL: From natural language data science questions to queries on
  graphs: analysing patients conditions &amp; treatments</title>
    <summary>  This paper introduces NLDS-QL, a translator of data science questions
expressed in natural language (NL) into data science queries on graph
databases. Our translator is based on a simplified NL described by a grammar
that specifies sentences combining keywords to refer to operations on graphs
with the vocabulary of the graph schema. The demonstration proposed in this
paper shows NLDS-QL in action within a scenario to explore and analyse a graph
base on patient diagnoses generated with the open-source Synthea.
</summary>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <author>
      <name>Karim Dao</name>
    </author>
    <author>
      <name>Mirian Halfeld Ferrari Alves</name>
    </author>
    <link href="http://arxiv.org/abs/2208.10415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02089v1</id>
    <updated>2022-09-05T18:24:59Z</updated>
    <published>2022-09-05T18:24:59Z</published>
    <title>Compressing integer lists with Contextual Arithmetic Trits</title>
    <summary>  Inverted indexes allow to query large databases without needing to search in
the database at each query. An important line of research is to construct the
most efficient inverted indexes, both in terms of compression ratio and time
efficiency. In this article, we show how to use trit encoding, combined with
contextual methods for computing inverted indexes. We perform an extensive
study of different variants of these methods and show that our method
consistently outperforms the Binary Interpolative Method -- which is one of the
golden standards in this topic -- with respect to compression size. We apply
our methods to a variety of datasets and make available the source code that
produced the results, together with all our datasets.
</summary>
    <author>
      <name>Yann Barsamian</name>
    </author>
    <author>
      <name>André Chailloux</name>
    </author>
    <link href="http://arxiv.org/abs/2209.02089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.10475v1</id>
    <updated>2022-09-21T16:15:47Z</updated>
    <published>2022-09-21T16:15:47Z</published>
    <title>Designing PIDs for Reproducible Science Using Time-Series Data</title>
    <summary>  As part of the investigation done by the IEEE Standards Association P2957
Working Group, called Big Data Governance and Metadata Management, the use of
persistent identifiers (PIDs) is looked at for tackling the problem of
reproducible research and science. This short paper proposes a preliminary
method using PIDs to reproduce research results using time-series data.
Furthermore, we feel it is possible to use the methodology and design for other
types of datasets.
</summary>
    <author>
      <name>Wen Ting Maria Tu</name>
    </author>
    <author>
      <name>Stephen Makonin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to MTSR 2022 - 16th International Conference on Metadata
  and Semantics Research</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.10475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.10475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.02237v1</id>
    <updated>2022-10-05T13:17:23Z</updated>
    <published>2022-10-05T13:17:23Z</published>
    <title>Dimensional Data KNN-Based Imputation</title>
    <summary>  Data Warehouses (DWs) are core components of Business Intelligence (BI).
Missing data in DWs have a great impact on data analyses. Therefore, missing
data need to be completed. Unlike other existing data imputation methods mainly
adapted for facts, we propose a new imputation method for dimensions. This
method contains two steps: 1) a hierarchical imputation and 2) a k-nearest
neighbors (KNN) based imputation. Our solution has the advantage of taking into
account the DW structure and dependency constraints. Experimental assessments
validate our method in terms of effectiveness and efficiency.
</summary>
    <author>
      <name>Yuzhao Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-15740-0_23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-15740-0_23" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">26th European Conference on Advances in Databases and Information
  Systems (ADBIS 2022), Sep 2020, Turin, Italy. pp.315-329</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.02237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13136v1</id>
    <updated>2022-10-24T11:42:42Z</updated>
    <published>2022-10-24T11:42:42Z</published>
    <title>Path association rule mining</title>
    <summary>  Graph association rule mining is a data mining technique used for discovering
regularities in graph data. In this study, we propose a novel concept, {\it
path association rule mining}, to discover the correlations of path patterns
that frequently appear in a given graph. Reachability path patterns (i.e.,
existence of paths from a vertex to another vertex) are applied in our concept
to discover diverse regularities. We show that the problem is NP-hard, and we
develop an efficient algorithm in which the anti-monotonic property is used on
path patterns. Subsequently, we develop approximation and parallelization
techniques to efficiently and scalably discover rules. We use real-life graphs
to experimentally verify the effective
</summary>
    <author>
      <name>Yuya Sasaki</name>
    </author>
    <link href="http://arxiv.org/abs/2210.13136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.17086v1</id>
    <updated>2022-10-31T06:23:05Z</updated>
    <published>2022-10-31T06:23:05Z</published>
    <title>EEMARQ: Efficient Lock-Free Range Queries with Memory Reclamation</title>
    <summary>  Multi-Version Concurrency Control (MVCC) is a common mechanism for achieving
linearizable range queries in database systems and concurrent data-structures.
The core idea is to keep previous versions of nodes to serve range queries,
while still providing atomic reads and updates. Existing concurrent
data-structure implementations, that support linearizable range queries, are
either slow, use locks, or rely on blocking reclamation schemes. We present
EEMARQ, the first scheme that uses MVCC with lock-free memory reclamation to
obtain a fully lock-free data-structure supporting linearizable inserts,
deletes, contains, and range queries. Evaluation shows that EEMARQ outperforms
existing solutions across most workloads, with lower space overhead and while
providing full lock freedom.
</summary>
    <author>
      <name>Gali Sheffi</name>
    </author>
    <author>
      <name>Pedro Ramalhete</name>
    </author>
    <author>
      <name>Erez Petrank</name>
    </author>
    <link href="http://arxiv.org/abs/2210.17086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.17086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05416v1</id>
    <updated>2022-11-10T08:46:47Z</updated>
    <published>2022-11-10T08:46:47Z</published>
    <title>Wikidata-lite for Knowledge Extraction and Exploration</title>
    <summary>  Wikidata is the largest collaborative general knowledge graph supported by a
worldwide community. It includes many helpful topics for knowledge exploration
and data science applications. However, due to the enormous size of Wikidata,
it is challenging to retrieve a large amount of data with millions of results,
make complex queries requiring large aggregation operations, or access too many
statement references. This paper introduces our preliminary works on
Wikidata-lite, a toolkit to build a database offline for knowledge extraction
and exploration, e.g., retrieving item information, statements, provenances, or
searching entities by their keywords and attributes. Wikidata-lite has high
performance and memory efficiency, much faster than the official Wikidata
SPARQL endpoint for big queries. The Wikidata-lite repository is available at
https://github.com/phucty/wikidb.
</summary>
    <author>
      <name>Phuc Nguyen</name>
    </author>
    <author>
      <name>Hideaki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, workshop paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12996v2</id>
    <updated>2022-12-14T00:22:09Z</updated>
    <published>2022-11-22T17:46:56Z</published>
    <title>Converting OpenStreetMap Data to Road Networks for Downstream
  Applications</title>
    <summary>  We study how to convert OpenStreetMap data to road networks for downstream
applications. OpenStreetMap data has different formats. Extensible Markup
Language (XML) is one of them. OSM data consist of nodes, ways, and relations.
We process OSM XML data to extract the information of nodes and ways to obtain
the map of streets of the Memphis area. We can use this map for different
downstream applications.
</summary>
    <author>
      <name>Md Kaisar Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2211.12996v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12996v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13170v1</id>
    <updated>2022-11-23T17:47:42Z</updated>
    <published>2022-11-23T17:47:42Z</published>
    <title>The World of Graph Databases from An Industry Perspective</title>
    <summary>  Rapidly growing social networks and other graph data have created a high
demand for graph technologies in the market. A plethora of graph databases,
systems, and solutions have emerged, as a result. On the other hand, graph has
long been a well studied area in the database research community. Despite the
numerous surveys on various graph research topics, there is a lack of survey on
graph technologies from an industry perspective. The purpose of this paper is
to provide the research community with an industrial perspective on the graph
database landscape, so that graph researcher can better understand the industry
trend and the challenges that the industry is facing, and work on solutions to
help address these problems.
</summary>
    <author>
      <name>Yuanyuan Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 papers, 3 figures, to appear in SIGMOD Record</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.05595v1</id>
    <updated>2022-11-26T20:24:15Z</updated>
    <published>2022-11-26T20:24:15Z</published>
    <title>A new PCA-based utility measure for synthetic data evaluation</title>
    <summary>  Data synthesis is a privacy enhancing technology aiming to produce realistic
and timely data when real data is hard to obtain. Utility of synthetic data
generators (SDGs) has been investigated through different utility metrics.
These metrics have been found to generate conflicting conclusions making direct
comparison of SDGs surprisingly difficult. Moreover, prior research found no
correlation between popular metrics, concluding they tackle different
utility-dimensions. This paper aggregates four popular utility metrics
(representing different utility dimensions) into one using
principal-component-analysis and checks whether the new measure can generate
synthetic data that perform well in real-life. The new measure is used to
compare four well-recognized SDGs.
</summary>
    <author>
      <name>F. K. Dankar</name>
    </author>
    <author>
      <name>M. K. Ibrahim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures, 8 tables, 1 appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.05595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.05595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.05682v1</id>
    <updated>2022-12-12T03:38:34Z</updated>
    <published>2022-12-12T03:38:34Z</published>
    <title>Privacy-Preserving Record Linkage</title>
    <summary>  Given several databases containing person-specific data held by different
organizations, Privacy-Preserving Record Linkage (PPRL) aims to identify and
link records that correspond to the same entity/individual across different
databases based on the matching of personal identifying attributes, such as
name and address, without revealing the actual values in these attributes due
to privacy concerns. This reference work entry defines the PPRL problem,
reviews the literature and key findings, and discusses applications and
research challenges.
</summary>
    <author>
      <name>Dinusha Vatsalan</name>
    </author>
    <author>
      <name>Dimitrios Karapiperis</name>
    </author>
    <author>
      <name>Vassilios S. Verykios</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-63962-8_17-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-63962-8_17-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PP. 1 - 10</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Springer Encyclopedia of Big Data Technologies, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.05682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.05682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06043v1</id>
    <updated>2022-11-03T03:32:22Z</updated>
    <published>2022-11-03T03:32:22Z</published>
    <title>Introducing Hermes: Executing Clinical Quality Language (CQL) at over 66
  Million Resources per Second (inexpensively)</title>
    <summary>  Clinical Quality Language (CQL) has emerged as a standard for rule
representation in Clinical Decision Support (CDS) and Electronic Clinical
Quality Measurement (eCQM) in healthcare. While open-source reference
implementations and a few commercial engines exist, there is still a market
need for high-performance engines that can execute CQL queries on the scales of
millions of patients. We introduce the \Hermes{} engine as the world's fastest
commercial CQL execution engine.
</summary>
    <author>
      <name>Angelo Kastroulis</name>
    </author>
    <author>
      <name>Paolo Bonfini</name>
    </author>
    <author>
      <name>Anastasios Litsas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, 2 appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09271v2</id>
    <updated>2022-12-20T17:03:30Z</updated>
    <published>2022-12-19T06:52:13Z</published>
    <title>Very Large Language Model as a Unified Methodology of Text Mining</title>
    <summary>  Text data mining is the process of deriving essential information from
language text. Typical text mining tasks include text categorization, text
clustering, topic modeling, information extraction, and text summarization.
Various data sets are collected and various algorithms are designed for the
different types of tasks. In this paper, I present a blue sky idea that very
large language model (VLLM) will become an effective unified methodology of
text mining. I discuss at least three advantages of this new methodology
against conventional methods. Finally I discuss the challenges in the design
and development of VLLM techniques for text mining.
</summary>
    <author>
      <name>Meng Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.09271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01189v1</id>
    <updated>2023-01-03T16:42:27Z</updated>
    <published>2023-01-03T16:42:27Z</published>
    <title>On the long-term archiving of research data</title>
    <summary>  Accessing research data at any time is what FAIR (Findable Accessible
Interoperable Reusable) data sharing aims to achieve at scale. Yet, we argue
that it is not sustainable to keep accumulating and maintaining all datasets
for rapid access, considering the monetary and ecological cost of maintaining
repositories. Here, we address the issue of cold data storage: when to dispose
of data for offline storage, how can this be done while maintaining FAIR
principles and who should be responsible for cold archiving and long-term
preservation.
</summary>
    <author>
      <name>Cyril Pernet</name>
    </author>
    <author>
      <name>Claus Svarer</name>
    </author>
    <author>
      <name>Ross Blair</name>
    </author>
    <author>
      <name>John D. Van Horn</name>
    </author>
    <author>
      <name>Russell A. Poldrack</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Editorial style paper, supported by an analysis of the OpenNeuro
  repository, about the long term FAIR management of datasets</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.01189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10673v2</id>
    <updated>2023-01-26T05:26:43Z</updated>
    <published>2023-01-25T16:21:11Z</published>
    <title>An Overview on Cloud Distributed Databases for Business Environments</title>
    <summary>  Cloud-based distributed databases are a popular choice for many current
applications, especially those that run over the Internet. By incorporating
distributed database systems within cloud environments, it has enabled
businesses to scale operations to a global level, all while achieving desired
standards of system reliability, availability, and responsiveness. Cloud
providers offer infrastructure and management tools for distributed databases
as Database-as-a-Service (DBaaS), re-purposing the investment by businesses
towards database services. This paper reviews the functionality of these
services, by highlighting Amazon Relational Data Service (RDS), suited for
handling relational distributed databases.
</summary>
    <author>
      <name>Allan Vikiru</name>
    </author>
    <author>
      <name>Mfadhili Muiruri</name>
    </author>
    <author>
      <name>Ismail Ateya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.10673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M14, 68U35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; C.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.04106v1</id>
    <updated>2023-02-08T15:00:01Z</updated>
    <published>2023-02-08T15:00:01Z</published>
    <title>Detecting Data Type Inconsistencies in a Property Graph Database</title>
    <summary>  Some property graph databases do not have a fixed schema, which can result in
data type inconsistencies for properties on nodes and relationships, especially
when importing data into a running database. Here we present a tool which can
rapidly produce a detailed report on every property in the graph. When executed
on a large knowledge graph, it allowed us to debug a complex ETL process and
enforce 100% data type consistency.
</summary>
    <author>
      <name>Joshua R. Porter</name>
    </author>
    <author>
      <name>Michael N. Young</name>
    </author>
    <author>
      <name>Aleks Y. M. Ontman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, general approach applied to production databases</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.04106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08019v1</id>
    <updated>2023-02-16T01:32:00Z</updated>
    <published>2023-02-16T01:32:00Z</published>
    <title>TransEdge: Supporting Efficient Read Queries Across Untrusted Edge Nodes</title>
    <summary>  We propose Transactional Edge (TransEdge), a distributed transaction
processing system for untrusted environments such as edge computing systems.
What distinguishes TransEdge is its focus on efficient support for read-only
transactions. TransEdge allows reading from different partitions consistently
using one round in most cases and no more than two rounds in the worst case.
TransEdge design is centered around this dependency tracking scheme including
the consensus and transaction processing protocols. Our performance evaluation
shows that TransEdge's snapshot read-only transactions achieve an 9-24x speedup
compared to current byzantine systems.
</summary>
    <author>
      <name>Abhishek A. Singh</name>
    </author>
    <author>
      <name>Aasim Khan</name>
    </author>
    <author>
      <name>Sharad Mehrotra</name>
    </author>
    <author>
      <name>Faisal Nawab</name>
    </author>
    <link href="http://arxiv.org/abs/2302.08019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08028v3</id>
    <updated>2024-02-22T20:44:58Z</updated>
    <published>2023-03-02T00:55:46Z</published>
    <title>EdgeServe: A Streaming System for Decentralized Model Serving</title>
    <summary>  The relevant features for a machine learning task may arrive as one or more
continuous streams of data. Serving machine learning models over streams of
data creates a number of interesting systems challenges in managing data
routing, time-synchronization, and rate control. This paper presents EdgeServe,
a distributed streaming system that can serve predictions from machine learning
models in real time. We evaluate EdgeServe on three streaming prediction tasks:
(1) human activity recognition, (2) autonomous driving, and (3) network
intrusion detection.
</summary>
    <author>
      <name>Ted Shaowang</name>
    </author>
    <author>
      <name>Sanjay Krishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.08028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08193v1</id>
    <updated>2023-03-14T19:29:46Z</updated>
    <published>2023-03-14T19:29:46Z</published>
    <title>RODD: Robust Outlier Detection in Data Cubes</title>
    <summary>  Data cubes are multidimensional databases, often built from several separate
databases, that serve as flexible basis for data analysis. Surprisingly,
outlier detection on data cubes has not yet been treated extensively. In this
work, we provide the first framework to evaluate robust outlier detection
methods in data cubes (RODD). We introduce a novel random forest-based outlier
detection approach (RODD-RF) and compare it with more traditional methods based
on robust location estimators. We propose a general type of test data and
examine all methods in a simulation study. Moreover, we apply ROOD-RF to real
world data. The results show that RODD-RF can lead to improved outlier
detection.
</summary>
    <author>
      <name>Lara Kuhlmann</name>
    </author>
    <author>
      <name>Daniel Wilmes</name>
    </author>
    <author>
      <name>Emmanuel Müller</name>
    </author>
    <author>
      <name>Markus Pauly</name>
    </author>
    <author>
      <name>Daniel Horn</name>
    </author>
    <link href="http://arxiv.org/abs/2303.08193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08697v1</id>
    <updated>2023-03-15T15:31:51Z</updated>
    <published>2023-03-15T15:31:51Z</published>
    <title>Mirror: A Natural Language Interface for Data Querying, Summarization,
  and Visualization</title>
    <summary>  We present Mirror, an open-source platform for data exploration and analysis
powered by large language models. Mirror offers an intuitive natural language
interface for querying databases, and automatically generates executable SQL
commands to retrieve relevant data and summarize it in natural language. In
addition, users can preview and manually edit the generated SQL commands to
ensure the accuracy of their queries. Mirror also generates visualizations to
facilitate understanding of the data. Designed with flexibility and human input
in mind, Mirror is suitable for both experienced data analysts and
non-technical professionals looking to gain insights from their data.
</summary>
    <author>
      <name>Canwen Xu</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <author>
      <name>Penghan Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3543873.3587309</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3543873.3587309" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Web Conference (WWW 2023) Demo</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.08697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.12804v1</id>
    <updated>2023-03-14T13:31:19Z</updated>
    <published>2023-03-14T13:31:19Z</published>
    <title>Features matching using natural language processing</title>
    <summary>  The feature matching is a basic step in matching different datasets. This
article proposes shows a new hybrid model of a pretrained Natural Language
Processing (NLP) based model called BERT used in parallel with a statistical
model based on Jaccard similarity to measure the similarity between list of
features from two different datasets. This reduces the time required to search
for correlations or manually match each feature from one dataset to another.
</summary>
    <author>
      <name>Muhammad Danial Khilji</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijci.2023.120218</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijci.2023.120218" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, International Conference on NLP &amp; AI (NLPAI
  2023)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 12,
  No.2, April 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.12804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.12804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16577v1</id>
    <updated>2023-03-29T10:26:09Z</updated>
    <published>2023-03-29T10:26:09Z</published>
    <title>NoSQL Schema Design for Time-Dependent Workloads</title>
    <summary>  In this paper, we propose a schema optimization method for time-dependent
workloads for NoSQL databases. In our proposed method, we migrate schema
according to changing workloads, and the estimated cost of execution and
migration are formulated and minimized as a single integer linear programming
problem. Furthermore, we propose a method to reduce the number of optimization
candidates by iterating over the time dimension abstraction and optimizing the
workload while updating constraints.
</summary>
    <author>
      <name>Yusuke Wakuta</name>
    </author>
    <author>
      <name>Michael Mior</name>
    </author>
    <author>
      <name>Teruyoshi Zenmyo</name>
    </author>
    <author>
      <name>Yuya Sasaki</name>
    </author>
    <author>
      <name>Makoto Onizuka</name>
    </author>
    <link href="http://arxiv.org/abs/2303.16577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17141v1</id>
    <updated>2023-03-30T04:16:19Z</updated>
    <published>2023-03-30T04:16:19Z</published>
    <title>A declarative approach to data narration</title>
    <summary>  This vision paper lays the preliminary foundations for Data Narrative
Management Systems (DNMS), systems that enable the storage, sharing, and
manipulation of data narratives. We motivate the need for such formal
foundations and introduce a simple logical framework inspired by the relational
model. The core of this framework is a Data Narrative Manipulation Language
inspired by the extended relational algebra. We illustrate its use via examples
and discuss the main challenges for the implementation of this vision.
</summary>
    <author>
      <name>Patrick Marcel</name>
    </author>
    <author>
      <name>Veronika Peralta</name>
    </author>
    <author>
      <name>Faten El Outa</name>
    </author>
    <author>
      <name>Panos Vassiliadis</name>
    </author>
    <link href="http://arxiv.org/abs/2303.17141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.18042v1</id>
    <updated>2023-03-31T13:22:28Z</updated>
    <published>2023-03-31T13:22:28Z</published>
    <title>Scardina: Scalable Join Cardinality Estimation by Multiple Density
  Estimators</title>
    <summary>  In recent years, machine learning-based cardinality estimation methods are
replacing traditional methods. This change is expected to contribute to one of
the most important applications of cardinality estimation, the query optimizer,
to speed up query processing. However, none of the existing methods do not
precisely estimate cardinalities when relational schemas consist of many tables
with strong correlations between tables/attributes. This paper describes that
multiple density estimators can be combined to effectively target the
cardinality estimation of data with large and complex schemas having strong
correlations. We propose Scardina, a new join cardinality estimation method
using multiple partitioned models based on the schema structure.
</summary>
    <author>
      <name>Ryuichi Ito</name>
    </author>
    <author>
      <name>Yuya Sasaki</name>
    </author>
    <author>
      <name>Chuan Xiao</name>
    </author>
    <author>
      <name>Makoto Onizuka</name>
    </author>
    <link href="http://arxiv.org/abs/2303.18042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.18042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.18142v1</id>
    <updated>2023-03-31T15:26:42Z</updated>
    <published>2023-03-31T15:26:42Z</published>
    <title>Shirakami: A Hybrid Concurrency Control Protocol for Tsurugi Relational
  Database System</title>
    <summary>  Modern real-world transactional workloads such as bills of materials or
telecommunication billing need to process both short transactions and long
transactions. Recent concurrency control protocols do not cope with such
workloads since they assume only classical workloads (i.e., YCSB and TPC-C)
that have relatively short transactions. To this end, we proposed a new
concurrency control protocol Shirakami. Shirakami has two sub-protocols.
Shirakami-LTX protocol is for long transactions based on multiversion
concurrency control and Shirakami-OCC protocol is for short transactions based
on Silo. Shirakami naturally integrates them with write preservation method and
epoch-based synchronization. Shirakami is a module in Tsurugi system, which is
a production-purpose relational database system.
</summary>
    <author>
      <name>Takashi Kambayashi</name>
    </author>
    <author>
      <name>Takayuki Tanabe</name>
    </author>
    <author>
      <name>Takashi Hoshino</name>
    </author>
    <author>
      <name>Hideyuki Kawashima</name>
    </author>
    <link href="http://arxiv.org/abs/2303.18142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.18142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.02323v2</id>
    <updated>2023-04-08T07:29:25Z</updated>
    <published>2023-04-05T09:26:17Z</published>
    <title>FASTAGEDS: Fast Approximate Graph Entity Dependency Discovery</title>
    <summary>  This paper studies the discovery of approximate rules in property graphs. We
propose a semantically meaningful measure of error for mining graph entity
dependencies (GEDs) at almost hold, to tolerate errors and inconsistencies that
exist in real-world graphs. We present a new characterisation of GED
satisfaction, and devise a depth-first search strategy to traverse the search
space of candidate rules efficiently. Further, we perform experiments to
demonstrate the feasibility and scalability of our solution, FASTAGEDS, with
three real-world graphs.
</summary>
    <author>
      <name>Guangtong Zhou</name>
    </author>
    <author>
      <name>Selasi Kwashie</name>
    </author>
    <author>
      <name>Yidi Zhang</name>
    </author>
    <author>
      <name>Michael Bewong</name>
    </author>
    <author>
      <name>Vincent M. Nofong</name>
    </author>
    <author>
      <name>Debo Cheng</name>
    </author>
    <author>
      <name>Keqing He</name>
    </author>
    <author>
      <name>Zaiwen Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:2301.06264</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.02323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.06811v1</id>
    <updated>2023-04-13T20:28:51Z</updated>
    <published>2023-04-13T20:28:51Z</published>
    <title>SIGNAL -- The SAP Signavio Analytics Query Language</title>
    <summary>  This paper provides an introduction to and discussion of SIGNAL, an
industry-scale process data querying language and engine for large-scale
cloud-based systems that is developed by SAP Signavio. SIGNAL is optimized for
fast read access to process data in event log format and utilizes an in-memory
columnar store to this end. To facilitate usability, SIGNAL uses an SQL-like
syntax with additional domain-specific querying features and in particular
row-pattern matching-based temporal operators. Also, the paper highlights
research challenges related to process querying that are informed by the
implementation and application of SIGNAL.
</summary>
    <author>
      <name>Timotheus Kampik</name>
    </author>
    <author>
      <name>Andre Lücke</name>
    </author>
    <author>
      <name>Jörn Horstmann</name>
    </author>
    <author>
      <name>Mark Wheeler</name>
    </author>
    <author>
      <name>David Eickhoff</name>
    </author>
    <link href="http://arxiv.org/abs/2304.06811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.06811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08285v1</id>
    <updated>2023-04-17T13:56:20Z</updated>
    <published>2023-04-17T13:56:20Z</published>
    <title>DIALITE: Discover, Align and Integrate Open Data Tables</title>
    <summary>  We demonstrate a novel table discovery pipeline called DIALITE that allows
users to discover, integrate and analyze open data tables. DIALITE has three
main stages. First, it allows users to discover tables from open data platforms
using state-of-the-art table discovery techniques. Second, DIALITE integrates
the discovered tables to produce an integrated table. Finally, it allows users
to analyze the integration result by applying different downstreaming tasks
over it. Our pipeline is flexible such that the user can easily add and compare
additional discovery and integration algorithms.
</summary>
    <author>
      <name>Aamod Khatiwada</name>
    </author>
    <author>
      <name>Roee Shraga</name>
    </author>
    <author>
      <name>Renée J. Miller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3555041.3589732</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3555041.3589732" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.08285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11996v4</id>
    <updated>2024-06-04T21:27:50Z</updated>
    <published>2023-04-24T11:01:42Z</published>
    <title>Applications of Information Inequalities to Database Theory Problems</title>
    <summary>  The paper describes several applications of information inequalities to
problems in database theory. The problems discussed include: upper bounds of a
query's output, worst-case optimal join algorithms, the query domination
problem, and the implication problem for approximate integrity constraints. The
paper is self-contained: all required concepts and results from information
inequalities are introduced here, gradually, and motivated by database
problems.
</summary>
    <author>
      <name>Dan Suciu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was invited for LICS'2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.11996v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11996v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.07608v1</id>
    <updated>2023-05-12T16:54:24Z</updated>
    <published>2023-05-12T16:54:24Z</published>
    <title>Torrent Driven (TD) Coin: A Crypto Coin with Built In Distributed Data
  Storage System</title>
    <summary>  In recent years decentralized currencies developed through Blockchains are
increasingly becoming popular because of their transparent nature and absence
of a central controlling authority. Though a lot of computation power, disk
space, and energy are being used to run this system, most of these resources
are dedicated to just keeping the bad actors away by using Proof of Work, Proof
of Stake, Proof of Space, etc., consensus. In this paper, we discuss a way to
combine those consensus mechanism and modify the defense system to create
actual values for the end-users by providing a solution for securely storing
their data in a decentralized manner without compromising the integrity of the
blockchain.
</summary>
    <author>
      <name>Anirudha Paul</name>
    </author>
    <link href="http://arxiv.org/abs/2305.07608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.07608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08236v1</id>
    <updated>2023-05-14T19:59:41Z</updated>
    <published>2023-05-14T19:59:41Z</published>
    <title>Puzzling over Subsequence-Query Extensions: Disjunction and Generalised
  Gaps</title>
    <summary>  A query model for sequence data was introduced in [11] in the form of
subsequence-queries with wildcards and gap-size constraints (swg-queries, for
short). These queries consist of a pattern over an alphabet of variables and
types, as well as a global window size and a number of local gap-size
constraints. We propose two new extensions of swg-queries, which both enrich
the expressive power of swg-queries in different ways: subsequence-queries with
generalised gap-size constraints (swgg-queries, for short) and disjunctive
subsequence-queries (dswg-queries, for short). We discuss a suitable
characterisation of containment, a classical property considered in database
theory, and adapt results concerning the discovery of swg-queries to both,
swgg-queries and dswg-queries.
</summary>
    <author>
      <name>André Frochaux</name>
    </author>
    <author>
      <name>Sarah Kleest-Meißner</name>
    </author>
    <link href="http://arxiv.org/abs/2305.08236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08583v1</id>
    <updated>2023-05-15T12:10:03Z</updated>
    <published>2023-05-15T12:10:03Z</published>
    <title>Towards efficient multilayer network data management</title>
    <summary>  Real-world multilayer networks can be very large and there can be multiple
choices regarding what should be modeled as a layer. Therefore, there is a need
for their effective storage and manipulation. Currently, multilayer network
analysis software use different data structures and manipulation operators. We
aim to categorize operators in order to assess which structures work best for
certain operator classes and data features. In this work, we propose a
preliminary taxonomy of layer and data manipulation operators. We also design
and execute a benchmark of select software and operators to identify potential
for optimization.
</summary>
    <author>
      <name>Georgios Panayiotou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI, UB</arxiv:affiliation>
    </author>
    <author>
      <name>Matteo Magnani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI, UB</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Pinaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI, UB</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">French Regional Conference on Complex Systems FRCCS 2023, Roberto
  Interdonato, Cyrille Bertelle, May 2023, Le Havre, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.08583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.15263v1</id>
    <updated>2023-05-24T15:52:01Z</updated>
    <published>2023-05-24T15:52:01Z</published>
    <title>ARULESPY: Exploring Association Rules and Frequent Itemsets in Python</title>
    <summary>  The R arules package implements a comprehensive infrastructure for
representing, manipulating, and analyzing transaction data and patterns using
frequent itemsets and association rules. The package also provides a wide range
of interest measures and mining algorithms, including the code of Christian
Borgelt's popular and efficient C implementations of the association mining
algorithms Apriori and Eclat, and optimized C/C++ code for mining and
manipulating association rules using sparse matrix representation. This
document describes the new Python package arulespy, which makes this
infrastructure available for Python users.
</summary>
    <author>
      <name>Michael Hahsler</name>
    </author>
    <link href="http://arxiv.org/abs/2305.15263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.15263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.17723v1</id>
    <updated>2023-05-28T13:42:34Z</updated>
    <published>2023-05-28T13:42:34Z</published>
    <title>SAP HANA Data Volume Management</title>
    <summary>  Today information technology is a data-driven environment. The role of data
is to empower business leaders to make decisions based on facts, trends, and
statistical numbers. SAP is no exception. In modern days many companies use
business suites like SAP on HANA S/4 or ERP or SAP Business Warehouse and other
non-SAP applications and run those on HANA databases for faster processing.
While HANA is an extremely powerful in-memory database, growing business data
has an impact on the overall performance and budget of the organization. This
paper presents best practices to reduce the overall data footprint of HANA
databases for three use cases like SAP Business Suite on HANA, SAP Business
Warehouse, and Native HANA database.
</summary>
    <author>
      <name>Subhadip Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2305.17723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.17723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01028v3</id>
    <updated>2024-10-10T14:42:25Z</updated>
    <published>2023-06-01T13:49:18Z</published>
    <title>ITR: Grammar-based Graph Compression Supporting Fast Triple Queries</title>
    <summary>  Neighborhood queries and triple queries are the most common queries on
graphs; thus, it is desirable to answer them efficiently on compressed data
structures. We present a compression scheme called Incidence-Type-RePair (ITR)
for graphs with labeled nodes and labeled edges based on RePair and apply the
scheme to network, version, and RDF graphs. We show that ITR performs
neighborhood queries and triple queries within only a few milliseconds and
thereby outperforms existing RePair-based solutions on graphs while providing a
compression size comparable to existing graph compressors.
</summary>
    <author>
      <name>Enno Adler</name>
    </author>
    <author>
      <name>Stefan Böttcher</name>
    </author>
    <author>
      <name>Rita Hartel</name>
    </author>
    <link href="http://arxiv.org/abs/2306.01028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07085v1</id>
    <updated>2023-06-12T12:57:34Z</updated>
    <published>2023-06-12T12:57:34Z</published>
    <title>Extracting JSON Schemas with Tagged Unions</title>
    <summary>  With data lakes and schema-free NoSQL document stores, extracting a
descriptive schema from JSON data collections is an acute challenge. In this
paper, we target the discovery of tagged unions, a JSON Schema design pattern
where the value of one property of an object (the tag) conditionally implies
subschemas for sibling properties. We formalize these implications as
conditional functional dependencies and capture them using the JSON Schema
operators if-then-else. We further motivate our heuristics to avoid
overfitting. Experiments with our prototype implementation are promising, and
show that this form of tagged unions can successfully be detected in real-world
GeoJSON and TopoJSON datasets. In discussing future work, we outline how our
approach can be extended further.
</summary>
    <author>
      <name>Stefan Klessinger</name>
    </author>
    <author>
      <name>Meike Klettke</name>
    </author>
    <author>
      <name>Uta Störl</name>
    </author>
    <author>
      <name>Stefanie Scherzinger</name>
    </author>
    <link href="http://arxiv.org/abs/2306.07085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.01718v1</id>
    <updated>2023-07-04T13:45:04Z</updated>
    <published>2023-07-04T13:45:04Z</published>
    <title>A Prototype for a Controlled and Valid RDF Data Production Using SHACL</title>
    <summary>  The paper introduces a tool prototype that combines SHACL's capabilities with
ad-hoc validation functions to create a controlled and user-friendly form
interface for producing valid RDF data. The proposed tool is developed within
the context of the OpenCitations Data Model (OCDM) use case. The paper
discusses the current status of the tool, outlines the future steps required
for achieving full functionality, and explores the potential applications and
benefits of the tool.
</summary>
    <author>
      <name>Elia Rizzetto</name>
    </author>
    <author>
      <name>Arcangelo Massari</name>
    </author>
    <author>
      <name>Ivan Heibi</name>
    </author>
    <author>
      <name>Silvio Peroni</name>
    </author>
    <link href="http://arxiv.org/abs/2307.01718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.01718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.12749v1</id>
    <updated>2023-07-24T12:41:11Z</updated>
    <published>2023-07-24T12:41:11Z</published>
    <title>MorphStream: Scalable Processing of Transactions over Streams on
  Multicores</title>
    <summary>  Transactional Stream Processing Engines (TSPEs) form the backbone of modern
stream applications handling shared mutable states. Yet, the full potential of
these systems, specifically in exploiting parallelism and implementing dynamic
scheduling strategies, is largely unexplored. We present MorphStream, a TSPE
designed to optimize parallelism and performance for transactional stream
processing on multicores. Through a unique three-stage execution paradigm
(i.e., planning, scheduling, and execution), MorphStream enables dynamic
scheduling and parallel processing in TSPEs. Our experiment showcased
MorphStream outperforms current TSPEs across various scenarios and offers
support for windowed state transactions and non-deterministic state access,
demonstrating its potential for broad applicability.
</summary>
    <author>
      <name>Yancan Mao</name>
    </author>
    <author>
      <name>Jianjun Zhao</name>
    </author>
    <author>
      <name>Zhonghao Yang</name>
    </author>
    <author>
      <name>Shuhao Zhang</name>
    </author>
    <author>
      <name>Haikun Liu</name>
    </author>
    <author>
      <name>Volker Markl</name>
    </author>
    <link href="http://arxiv.org/abs/2307.12749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.12749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.13219v1</id>
    <updated>2023-07-25T03:11:18Z</updated>
    <published>2023-07-25T03:11:18Z</published>
    <title>A Primer on the Data Cleaning Pipeline</title>
    <summary>  The availability of both structured and unstructured databases, such as
electronic health data, social media data, patent data, and surveys that are
often updated in real time, among others, has grown rapidly over the past
decade. With this expansion, the statistical and methodological questions
around data integration, or rather merging multiple data sources, has also
grown. Specifically, the science of the ``data cleaning pipeline'' contains
four stages that allow an analyst to perform downstream tasks, predictive
analyses, or statistical analyses on ``cleaned data.'' This article provides a
review of this emerging field, introducing technical terminology and commonly
used methods.
</summary>
    <author>
      <name>Rebecca C. Steorts</name>
    </author>
    <link href="http://arxiv.org/abs/2307.13219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.13219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15255v1</id>
    <updated>2023-07-28T01:40:49Z</updated>
    <published>2023-07-28T01:40:49Z</published>
    <title>Predicate Transfer: Efficient Pre-Filtering on Multi-Join Queries</title>
    <summary>  This paper presents predicate transfer, a novel method that optimizes join
performance by pre-filtering tables to reduce the join input sizes. Predicate
transfer generalizes Bloom join, which conducts pre-filtering within a single
join operation, to multi-table joins such that the filtering benefits can be
significantly increased. Predicate transfer is inspired by the seminal
theoretical results by Yannakakis, which uses semi-joins to pre-filter acyclic
queries. Predicate transfer generalizes the theoretical results to any join
graphs and use Bloom filters to replace semi-joins leading to significant
speedup. Evaluation shows predicate transfer can outperform Bloom join by 3.1x
on average on TPC-H benchmark.
</summary>
    <author>
      <name>Yifei Yang</name>
    </author>
    <author>
      <name>Hangdong Zhao</name>
    </author>
    <author>
      <name>Xiangyao Yu</name>
    </author>
    <author>
      <name>Paraschos Koutris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.15255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.00184v1</id>
    <updated>2023-07-31T22:41:17Z</updated>
    <published>2023-07-31T22:41:17Z</published>
    <title>Attribution-Scores in Data Management and Explainable Machine Learning</title>
    <summary>  We describe recent research on the use of actual causality in the definition
of responsibility scores as explanations for query answers in databases, and
for outcomes from classification models in machine learning. In the case of
databases, useful connections with database repairs are illustrated and
exploited. Repairs are also used to give a quantitative measure of the
consistency of a database. For classification models, the responsibility score
is properly extended and illustrated. The efficient computation of Shap-score
is also analyzed and discussed. The emphasis is placed on work done by the
author and collaborators.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper associated to ADBIS23 tutorial. To appear. arXiv admin note:
  substantial text overlap with arXiv:2303.02829, arXiv:2106.10562</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.00184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.00184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.06815v2</id>
    <updated>2023-12-22T19:18:21Z</updated>
    <published>2023-08-13T17:12:14Z</published>
    <title>Optimizing the cloud? Don't train models. Build oracles!</title>
    <summary>  We propose cloud oracles, an alternative to machine learning for online
optimization of cloud configurations. Our cloud oracle approach guarantees
complete accuracy and explainability of decisions for problems that can be
formulated as parametric convex optimizations. We give experimental evidence of
this technique's efficacy and share a vision of research directions for
expanding its applicability.
</summary>
    <author>
      <name>Tiemo Bang</name>
    </author>
    <author>
      <name>Conor Power</name>
    </author>
    <author>
      <name>Siavash Ameli</name>
    </author>
    <author>
      <name>Natacha Crooks</name>
    </author>
    <author>
      <name>Joseph M. Hellerstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera-ready publication for CIDR'24:
  https://www.cidrdb.org/cidr2024/papers/p47-bang.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.06815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.06815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.09569v1</id>
    <updated>2023-08-18T14:02:01Z</updated>
    <published>2023-08-18T14:02:01Z</published>
    <title>Cost-Intelligent Data Analytics in the Cloud</title>
    <summary>  For decades, database research has focused on optimizing performance under
fixed resources. As more and more database applications move to the public
cloud, we argue that it is time to make cost a first-class citizen when solving
database optimization problems. In this paper, we introduce the concept of cost
intelligence and envision the architecture of a cloud data warehouse designed
for that. We investigate two critical challenges to achieving cost intelligence
in an analytical system: automatic resource deployment and cost-oriented
auto-tuning. We describe our system architecture with an emphasis on the
components that are missing in today's cloud data warehouses. Each of these new
components represents unique research opportunities in this much-needed
research area.
</summary>
    <author>
      <name>Huanchen Zhang</name>
    </author>
    <author>
      <name>Yihao Liu</name>
    </author>
    <author>
      <name>Jiaqi Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2308.09569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.11008v1</id>
    <updated>2023-08-17T11:39:41Z</updated>
    <published>2023-08-17T11:39:41Z</published>
    <title>Approximating Clustering for Memory Management and request processing</title>
    <summary>  Clustering is a crucial tool for analyzing data in virtually every scientific
and engineering discipline. There are more scalable solutions framed to enable
time and space clustering for the future large-scale data analyses. As a
result, hardware and software innovations that can significantly improve data
efficiency and performance of the data clustering techniques are necessary to
make the future large-scale data analysis practical. This paper proposes a
novel mechanism for computing bit-serial medians. We propose a novel method,
two-parameter terms that enables in computation within the data arrays
</summary>
    <author>
      <name>D. D. D. Suribabu</name>
    </author>
    <author>
      <name>T. Hitendra Sarma</name>
    </author>
    <author>
      <name>B. Eswar Reddy</name>
    </author>
    <link href="http://arxiv.org/abs/2308.11008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.11008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.14617v1</id>
    <updated>2023-08-28T14:38:06Z</updated>
    <published>2023-08-28T14:38:06Z</published>
    <title>Towards "all-inclusive" Data Preparation to ensure Data Quality</title>
    <summary>  Data preparation, especially data cleaning, is very important to ensure data
quality and to improve the output of automated decision systems. Since there is
no single tool that covers all steps required, a combination of tools -- namely
a data preparation pipeline -- is required. Such process comes with a number of
challenges. We outline the challenges and describe the different tasks we want
to analyze in our future research to address these. A test data generator which
we implemented to constitute the basis for our future work will also be
introduced in detail.
</summary>
    <author>
      <name>Valerie Restat</name>
    </author>
    <link href="http://arxiv.org/abs/2308.14617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.14617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03045v1</id>
    <updated>2023-09-06T14:39:18Z</updated>
    <published>2023-09-06T14:39:18Z</published>
    <title>An Evaluation of Software Sketches</title>
    <summary>  This work presents a detailed evaluation of Rust (software) implementations
of several popular sketching solutions, as well as recently proposed
optimizations. We compare these solutions in terms of computational speed,
memory consumption, and several approximation error metrics. Overall, we find a
simple hashing based solution employed with the Nitro sampling technique [22]
gives the best trade-off between memory, error and speed. Our findings also
include some novel insights about how to best combine sampling with Counting
Cuckoo filters depending on the application.
</summary>
    <author>
      <name>Roy Friedman</name>
    </author>
    <link href="http://arxiv.org/abs/2309.03045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.05178v1</id>
    <updated>2023-09-11T00:14:49Z</updated>
    <published>2023-09-11T00:14:49Z</published>
    <title>Quantifying Uncertainty in Aggregate Queries over Integrated Datasets</title>
    <summary>  Data integration is a notoriously difficult and heuristic-driven process,
especially when ground-truth data are not readily available. This paper
presents a measure of uncertainty by providing maximal and minimal ranges of a
query outcome in two-table, one-to-many data integration workflows. Users can
use these query results to guide a search through different matching
parameters, similarity metrics, and constraints. Even though there are
exponentially many such matchings, we show that in appropriately constrained
circumstances that this result range can be calculated in polynomial time with
bipartite graph matching. We evaluate this on real-world datasets and synthetic
datasets, and find that uncertainty estimates are more robust when a
graph-matching based approach is used for data integration.
</summary>
    <author>
      <name>Deniz Turkcapar</name>
    </author>
    <author>
      <name>Sanjay Krishnan</name>
    </author>
    <link href="http://arxiv.org/abs/2309.05178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.12059v2</id>
    <updated>2024-05-24T06:44:27Z</updated>
    <published>2023-09-21T13:25:11Z</published>
    <title>A Dichotomy in the Complexity of Consistent Query Answering for Two Atom
  Queries With Self-Join</title>
    <summary>  We consider the dichotomy conjecture for consistent query answering under
primary key constraints. It states that, for every fixed Boolean conjunctive
query q, testing whether q is certain (i.e. whether it evaluates to true over
all repairs of a given inconsistent database) is either polynomial time or
coNP-complete. This conjecture has been verified for self-join-free and path
queries. We show that it also holds for queries with two atoms.
</summary>
    <author>
      <name>Anantha Padmanabha</name>
    </author>
    <author>
      <name>Luc Segoufin</name>
    </author>
    <author>
      <name>Cristina Sirangelo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3651137</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3651137" rel="related"/>
    <link href="http://arxiv.org/abs/2309.12059v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.12059v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.17072v1</id>
    <updated>2023-09-29T09:09:04Z</updated>
    <published>2023-09-29T09:09:04Z</published>
    <title>MaaSDB: Spatial Databases in the Era of Large Language Models (Vision
  Paper)</title>
    <summary>  Large language models (LLMs) are advancing rapidly. Such models have
demonstrated strong capabilities in learning from large-scale (unstructured)
text data and answering user queries. Users do not need to be experts in
structured query languages to interact with systems built upon such models.
This provides great opportunities to reduce the barrier of information
retrieval for the general public. By introducing LLMs into spatial data
management, we envisage an LLM-based spatial database system to learn from both
structured and unstructured spatial data. Such a system will offer seamless
access to spatial knowledge for the users, thus benefiting individuals,
business, and government policy makers alike.
</summary>
    <author>
      <name>Jianzhong Qi</name>
    </author>
    <author>
      <name>Zuqing Li</name>
    </author>
    <author>
      <name>Egemen Tanin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3589132.3625597</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3589132.3625597" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear in ACM SIGSPATIAL 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.17072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.17072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.05020v1</id>
    <updated>2023-10-08T05:34:43Z</updated>
    <published>2023-10-08T05:34:43Z</published>
    <title>Harnessing Automation in Data Mining: A Review on the Impact of PyESAPI
  in Radiation Oncology Data Extraction and Management</title>
    <summary>  Data extraction and management are crucial components of research and
clinical workflows in Radiation Oncology (RO), where accurate and comprehensive
data are imperative to inform treatment planning and delivery. The advent of
automated data mining scripts, particularly using the Python Environment for
Scripting APIs (PyESAPI), has been a promising stride towards enhancing
efficiency, accuracy, and reliability in extracting data from RO Information
Systems (ROIS) and Treatment Planning Systems (TPS). This review dissects the
role, efficiency, and challenges of implementing PyESAPI in RO data extraction
and management, juxtaposing manual data extraction techniques and explicating
future avenues
</summary>
    <author>
      <name>Ghaith Alomari</name>
    </author>
    <author>
      <name>Anas Aljarah</name>
    </author>
    <link href="http://arxiv.org/abs/2310.05020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.05020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.12658v1</id>
    <updated>2023-10-19T11:27:57Z</updated>
    <published>2023-10-19T11:27:57Z</published>
    <title>phyloDB: A framework for large-scale phylogenetic analysis</title>
    <summary>  phyloDB is a modular and extensible framework for large-scale phylogenetic
analyses, which are essential for understanding epidemics evolution. It relies
on the Neo4j graph database for data storage and processing, providing a schema
and an API for representing and querying phylogenetic data. Custom algorithms
are also supported, allowing to perform heavy computations directly over the
data, and to store results in the database. Multiple computation results are
stored as multilayer networks, promoting and facilitating comparative analyses,
as well as avoiding unnecessary ab initio computations. The experimental
evaluation results showcase that phyloDB is efficient and scalable with respect
to both API operations and algorithms execution.
</summary>
    <author>
      <name>Bruno Lourenço</name>
    </author>
    <author>
      <name>Cátia Vaz</name>
    </author>
    <author>
      <name>Miguel E. Coimbra</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2012.13363</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.12658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.12658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.15360v1</id>
    <updated>2023-10-23T20:46:39Z</updated>
    <published>2023-10-23T20:46:39Z</published>
    <title>Algorithm for Invalidation of Cached Results of Queries to a Single
  Table</title>
    <summary>  One of the most popular setups for a back-end of a high performance website
consists of a relational database and a cache which stores results of performed
queries. Several application frameworks support caching of queries made to the
database, but few of them handle cache invalidation correctly, resorting to
simpler solutions such as short TTL values, or flushing the whole cache after
any write to the database. In this paper a simple, correct, efficient and
tested in real world application solution is presented, which allows for
infinite TTL, and very fine grained cache invalidation. Algorithm is proven to
be correct in a concurrent environment, both theoretically and in practice.
</summary>
    <author>
      <name>Jakub Łopuszański</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">my rejected submission to SIGMOD 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.15360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.15360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.03574v1</id>
    <updated>2023-11-06T22:19:30Z</updated>
    <published>2023-11-06T22:19:30Z</published>
    <title>Fuzzy Relational Databases via Associative Arrays</title>
    <summary>  The increasing rise in artificial intelligence has made the use of imprecise
language in computer programs like ChatGPT more prominent. Fuzzy logic
addresses this form of imprecise language by introducing the concept of fuzzy
sets, where elements belong to the set with a certain membership value (called
the fuzzy value). This paper combines fuzzy data with relational algebra to
provide the mathematical foundation for a fuzzy database querying language,
describing various useful operations in the language of linear algebra and
multiset operations, in addition to rigorously proving key identities.
</summary>
    <author>
      <name>Kevin Min</name>
    </author>
    <author>
      <name>Hayden Jananthan</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, accepted to IEEE URTC 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.03574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.03574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.07826v1</id>
    <updated>2023-11-14T01:05:47Z</updated>
    <published>2023-11-14T01:05:47Z</published>
    <title>Adaptive Search Optimization: Dynamic Algorithm Selection and Caching
  for Enhanced Database Performance</title>
    <summary>  Efficient search operations in databases are paramount for timely retrieval
of information various applications. This research introduces a novel approach,
combining dynamicalgorithm1 selection and caching2 strategies, to optimize
search performance. The proposed dynamic search algorithm intelligently
switches between Binary3 and Interpolation 4 Search based on dataset
characteristics, significantly improving efficiency for non-uniformly
distributed data. Additionally, a robust caching mechanism5 stores and
retrieves previous search results, further enhancing computational efficiency6.
Theoretical analysis and extensive experiments demonstrate the effectiveness of
the approach, showcasing its potential to revolutionize database performance7
in scenarios with diverse data distributions. This research contributes
valuable insights and practical solutions to the realm of database
optimization, offering a promising avenue for enhancing search operations in
real-world applications
</summary>
    <author>
      <name>Hakikat Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.34751.69281</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.34751.69281" rel="related"/>
    <link href="http://arxiv.org/abs/2311.07826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.07826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.08795v1</id>
    <updated>2023-11-15T09:08:57Z</updated>
    <published>2023-11-15T09:08:57Z</published>
    <title>Advancements and Challenges in Object-Centric Process Mining: A
  Systematic Literature Review</title>
    <summary>  Recent years have seen the emergence of object-centric process mining
techniques. Born as a response to the limitations of traditional process mining
in analyzing event data from prevalent information systems like CRM and ERP,
these techniques aim to tackle the deficiency, convergence, and divergence
issues seen in traditional event logs. Despite the promise, the adoption in
real-world process mining analyses remains limited. This paper embarks on a
comprehensive literature review of object-centric process mining, providing
insights into the current status of the discipline and its historical
trajectory.
</summary>
    <author>
      <name>Alessandro Berti</name>
    </author>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Wil M. P. van der Aalst</name>
    </author>
    <link href="http://arxiv.org/abs/2311.08795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.08795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10964v1</id>
    <updated>2023-11-18T04:24:28Z</updated>
    <published>2023-11-18T04:24:28Z</published>
    <title>A Novel Method for Curating Quanti-Qualitative Content</title>
    <summary>  This paper proposes a Researcher-in-the-Loop (RITL) guided content curation
approach for quanti-qualitative research methods that uses a version control
system based on consensus. The paper introduces a workflow for
quanti-qualitative research processes that produces and consumes content
versions through collaborative phases validated through consensus protocols
performed by research teams. We argue that content versioning is a critical
component that supports the research process's reproducibility, traceability,
and rationale. We propose a curation framework that provides methods,
protocols, and tools for supporting the RITL approach to managing the content
produced by quanti-qualitative methods. The paper reports a validation
experiment using a use case about the study on disseminating political
statements in graffiti.
</summary>
    <author>
      <name>Alejandro Adorjan</name>
    </author>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <author>
      <name>Regina Motz</name>
    </author>
    <link href="http://arxiv.org/abs/2311.10964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10969v1</id>
    <updated>2023-11-18T04:37:07Z</updated>
    <published>2023-11-18T04:37:07Z</published>
    <title>MATILDA: Inclusive Data Science Pipelines Design through Computational
  Creativity</title>
    <summary>  We argue for the need for a new generation of data science solutions that can
democratize recent advances in data engineering and artificial intelligence for
non-technical users from various disciplines, enabling them to unlock the full
potential of these solutions. To do so, we adopt an approach whereby
computational creativity and conversational computing are combined to guide
non-specialists intuitively to explore and extract knowledge from data
collections. The paper introduces MATILDA, a creativity-based data science
design platform, showing how it can support the design process of data science
pipelines guided by human and computational creativity.
</summary>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <author>
      <name>Santiago Negrete-Yankelevich</name>
    </author>
    <author>
      <name>Javier A. Espinosa-Oviedo</name>
    </author>
    <author>
      <name>Khalid Belhajjame</name>
    </author>
    <author>
      <name>José-Luis Zechinelli-Martini</name>
    </author>
    <link href="http://arxiv.org/abs/2311.10969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.15730v1</id>
    <updated>2023-11-27T11:27:38Z</updated>
    <published>2023-11-27T11:27:38Z</published>
    <title>Analytical Queries: A Comprehensive Survey</title>
    <summary>  Modern hardware heterogeneity brings efficiency and performance opportunities
for analytical query processing. In the presence of continuous data volume and
complexity growth, bridging the gap between recent hardware advancements and
the data processing tools ecosystem is paramount for improving the speed of ETL
and model development. In this paper, we present a comprehensive overview of
existing analytical query processing approaches as well as the use and design
of systems that use heterogeneous hardware for the task. We then analyze
state-of-the-art solutions and identify missing pieces. The last two chapters
discuss the identified problems and present our view on how the ecosystem
should evolve.
</summary>
    <author>
      <name>Petr Kurapov</name>
    </author>
    <author>
      <name>Areg Melik-Adamyan</name>
    </author>
    <link href="http://arxiv.org/abs/2311.15730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.15730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.06517v1</id>
    <updated>2023-12-11T16:46:57Z</updated>
    <published>2023-12-11T16:46:57Z</published>
    <title>Facilitating Digital Agriculture with Simple Databases</title>
    <summary>  As an on-ramp to databases, we offer several well-structured private database
templates as open source resources for agriculturalists, particularly those
with modest spreadsheet skills. These farmer-oriented Air table databases use
simple data-validated forms, with the look and feel of a customized app, to
yield operational data that is tidy, machine- and human-readable, editable, and
exportable for analysis in other software. Such data can facilitate logistics,
provide contextual metadata, and improve enterprise analysis. A recorded
workshop explaining how to build a database for activity records is presented.
These resources may facilitate infusion of digital agriculture principles
through Extension and structured educational programming.
</summary>
    <author>
      <name>Dennis Buckmaster</name>
    </author>
    <author>
      <name>Sami Basir</name>
    </author>
    <author>
      <name>Hanae Sakata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 table, 1 figure. Journal of Extension Tools of the Trade,
  in press</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.06517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.06517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4; E.0; K.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.14530v1</id>
    <updated>2023-12-22T08:53:48Z</updated>
    <published>2023-12-22T08:53:48Z</published>
    <title>ZodiacEdge: a Datalog Engine With Incremental Rule Set Maintenance</title>
    <summary>  In this paper, we tackle the incremental maintenance of Datalog inference
materialisation when the rule set can be updated. This is particularly relevant
in the context of the Internet of Things and Edge computing where smart devices
may need to reason over newly acquired knowledge represented as Datalog rules.
Our solution is based on an adaptation of a stratification strategy applied to
a dependency hypergraph whose nodes correspond to rule sets in a Datalog
program. Our implementation supports recursive rules containing both negation
and aggregation. We demonstrate the effectiveness of our system on real and
synthetic data.
</summary>
    <author>
      <name>Weiqin Xu</name>
    </author>
    <author>
      <name>Olivier Curé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures, 3 tables, 4 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.14530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.05712v3</id>
    <updated>2024-08-18T22:02:30Z</updated>
    <published>2024-01-11T07:27:58Z</published>
    <title>BOD: Blindly Optimal Data Discovery</title>
    <summary>  Combining discovery and augmentation is important in the era of data usage
when it comes to predicting the outcome of tasks. However, having to ask the
user the utility function to discover the goal to achieve the optimal small
rightful dataset is not an optimal solution. The existing solutions do not make
good use of this combination, hence underutilizing the data. In this paper, we
introduce a novel goal-oriented framework, called BOD: Blindly Optimal Data
Discovery, that involves humans in the loop and comparing utility scores every
time querying in the process without knowing the utility function. This
establishes the promise of using BOD: Blindly Optimal Data Discovery for modern
data science solutions.
</summary>
    <author>
      <name>Thomas Hoang</name>
    </author>
    <link href="http://arxiv.org/abs/2401.05712v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.05712v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13677v1</id>
    <updated>2023-11-30T12:09:14Z</updated>
    <published>2023-11-30T12:09:14Z</published>
    <title>Process Mining for Unstructured Data: Challenges and Research Directions</title>
    <summary>  The application of process mining for unstructured data might significantly
elevate novel insights into disciplines where unstructured data is a common
data format. To efficiently analyze unstructured data by process mining and to
convey confidence into the analysis result, requires bridging multiple
challenges. The purpose of this paper is to discuss these challenges, present
initial solutions and describe future research directions. We hope that this
article lays the foundations for future collaboration on this topic.
</summary>
    <author>
      <name>Agnes Koschmider</name>
    </author>
    <author>
      <name>Milda Aleknonytė-Resch</name>
    </author>
    <author>
      <name>Frederik Fonger</name>
    </author>
    <author>
      <name>Christian Imenkamp</name>
    </author>
    <author>
      <name>Arvid Lepsien</name>
    </author>
    <author>
      <name>Kaan Apaydin</name>
    </author>
    <author>
      <name>Maximilian Harms</name>
    </author>
    <author>
      <name>Dominik Janssen</name>
    </author>
    <author>
      <name>Dominic Langhammer</name>
    </author>
    <author>
      <name>Tobias Ziolkowski</name>
    </author>
    <author>
      <name>Yorck Zisgen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18420/modellierung2024_012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18420/modellierung2024_012" rel="related"/>
    <link href="http://arxiv.org/abs/2401.13677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.16552v1</id>
    <updated>2024-01-29T20:38:29Z</updated>
    <published>2024-01-29T20:38:29Z</published>
    <title>ONDA: ONline Database Architect</title>
    <summary>  Database modeling is a key activity towards the fulfillment of storage
requirements. Despite the availability of several database modeling tools for
developers, these often come with associated costs, setup complexities,
usability challenges, or dependency on specific operating systems. In this
paper we present ONDA, a web-based tool developed at the University of Coimbra,
that allows the creation of Entity-Relationship diagrams, visualization of
physical models, and generation of SQL code for various database engines. ONDA
is freely available at https://onda.dei.uc.pt and was created with the
intention of supporting teaching activities at university-level database
courses. At the time of writing, the tool being used by more than three hundred
university students every academic year.
</summary>
    <author>
      <name>Nuno Laranjeiro</name>
    </author>
    <author>
      <name>Alexandre Miguel Pinto</name>
    </author>
    <link href="http://arxiv.org/abs/2401.16552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.03464v1</id>
    <updated>2024-02-05T19:19:08Z</updated>
    <published>2024-02-05T19:19:08Z</published>
    <title>A Fuzzy Approach to Record Linkages</title>
    <summary>  Record Linkage is the process of identifying and unifying records from
various independent data sources. Existing strategies, which can be either
deterministic or probabilistic, often fail to link records satisfactorily under
uncertainty. This paper describes an indigenously (locally) developed fuzzy
linkage method, based on fuzzy set techniques, which can effectively account
for this uncertainty prevalent in the disparate data sources and address the
shortcomings of the existing approaches. Extensive testing, evaluation and
comparisons have demonstrated the efficacy of this fuzzy approach for record
linkages.
</summary>
    <author>
      <name>Pratik K. Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal Paper (9 pages, 6 Figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.03464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.03464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3; H.4; E.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.07478v1</id>
    <updated>2024-02-12T08:33:53Z</updated>
    <published>2024-02-12T08:33:53Z</published>
    <title>A Comparison of Different Representations of Ordinal Patterns and Their
  Usability in Data Analysis</title>
    <summary>  We describe and analyze different approaches to represent ordinal patterns.
All of these can be found in the literature. The most important representations
(plus sub-classes) are compared in terms of their applicability from different
angles. Namely we consider digital implementation, inverse patterns and ties
between values. At the end we provide a guideline on which occasions which
representation should be used.
</summary>
    <author>
      <name>Alexander Schnurr</name>
    </author>
    <author>
      <name>Angelika Silbernagel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, published in conference proceedings of ICECET
  2023 (2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.07478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.07478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.00981v1</id>
    <updated>2024-03-01T21:06:11Z</updated>
    <published>2024-03-01T21:06:11Z</published>
    <title>A Conceptual Model for Data Storytelling Highlights in Business
  Intelligence Environments</title>
    <summary>  We introduce a conceptual model for highlights to support data analysis and
storytelling in the domain of Business Intelligence, via the automated
extraction, representation, and exploitation of highlights revealing key facts
that are hidden in the data with which a data analyst works. The model builds
on the concepts of Holistic and Elementary Highlights, along with their
context, constituents and interrelationships, whose synergy can identify
internal properties, patterns and key facts in a dataset being analyzed.
</summary>
    <author>
      <name>Panos Vassiliadis</name>
    </author>
    <author>
      <name>Patrick Marcel</name>
    </author>
    <author>
      <name>Faten El Outa</name>
    </author>
    <author>
      <name>Veronika Peralta</name>
    </author>
    <author>
      <name>Dimos Gkitsakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.00981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.00981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.02856v1</id>
    <updated>2024-03-05T10:56:25Z</updated>
    <published>2024-03-05T10:56:25Z</published>
    <title>Quantum Data Management: From Theory to Opportunities</title>
    <summary>  Quantum computing has emerged as a transformative tool for future data
management. Classical problems in database domains, including query
optimization, data integration, and transaction management, have recently been
addressed using quantum computing techniques. This tutorial aims to establish
the theoretical foundation essential for enhancing methodologies and practical
implementations in this line of research. Moreover, this tutorial takes a
forward-looking approach by delving into recent strides in quantum internet
technologies and the nonlocality theory. We aim to shed light on the uncharted
territory of future data systems tailored for the quantum internet.
</summary>
    <author>
      <name>Rihan Hai</name>
    </author>
    <author>
      <name>Shih-Han Hung</name>
    </author>
    <author>
      <name>Sebastian Feld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 40th IEEE International Conference on Data
  Engineering (ICDE 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.02856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.02856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.13694v2</id>
    <updated>2025-01-28T17:30:16Z</updated>
    <published>2024-03-20T15:56:31Z</published>
    <title>Overview of Publicly Available Degradation Data Sets for Tasks within
  Prognostics and Health Management</title>
    <summary>  Central to the efficacy of prognostics and health management methods is the
acquisition and analysis of degradation data, which encapsulates the evolving
health condition of engineering systems over time. Degradation data serves as a
rich source of information, offering invaluable insights into the underlying
degradation processes, failure modes, and performance trends of engineering
systems. This paper provides an overview of publicly available degradation data
sets.
</summary>
    <author>
      <name>Fabian Mauthe</name>
    </author>
    <author>
      <name>Christopher Braun</name>
    </author>
    <author>
      <name>Julian Raible</name>
    </author>
    <author>
      <name>Peter Zeiler</name>
    </author>
    <author>
      <name>Marco F. Huber</name>
    </author>
    <link href="http://arxiv.org/abs/2403.13694v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.13694v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.02276v1</id>
    <updated>2024-04-02T20:00:14Z</updated>
    <published>2024-04-02T20:00:14Z</published>
    <title>Heterogeneous Data Access Model for Concurrency Control and Methods to
  Deal with High Data Contention</title>
    <summary>  OLTP has stringent performance requirements defined by Service Level
Agreements. Transaction response time is used to determine the maximum
throughout in benchmarks. Capacity planning tools for OLTP performance are
based on queueing network models for hardware resources and database lock
contention has a secondary effect on performance. With ever increasing levels
of e-commerce and surges in OLTP traffic we discuss the need for studies of
database workloads to develop more realistic lock/latch contention models.
Predictive formulas to model increased load leading to thrashing for txns with
identical and nonidentical steps are presented. We review concurrency control
methods to reduce the level of lock/data conflicts in high contention
environments.
</summary>
    <author>
      <name>Alexander Thomasian</name>
    </author>
    <link href="http://arxiv.org/abs/2404.02276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.02276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.05778v2</id>
    <updated>2024-04-11T01:44:01Z</updated>
    <published>2024-04-08T14:38:12Z</published>
    <title>Database-Driven Mathematical Inquiry</title>
    <summary>  Recent advances in computing have changed not only the nature of mathematical
computation, but mathematical proof and inquiry itself. While artificial
intelligence and formalized mathematics have been the major topics of this
conversation, this paper explores another class of tools for advancing
mathematics research: databases of mathematical objects that enable semantic
search. In addition to defining and exploring examples of these tools, we
illustrate a particular line of research that was inspired and enabled by one
such database.
</summary>
    <author>
      <name>Steven Clontz</name>
    </author>
    <link href="http://arxiv.org/abs/2404.05778v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.05778v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A35, 54-04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.10086v1</id>
    <updated>2024-04-15T18:50:30Z</updated>
    <published>2024-04-15T18:50:30Z</published>
    <title>Empowering Enterprise Development by Building and Deploying Admin
  Dashboard using Refine Framework</title>
    <summary>  This project proposes the development of an advanced admin dashboard tailored
for enterprise development, leveraging the Refine framework, Ant Design, and
GraphQL API. It promises heightened operational efficiency by optimizing
backend integration and employing GraphQL's dynamic data subscription for
real-time insights. With an emphasis on modern aesthetics and user-centric
design, it ensures seamless data visualization and management. Key
functionalities encompass user administration, data visualization, CRUD
operations, real-time notifications, and seamless integration with existing
systems. The deliverable includes a deployable dashboard alongside
comprehensive documentation, aiming to empower enterprise teams with a
cutting-edge, data-driven solution.
</summary>
    <author>
      <name>Sai Teja Gajjala</name>
    </author>
    <author>
      <name>Devi Deepak Manchala</name>
    </author>
    <author>
      <name>Bhargav Gummadelly</name>
    </author>
    <author>
      <name>Naga Sailaja K</name>
    </author>
    <link href="http://arxiv.org/abs/2404.10086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.10086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.12552v1</id>
    <updated>2024-04-19T00:13:25Z</updated>
    <published>2024-04-19T00:13:25Z</published>
    <title>Cocoon: Semantic Table Profiling Using Large Language Models</title>
    <summary>  Data profilers play a crucial role in the preprocessing phase of data
analysis by identifying quality issues such as missing, extreme, or erroneous
values. Traditionally, profilers have relied solely on statistical methods,
which lead to high false positives and false negatives. For example, they may
incorrectly flag missing values where such absences are expected and normal
based on the data's semantic context. To address these, we introduce Cocoon, a
data profiling system that integrates LLMs to imbue statistical profiling with
semantics. Cocoon enhances traditional profiling methods by adding a three-step
process: Semantic Context, Semantic Profile, and Semantic Review. Our user
studies show that Cocoon is highly effective at accurately discerning whether
anomalies are genuine errors requiring correction or acceptable variations
based on the semantics for real-world datasets.
</summary>
    <author>
      <name>Zezhou Huang</name>
    </author>
    <author>
      <name>Eugene Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2404.12552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.12552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13091v1</id>
    <updated>2024-04-18T18:42:30Z</updated>
    <published>2024-04-18T18:42:30Z</published>
    <title>A process mining-based error correction approach to improve data quality
  of an IoT-sourced event log</title>
    <summary>  Internet of Things (IoT) systems are vulnerable to data collection errors and
these errors can significantly degrade the quality of collected data, impact
data analysis and lead to inaccurate or distorted results. This article
emphasizes the importance of evaluating data quality and errors before
proceeding with analysis and considering the effectiveness of error correction
methods for a smart home use case.
</summary>
    <author>
      <name>Mohsen Shirali</name>
    </author>
    <author>
      <name>Zahra Ahmadi</name>
    </author>
    <author>
      <name>Carlos Fernández-Llatas</name>
    </author>
    <author>
      <name>Jose-Luis Bayo-Monton</name>
    </author>
    <author>
      <name>Gemma Di Federico</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3680289</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3680289" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.13091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0; H.4.0; J.3; J.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.07081v1</id>
    <updated>2024-05-11T19:32:27Z</updated>
    <published>2024-05-11T19:32:27Z</published>
    <title>T-curator: a trust based curation tool for LOD logs</title>
    <summary>  Nowadays, companies are racing towards Linked Open Data (LOD) to improve
their added value, but they are ignoring their SPARQL query logs. If well
curated, these logs can present an asset for decision makers. A naive and
straightforward use of these logs is too risky because their provenance and
quality are highly questionable. Users of these logs in a trusted way have to
be assisted by providing them with in-depth knowledge of the whole LOD
environment and tools to curate these logs. In this paper, we propose an
interactive and intuitive trust based tool that can be used to curate these LOD
logs before exploiting them. This tool is proposed to support our approach
proposed in our previous work Lanasri et al. [2020].
</summary>
    <author>
      <name>Dihia Lanasri</name>
    </author>
    <link href="http://arxiv.org/abs/2405.07081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.07081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.10045v2</id>
    <updated>2024-06-27T08:12:59Z</updated>
    <published>2024-05-16T12:29:12Z</published>
    <title>Global Benchmark Database</title>
    <summary>  This paper presents Global Benchmark Database (GBD), a comprehensive suite of
tools for provisioning and sustainably maintaining benchmark instances and
their metadata. The availability of benchmark metadata is essential for many
tasks in empirical research, e.g., for the data-driven compilation of
benchmarks, the domain-specific analysis of runtime experiments, or the
instance-specific selection of solvers. In this paper, we introduce the data
model of GBD as well as its interfaces and provide examples of how to interact
with them. We also demonstrate the integration of custom data sources and
explain how to extend GBD with additional problem domains, instance formats and
feature extractors.
</summary>
    <author>
      <name>Markus Iser</name>
    </author>
    <author>
      <name>Christoph Jabs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.SAT.2024.18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.SAT.2024.18" rel="related"/>
    <link href="http://arxiv.org/abs/2405.10045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.10045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.14435v1</id>
    <updated>2024-05-23T11:03:09Z</updated>
    <published>2024-05-23T11:03:09Z</published>
    <title>High-Level Event Mining: Overview and Future Work</title>
    <summary>  Process mining traditionally relies on input consisting of low-level events
that capture individual activities, such as filling out a form or processing a
product. However, many of the complex problems inherent in processes, such as
bottlenecks and compliance issues, extend beyond the scope of individual events
and process instances. Consider congestion, for instance, it can involve and
impact numerous cases, much like how a traffic jam affects many cars
simultaneously. High-level event mining seeks to address such phenomena using
the regular event data available. This report offers an extensive and
comprehensive overview at existing work and challenges encountered when lifting
the perspective from individual events and cases to system-level events.
</summary>
    <author>
      <name>Bianka Bakullari</name>
    </author>
    <author>
      <name>Wil M. P. van der Aalst</name>
    </author>
    <link href="http://arxiv.org/abs/2405.14435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.14435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04995v4</id>
    <updated>2024-06-12T12:16:11Z</updated>
    <published>2024-06-07T15:06:36Z</published>
    <title>Data2Neo - A Tool for Complex Neo4j Data Integration</title>
    <summary>  This paper introduces Data2Neo, an open-source Python library for converting
relational data into knowledge graphs stored in Neo4j databases. With extensive
customization options and support for continuous online data integration from
various data sources, Data2Neo is designed to be user-friendly, efficient, and
scalable to large datasets. The tool significantly lowers the barrier to entry
for creating and using knowledge graphs, making this increasingly popular form
of data representation accessible to a wider audience. The code is available at
https://github.com/jkminder/data2neo .
</summary>
    <author>
      <name>Julian Minder</name>
    </author>
    <author>
      <name>Laurence Brandenberger</name>
    </author>
    <author>
      <name>Luis Salamanca</name>
    </author>
    <author>
      <name>Frank Schweitzer</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04995v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04995v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.05417v1</id>
    <updated>2024-06-08T09:41:08Z</updated>
    <published>2024-06-08T09:41:08Z</published>
    <title>Optimizing Navigational Graph Queries</title>
    <summary>  We study the optimization of navigational graph queries, i.e., queries which
combine recursive and pattern-matching fragments. Current approaches to their
evaluation are not effective in practice. Towards addressing this, we present a
number of novel powerful optimization techniques which aim to constrain the
intermediate results during query evaluation. We show how these techniques can
be planned effectively and executed efficiently towards the first practical
evaluation solution for complex navigational queries on real-world workloads.
Indeed, our experimental results show several orders of magnitude improvement
in query evaluation performance over state-of-the-art techniques on a wide
range of queries on diverse datasets.
</summary>
    <author>
      <name>Thomas Mulder</name>
    </author>
    <author>
      <name>George Fletcher</name>
    </author>
    <author>
      <name>Nikolay Yakovets</name>
    </author>
    <link href="http://arxiv.org/abs/2406.05417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.05417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.15655v1</id>
    <updated>2024-06-21T21:20:57Z</updated>
    <published>2024-06-21T21:20:57Z</published>
    <title>ProBE: Proportioning Privacy Budget for Complex Exploratory Decision
  Support</title>
    <summary>  This paper studies privacy in the context of complex decision support queries
composed of multiple conditions on different aggregate statistics combined
using disjunction and conjunction operators. Utility requirements for such
queries necessitate the need for private mechanisms that guarantee a bound on
the false negative and false positive errors. This paper formally defines
complex decision support queries and their accuracy requirements, and provides
algorithms that proportion the existing budget to optimally minimize privacy
loss while supporting a bounded guarantee on the accuracy. Our experimental
results on multiple real-life datasets show that our algorithms successfully
maintain such utility guarantees, while also minimizing privacy loss.
</summary>
    <author>
      <name>Nada Lahjouji</name>
    </author>
    <author>
      <name>Sameera Ghayyur</name>
    </author>
    <author>
      <name>Xi He</name>
    </author>
    <author>
      <name>Sharad Mehrotra</name>
    </author>
    <link href="http://arxiv.org/abs/2406.15655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.15655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.01127v2</id>
    <updated>2024-08-25T14:51:54Z</updated>
    <published>2024-07-01T09:49:21Z</published>
    <title>Tractable Circuits in Database Theory</title>
    <summary>  This work reviews how database theory uses tractable circuit classes from
knowledge compilation. We present relevant query evaluation tasks, and notions
of tractable circuits. We then show how these tractable circuits can be used to
address database tasks. We first focus on Boolean provenance and its
applications for aggregation tasks, in particular probabilistic query
evaluation. We study these for Monadic Second Order (MSO) queries on trees, and
for safe Conjunctive Queries (CQs) and Union of Conjunctive Queries (UCQs). We
also study circuit representations of query answers, and their applications to
enumeration tasks: both in the Boolean setting (for MSO) and the multivalued
setting (for CQs and UCQs).
</summary>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <author>
      <name>Florent Capelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages including 12 pages of main text</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.01127v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.01127v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.05096v1</id>
    <updated>2024-07-06T14:53:14Z</updated>
    <published>2024-07-06T14:53:14Z</published>
    <title>Database Technology Evolution III: Knowledge Graphs and Linked Data</title>
    <summary>  This paper reviews the changes for database technology represented by the
current development of the draft international standard ISO 39075 (Database
Languages - GQL), which seeks a unified specification for property graphs and
knowledge graphs. This paper examines these current developments as part of our
review of the evolution of database technology, and their relation to the
longer-term goal of supporting the Semantic Web using relational technology.
</summary>
    <author>
      <name>Malcolm Crowe</name>
    </author>
    <author>
      <name>Fritz Laux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IARIA, 2024, ISBN: 978-1-68558-180-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.05096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10440v1</id>
    <updated>2024-05-09T12:48:43Z</updated>
    <published>2024-05-09T12:48:43Z</published>
    <title>A novel multi-threaded web crawling model</title>
    <summary>  This paper proposes a novel model for web crawling suitable for large-scale
web data acquisition. This model first divides web data into several sub-data,
with each sub-data corresponding to a thread task. In each thread task, web
crawling tasks are concurrently executed, and the crawled data are stored in a
buffer queue, awaiting further parsing. The parsing process is also divided
into several threads. By establishing the model and continuously conducting
crawler tests, it is found that this model is significantly optimized compared
to single-threaded approaches.
</summary>
    <author>
      <name>Weijie. Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.10440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.12793v1</id>
    <updated>2024-06-19T06:01:28Z</updated>
    <published>2024-06-19T06:01:28Z</published>
    <title>Data Collection and Labeling Techniques for Machine Learning</title>
    <summary>  Data collection and labeling are critical bottlenecks in the deployment of
machine learning applications. With the increasing complexity and diversity of
applications, the need for efficient and scalable data collection and labeling
techniques has become paramount. This paper provides a review of the
state-of-the-art methods in data collection, data labeling, and the improvement
of existing data and models. By integrating perspectives from both the machine
learning and data management communities, we aim to provide a holistic view of
the current landscape and identify future research directions.
</summary>
    <author>
      <name>Qianyu Huang</name>
    </author>
    <author>
      <name>Tongfang Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2407.12793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18604v1</id>
    <updated>2024-07-26T08:55:00Z</updated>
    <published>2024-07-26T08:55:00Z</published>
    <title>Turning Multidimensional Big Data Analytics into Practice: Design and
  Implementation of ClustCube Big-Data Tools in Real-Life Scenarios</title>
    <summary>  Multidimensional Big Data Analytics is an emerging area that marries the
capabilities of OLAP with modern Big Data Analytics. Essentially, the idea is
engrafting multidimensional models into Big Data analytics processes to gain
into expressive power of the overall discovery task. ClustCube is a
state-of-the-art model that combines OLAP and Clustering, thus delving into
practical and well-understood advantages in the context of real-life
applications and systems. In this paper, we show how ClustCube can effectively
and efficiently realizing nice tools for supporting Multidimensional Big Data
Analytics, and assess these tools in the context of real-life research
projects.
</summary>
    <author>
      <name>Alfredo Cuzzocrea</name>
    </author>
    <author>
      <name>Abderraouf Hafsaoui</name>
    </author>
    <author>
      <name>Ismail Benlaredj</name>
    </author>
    <link href="http://arxiv.org/abs/2407.18604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.19095v1</id>
    <updated>2024-07-26T21:20:57Z</updated>
    <published>2024-07-26T21:20:57Z</published>
    <title>Towards A More Reasonable Semantic Web</title>
    <summary>  We aim to accelerate the original vision of the semantic web by revisiting
design decisions that have defined the semantic web up until now. We propose a
shift in direction that more broadly embraces existing data infrastructure by
reconsidering the semantic web's logical foundations. We argue to shift
attention away from description logic, which has so far underpinned the
semantic web, to a different fragment of first-order logic. We argue, using
examples from the (geo)spatial domain, that by doing so, the semantic web can
be approached as a traditional data migration and integration problem at a
massive scale. That way, a huge amount of existing tools and theories can be
deployed to the semantic web's benefit, and the original vision of ontology as
shared abstraction be reinvigorated.
</summary>
    <author>
      <name>Vleer Doing</name>
    </author>
    <author>
      <name>Ryan Wisnesky</name>
    </author>
    <link href="http://arxiv.org/abs/2407.19095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.19095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07607v1</id>
    <updated>2024-08-01T15:19:08Z</updated>
    <published>2024-08-01T15:19:08Z</published>
    <title>The MAGIC of Data Management: Understanding the Value and Activities of
  Data Management</title>
    <summary>  In an era dominated by information technology, the critical discipline of
data management remains undervalued compared to the innovations it enables,
such as artificial intelligence and social media. The ambiguity surrounding
what constitutes data management and its associated activities complicates
efforts to explain its importance and ensure data are collected, stored and
used in a way that maximizes value and avoids failures. This paper aims to
address these shortcomings by presenting a simple framework for understanding
data management, referred to as MAGIC. MAGIC encompasses five key activities:
Modeling, Acquisition, Governance, Infrastructuring, and Consumption support
tasks. By delineating these components, the MAGIC framework provides a clear,
accessible approach to data management that can be used for teaching, research
and practice.
</summary>
    <author>
      <name>Roman Lukyanenko</name>
    </author>
    <link href="http://arxiv.org/abs/2408.07607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.15776v2</id>
    <updated>2025-01-27T16:08:53Z</updated>
    <published>2024-08-28T13:15:14Z</published>
    <title>Enumeration of Minimal Hitting Sets Parameterized by Treewidth</title>
    <summary>  Enumerating the minimal hitting sets of a hypergraph is a problem which
arises in many data management applications that include constraint mining,
discovering unique column combinations, and enumerating database repairs.
Previously, Eiter et al. showed that the minimal hitting sets of an $n$-vertex
hypergraph, with treewidth $w$, can be enumerated with delay $O^*(n^{w})$
(ignoring polynomial factors), with space requirements that scale with the
output size. We improve this to fixed-parameter-linear delay, following an FPT
preprocessing phase. The memory consumption of our algorithm is exponential
with respect to the treewidth of the hypergraph.
</summary>
    <author>
      <name>Batya Kenig</name>
    </author>
    <author>
      <name>Dan Shlomo Mizrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICDT 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.15776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.15776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.01648v1</id>
    <updated>2024-09-03T06:36:39Z</updated>
    <published>2024-09-03T06:36:39Z</published>
    <title>Computing Range Consistent Answers to Aggregation Queries via Rewriting</title>
    <summary>  We consider the problem of answering conjunctive queries with aggregation on
database instances that may violate primary key constraints. In SQL, these
queries follow the SELECT-FROM-WHERE-GROUP BY format, where the WHERE-clause
involves a conjunction of equalities, and the SELECT-clause can incorporate
aggregate operators like MAX, MIN, SUM, AVG, or COUNT. Repairs of a database
instance are defined as inclusion-maximal subsets that satisfy all primary
keys. For a given query, our primary objective is to identify repairs that
yield the lowest aggregated value among all possible repairs. We particularly
investigate queries for which this lowest aggregated value can be determined
through a rewriting in first-order logic with aggregate operators.
</summary>
    <author>
      <name>Aziz Amezian El Khalfioui</name>
    </author>
    <author>
      <name>Jef Wijsen</name>
    </author>
    <link href="http://arxiv.org/abs/2409.01648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.01648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.04498v1</id>
    <updated>2024-09-06T15:41:24Z</updated>
    <published>2024-09-06T15:41:24Z</published>
    <title>Graph versioning for evolving urban data</title>
    <summary>  The continuous evolution of cities poses significant challenges in terms of
managing and understanding their complex dynamics. With the increasing demand
for transparency and the growing availability of open urban data, it has become
important to ensure the reproducibility of scientific research and computations
in urban planning. To understand past decisions and other possible scenarios,
we require solutions that go beyond the management of urban knowledge graphs.
In this work, we explore existing solutions and their limits and explain the
need and possible approaches for querying across multiple graph versions.
</summary>
    <author>
      <name>Jey Puget Gil</name>
    </author>
    <author>
      <name>Emmanuel Coquery</name>
    </author>
    <author>
      <name>John Samuel</name>
    </author>
    <author>
      <name>Gilles Gesquiere</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BDA, 23-26 oct. 2023, Montpellier, FRANCE</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.04498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.08013v1</id>
    <updated>2024-09-12T12:56:43Z</updated>
    <published>2024-09-12T12:56:43Z</published>
    <title>DPconv: Super-Polynomially Faster Join Ordering</title>
    <summary>  We revisit the join ordering problem in query optimization. The standard
exact algorithm, DPccp, has a worst-case running time of $O(3^n)$. This is
prohibitively expensive for large queries, which are not that uncommon anymore.
We develop a new algorithmic framework based on subset convolution. DPconv
achieves a super-polynomial speedup over DPccp, breaking the $O(3^n)$
time-barrier for the first time. We show that the instantiation of our
framework for the $C_\max$ cost function is up to 30x faster than DPccp for
large clique queries.
</summary>
    <author>
      <name>Mihail Stoian</name>
    </author>
    <author>
      <name>Andreas Kipf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at SIGMOD 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.08013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.08013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.10635v1</id>
    <updated>2024-09-16T18:06:41Z</updated>
    <published>2024-09-16T18:06:41Z</published>
    <title>Development of Data Evaluation Benchmark for Data Wrangling
  Recommendation System</title>
    <summary>  CoWrangler is a data-wrangling recommender system designed to streamline data
processing tasks. Recognizing that data processing is often time-consuming and
complex for novice users, we aim to simplify the decision-making process
regarding the most effective subsequent data operation. By analyzing over
10,000 Kaggle notebooks spanning approximately 1,000 datasets, we derive
insights into common data processing strategies employed by users across
various tasks. This analysis helps us understand how dataset quality influences
wrangling operations, informing our ongoing efforts to possibly expand our
dataset sources in the future.
</summary>
    <author>
      <name>Yuqing Wang</name>
    </author>
    <author>
      <name>Anna Fariha</name>
    </author>
    <link href="http://arxiv.org/abs/2409.10635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.10635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.01760v1</id>
    <updated>2024-10-02T17:14:47Z</updated>
    <published>2024-10-02T17:14:47Z</published>
    <title>Competitive Ratio of Online Caching with Predictions: Lower and Upper
  Bounds</title>
    <summary>  We address the problem of learning-augmented online caching in the scenario
when each request is accompanied by a prediction of the next occurrence of the
requested page. We improve currently known bounds on the competitive ratio of
the BlindOracle algorithm, which evicts a page predicted to be requested last.
We also prove a lower bound on the competitive ratio of any randomized
algorithm and show that a combination of the BlindOracle with the Marker
algorithm achieves a competitive ratio that is optimal up to some constant.
</summary>
    <author>
      <name>Daniel Skachkov</name>
    </author>
    <author>
      <name>Denis Ponomaryov</name>
    </author>
    <author>
      <name>Yuri Dorn</name>
    </author>
    <author>
      <name>Alexander Demin</name>
    </author>
    <link href="http://arxiv.org/abs/2410.01760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.01760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.01978v2</id>
    <updated>2025-03-22T20:58:34Z</updated>
    <published>2024-10-02T19:35:35Z</published>
    <title>LLM+KG@VLDB'24 Workshop Summary</title>
    <summary>  The unification of large language models (LLMs) and knowledge graphs (KGs)
has emerged as a hot topic. At the LLM+KG'24 workshop, held in conjunction with
VLDB 2024 in Guangzhou, China, one of the key themes explored was important
data management challenges and opportunities due to the effective interaction
between LLMs and KGs. This report outlines the major directions and approaches
presented by various speakers during the LLM+KG'24 workshop.
</summary>
    <author>
      <name>Arijit Khan</name>
    </author>
    <author>
      <name>Tianxing Wu</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at ACM SIGMOD Record 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.01978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.01978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.06062v4</id>
    <updated>2025-02-10T10:55:08Z</updated>
    <published>2024-10-08T14:09:12Z</published>
    <title>LLM-based SPARQL Query Generation from Natural Language over Federated
  Knowledge Graphs</title>
    <summary>  We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.
</summary>
    <author>
      <name>Vincent Emonet</name>
    </author>
    <author>
      <name>Jerven Bolleman</name>
    </author>
    <author>
      <name>Severine Duvaud</name>
    </author>
    <author>
      <name>Tarcisio Mendes de Farias</name>
    </author>
    <author>
      <name>Ana Claudia Sima</name>
    </author>
    <link href="http://arxiv.org/abs/2410.06062v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.06062v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.09244v1</id>
    <updated>2024-10-11T20:41:04Z</updated>
    <published>2024-10-11T20:41:04Z</published>
    <title>Using off-the-shelf LLMs to query enterprise data by progressively
  revealing ontologies</title>
    <summary>  Ontologies are known to improve the accuracy of Large Language Models (LLMs)
when translating natural language queries into a formal query language like SQL
or SPARQL. There are two ways to leverage ontologies when working with LLMs.
One is to fine-tune the model, i.e., to enhance it with specific domain
knowledge. Another is the zero-shot prompting approach, where the ontology is
provided as part of the input question. Unfortunately, modern enterprises
typically have ontologies that are too large to fit in a prompt due to LLM's
token size limitations. We present a solution that incrementally reveals "just
enough" of an ontology that is needed to answer a given question.
</summary>
    <author>
      <name>C. Civili</name>
    </author>
    <author>
      <name>E. Sherkhonov</name>
    </author>
    <author>
      <name>R. E. K. Stirewalt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.09244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.09244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.12056v1</id>
    <updated>2024-10-15T20:51:56Z</updated>
    <published>2024-10-15T20:51:56Z</published>
    <title>Utilizing Spatiotemporal Data Analytics to Pinpoint Outage Location</title>
    <summary>  Understanding the exact fault location in the post-event analysis is the key
to improving the accuracy of outage management. Unfortunately, the fault
location is not generally well documented during the restoration process,
creating a big challenge for post-event analysis. By utilizing various data
source systems, including outage management system (OMS) data, asset geospatial
information system (GIS) data, and vehicle location data, this paper creates a
novel method to pinpoint the outage location accurately to create additional
insights for distribution operations and performance teams during the
post-event analysis.
</summary>
    <author>
      <name>Reddy Mandati</name>
    </author>
    <author>
      <name>Po-Chen Chen</name>
    </author>
    <author>
      <name>Vladyslav Anderson</name>
    </author>
    <author>
      <name>Bishwa Sapkota</name>
    </author>
    <author>
      <name>Michael Jarrell Warren</name>
    </author>
    <author>
      <name>Bobby Besharati</name>
    </author>
    <author>
      <name>Ankush Agarwal</name>
    </author>
    <author>
      <name>Samuel Johnston III</name>
    </author>
    <link href="http://arxiv.org/abs/2410.12056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.12056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.15547v1</id>
    <updated>2024-10-21T00:29:40Z</updated>
    <published>2024-10-21T00:29:40Z</published>
    <title>Data Cleaning Using Large Language Models</title>
    <summary>  Data cleaning is a crucial yet challenging task in data analysis, often
requiring significant manual effort. To automate data cleaning, previous
systems have relied on statistical rules derived from erroneous data, resulting
in low accuracy and recall. This work introduces Cocoon, a novel data cleaning
system that leverages large language models for rules based on semantic
understanding and combines them with statistical error detection. However, data
cleaning is still too complex a task for current LLMs to handle in one shot. To
address this, we introduce Cocoon, which decomposes complex cleaning tasks into
manageable components in a workflow that mimics human cleaning processes. Our
experiments show that Cocoon outperforms state-of-the-art data cleaning systems
on standard benchmarks.
</summary>
    <author>
      <name>Shuo Zhang</name>
    </author>
    <author>
      <name>Zezhou Huang</name>
    </author>
    <author>
      <name>Eugene Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.22846v1</id>
    <updated>2024-10-30T09:27:43Z</updated>
    <published>2024-10-30T09:27:43Z</published>
    <title>Knowledge Graph Based Visual Search Application</title>
    <summary>  The FAIR data principles advocate for making scientific and research datasets
'Findable' and 'Accessible'. Yet, the sheer volume and diversity of these
datasets present significant challenges. Despite advancements in data search
technologies, techniques for representing search results are still traditional
and inadequate, often returning extraneous results. To address these issues, we
developed a knowledge graph based visual search application designed to enhance
data search for Earth System Scientists. This application utilizes various
chart widgets and a knowledge graph at the backend, connecting two disparate
data repositories.
</summary>
    <author>
      <name>Pawandeep Kaur Betz</name>
    </author>
    <author>
      <name>Tobias Hecking</name>
    </author>
    <author>
      <name>Andreas Schreiber</name>
    </author>
    <author>
      <name>Andreas Gerndt</name>
    </author>
    <link href="http://arxiv.org/abs/2410.22846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.22846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02862v1</id>
    <updated>2024-11-05T07:10:00Z</updated>
    <published>2024-11-05T07:10:00Z</published>
    <title>The Unreasonable Effectiveness of LLMs for Query Optimization</title>
    <summary>  Recent work in database query optimization has used complex machine learning
strategies, such as customized reinforcement learning schemes. Surprisingly, we
show that LLM embeddings of query text contain useful semantic information for
query optimization. Specifically, we show that a simple binary classifier
deciding between alternative query plans, trained only on a small number of
labeled embedded query vectors, can outperform existing heuristic systems.
Although we only present some preliminary results, an LLM-powered query
optimizer could provide significant benefits, both in terms of performance and
simplicity.
</summary>
    <author>
      <name>Peter Akioyamen</name>
    </author>
    <author>
      <name>Zixuan Yi</name>
    </author>
    <author>
      <name>Ryan Marcus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Machine Learning for Systems Workshop at NeurIPS
  2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.02862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.04304v1</id>
    <updated>2024-11-06T23:07:14Z</updated>
    <published>2024-11-06T23:07:14Z</published>
    <title>Don't go gaga with GIGO</title>
    <summary>  We revisit integrity checking in relational and deductive databases with an
approach that tolerates erroneous, inconsistent data. In particular, we relax
the fundamental prerequisite that, in order to apply any method for simplified
integrity checking, all data must initially have integrity. As opposed to a
long-standing belief, integrity in the old state before the update is not
needed for a correct application of simplification methods. Rather, we show
that correct simplifications preserve what was consistent across updates. We
formally characterize this property, that we call inconsistency tolerance, and
state its validity for some well-known methods for integrity checking.
</summary>
    <author>
      <name>Hendrik Decker</name>
    </author>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, no figures. arXiv admin note: substantial text overlap with
  arXiv:1312.2353</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.04304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.04304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.13278v1</id>
    <updated>2024-11-20T12:46:41Z</updated>
    <published>2024-11-20T12:46:41Z</published>
    <title>Introducing Schema Inference as a Scalable SQL Function [Extended
  Version]</title>
    <summary>  This paper introduces a novel approach to schema inference as an on-demand
function integrated directly within a DBMS, targeting NoSQL databases where
schema flexibility can create challenges. Unlike previous methods relying on
external frameworks like Apache Spark, our solution enables schema inference as
a SQL function, allowing users to infer schemas natively within the DBMS.
Implemented in Apache AsterixDB, it performs schema discovery in two phases,
local inference and global schema merging, leveraging internal resources for
improved performance. Experiments with real world datasets show up to a two
orders of magnitude performance boost over external methods, enhancing
usability and scalability.
</summary>
    <author>
      <name>Calvin Dani</name>
    </author>
    <author>
      <name>Shiva Jahangiri</name>
    </author>
    <author>
      <name>Thomas Hütter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of EDBT 2025 submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.13278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.13278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.14968v1</id>
    <updated>2024-11-22T14:24:51Z</updated>
    <published>2024-11-22T14:24:51Z</published>
    <title>Optimization Strategies for Parallel Computation of Skylines</title>
    <summary>  Skyline queries are one of the most widely adopted tools for Multi-Criteria
Analysis, with applications covering diverse domains, including, e.g., Database
Systems, Data Mining, and Decision Making. Skylines indeed offer a useful
overview of the most suitable alternatives in a dataset, while discarding all
the options that are dominated by (i.e., worse than) others.
  The intrinsically quadratic complexity associated with skyline computation
has pushed researchers to identify strategies for parallelizing the task,
particularly by partitioning the dataset at hand. In this paper, after
reviewing the main partitioning approaches available in the relevant
literature, we propose two orthogonal optimization strategies for reducing the
computational overhead, and compare them experimentally in a multi-core
environment equipped with PySpark.
</summary>
    <author>
      <name>Paolo Ciaccia</name>
    </author>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.14968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.14968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.16254v1</id>
    <updated>2024-11-25T10:15:38Z</updated>
    <published>2024-11-25T10:15:38Z</published>
    <title>Asynchronous I/O -- With Great Power Comes Great Responsibility</title>
    <summary>  The performance of storage hardware has improved vastly recently, leaving the
traditional I/O stack incapable of exploiting these gains due to increasingly
large relative overheads. Newer asynchronous I/O APIs, such as io_uring, have
significantly improved performance by reducing such overheads, but exhibit
limited adoption in practice. In this paper, we discuss the complexities that
the usage of these contemporary I/O APIs introduces to applications, which we
believe are mostly responsible for their low adoption rate. Finally, we share
implications and trade offs made by architectures that may be used to integrate
asynchronous I/O into DB applications.
</summary>
    <author>
      <name>Constantin Pestka</name>
    </author>
    <author>
      <name>Marcus Paradies</name>
    </author>
    <author>
      <name>Matthias Pohl</name>
    </author>
    <link href="http://arxiv.org/abs/2411.16254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.16254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.16742v1</id>
    <updated>2024-11-23T19:20:24Z</updated>
    <published>2024-11-23T19:20:24Z</published>
    <title>Text-to-SQL Calibration: No Need to Ask -- Just Rescale Model
  Probabilities</title>
    <summary>  Calibration is crucial as large language models (LLMs) are increasingly
deployed to convert natural language queries into SQL for commercial databases.
In this work, we investigate calibration techniques for assigning confidence to
generated SQL queries. We show that a straightforward baseline -- deriving
confidence from the model's full-sequence probability -- outperforms recent
methods that rely on follow-up prompts for self-checking and confidence
verbalization. Our comprehensive evaluation, conducted across two widely-used
Text-to-SQL benchmarks and multiple LLM architectures, provides valuable
insights into the effectiveness of various calibration strategies.
</summary>
    <author>
      <name>Ashwin Ramachandran</name>
    </author>
    <author>
      <name>Sunita Sarawagi</name>
    </author>
    <link href="http://arxiv.org/abs/2411.16742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.16742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.04294v1</id>
    <updated>2024-12-05T16:13:37Z</updated>
    <published>2024-12-05T16:13:37Z</published>
    <title>A Formalization of Top-Down Unnesting</title>
    <summary>  When writing SQL queries, it is often convenient to use correlated
subqueries. However, for the database engine, these correlated queries are very
difficult to evaluate efficiently. The query optimizer will therefore try to
eliminate the correlations, a process referred to as unnesting.
  Recent work has introduced a single pass top-down algorithm for unnesting
arbitrary SQL queries. That work did not include a formal proof of correctness,
though. In this work we provide the missing formalization by formally defining
the operator semantics and proving that the unnesting algorithm is correct.
</summary>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/2412.04294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.04294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.07786v1</id>
    <updated>2024-11-25T19:57:16Z</updated>
    <published>2024-11-25T19:57:16Z</published>
    <title>Towards Agentic Schema Refinement</title>
    <summary>  Large enterprise databases can be complex and messy, obscuring the data
semantics needed for analytical tasks. We propose a semantic layer in-between
the database and the user as a set of small and easy-to-interpret database
views, effectively acting as a refined version of the schema. To discover these
views, we introduce a multi-agent Large Language Model (LLM) simulation where
LLM agents collaborate to iteratively define and refine views with minimal
input. Our approach paves the way for LLM-powered exploration of unwieldy
databases.
</summary>
    <author>
      <name>Agapi Rissaki</name>
    </author>
    <author>
      <name>Ilias Fountalis</name>
    </author>
    <author>
      <name>Nikolaos Vasiloglou</name>
    </author>
    <author>
      <name>Wolfgang Gatterbauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the Table Representation Learning Workshop, NeurIPS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.07786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.08583v1</id>
    <updated>2024-12-11T17:59:58Z</updated>
    <published>2024-12-11T17:59:58Z</published>
    <title>A Principled Solution to the Disjunction Problem of Diagrammatic Query
  Representations</title>
    <summary>  Finding unambiguous diagrammatic representations for first-order logical
formulas and relational queries with arbitrarily nested disjunctions has been a
surprisingly long-standing unsolved problem. We refer to this problem as the
disjunction problem (of diagrammatic query representations).
  This work solves the disjunction problem. Our solution unifies, generalizes,
and overcomes the shortcomings of prior approaches for disjunctions. It extends
the recently proposed Relational Diagrams and is identical for disjunction-free
queries. However, it can preserve the relational patterns and the safety for
all well-formed Tuple Relational Calculus (TRC) queries, even with arbitrary
disjunctions. Additionally, its size is proportional to the original TRC query
and can thus be exponentially more succinct than Relational Diagrams.
</summary>
    <author>
      <name>Wolfgang Gatterbauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 27 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.08583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.08583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12938v1</id>
    <updated>2024-12-17T14:18:07Z</updated>
    <published>2024-12-17T14:18:07Z</published>
    <title>A Conceptual Model of Intelligent Multimedia Data Rendered using Flying
  Light Specks</title>
    <summary>  A Flying Light Speck, FLS, is a miniature sized drone configured with light
sources to illuminate 3D multimedia objects in a fixed volume, an FLS display.
A swarm of FLSs may provide haptic interactions by exerting force back at a
user's touch. This paper presents a conceptual model for the multimedia data to
enable content-based queries. The model empowers users of an FLS display to
annotate the illuminations by adding semantics to the data, extending a
multimedia repository with information and knowledge. We present a core
conceptual model and demonstrate its extensions for two diverse applications,
authoring tools with entertainment and MRI scans with healthcare.
</summary>
    <author>
      <name>Nima Yazdani</name>
    </author>
    <author>
      <name>Hamed Alimohammadzadeh</name>
    </author>
    <author>
      <name>Shahram Ghandeharizadeh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.61981/ZFSH2309</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.61981/ZFSH2309" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in the First International Conference on Holodecks</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.12938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.12938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.17892v1</id>
    <updated>2024-12-23T18:39:11Z</updated>
    <published>2024-12-23T18:39:11Z</published>
    <title>LLM-Driven Feedback for Enhancing Conceptual Design Learning in Database
  Systems Courses</title>
    <summary>  The integration of LLM-generated feedback into educational settings has shown
promise in enhancing student learning outcomes. This paper presents a novel
LLM-driven system that provides targeted feedback for conceptual designs in a
Database Systems course. The system converts student-created
entity-relationship diagrams (ERDs) into JSON format, allows the student to
prune the diagram by isolating a relationship, extracts relevant requirements
for the selected relationship, and utilizes a large language model (LLM) to
generate detailed feedback. Additionally, the system creates a tailored set of
questions and answers to further aid student understanding. Our pilot
implementation in a Database System course demonstrates effective feedback
generation that helped the students improve their design skills.
</summary>
    <author>
      <name>Sara Riazi</name>
    </author>
    <author>
      <name>Pedram Rooshenas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3641554.3701940</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3641554.3701940" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGCSE 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2412.17892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.17892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18022v1</id>
    <updated>2024-12-23T22:34:40Z</updated>
    <published>2024-12-23T22:34:40Z</published>
    <title>Trustworthy and Efficient LLMs Meet Databases</title>
    <summary>  In the rapidly evolving AI era with large language models (LLMs) at the core,
making LLMs more trustworthy and efficient, especially in output generation
(inference), has gained significant attention. This is to reduce plausible but
faulty LLM outputs (a.k.a hallucinations) and meet the highly increased
inference demands. This tutorial explores such efforts and makes them
transparent to the database community. Understanding these efforts is essential
in harnessing LLMs in database tasks and adapting database techniques to LLMs.
Furthermore, we delve into the synergy between LLMs and databases, highlighting
new opportunities and challenges in their intersection. This tutorial aims to
share with database researchers and practitioners essential concepts and
strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining
in the intersection between LLMs and databases.
</summary>
    <author>
      <name>Kyoungmin Kim</name>
    </author>
    <author>
      <name>Anastasia Ailamaki</name>
    </author>
    <link href="http://arxiv.org/abs/2412.18022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18143v1</id>
    <updated>2024-12-24T03:55:24Z</updated>
    <published>2024-12-24T03:55:24Z</published>
    <title>NoSQL Graph Databases: an overview</title>
    <summary>  Graphs are the most suitable structures for modeling objects and interactions
in applications where component inter-connectivity is a key feature. There has
been increased interest in graphs to represent domains such as social networks,
web site link structures, and biology. Graph stores recently rose to prominence
along the NoSQL movement. In this work we will focus on NOSQL graph databases,
describing their peculiarities that sets them apart from other data storage and
management solutions, and how they differ among themselves. We will also
analyze in-depth two different graph database management systems - AllegroGraph
and Neo4j that uses the most popular graph models used by NoSQL stores in
practice: the resource description framework (RDF) and the labeled property
graph (LPG), respectively.
</summary>
    <author>
      <name>Veronica Santos</name>
    </author>
    <author>
      <name>Bruno Cuconato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Monografias em Ci\^encia da Computa\c{c}\~ao, PUC-Rio</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.18143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.20871v1</id>
    <updated>2024-12-30T11:21:15Z</updated>
    <published>2024-12-30T11:21:15Z</published>
    <title>Simplified integrity checking for an expressive class of denial
  constraints</title>
    <summary>  Data integrity is crucial for ensuring data correctness and quality,
maintained through integrity constraints that must be continuously checked,
especially in data-intensive systems like OLTP. While DBMSs handle common
constraints well, complex constraints often require ad-hoc solutions. Research
since the 1980s has focused on automatic and simplified integrity constraint
checking, leveraging the assumption that databases are consistent before
updates. This paper discusses using program transformation operators to
generate simplified integrity constraints, focusing on complex constraints
expressed in denial form. In particular, we target a class of integrity
constraints, called extended denials, which are more general than
tuple-generating dependencies and equality-generating dependencies. These
techniques can be readily applied to standard database practices and can be
directly translated into SQL.
</summary>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.20871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.03850v1</id>
    <updated>2025-01-07T15:07:07Z</updated>
    <published>2025-01-07T15:07:07Z</published>
    <title>Partitioning Strategies for Parallel Computation of Flexible Skylines</title>
    <summary>  While classical skyline queries identify interesting data within large
datasets, flexible skylines introduce preferences through constraints on
attribute weights, and further reduce the data returned. However, computing
these queries can be time-consuming for large datasets. We propose and
implement a parallel computation scheme consisting of a parallel phase followed
by a sequential phase, and apply it to flexible skylines. We assess the
additional effect of an initial filtering phase to reduce dataset size before
parallel processing, and the elimination of the sequential part (the most
time-consuming) altogether. All our experiments are executed in the PySpark
framework for a number of different datasets of varying sizes and dimensions.
</summary>
    <author>
      <name>Emilio De Lorenzis</name>
    </author>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.03850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.03850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.04347v1</id>
    <updated>2025-01-08T08:42:51Z</updated>
    <published>2025-01-08T08:42:51Z</published>
    <title>Keyword Search in the Deep Web</title>
    <summary>  The Deep Web is constituted by data that are accessible through Web pages,
but not readily indexable by search engines as they are returned in dynamic
pages. In this paper we propose a conceptual framework for answering keyword
queries on Deep Web sources represented as relational tables with so-called
access limitations. We formalize the notion of optimal answer, characterize
queries for which an answer can be found, and present a method for query
processing based on the construction of a query plan that minimizes the
accesses to the data sources.
</summary>
    <author>
      <name>Andrea Calì</name>
    </author>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <author>
      <name>Riccardo Torlone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-46397-1_20</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-46397-1_20" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 3 figures. A short version appeared in ER 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ER 2016: 260-268 [short version]</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.04347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.06978v1</id>
    <updated>2025-01-12T23:30:30Z</updated>
    <published>2025-01-12T23:30:30Z</published>
    <title>Towards a visually interpretable analysis of Two-Phase Locking
  membership</title>
    <summary>  Two-phase locking (2PL) is a consolidated policy commonly adopted by Database
Management Systems to enforce serializability of a schedule. While the policy
is well understood, both in its standard and in the strict version,
automatically deriving a suitable tabular/graphical analysis of schedules with
respect to 2PL is far from trivial, and requires several technicalities that do
not straightforwardly translate to visual cues. In this paper, we delve into
the details of the development of a tool for 2PL analysis.
</summary>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.06978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.07689v1</id>
    <updated>2025-01-13T21:05:04Z</updated>
    <published>2025-01-13T21:05:04Z</published>
    <title>Real-Time Outlier Connections Detection in Databases Network Traffic</title>
    <summary>  The article describes a practical method for detecting outlier database
connections in real-time. Outlier connections are detected with a specified
level of confidence. The method is based on generalized security rules and a
simple but effective real-time machine learning mechanism. The described method
is non-intrusive to the database and does not depend on the type of database.
The method is used to proactively control access even before database
connection is established, minimize false positives, and maintain the required
response speed to detected database connection outliers. The capabilities of
the system are demonstrated with several examples of outliers in real-world
scenarios.
</summary>
    <author>
      <name>Leonid Rodniansky</name>
    </author>
    <author>
      <name>Tania Butovsky</name>
    </author>
    <author>
      <name>Mikhail Shpak</name>
    </author>
    <link href="http://arxiv.org/abs/2501.07689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.07689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.02495v3</id>
    <updated>2025-02-28T00:40:10Z</updated>
    <published>2025-02-04T17:12:23Z</published>
    <title>The Causal-Effect Score in Data Management</title>
    <summary>  The Causal Effect (CE) is a numerical measure of causal influence of
variables on observed results. Despite being widely used in many areas, only
preliminary attempts have been made to use CE as an attribution score in data
management, to measure the causal strength of tuples for query answering in
databases. In this work, we introduce, generalize and investigate the so-called
Causal-Effect Score in the context of classical and probabilistic databases.
</summary>
    <author>
      <name>Felipe Azua</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 4th Conference on Causal Learning and
  Reasoning, 2025. This is the camera-ready version, and included a couple of
  new references</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.02495v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.02495v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.11459v1</id>
    <updated>2025-02-17T05:33:50Z</updated>
    <published>2025-02-17T05:33:50Z</published>
    <title>Towards Responsible and Fair Data Science: Resource Allocation for
  Inclusive and Sustainable Analytics</title>
    <summary>  This project addresses the challenges of responsible and fair resource
allocation in data science (DS), focusing on DS queries evaluation. Current DS
practices often overlook the broader socio-economic, environmental, and ethical
implications, including data sovereignty, fairness, and inclusivity. By
integrating a decolonial perspective, the project aims to establish innovative
fairness metrics that respect cultural and contextual diversity, optimise
computational and energy efficiency, and ensure equitable participation of
underrepresented communities. The research includes developing algorithms to
align resource allocation with fairness constraints, incorporating ethical and
sustainability considerations, and fostering interdisciplinary collaborations
to bridge technical advancements and societal impact gaps. This work aims to
reshape into an equitable, transparent, and community-empowering practice
challenging the technological power developed by the Big Tech.
</summary>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <link href="http://arxiv.org/abs/2502.11459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.11459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14745v1</id>
    <updated>2025-02-20T17:16:10Z</updated>
    <published>2025-02-20T17:16:10Z</published>
    <title>SQL4NN: Validation and expressive querying of models as data</title>
    <summary>  We consider machine learning models, learned from data, to be an important,
intensional, kind of data in themselves. As such, various analysis tasks on
models can be thought of as queries over this intensional data, often combined
with extensional data such as data for training or validation. We demonstrate
that relational database systems and SQL can actually be well suited for many
such tasks.
</summary>
    <author>
      <name>Mark Gerarts</name>
    </author>
    <author>
      <name>Juno Steegmans</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/2502.14745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.18221v1</id>
    <updated>2025-02-25T14:05:44Z</updated>
    <published>2025-02-25T14:05:44Z</published>
    <title>Improving Unstructured Data Quality via Updatable Extracted Views</title>
    <summary>  Improving data quality in unstructured documents is a long-standing
challenge. Unstructured data, especially in textual form, inherently lacks
defined semantics, which poses significant challenges for effective processing
and for ensuring data quality. We propose leveraging information extraction
algorithms to design, apply, and explain data cleaning processes for documents.
Specifically, for a simple document update model, we identify and verify a set
of sufficient conditions for rule-based extraction programs to qualify for
inclusion in our document cleaning framework. Through experiments conducted on
medical records, we demonstrate that our approach provides an effective
framework for identifying and correcting data quality problems, thereby
highlighting its practical value in real-world applications.
</summary>
    <author>
      <name>Besat Kassaie</name>
    </author>
    <author>
      <name>Frank Wm. Tompa</name>
    </author>
    <link href="http://arxiv.org/abs/2502.18221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; I.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02688v1</id>
    <updated>2025-03-04T15:02:15Z</updated>
    <published>2025-03-04T15:02:15Z</published>
    <title>A user-friendly SPARQL query editor powered by lightweight metadata</title>
    <summary>  SPARQL query editors often lack intuitive interfaces to aid SPARQL-savvy
users to write queries. To address this issue, we propose an easy-to-deploy,
triple store-agnostic and open-source query editor that offers three main
features: (i) automatic query example rendering, (ii) precise autocomplete
based on existing triple patterns including within SERVICE clauses, and (iii) a
data-aware schema visualization. It can be easily set up with a custom HTML
element. The tool has been successfully tested on various public endpoints, and
is deployed online at https://sib-swiss.github.io/sparql-editor with
open-source code available at https://github.com/sib-swiss/sparql-editor.
</summary>
    <author>
      <name>Vincent Emonet</name>
    </author>
    <author>
      <name>Ana-Claudia Sima</name>
    </author>
    <author>
      <name>Tarcisio Mendes de Farias</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14469v1</id>
    <updated>2025-03-18T17:45:32Z</updated>
    <published>2025-03-18T17:45:32Z</published>
    <title>Attribution Score Alignment in Explainable Data Management</title>
    <summary>  Different attribution-scores have been proposed to quantify the relevance of
database tuples for a query answer from a database. Among them, we find Causal
Responsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal
Effect. They have been analyzed in isolation, mainly in terms of computational
properties. In this work, we start an investigation into the alignment of these
scores on the basis of the queries at hand; that is, on whether they induce
compatible rankings of tuples. We are able to identify vast classes of queries
for which some pairs of scores are always aligned, and others for which they
are not. It turns out that the presence of exogenous tuples makes a crucial
difference in this regard.
</summary>
    <author>
      <name>Felipe Azua</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16042v1</id>
    <updated>2025-03-20T11:21:01Z</updated>
    <published>2025-03-20T11:21:01Z</published>
    <title>Sustainable Open-Data Management for Field Research: A Cloud-Based
  Approach in the Underlandscape Project</title>
    <summary>  Field-based research projects require a robust suite of ICT services to
support data acquisition, documentation, storage, and dissemination. A key
challenge lies in ensuring the sustainability of data management - not only
during the project's funded period but also beyond its conclusion, when
maintenance and support often depend on voluntary efforts. In the
Underlandscape project, we tackled this challenge by extensively leveraging
public cloud services while minimizing reliance on complex custom
infrastructure. This paper provides a comprehensive overview of the project's
final infrastructure, detailing the adopted data formats, the cloud-based
solutions enabling data management, and the custom applications developed for
system integration.
</summary>
    <author>
      <name>Augusto Ciuffoletti</name>
    </author>
    <author>
      <name>Letizia Chiti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.13940.05761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.13940.05761" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08524v1</id>
    <updated>2017-07-26T16:21:32Z</updated>
    <published>2017-07-26T16:21:32Z</published>
    <title>The Shape Metric for Clustering Algorithms</title>
    <summary>  We construct a method by which we can calculate the precision with which an
algorithm identifies the shape of a cluster. We present our results for several
well known clustering algorithms and suggest ways to improve performance for
newer algorithms.
</summary>
    <author>
      <name>Clark Alexander</name>
    </author>
    <author>
      <name>Sofya Akhmametyeva</name>
    </author>
    <link href="http://arxiv.org/abs/1707.08524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08978v2</id>
    <updated>2015-12-02T21:30:41Z</updated>
    <published>2015-06-30T07:55:46Z</published>
    <title>Big Data Technology Literature Review</title>
    <summary>  A short overview of various algorithms and technologies that are helpful for
big data storage and manipulation. Includes pointers to papers for further
reading, and, where applicable, pointers to open source projects implementing a
described storage type.
</summary>
    <author>
      <name>Michael Bar-Sinai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.04846v1</id>
    <updated>2023-01-12T07:17:42Z</updated>
    <published>2023-01-12T07:17:42Z</published>
    <title>Algebraic Model Management: A Survey</title>
    <summary>  We survey the field of model management and describe a new model management
approach based on algebraic specification.
</summary>
    <author>
      <name>Patrick Schultz</name>
    </author>
    <author>
      <name>David I. Spivak</name>
    </author>
    <author>
      <name>Ryan Wisnesky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-72044-9_5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-72044-9_5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in WADT 2016, Recent Trends in Algebraic Development
  Techniques</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.04846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.07354v3</id>
    <updated>2024-06-10T22:29:14Z</updated>
    <published>2023-07-14T14:02:20Z</published>
    <title>PG-Triggers: Triggers for Property Graphs</title>
    <summary>  Graph databases are emerging as the leading data management technology for
storing large knowledge graphs; significant efforts are ongoing to produce new
standards (such as the Graph Query Language, GQL), as well as enrich them with
properties, types, schemas, and keys. In this article, we introduce
PG-Triggers, a complete proposal for adding triggers to Property Graphs, along
the direction marked by the SQL3 Standard. We define the syntax and semantics
of PG-Triggers and then illustrate how they can be implemented on top of Neo4j,
one of the most popular graph databases. In particular, we introduce a
syntax-directed translation from PG-Triggers into Neo4j, which makes use of the
so-called {\it APOC triggers}; APOC is a community-contributed library for
augmenting the Cypher query language supported by Neo4j. We also cover
Memgraph, and show that our approach applies to this system in a similar way.
We illustrate the use of PG-Triggers through a life science application
inspired by the COVID-19 pandemic. The main objective of this article is to
introduce an active database standard for graph databases as a first-class
citizen at a time when reactive graph management is in its infancy, so as to
minimize the conversion efforts towards a full-fledged standard proposal.
</summary>
    <author>
      <name>Stefano Ceri</name>
    </author>
    <author>
      <name>Anna Bernasconi</name>
    </author>
    <author>
      <name>Alessia Gagliardi</name>
    </author>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <author>
      <name>Luigi Bellomarini</name>
    </author>
    <author>
      <name>Davide Magnanimi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3626246.3653386</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3626246.3653386" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Companion of the 2024 International Conference on Management of
  Data (SIGMOD/PODS '24). Association for Computing Machinery, New York, NY,
  USA, 373-385</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2307.07354v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07354v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106021v1</id>
    <updated>2001-06-11T11:46:14Z</updated>
    <published>2001-06-11T11:46:14Z</published>
    <title>Object-oriented solutions</title>
    <summary>  In this paper are briefly outlined the motivations, mathematical ideas in
use, pre-formalization and assumptions, object-as-functor construction, `soft'
types and concept constructions, case study for concepts based on variable
domains, extracting a computational background, and examples of evaluations.
</summary>
    <author>
      <name>Viacheslav Wolfengagen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2-nd International Workshop on Advances in
  Databases and Information Systems, ADBIS'95, Moscow, June 27 --30, 1995, Vol.
  1, pp. 235--246</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0106021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.1; F.1; F.4.1; D.1.1; H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406058v2</id>
    <updated>2004-07-14T12:47:52Z</updated>
    <published>2004-06-29T11:05:29Z</published>
    <title>Proofs of Zero Knowledge</title>
    <summary>  We present a protocol for verification of ``no such entry'' replies from
databases. We introduce a new cryptographic primitive as the underlying
structure, the keyed hash tree, which is an extension of Merkle's hash tree. We
compare our scheme to Buldas et al.'s Undeniable Attesters and Micali et al.'s
Zero Knowledge Sets.
</summary>
    <author>
      <name>Matthias Bauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; H.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-lat/0003009v1</id>
    <updated>2000-03-14T15:36:30Z</updated>
    <published>2000-03-14T15:36:30Z</published>
    <title>Data storage issues in lattice QCD calculations</title>
    <summary>  I describe some of the data management issues in lattice Quantum
Chromodynamics calculations. I focus on the experience of the UKQCD
collaboration. I describe an attempt to use a relational database to store part
of the data produced by a lattice QCD calculation.
</summary>
    <author>
      <name>Craig McNeile</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited talk at: Workshop on Advanced Data Storage / Management
  Techniques for HPC, 23rd - 25th February 2000, Daresbury Laboratory,
  Warrington, UK. 10 pages html</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-lat/0003009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-lat/0003009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3949v1</id>
    <updated>2007-05-27T12:36:58Z</updated>
    <published>2007-05-27T12:36:58Z</published>
    <title>Translating a first-order modal language to relational algebra</title>
    <summary>  This paper is about Kripke structures that are inside a relational database
and queried with a modal language. At first the modal language that is used is
introduced, followed by a definition of the database and relational algebra.
Based on these definitions two things are presented: a mapping from components
of the modal structure to a relational database schema and instance, and a
translation from queries in the modal language to relational algebra queries.
</summary>
    <author>
      <name>Yeb Havinga</name>
    </author>
    <link href="http://arxiv.org/abs/0705.3949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1816v1</id>
    <updated>2008-06-11T09:05:21Z</updated>
    <published>2008-06-11T09:05:21Z</published>
    <title>Cardinality heterogeneities in Web service composition: Issues and
  solutions</title>
    <summary>  Data exchanges between Web services engaged in a composition raise several
heterogeneities. In this paper, we address the problem of data cardinality
heterogeneity in a composition. Firstly, we build a theoretical framework to
describe different aspects of Web services that relate to data cardinality, and
secondly, we solve this problem by developing a solution for cardinality
mediation based on constraint logic programming.
</summary>
    <author>
      <name>M. Mrissa</name>
    </author>
    <author>
      <name>Ph. Thiran</name>
    </author>
    <author>
      <name>J-M. Jacquet</name>
    </author>
    <author>
      <name>D. Benslimane</name>
    </author>
    <author>
      <name>Z. Maamar</name>
    </author>
    <link href="http://arxiv.org/abs/0806.1816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0518v1</id>
    <updated>2008-08-04T22:11:45Z</updated>
    <published>2008-08-04T22:11:45Z</published>
    <title>Building a terminology network for search: the KoMoHe project</title>
    <summary>  The paper reports about results on the GESIS-IZ project "Competence Center
Modeling and Treatment of Semantic Heterogeneity" (KoMoHe). KoMoHe supervised a
terminology mapping effort, in which 'cross-concordances' between major
controlled vocabularies were organized, created and managed. In this paper we
describe the establishment and implementation of cross-concordances for search
in a digital library (DL).
</summary>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Vivien Petras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18452/1264</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18452/1264" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figure, Dublin Core Conference 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1413v3</id>
    <updated>2011-06-21T17:05:53Z</updated>
    <published>2009-07-09T06:51:54Z</published>
    <title>Privacy constraints in regularized convex optimization</title>
    <summary>  This paper is withdrawn due to some errors, which are corrected in
arXiv:0912.0071v4 [cs.LG].
</summary>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <author>
      <name>Anand D. Sarwate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors due to some errors.
  Corrections have been included in arXiv:0912.0071v4</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.1413v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1413v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2299v1</id>
    <updated>2010-12-10T15:50:36Z</updated>
    <published>2010-12-10T15:50:36Z</published>
    <title>A Simple Correctness Proof for Magic Transformation</title>
    <summary>  The paper presents a simple and concise proof of correctness of the magic
transformation. We believe it may provide a useful example of formal reasoning
about logic programs.
  The correctness property concerns the declarative semantics. The proof,
however, refers to the operational semantics (LD-resolution) of the source
programs. Its conciseness is due to applying a suitable proof method.
</summary>
    <author>
      <name>Wlodzimierz Drabent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to "Theory and Practice of Logic Programming"</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.2299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4187v1</id>
    <updated>2012-09-19T08:58:52Z</updated>
    <published>2012-09-19T08:58:52Z</published>
    <title>PaxosLease: Diskless Paxos for Leases</title>
    <summary>  This paper describes PaxosLease, a distributed algorithm for lease
negotiation. PaxosLease is based on Paxos, but does not require disk writes or
clock synchrony. PaxosLease is used for master lease negotation in the
open-source Keyspace and ScalienDB replicated key-value stores.
</summary>
    <author>
      <name>Márton Trencséni</name>
    </author>
    <author>
      <name>Attila Gazsó</name>
    </author>
    <author>
      <name>Holger Reinhardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7544v1</id>
    <updated>2013-04-29T00:30:36Z</updated>
    <published>2013-04-29T00:30:36Z</published>
    <title>Monoidify! Monoids as a Design Principle for Efficient MapReduce
  Algorithms</title>
    <summary>  It is well known that since the sort/shuffle stage in MapReduce is costly,
local aggregation is one important principle to designing efficient algorithms.
This short paper represents an attempt to more clearly articulate this design
principle in terms of monoids, which generalizes the use of combiners and the
in-mapper combining pattern.
</summary>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.4927v2</id>
    <updated>2013-09-20T07:58:30Z</updated>
    <published>2013-09-19T10:59:05Z</published>
    <title>A finite axiomatization of conditional independence and inclusion
  dependencies</title>
    <summary>  We present a complete finite axiomatization of the unrestricted implication
problem for inclusion and conditional independence atoms in the context of
dependence logic. For databases, our result implies a finite axiomatization of
the unrestricted implication problem for inclusion, functional, and embedded
multivalued dependencies in the unirelational case.
</summary>
    <author>
      <name>Miika Hannula</name>
    </author>
    <author>
      <name>Juha Kontinen</name>
    </author>
    <link href="http://arxiv.org/abs/1309.4927v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4927v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03C80" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1316v1</id>
    <updated>2013-10-04T15:21:25Z</updated>
    <published>2013-10-04T15:21:25Z</published>
    <title>A note on monadic datalog on unranked trees</title>
    <summary>  In the article 'Recursive queries on trees and data trees' (ICDT'13),
Abiteboul et al., asked whether the containment problem for monadic datalog
over unordered unranked labeled trees using the child relation and the
descendant relation is decidable. This note gives a positive answer to this
question, as well as an overview of the relative expressive power of monadic
datalog on various representations of unranked trees.
</summary>
    <author>
      <name>André Frochaux</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <link href="http://arxiv.org/abs/1310.1316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5685v1</id>
    <updated>2013-11-22T09:22:03Z</updated>
    <published>2013-11-22T09:22:03Z</published>
    <title>Data Challenges in High-Performance Risk Analytics</title>
    <summary>  Risk Analytics is important to quantify, manage and analyse risks from the
manufacturing to the financial setting. In this paper, the data challenges in
the three stages of the high-performance risk analytics pipeline, namely risk
modelling, portfolio risk management and dynamic financial analysis is
presented.
</summary>
    <author>
      <name>Blesson Varghese</name>
    </author>
    <author>
      <name>Andrew Rau-Chaplin</name>
    </author>
    <link href="http://arxiv.org/abs/1311.5685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03327v1</id>
    <updated>2017-07-05T19:02:22Z</updated>
    <published>2017-07-05T19:02:22Z</published>
    <title>PlumX As a Potential Tool to Assess the Macroscopic Multidimensional
  Impact of Books</title>
    <summary>  The main purpose of this macro-study is to shed light on the broad impact of
books. For this purpose, the impact of a very large collection of books has
been analyzed by using PlumX, an analytical tool providing a great number of
different metrics provided by various tools.
</summary>
    <author>
      <name>Daniel Torres-Salinas</name>
    </author>
    <author>
      <name>Christian Gumpenberger</name>
    </author>
    <author>
      <name>Juan Gorraiz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frma.2017.00005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frma.2017.00005" rel="related"/>
    <link href="http://arxiv.org/abs/1707.03327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05730v1</id>
    <updated>2015-02-19T21:25:11Z</updated>
    <published>2015-02-19T21:25:11Z</published>
    <title>Designing Applications with Distributed Databases in a Hybrid Cloud</title>
    <summary>  Designing applications for use in a hybrid cloud has many features. These
include dynamic virtualization management and an unknown route switching
customers. This makes it impossible to evaluate the query and hence the optimal
distribution of data. In this paper, we formulate the main challenges of
designing and simulation offer installation for processing.
</summary>
    <author>
      <name>Evgeniy Pluzhnik</name>
    </author>
    <author>
      <name>Oleg Lukyanchikov</name>
    </author>
    <author>
      <name>Evgeny Nikulchev</name>
    </author>
    <author>
      <name>Simon Payain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in WIT Transactions of Information and Communication Technologies,
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04032v1</id>
    <updated>2018-02-12T13:31:27Z</updated>
    <published>2018-02-12T13:31:27Z</published>
    <title>Average Size of Implicational Bases</title>
    <summary>  Implicational bases are objects of interest in formal concept analysis and
its applications. Unfortunately, even the smallest base, the Duquenne-Guigues
base, has an exponential size in the worst case. In this paper, we use results
on the average number of minimal transversals in random hypergraphs to show
that the base of proper premises is, on average, of quasi-polynomial size.
</summary>
    <author>
      <name>Giacomo Kahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandre Bazin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Le2i</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CLA, Jun 2018, Olomouc, Czech Republic</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.04032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00914v1</id>
    <updated>2018-04-03T11:33:18Z</updated>
    <published>2018-04-03T11:33:18Z</published>
    <title>Database Consistency Models</title>
    <summary>  A data store allows application processes to put and get data from a shared
memory. In general, a data store cannot be modelled as a strictly sequential
process. Applications observe non-sequential behaviours, called anomalies. The
set of pos- sible behaviours, and conversely of possible anomalies, constitutes
the consistency model of the data store.
</summary>
    <author>
      <name>Marc Shapiro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DELYS, Inria, LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Sutra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-63962-8\_203-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-63962-8\_203-1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sherif Sakr; Albert Zomaya. Encyclopedia of Big Data Technologies,
  Springer, 2017, 978-3-319-63962-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.00914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04760v1</id>
    <updated>2020-01-14T13:19:41Z</updated>
    <published>2020-01-14T13:19:41Z</published>
    <title>Simulation computation in grammar-compressed graphs</title>
    <summary>  Like [1], we present an algorithm to compute the simulation of a query
pattern in a graph of labeled nodes and unlabeled edges. However, our algorithm
works on a compressed graph grammar, instead of on the original graph. The
speed-up of our algorithm compared to the algorithm in [1] grows with the size
of the graph and with the compression strength.
</summary>
    <author>
      <name>Stefan Böttcher</name>
    </author>
    <author>
      <name>Rita Hartel</name>
    </author>
    <author>
      <name>Sven Peeters</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03736v1</id>
    <updated>2020-03-08T07:15:48Z</updated>
    <published>2020-03-08T07:15:48Z</published>
    <title>DeepLENS: Deep Learning for Entity Summarization</title>
    <summary>  Entity summarization has been a prominent task over knowledge graphs. While
existing methods are mainly unsupervised, we present DeepLENS, a simple yet
effective deep learning model where we exploit textual semantics for encoding
triples and we score each candidate triple based on its interdependence on
other triples. DeepLENS significantly outperformed existing methods on a public
benchmark.
</summary>
    <author>
      <name>Qingxia Liu</name>
    </author>
    <author>
      <name>Gong Cheng</name>
    </author>
    <author>
      <name>Yuzhong Qu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, submitted to DL4KG 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.03736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06868v2</id>
    <updated>2020-05-25T04:24:18Z</updated>
    <published>2020-03-15T17:00:37Z</published>
    <title>Causality-based Explanation of Classification Outcomes</title>
    <summary>  We propose a simple definition of an explanation for the outcome of a
classifier based on concepts from causality. We compare it with previously
proposed notions of explanation, and study their complexity. We conduct an
experimental evaluation with two real datasets from the financial domain.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Jordan Li</name>
    </author>
    <author>
      <name>Maximilian Schleich</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <author>
      <name>Zografoula Vagena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1125v1</id>
    <updated>2012-05-05T12:19:33Z</updated>
    <published>2012-05-05T12:19:33Z</published>
    <title>Application Of Data Mining In Bioinformatics</title>
    <summary>  This article highlights some of the basic concepts of bioinformatics and data
mining. The major research areas of bioinformatics are highlighted. The
application of data mining in the domain of bioinformatics is explained. It
also highlights some of the current challenges and opportunities of data mining
in bioinformatics.
</summary>
    <author>
      <name>Khalid Raza</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Indian Journal of Computer Science and Engineering 1(2):114-118
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.1125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5745v1</id>
    <updated>2012-05-25T16:18:29Z</updated>
    <published>2012-05-25T16:18:29Z</published>
    <title>Generic Expression Hardness Results for Primitive Positive Formula
  Comparison</title>
    <summary>  We study the expression complexity of two basic problems involving the
comparison of primitive positive formulas: equivalence and containment. In
particular, we study the complexity of these problems relative to finite
relational structures. We present two generic hardness results for the studied
problems, and discuss evidence that they are optimal and yield, for each of the
problems, a complexity trichotomy.
</summary>
    <author>
      <name>Simone Bova</name>
    </author>
    <author>
      <name>Hubie Chen</name>
    </author>
    <author>
      <name>Matthew Valeriote</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04517v1</id>
    <updated>2019-08-13T07:14:01Z</updated>
    <published>2019-08-13T07:14:01Z</published>
    <title>Beyond the Inverted Index</title>
    <summary>  In this paper, a new data structure named group-list is proposed. The
group-list is as simple as the inverted index. However, the group-list divides
document identifiers in an inverted index into groups, which makes it more
efficient when it is used to perform the intersection or union operation on
document identifiers. The experimental results on a synthetic dataset show that
the group-list outperforms the inverted index.
</summary>
    <author>
      <name>Zhi-Hong Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, and 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07011v1</id>
    <updated>2020-10-09T11:08:06Z</updated>
    <published>2020-10-09T11:08:06Z</published>
    <title>Database (Lecture) Streams on the Cloud: An Experience Report on
  Teaching an Undergrad Database Lecture during a Pandemic</title>
    <summary>  This is an experience report on teaching the undergrad lecture Big Data
Engineering at Saarland University in summer term 2020 online. We describe our
teaching philosophy, the tools used, what worked and what did not work. As we
received extremely positive feedback from the students, in the future, we will
continue to use the same teaching model for other lectures.
</summary>
    <author>
      <name>Jens Dittrich</name>
    </author>
    <author>
      <name>Marcel Maltry</name>
    </author>
    <link href="http://arxiv.org/abs/2010.07011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14766v1</id>
    <updated>2021-06-21T09:24:27Z</updated>
    <published>2021-06-21T09:24:27Z</published>
    <title>A Logical Model for joining Property Graphs</title>
    <summary>  The present paper upgrades the logical model required to exploit materialized
views over property graphs as intended in the seminal paper "A Join Operator
for Property Graphs". Furthermore, we provide some computational complexity
proofs strengthening the contribution of a forthcoming graph equi-join
algorithm proposed in a recently accepted paper.
</summary>
    <author>
      <name>Giacomo Bergami</name>
    </author>
    <link href="http://arxiv.org/abs/2106.14766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.06523v1</id>
    <updated>2022-01-17T16:57:53Z</updated>
    <published>2022-01-17T16:57:53Z</published>
    <title>Patterns of near-crash events in a naturalistic driving dataset:
  applying rules mining</title>
    <summary>  This study aims to explore the associations between near-crash events and
road geometry and trip features by investigating a naturalistic driving dataset
and a corresponding roadway inventory dataset using an association rule mining
method.
</summary>
    <author>
      <name>Xiaoqiang Kong</name>
    </author>
    <author>
      <name>Subasish Das</name>
    </author>
    <author>
      <name>Hongmin Zhou</name>
    </author>
    <author>
      <name>Yunlong Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.aap.2021.106346</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.aap.2021.106346" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Accident Analysis &amp; Prevention (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.06523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3690v1</id>
    <updated>2014-01-15T18:04:36Z</updated>
    <published>2014-01-15T18:04:36Z</published>
    <title>FindStat - the combinatorial statistics database</title>
    <summary>  The FindStat project at www.FindStat.org provides an online platform for
mathematicians, particularly for combinatorialists, to gather information about
combinatorial statistics and their relations. This outline provides an overview
over the project.
</summary>
    <author>
      <name>Chris Berg</name>
    </author>
    <author>
      <name>Viviane Pons</name>
    </author>
    <author>
      <name>Travis Scrimshaw</name>
    </author>
    <author>
      <name>Jessica Striker</name>
    </author>
    <author>
      <name>Christian Stump</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, project description</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4872v1</id>
    <updated>2014-01-20T11:58:23Z</updated>
    <published>2014-01-20T11:58:23Z</published>
    <title>Classification of IDS Alerts with Data Mining Techniques</title>
    <summary>  A data mining technique to reduce the amount of false alerts within an IDS
system is proposed. The new technique achieves an accuracy of 99% compared to
97% by the current systems.
</summary>
    <author>
      <name>Hany Nashat Gabra</name>
    </author>
    <author>
      <name>Ayman Mohammad Bahaa-Eldin</name>
    </author>
    <author>
      <name>Huda Korashy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2012 International Conference on Internet Study (NETs2012), Bangkok,
  Thailand</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.4872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09897v1</id>
    <updated>2019-02-26T12:49:32Z</updated>
    <published>2019-02-26T12:49:32Z</published>
    <title>An Abstract View on the De-anonymization Process</title>
    <summary>  Over the recent years, the availability of datasets containing personal, but
anonymized information has been continuously increasing. Extensive research has
revealed that such datasets are vulnerable to privacy breaches: being able to
reveal sensitive information about individuals through deanonymization methods.
Here, we provide a taxonomy of the research in de-anonymization.
</summary>
    <author>
      <name>Alexandros Bampoulidis</name>
    </author>
    <author>
      <name>Mihai Lupu</name>
    </author>
    <link href="http://arxiv.org/abs/1902.09897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08010v1</id>
    <updated>2019-04-16T23:16:56Z</updated>
    <published>2019-04-16T23:16:56Z</published>
    <title>How to define co-occurrence in different domains of study?</title>
    <summary>  This position paper presents a comparative study of co-occurrences. Some
similarities and differences in the definition exist depending on the research
domain (e.g. linguistics, NLP, computer science). This paper discusses these
points, and deals with the methodological aspects in order to identify
co-occurrences in a multidisciplinary paradigm.
</summary>
    <author>
      <name>Mathieu Roche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CICLING'2018 (International Conference on Computational Linguistics
  and Intelligent Text Processing) - March 18 to 24, 2018 - Hanoi, Vietnam (not
  published in CICLING proceedings)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.08010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05937v2</id>
    <updated>2020-09-15T02:12:39Z</updated>
    <published>2019-06-13T21:35:19Z</published>
    <title>A Complete Language for Faceted Dataflow Programs</title>
    <summary>  We present a complete categorical axiomatization of a wide class of dataflow
programs. This gives a three-dimensional diagrammatic language for workflows,
more expressive than the directed acyclic graphs generally used for this
purpose. This calls for an implementation of these representations in data
transformation tools.
</summary>
    <author>
      <name>Antonin Delpeuch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oxford</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.323.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.323.1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ACT 2019, arXiv:2009.06334</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 323, 2020, pp. 1-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.05937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08092v2</id>
    <updated>2019-08-20T13:23:17Z</updated>
    <published>2019-06-19T13:35:41Z</published>
    <title>A survey of OpenRefine reconciliation services</title>
    <summary>  We review the services implementing the OpenRefine reconciliation API,
comparing their design to the state of the art in record linkage. Due to the
design of the API, the matching scores returned by the services are of little
help to guide matching decisions. This suggests possible improvements to the
specifications of the API, which could improve user workflows by giving more
control over the scoring mechanism to the client.
</summary>
    <author>
      <name>Antonin Delpeuch</name>
    </author>
    <link href="http://arxiv.org/abs/1906.08092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01184v2</id>
    <updated>2020-12-19T22:03:07Z</updated>
    <published>2020-11-27T17:02:26Z</published>
    <title>Feedback from the participants of the ADBIS, TPDL and EDA 2020 joint
  conferences</title>
    <summary>  This paper presents the way the joint ADBIS, TPDL and EDA 2020 conferences
were organized online and the results of the participant survey conducted
thereafter. We present the lessons learned from the participants' feedback.
</summary>
    <author>
      <name>Pegdwendé Sawadogo</name>
    </author>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <author>
      <name>Fabien Duchateau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.01184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0; H.0; I.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01108v1</id>
    <updated>2021-12-02T10:33:18Z</updated>
    <published>2021-12-02T10:33:18Z</published>
    <title>A short note on the counting complexity of conjunctive queries</title>
    <summary>  This note closes a minor gap in the literature on the counting complexity of
conjunctive queries by showing that queries that are not free-connex do not
have a linear time counting algorithm under standard complexity assumptions.
More generally, it is shown that the so-called quantified star size is a lower
bound for the exponent in the runtime of any counting algorithm for conjunctive
queries.
</summary>
    <author>
      <name>Stefan Mengel</name>
    </author>
    <link href="http://arxiv.org/abs/2112.01108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12110v1</id>
    <updated>2022-09-25T00:34:39Z</updated>
    <published>2022-09-25T00:34:39Z</published>
    <title>Answer-Set Programs for Repair Updates and Counterfactual Interventions</title>
    <summary>  We briefly describe -- mainly through very simple examples -- different kinds
of answer-set programs with annotations that have been proposed for specifying:
database repairs and consistent query answering; secrecy view and query
evaluation with them; counterfactual interventions for causality in databases;
and counterfactual-based explanations in machine learning.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Festschrift volume</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.12417v1</id>
    <updated>2023-04-24T19:49:14Z</updated>
    <published>2023-04-24T19:49:14Z</published>
    <title>DONUT -- Creation, Development, and Opportunities of a Database</title>
    <summary>  DONUT is a database of papers about practical, real-world uses of Topological
Data Analysis (TDA). Its original seed was planted in a group chat formed
during the HIM Spring School on Applied and Computational Algebraic Topology in
April 2017. This document describes the creation, curation, and maintenance
process of the database.
</summary>
    <author>
      <name>Barbara Giunti</name>
    </author>
    <author>
      <name>Jānis Lazovskis</name>
    </author>
    <author>
      <name>Bastian Rieck</name>
    </author>
    <link href="http://arxiv.org/abs/2304.12417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.12417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.05907v2</id>
    <updated>2024-08-20T17:25:56Z</updated>
    <published>2023-08-11T02:09:37Z</published>
    <title>Simple Analysis of Priority Sampling</title>
    <summary>  We prove a tight upper bound on the variance of the priority sampling method
(aka sequential Poisson sampling). Our proof is significantly shorter and
simpler than the original proof given by Mario Szegedy at STOC 2006, which
resolved a conjecture by Duffield, Lund, and Thorup.
</summary>
    <author>
      <name>Majid Daliri</name>
    </author>
    <author>
      <name>Juliana Freire</name>
    </author>
    <author>
      <name>Christopher Musco</name>
    </author>
    <author>
      <name>Aécio Santos</name>
    </author>
    <author>
      <name>Haoxiang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.05907v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.05907v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.05668v1</id>
    <updated>2023-11-09T14:56:27Z</updated>
    <published>2023-11-09T14:56:27Z</published>
    <title>The Biological Data Sustainability Paradox</title>
    <summary>  Biological data in digital form has become a, if not the, driving force
behind innovations in biology, medicine, and the environment. No study and no
model would be complete without access to digital data (including text)
collected by others and available in public repositories. With this ascent in
the fundamental importance of data for reproducible scientific progress has
come a troubling paradox.
</summary>
    <author>
      <name>Terence R. Johnson</name>
    </author>
    <author>
      <name>Philip E. Bourne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages 5852 words 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.05668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.05668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.03067v1</id>
    <updated>2024-03-05T15:56:48Z</updated>
    <published>2024-03-05T15:56:48Z</published>
    <title>Enumeration for MSO-Queries on Compressed Trees</title>
    <summary>  We present a linear preprocessing and output-linear delay enumeration
algorithm for MSO-queries over trees that are compressed in the
well-established grammar-based framework. Time bounds are measured with respect
to the size of the compressed representation of the tree. Our result extends
previous work on the enumeration of MSO-queries over uncompressed trees and on
the enumeration of document spanners over compressed text documents.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Markus L. Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/2403.03067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.03067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809011v1</id>
    <updated>1998-09-05T00:29:54Z</updated>
    <published>1998-09-05T00:29:54Z</published>
    <title>Microsoft TerraServer</title>
    <summary>  The Microsoft TerraServer stores aerial and satellite images of the earth in
a SQL Server Database served to the public via the Internet. It is the world's
largest atlas, combining five terabytes of image data from the United States
Geodetic Survey, Sovinformsputnik, and Encarta Virtual Globe. Internet browsers
provide intuitive spatial and gazetteer interfaces to the data. The TerraServer
is also an E-Commerce application. Users can buy the right to use the imagery
using Microsoft Site Servers managed by the USGS and Aerial Images. This paper
describes the TerraServer's design and implementation.
</summary>
    <author>
      <name>Tom Barclay</name>
    </author>
    <author>
      <name>Robert Eberl</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>John Nordlinger</name>
    </author>
    <author>
      <name>Guru Raghavendran</name>
    </author>
    <author>
      <name>Don Slutz</name>
    </author>
    <author>
      <name>Greg Smith</name>
    </author>
    <author>
      <name>Phil Smoot</name>
    </author>
    <author>
      <name>John Hoffman</name>
    </author>
    <author>
      <name>Natt Robb III</name>
    </author>
    <author>
      <name>Hedy Rossmeissl</name>
    </author>
    <author>
      <name>Beth Duff</name>
    </author>
    <author>
      <name>George Lee</name>
    </author>
    <author>
      <name>Theresa Mathesmier</name>
    </author>
    <author>
      <name>Randall Sunne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original file at
  http://research.microsoft.com/~gray/TerraServer_TR.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.2.8;H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809023v2</id>
    <updated>1998-09-18T19:04:42Z</updated>
    <published>1998-09-17T14:41:07Z</published>
    <title>Similarity-Based Queries for Time Series Data</title>
    <summary>  We study a set of linear transformations on the Fourier series representation
of a sequence that can be used as the basis for similarity queries on
time-series data. We show that our set of transformations is rich enough to
formulate operations such as moving average and time warping. We present a
query processing algorithm that uses the underlying R-tree index of a
multidimensional data set to answer similarity queries efficiently. Our
experiments show that the performance of this algorithm is competitive to that
of processing ordinary (exact match) queries using the index, and much faster
than sequential scanning. We relate our transformations to the general
framework for similarity queries of Jagadish et al.
</summary>
    <author>
      <name>Davood Rafiei</name>
    </author>
    <author>
      <name>Alberto Mendelzon</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACM SIGMOD Intl. Conf. on Management of
  Data, pages 13-24, Tucson, Arizona, May 1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811013v1</id>
    <updated>1998-11-09T05:09:36Z</updated>
    <published>1998-11-09T05:09:36Z</published>
    <title>The Asilomar Report on Database Research</title>
    <summary>  The database research community is rightly proud of success in basic
research, and its remarkable record of technology transfer. Now the field needs
to radically broaden its research focus to attack the issues of capturing,
storing, analyzing, and presenting the vast array of online data. The database
research community should embrace a broader research agenda -- broadening the
definition of database management to embrace all the content of the Web and
other online data stores, and rethinking our fundamental assumptions in light
of technology shifts. To accelerate this transition, we recommend changing the
way research results are evaluated and presented. In particular, we advocate
encouraging more speculative and long-range work, moving conferences to a
poster format, and publishing all research literature on the Web.
</summary>
    <author>
      <name>Phil Bernstein</name>
    </author>
    <author>
      <name>Michael Brodie</name>
    </author>
    <author>
      <name>Stefano Ceri</name>
    </author>
    <author>
      <name>David DeWitt</name>
    </author>
    <author>
      <name>Mike Franklin</name>
    </author>
    <author>
      <name>Hector Garcia-Molina</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Jerry Held</name>
    </author>
    <author>
      <name>Joe Hellerstein</name>
    </author>
    <author>
      <name>H. V. Jagadish</name>
    </author>
    <author>
      <name>Michael Lesk</name>
    </author>
    <author>
      <name>Dave Maier</name>
    </author>
    <author>
      <name>Jeff Naughton</name>
    </author>
    <author>
      <name>Hamid Pirahesh</name>
    </author>
    <author>
      <name>Mike Stonebraker</name>
    </author>
    <author>
      <name>Jeff Ullman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages in HTML; an original in MSword at
  http://research.microsoft.com/~gray/Asilomar_DB_98.doc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGMOD Record, December 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0;H.2;H.3;H.4;H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907016v1</id>
    <updated>1999-07-09T21:30:11Z</updated>
    <published>1999-07-09T21:30:11Z</published>
    <title>Microsoft TerraServer: A Spatial Data Warehouse</title>
    <summary>  The TerraServer stores aerial, satellite, and topographic images of the earth
in a SQL database available via the Internet. It is the world's largest online
atlas, combining five terabytes of image data from the United States Geological
Survey (USGS) and SPIN-2. This report describes the system-redesign based on
our experience over the last year. It also reports usage and operations results
over the last year -- over 2 billion web hits and over 20 Terabytes of imagry
served over the Internet. Internet browsers provide intuitive spatial and text
interfaces to the data. Users need no special hardware, software, or knowledge
to locate and browse imagery. This paper describes how terabytes of "Internet
unfriendly" geo-spatial images were scrubbed and edited into hundreds of
millions of "Internet friendly" image tiles and loaded into a SQL data
warehouse. Microsoft TerraServer demonstrates that general-purpose relational
database technology can manage large scale image repositories, and shows that
web browsers can be a good geospatial image presentation system.
</summary>
    <author>
      <name>Tom Barclay Jim Gray Don Slutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original MSword format at
  http://research.microsoft.com/~gray/papers/MS_TR_99_30_TerraServer.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9907016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2, H.2.4, H.2.8, H.3.5, H.5.1,J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9909016v1</id>
    <updated>1999-09-21T21:20:20Z</updated>
    <published>1999-09-21T21:20:20Z</published>
    <title>Least expected cost query optimization: an exercise in utility</title>
    <summary>  We identify two unreasonable, though standard, assumptions made by database
query optimizers that can adversely affect the quality of the chosen evaluation
plans. One assumption is that it is enough to optimize for the expected
case---that is, the case where various parameters (like available memory) take
on their expected value. The other assumption is that the parameters are
constant throughout the execution of the query. We present an algorithm based
on the ``System R''-style query optimization algorithm that does not rely on
these assumptions. The algorithm we present chooses the plan of the least
expected cost instead of the plan of least cost given some fixed value of the
parameters. In execution environments that exhibit a high degree of
variability, our techniques should result in better performance.
</summary>
    <author>
      <name>Francis C. Chu</name>
    </author>
    <author>
      <name>Joseph Y. Halpern</name>
    </author>
    <author>
      <name>Praveen Seshadri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper appears in Proceedings of the Eighteenth Annual ACM
  Symposium on Principles of Database Systems, 1999, pp. 138--147</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9909016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9909016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910019v1</id>
    <updated>1999-10-22T15:17:29Z</updated>
    <published>1999-10-22T15:17:29Z</published>
    <title>Consistent Checkpointing in Distributed Databases: Towards a Formal
  Approach</title>
    <summary>  Whether it is for audit or for recovery purposes, data checkpointing is an
important problem of distributed database systems. Actually, transactions
establish dependence relations on data checkpoints taken by data object
managers. So, given an arbitrary set of data checkpoints (including at least a
single data checkpoint from a data manager, and at most a data checkpoint from
each data manager), an important question is the following one: ``Can these
data checkpoints be members of a same consistent global checkpoint?''. This
paper answers this question by providing a necessary and sufficient condition
suited for database systems. Moreover, to show the usefulness of this
condition, two {\em non-intrusive} data checkpointing protocols are derived
from this condition. It is also interesting to note that this paper, by
exhibiting ``correspondences'', establishes a bridge between the data
object/transaction model and the process/message-passing model.
</summary>
    <author>
      <name>R. Baldoni</name>
    </author>
    <author>
      <name>F. Quaglia</name>
    </author>
    <author>
      <name>M. Raynal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9910019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910021v1</id>
    <updated>1999-10-25T16:30:20Z</updated>
    <published>1999-10-25T16:30:20Z</published>
    <title>Efficient and Extensible Algorithms for Multi Query Optimization</title>
    <summary>  Complex queries are becoming commonplace, with the growing use of decision
support systems. These complex queries often have a lot of common
sub-expressions, either within a single query, or across multiple such queries
run as a batch. Multi-query optimization aims at exploiting common
sub-expressions to reduce evaluation cost. Multi-query optimization has
hither-to been viewed as impractical, since earlier algorithms were exhaustive,
and explore a doubly exponential search space.
  In this paper we demonstrate that multi-query optimization using heuristics
is practical, and provides significant benefits. We propose three cost-based
heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple
modifications to the Volcano search strategy, and a greedy heuristic. Our
greedy heuristic incorporates novel optimizations that improve efficiency
greatly. Our algorithms are designed to be easily added to existing optimizers.
We present a performance study comparing the algorithms, using workloads
consisting of queries from the TPC-D benchmark. The study shows that our
algorithms provide significant benefits over traditional optimization, at a
very acceptable overhead in optimization time.
</summary>
    <author>
      <name>Prasan Roy</name>
    </author>
    <author>
      <name>S. Seshadri</name>
    </author>
    <author>
      <name>S. Sudarshan</name>
    </author>
    <author>
      <name>Siddhesh Bhobe</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9910021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003006v1</id>
    <updated>2000-03-02T08:05:24Z</updated>
    <published>2000-03-02T08:05:24Z</published>
    <title>Materialized View Selection and Maintenance Using Multi-Query
  Optimization</title>
    <summary>  Because the presence of views enhances query performance, materialized views
are increasingly being supported by commercial database/data warehouse systems.
Whenever the data warehouse is updated, the materialized views must also be
updated. However, whereas the amount of data entering a warehouse, the query
loads, and the need to obtain up-to-date responses are all increasing, the time
window available for making the warehouse up-to-date is shrinking. These trends
necessitate efficient techniques for the maintenance of materialized views.
  In this paper, we show how to find an efficient plan for maintenance of a
{\em set} of views, by exploiting common subexpressions between different view
maintenance expressions. These common subexpressions may be materialized
temporarily during view maintenance. Our algorithms also choose
subexpressions/indices to be materialized permanently (and maintained along
with other materialized views), to speed up view maintenance. While there has
been much work on view maintenance in the past, our novel contributions lie in
exploiting a recently developed framework for multiquery optimization to
efficiently find good view maintenance plans as above. In addition to faster
view maintenance, our algorithms can also be used to efficiently select
materialized views to speed up workloads containing queries.
</summary>
    <author>
      <name>Hoshi Mistry</name>
    </author>
    <author>
      <name>Prasan Roy</name>
    </author>
    <author>
      <name>Krithi Ramamritham</name>
    </author>
    <author>
      <name>S. Sudarshan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003043v1</id>
    <updated>2000-03-09T04:01:22Z</updated>
    <published>2000-03-09T04:01:22Z</published>
    <title>Automatic Classification of Text Databases through Query Probing</title>
    <summary>  Many text databases on the web are "hidden" behind search interfaces, and
their documents are only accessible through querying. Search engines typically
ignore the contents of such search-only databases. Recently, Yahoo-like
directories have started to manually organize these databases into categories
that users can browse to find these valuable resources. We propose a novel
strategy to automate the classification of search-only text databases. Our
technique starts by training a rule-based document classifier, and then uses
the classifier's rules to generate probing queries. The queries are sent to the
text databases, which are then classified based on the number of matches that
they produce for each query. We report some initial exploratory experiments
that show that our approach is promising to automatically characterize the
contents of text databases accessible on the web.
</summary>
    <author>
      <name>Panagiotis Ipeirotis</name>
    </author>
    <author>
      <name>Luis Gravano</name>
    </author>
    <author>
      <name>Mehran Sahami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007044v2</id>
    <updated>2001-06-11T08:13:05Z</updated>
    <published>2000-07-31T20:15:08Z</published>
    <title>Managing Periodically Updated Data in Relational Databases: A Stochastic
  Modeling Approach</title>
    <summary>  Recent trends in information management involve the periodic transcription of
data onto secondary devices in a networked environment, and the proper
scheduling of these transcriptions is critical for efficient data management.
To assist in the scheduling process, we are interested in modeling the
reduction of consistency over time between a relation and its replica, termed
obsolescence of data. The modeling is based on techniques from the field of
stochastic processes, and provides several stochastic models for content
evolution in the base relations of a database, taking referential integrity
constraints into account. These models are general enough to accommodate most
of the common scenarios in databases, including batch insertions and life spans
both with and without memory. As an initial "proof of concept" of the
applicability of our approach, we validate the insertion portion of our model
framework via experiments with real data feeds. We also discuss a set of
transcription protocols which make use of the proposed stochastic model.
</summary>
    <author>
      <name>Avigdor Gal</name>
    </author>
    <author>
      <name>Jonathan Eckstein</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0007044v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007044v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011041v1</id>
    <updated>2000-11-27T09:15:08Z</updated>
    <published>2000-11-27T09:15:08Z</published>
    <title>EquiX---A Search and Query Language for XML</title>
    <summary>  EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graphical abstract
syntax and a formal concrete syntax are presented for EquiX queries. In
addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.
</summary>
    <author>
      <name>Sara Cohen</name>
    </author>
    <author>
      <name>Yaron Kanza</name>
    </author>
    <author>
      <name>Yakov Kogan</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <author>
      <name>Alexander Serebrenik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report of Hebrew University Jerusalem Israel</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0011041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103004v1</id>
    <updated>2001-03-02T20:00:16Z</updated>
    <published>2001-03-02T20:00:16Z</published>
    <title>Rapid Application Evolution and Integration Through Document
  Metamorphosis</title>
    <summary>  The Harland document management system implements a data model in which
document (object) structure can be altered by mixin-style multiple inheritance
at any time. This kind of structural fluidity has long been supported by
knowledge-base management systems, but its use has primarily been in support of
reasoning and inference. In this paper, we report our experiences building and
supporting several non-trivial applications on top of this data model. Based on
these experiences, we argue that structural fluidity is convenient for
data-intensive applications other than knowledge-base management. Specifically,
we suggest that this flexible data model is a natural fit for the decoupled
programming methodology that arises naturally when using enterprise component
frameworks.
</summary>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <author>
      <name>Ian E. Smith</name>
    </author>
    <author>
      <name>James D. Thornton</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0103004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; D.2.11; K.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104008v1</id>
    <updated>2001-04-03T16:27:48Z</updated>
    <published>2001-04-03T16:27:48Z</published>
    <title>Event Indexing Systems for Efficient Selection and Analysis of HERA Data</title>
    <summary>  The design and implementation of two software systems introduced to improve
the efficiency of offline analysis of event data taken with the ZEUS Detector
at the HERA electron-proton collider at DESY are presented. Two different
approaches were made, one using a set of event directories and the other using
a tag database based on a commercial object-oriented database management
system. These are described and compared. Both systems provide quick direct
access to individual collision events in a sequential data store of several
terabytes, and they both considerably improve the event analysis efficiency. In
particular the tag database provides a very flexible selection mechanism and
can dramatically reduce the computing time needed to extract small subsamples
from the total event sample. Gains as large as a factor 20 have been obtained.
</summary>
    <author>
      <name>L. A. T. Bauerdick</name>
    </author>
    <author>
      <name>Adrian Fox-Murphy</name>
    </author>
    <author>
      <name>Tobias Haas</name>
    </author>
    <author>
      <name>Stefan Stonjek</name>
    </author>
    <author>
      <name>Enrico Tassi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0010-4655(01)00162-X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0010-4655(01)00162-X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Computer Physics Communications</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Comput.Phys.Commun. 137 (2001) 236-246</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0104008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.3.1; H.3.3; H.3.4; J.2; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106012v1</id>
    <updated>2001-06-07T17:29:03Z</updated>
    <published>2001-06-07T17:29:03Z</published>
    <title>Computational Properties of Metaquerying Problems</title>
    <summary>  Metaquerying is a datamining technology by which hidden dependencies among
several database relations can be discovered. This tool has already been
successfully applied to several real-world applications. Recent papers provide
only preliminary results about the complexity of metaquerying. In this paper we
define several variants of metaquerying that encompass, as far as we know, all
variants defined in the literature. We study both the combined complexity and
the data complexity of these variants. We show that, under the combined
complexity measure, metaquerying is generally intractable (unless P=NP), lying
sometimes quite high in the complexity hierarchies (as high as NP^PP),
depending on the characteristics of the plausibility index. However, we are
able to single out some tractable and interesting metaquerying cases (whose
combined complexity is LOGCFL-complete). As for the data complexity of
metaquerying, we prove that, in general, this is in TC0, but lies within AC0 in
some simpler cases. Finally, we discuss implementation of metaqueries, by
providing algorithms to answer them.
</summary>
    <author>
      <name>F. Angiulli</name>
    </author>
    <author>
      <name>R. Ben-Eliyahu-Zohary</name>
    </author>
    <author>
      <name>G. Ianni</name>
    </author>
    <author>
      <name>L. Palopoli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages. Partial and preliminary version appeared in Proc. of 19th
  Symposium on Principles of Database Systems, 2000, Dallas, pp. 237-244</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3;F.2.0;H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106055v1</id>
    <updated>2001-06-28T06:02:46Z</updated>
    <published>2001-06-28T06:02:46Z</published>
    <title>A Seamless Integration of Association Rule Mining with Database Systems</title>
    <summary>  The need for Knowledge and Data Discovery Management Systems (KDDMS) that
support ad hoc data mining queries has been long recognized. A significant
amount of research has gone into building tightly coupled systems that
integrate association rule mining with database systems. In this paper, we
describe a seamless integration scheme for database queries and association
rule discovery using a common query optimizer for both. Query trees of
expressions in an extended algebra are used for internal representation in the
optimizer. The algebraic representation is flexible enough to deal with
constrained association rule queries and other variations of association rule
specifications. We propose modularization to simplify the query tree for
complex tasks in data mining. It paves the way for making use of existing
algorithms for constructing query plans in the optimization process. How the
integration scheme we present will facilitate greater user control over the
data mining process is also discussed. The work described in this paper forms
part of a larger project for fully integrating data mining with database
management.
</summary>
    <author>
      <name>Raj P. Gopalan</name>
    </author>
    <author>
      <name>Tariq Nuruddin</name>
    </author>
    <author>
      <name>Yudho Giri Sucahyo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109040v1</id>
    <updated>2001-09-20T12:27:45Z</updated>
    <published>2001-09-20T12:27:45Z</published>
    <title>The Building of BODHI, a Bio-diversity Database System</title>
    <summary>  We have recently built a database system called BODHI, intended to store
plant bio-diversity information. It is based on an object-oriented modeling
approach and is developed completely around public-domain software. The unique
feature of BODHI is that it seamlessly integrates diverse types of data,
including taxonomic characteristics, spatial distributions, and genetic
sequences, thereby spanning the entire range from molecular to organism-level
information. A variety of sophisticated indexing strategies are incorporated to
efficiently access the various types of data, and a rule-based query processor
is employed for optimizing query execution. In this paper, we report on our
experiences in building BODHI and on its performance characteristics for a
representative set of queries.
</summary>
    <author>
      <name>B. J. Srikanta</name>
    </author>
    <author>
      <name>Jayant Haritsa</name>
    </author>
    <author>
      <name>Udaysankar Sen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109084v1</id>
    <updated>2001-09-24T20:42:05Z</updated>
    <published>2001-09-24T20:42:05Z</published>
    <title>The Internet and Community Networks: Case Studies of Five U.S. Cities</title>
    <summary>  This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland,
and Washington, DC) and explores strategies being employed by community
activists and local governments to create and sustain community networking
projects. In some cities, community networking initiatives are relatively
mature, while in others they are in early or intermediate stages. The paper
looks at several factors that help explain the evolution of community networks
in cities:
  1) Local government support; 2) Federal support 3) Degree of community
activism, often reflected by public-private partnerships that help support
community networks.
  In addition to these (more or less) measurable elements of local support, the
case studies enable description of the different objectives of community
networks in different cities. Several community networking projects aim to
improve the delivery of government services (e.g., Portland and Cleveland),
some have a job-training focus (e.g., Austin, Washington, DC), others are
oriented very explicitly toward community building (Nashville, DC), and others
toward neighborhood entrepreneurship (Portland and Cleveland).
  The paper ties the case studies together by asking whether community
technology initiatives contribute to social capital in the cities studied.
</summary>
    <author>
      <name>John B. Horrigan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29th TPRC Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.m Miscellaneous" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110044v1</id>
    <updated>2001-10-22T08:03:50Z</updated>
    <published>2001-10-22T08:03:50Z</published>
    <title>EquiX--A Search and Query Language for XML</title>
    <summary>  EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graph-based
abstract syntax and a formal concrete syntax are presented for EquiX queries.
In addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.
</summary>
    <author>
      <name>Sara Cohen</name>
    </author>
    <author>
      <name>Yaron Kanza</name>
    </author>
    <author>
      <name>Yakov Kogan</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <author>
      <name>Alexander Serebrenik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint of an article accepted for publication in Journal
  of the American Society for Information Science and Technology @ copyright
  2001 John Wiley &amp; Sons, Inc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5; H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110052v1</id>
    <updated>2001-10-25T08:55:57Z</updated>
    <published>2001-10-25T08:55:57Z</published>
    <title>Mragyati : A System for Keyword-based Searching in Databases</title>
    <summary>  The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query.
</summary>
    <author>
      <name>N. L. Sarda</name>
    </author>
    <author>
      <name>Ankur Jain</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0110052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.3.3; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111004v1</id>
    <updated>2001-11-01T22:07:01Z</updated>
    <published>2001-11-01T22:07:01Z</published>
    <title>The Relational Database Aspects of Argonne's ATLAS Control System</title>
    <summary>  The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans.
</summary>
    <author>
      <name>D. E. R. Quock</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <author>
      <name>F. H. Munson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <author>
      <name>K. J. Eder</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <author>
      <name>S. L. Dean</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICALEPCS 2001 Conference, PSN WEAP066, 3 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) WEAP066</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111006v1</id>
    <updated>2001-11-05T18:14:16Z</updated>
    <published>2001-11-05T18:14:16Z</published>
    <title>Proliferation of SDDS Support for Various Platforms and Languages</title>
    <summary>  Since Self-Describing Data Sets (SDDS) were first introduced, the source code
has been ported to many different operating systems and various languages. SDDS
is now available in C, Tcl, Java, Fortran, and Python. All of these versions
are supported on Solaris, Linux, and Windows. The C version of SDDS is also
supported on VxWorks. With the recent addition of the Java port, SDDS can now
be deployed on virtually any operating system. Due to this proliferation, SDDS
files serve to link not only a collection of C programs, but programs and
scripts in many languages on different operating systems. The platform
independent binary feature of SDDS also facilitates portability among operating
systems. This paper presents an overview of various benefits of SDDS platform
interoperability.
</summary>
    <author>
      <name>Robert Soliday</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">APS/ANL</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, submitted to ICALEPCS 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConfC011127:THAP031,2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111018v1</id>
    <updated>2001-11-08T18:28:46Z</updated>
    <published>2001-11-08T18:28:46Z</published>
    <title>Data Acquisition and Database Management System for Samsung
  Superconductor Test Facility</title>
    <summary>  In order to fulfill the test requirement of KSTAR (Korea Superconducting
Tokamak Advanced Research) superconducting magnet system, a large scale
superconducting magnet and conductor test facility, SSTF (Samsung
Superconductor Test Facility), has been constructed at Samsung Advanced
Institute of Technology. The computer system for SSTF DAC (Data Acquisition and
Control) is based on UNIX system and VxWorks is used for the real-time OS of
the VME system. EPICS (Experimental Physics and Industrial Control System) is
used for the communication between IOC server and client. A database program
has been developed for the efficient management of measured data and a Linux
workstation with PENTIUM-4 CPU is used for the database server. In this paper,
the current status of SSTF DAC system, the database management system and
recent test results are presented.
</summary>
    <author>
      <name>Y. Chu</name>
    </author>
    <author>
      <name>S. Baek</name>
    </author>
    <author>
      <name>H. Yonekawa</name>
    </author>
    <author>
      <name>A. Chertovskikh</name>
    </author>
    <author>
      <name>M. Kim</name>
    </author>
    <author>
      <name>J. S. Kim</name>
    </author>
    <author>
      <name>K. Park</name>
    </author>
    <author>
      <name>S. Baang</name>
    </author>
    <author>
      <name>Y. Chang</name>
    </author>
    <author>
      <name>J. H. Kim</name>
    </author>
    <author>
      <name>S. Lee</name>
    </author>
    <author>
      <name>B. Lim</name>
    </author>
    <author>
      <name>W. Chung</name>
    </author>
    <author>
      <name>H. Park</name>
    </author>
    <author>
      <name>K. Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, ICALEPCS 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) TUAP018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112013v1</id>
    <updated>2001-12-11T19:23:11Z</updated>
    <published>2001-12-11T19:23:11Z</published>
    <title>A Data Mining Framework for Optimal Product Selection in Retail
  Supermarket Data: The Generalized PROFSET Model</title>
    <summary>  In recent years, data mining researchers have developed efficient association
rule algorithms for retail market basket analysis. Still, retailers often
complain about how to adopt association rules to optimize concrete retail
marketing-mix decisions. It is in this context that, in a previous paper, the
authors have introduced a product selection model called PROFSET. This model
selects the most interesting products from a product assortment based on their
cross-selling potential given some retailer defined constraints. However this
model suffered from an important deficiency: it could not deal effectively with
supermarket data, and no provisions were taken to include retail category
management principles. Therefore, in this paper, the authors present an
important generalization of the existing model in order to make it suitable for
supermarket data as well, and to enable retailers to add category restrictions
to the model. Experiments on real world data obtained from a Belgian
supermarket chain produce very promising results and demonstrate the
effectiveness of the generalized PROFSET model.
</summary>
    <author>
      <name>Tom Brijs</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Gilbert Swinnen</name>
    </author>
    <author>
      <name>Koen Vanhoof</name>
    </author>
    <author>
      <name>Geert Wets</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0112013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202014v1</id>
    <updated>2002-02-12T23:47:20Z</updated>
    <published>2002-02-12T23:47:20Z</published>
    <title>Data Mining the SDSS SkyServer Database</title>
    <summary>  An earlier paper (Szalay et. al. "Designing and Mining MultiTerabyte
Astronomy Archives: The Sloan Digital Sky Survey," ACM SIGMOD 2000) described
the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty
database queries and twelve data visualization tasks that a good data
management system should support. We built a database and interfaces to support
both the query load and also a website for ad-hoc access. This paper reports on
the database design, describes the data loading pipeline, and reports on the
query implementation and performance. The queries typically translated to a
single SQL statement. Most queries run in less than 20 seconds, allowing
scientists to interactively explore the database. This paper is an in-depth
tour of those queries. Readers should first have studied the companion overview
paper Szalay et. al. "The SDSS SkyServer, Public Access to the Sloan Digital
Sky Server Data" ACM SIGMOND 2002.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex S. Szalay</name>
    </author>
    <author>
      <name>Ani R. Thakar</name>
    </author>
    <author>
      <name>Peter Z. Kunszt</name>
    </author>
    <author>
      <name>Christopher Stoughton</name>
    </author>
    <author>
      <name>Don Slutz</name>
    </author>
    <author>
      <name>Jan vandenBerg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, Original source is at
  http://research.microsoft.com/~gray/Papers/MSR_TR_O2_01_20_queries.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0202014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8;H.3.3; H.3.5;h.3.7;H.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202035v1</id>
    <updated>2002-02-21T05:23:51Z</updated>
    <published>2002-02-21T05:23:51Z</published>
    <title>Sprinkling Selections over Join DAGs for Efficient Query Optimization</title>
    <summary>  In optimizing queries, solutions based on AND/OR DAG can generate all
possible join orderings and select placements before searching for optimal
query execution strategy. But as the number of joins and selection conditions
increase, the space and time complexity to generate optimal query plan
increases exponentially. In this paper, we use join graph for a relational
database schema to either pre-compute all possible join orderings that can be
executed and store it as a join DAG or, extract joins in the queries to
incrementally build a history join DAG as and when the queries are executed.
The select conditions in the queries are appropriately placed in the retrieved
join DAG (or, history join DAG) to generate optimal query execution strategy.
We experimentally evaluate our query optimization technique on TPC-D/H query
sets to show their effectiveness over AND/OR DAG query optimization strategy.
Finally, we illustrate how our technique can be used for efficient multiple
query optimization and selection of materialized views in data warehousing
environments.
</summary>
    <author>
      <name>Satyanarayana R Valluri</name>
    </author>
    <author>
      <name>Soujanya Vadapalli</name>
    </author>
    <author>
      <name>Kamalakar Karlapalem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0202035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203028v3</id>
    <updated>2003-01-26T07:30:18Z</updated>
    <published>2002-03-27T06:31:58Z</published>
    <title>When to Update the sequential patterns of stream data?</title>
    <summary>  In this paper, we first define a difference measure between the old and new
sequential patterns of stream data, which is proved to be a distance. Then we
propose an experimental method, called TPD (Tradeoff between Performance and
Difference), to decide when to update the sequential patterns of stream data by
making a tradeoff between the performance of increasingly updating algorithms
and the difference of sequential patterns. The experiments for the incremental
updating algorithm IUS on two data sets show that generally, as the size of
incremental windows grows, the values of the speedup and the values of the
difference will decrease and increase respectively. It is also shown
experimentally that the incremental ratio determined by the TPD method does not
monotonically increase or decrease but changes in a range between 20 and 30
percentage for the IUS algorithm.
</summary>
    <author>
      <name>Qingguo Zheng</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <author>
      <name>Shilong Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0203028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0203028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204046v1</id>
    <updated>2002-04-22T18:32:44Z</updated>
    <published>2002-04-22T18:32:44Z</published>
    <title>Optimal Aggregation Algorithms for Middleware</title>
    <summary>  Let D be a database of N objects where each object has m fields. The objects
are given in m sorted lists (where the ith list is sorted according to the ith
field). Our goal is to find the top k objects according to a monotone
aggregation function t, while minimizing access to the lists. The problem
arises in several contexts. In particular Fagin (JCSS 1999) considered it for
the purpose of aggregating information in a multimedia database system.
  We are interested in instance optimality, i.e. that our algorithm will be as
good as any other (correct) algorithm on any instance. We provide and analyze
several instance optimal algorithms for the task, with various access costs and
models.
</summary>
    <author>
      <name>Ron Fagin</name>
    </author>
    <author>
      <name>Amnon Lotem</name>
    </author>
    <author>
      <name>Moni Naor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages. Preliminary version appeared in ACM PODS 2001, pp. 102-113</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205060v1</id>
    <updated>2002-05-23T14:15:55Z</updated>
    <published>2002-05-23T14:15:55Z</published>
    <title>Optimizing Queries Using a Meta-level Database</title>
    <summary>  Graph simulation (using graph schemata or data guides) has been successfully
proposed as a technique for adding structure to semistructured data. Design
patterns for description (such as meta-classes and homomorphisms between schema
layers), which are prominent in the object-oriented programming community,
constitute a generalization of this graph simulation approach.
  In this paper, we show description applicable to a wide range of data models
that have some notion of object (-identity), and propose to turn it into a data
model primitive much like, say, inheritance. We argue that such an extension
fills a practical need in contemporary data management. Then, we present
algebraic techniques for query optimization (using the notions of described and
description queries). Finally, in the semistructured setting, we discuss the
pruning of regular path queries (with nested conditions) using description
meta-data. In this context, our notion of meta-data extends graph schemata and
data guides by meta-level values, allowing to boost query performance and to
reduce the redundancy of data.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1, H.2.3, D.1.5, D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206004v1</id>
    <updated>2002-06-03T14:13:51Z</updated>
    <published>2002-06-03T14:13:51Z</published>
    <title>Mining All Non-Derivable Frequent Itemsets</title>
    <summary>  Recent studies on frequent itemset mining algorithms resulted in significant
performance improvements. However, if the minimal support threshold is set too
low, or the data is highly correlated, the number of frequent itemsets itself
can be prohibitively large. To overcome this problem, recently several
proposals have been made to construct a concise representation of the frequent
itemsets, instead of mining all frequent itemsets. The main goal of this paper
is to identify redundancies in the set of all frequent itemsets and to exploit
these redundancies in order to reduce the result of a mining operation. We
present deduction rules to derive tight bounds on the support of candidate
itemsets. We show how the deduction rules allow for constructing a minimal
representation for all frequent itemsets. We also present connections between
our proposal and recent proposals for concise representations and we give the
results of experiments on real-life datasets that show the effectiveness of the
deduction rules. In fact, the experiments even show that in many cases, first
mining the concise representation, and then creating the frequent itemsets from
this representation outperforms existing frequent set mining algorithms.
</summary>
    <author>
      <name>Toon Calders</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0206004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207093v1</id>
    <updated>2002-07-27T00:46:50Z</updated>
    <published>2002-07-27T00:46:50Z</published>
    <title>Preference Queries</title>
    <summary>  The handling of user preferences is becoming an increasingly important issue
in present-day information systems. Among others, preferences are used for
information filtering and extraction to reduce the volume of data presented to
the user. They are also used to keep track of user profiles and formulate
policies to improve and automate decision making.
  We propose here a simple, logical framework for formulating preferences as
preference formulas. The framework does not impose any restrictions on the
preference relations and allows arbitrary operation and predicate signatures in
preference formulas. It also makes the composition of preference relations
straightforward. We propose a simple, natural embedding of preference formulas
into relational algebra (and SQL) through a single winnow operator
parameterized by a preference formula. The embedding makes possible the
formulation of complex preference queries, e.g., involving aggregation, by
piggybacking on existing SQL constructs. It also leads in a natural way to the
definition of further, preference-related concepts like ranking. Finally, we
present general algebraic laws governing the winnow operator and its
interaction with other relational algebra operators. The preconditions on the
applicability of the laws are captured by logical formulas. The laws provide a
formal foundation for the algebraic optimization of preference queries. We
demonstrate the usefulness of our approach through numerous examples.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0207093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207094v1</id>
    <updated>2002-07-26T21:18:50Z</updated>
    <published>2002-07-26T21:18:50Z</published>
    <title>Answer Sets for Consistent Query Answering in Inconsistent Databases</title>
    <summary>  A relational database is inconsistent if it does not satisfy a given set of
integrity constraints. Nevertheless, it is likely that most of the data in it
is consistent with the constraints. In this paper we apply logic programming
based on answer sets to the problem of retrieving consistent information from a
possibly inconsistent database. Since consistent information persists from the
original database to every of its minimal repairs, the approach is based on a
specification of database repairs using disjunctive logic programs with
exceptions, whose answer set semantics can be represented and computed by
systems that implement stable model semantics. These programs allow us to
declare persistence by defaults and repairing changes by exceptions. We
concentrate mainly on logic programs for binary integrity constraints, among
which we find most of the integrity constraints found in practice.
</summary>
    <author>
      <name>Marcelo Arenas</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0207094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208013v1</id>
    <updated>2002-08-07T22:49:56Z</updated>
    <published>2002-08-07T22:49:56Z</published>
    <title>Petabyte Scale Data Mining: Dream or Reality?</title>
    <summary>  Science is becoming very data intensive1. Today's astronomy datasets with
tens of millions of galaxies already present substantial challenges for data
mining. In less than 10 years the catalogs are expected to grow to billions of
objects, and image archives will reach Petabytes. Imagine having a 100GB
database in 1996, when disk scanning speeds were 30MB/s, and database tools
were immature. Such a task today is trivial, almost manageable with a laptop.
We think that the issue of a PB database will be very similar in six years. In
this paper we scale our current experiments in data archiving and analysis on
the Sloan Digital Sky Survey2,3 data six years into the future. We analyze
these projections and look at the requirements of performing data mining on
such data sets. We conclude that the task scales rather well: we could do the
job today, although it would be expensive. There do not seem to be any
show-stoppers that would prevent us from storing and using a Petabyte dataset
six years from today.
</summary>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Jan vandenBerg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.461427</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.461427" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">originals at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-84</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIPE Astronmy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8;J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208015v1</id>
    <updated>2002-08-07T23:06:40Z</updated>
    <published>2002-08-07T23:06:40Z</published>
    <title>Spatial Clustering of Galaxies in Large Datasets</title>
    <summary>  Datasets with tens of millions of galaxies present new challenges for the
analysis of spatial clustering. We have built a framework that integrates a
database of object catalogs, tools for creating masks of bad regions, and a
fast (NlogN) correlation code. This system has enabled unprecedented efficiency
in carrying out the analysis of galaxy clustering in the SDSS catalog. A
similar approach is used to compute the three-dimensional spatial clustering of
galaxies on very large scales. We describe our strategy to estimate the effect
of photometric errors using a database. We discuss our efforts as an early
example of data-intensive science. While it would have been possible to get
these results without the framework we describe, it will be infeasible to
perform these computations on the future huge datasets without using this
framework.
</summary>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Tamas Budavari</name>
    </author>
    <author>
      <name>Andrew Connolly</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Takahiko Matsubara</name>
    </author>
    <author>
      <name>Adrian Pope</name>
    </author>
    <author>
      <name>Istvan Szapudi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.476761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.476761" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">original documents at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-86</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIPE Astronomy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3;H.2.8; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0210028v2</id>
    <updated>2002-10-30T18:27:58Z</updated>
    <published>2002-10-29T19:19:12Z</published>
    <title>Equivalences Among Aggregate Queries with Negation</title>
    <summary>  Query equivalence is investigated for disjunctive aggregate queries with
negated subgoals, constants and comparisons. A full characterization of
equivalence is given for the aggregation functions count, max, sum, prod,
toptwo and parity. A related problem is that of determining, for a given
natural number N, whether two given queries are equivalent over all databases
with at most N constants. We call this problem bounded equivalence. A complete
characterization of decidability of bounded equivalence is given. In
particular, it is shown that this problem is decidable for all the above
aggregation functions as well as for count distinct and average. For
quasilinear queries (i.e., queries where predicates that occur positively are
not repeated) it is shown that equivalence can be decided in polynomial time
for the aggregation functions count, max, sum, parity, prod, toptwo and
average. A similar result holds for count distinct provided that a few
additional conditions hold. The results are couched in terms of abstract
characteristics of aggregation functions, and new proof techniques are used.
Finally, the results above also imply that equivalence, under bag-set
semantics, is decidable for non-aggregate queries with negation.
</summary>
    <author>
      <name>Sara Cohen</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0210028v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0210028v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1;H.2.3;H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211020v2</id>
    <updated>2003-10-08T01:17:47Z</updated>
    <published>2002-11-15T20:01:54Z</published>
    <title>Monadic Datalog and the Expressive Power of Languages for Web
  Information Extraction</title>
    <summary>  Research on information extraction from Web pages (wrapping) has seen much
activity recently (particularly systems implementations), but little work has
been done on formally studying the expressiveness of the formalisms proposed or
on the theoretical foundations of wrapping. In this paper, we first study
monadic datalog over trees as a wrapping language. We show that this simple
language is equivalent to monadic second order logic (MSO) in its ability to
specify wrappers. We believe that MSO has the right expressiveness required for
Web information extraction and propose MSO as a yardstick for evaluating and
comparing wrappers. Along the way, several other results on the complexity of
query evaluation and query containment for monadic datalog over trees are
established, and a simple normal form for this language is presented. Using the
above results, we subsequently study the kernel fragment Elog$^-$ of the Elog
wrapping language used in the Lixto system (a visual wrapper generator).
Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed,
programs in this language can be entirely visually specified.
</summary>
    <author>
      <name>Georg Gottlob</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 3 figures, journal version of PODS 2002 paper, to appear in
  JACM</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.4.1; F.4.3; H.2.3; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211023v1</id>
    <updated>2002-11-20T04:54:19Z</updated>
    <published>2002-11-20T04:54:19Z</published>
    <title>SkyQuery: A WebService Approach to Federate Databases</title>
    <summary>  Traditional science searched for new objects and phenomena that led to
discoveries. Tomorrow's science will combine together the large pool of
information in scientific archives and make discoveries. Scienthists are
currently keen to federate together the existing scientific databases. The
major challenge in building a federation of these autonomous and heterogeneous
databases is system integration. Ineffective integration will result in defunct
federations and under utilized scientific data.
  Astronomy, in particular, has many autonomous archives spread over the
Internet. It is now seeking to federate these, with minimal effort, into a
Virtual Observatory that will solve complex distributed computing tasks such as
answering federated spatial join queries.
  In this paper, we present SkyQuery, a successful prototype of an evolving
federation of astronomy archives. It interoperates using the emerging Web
services standard. We describe the SkyQuery architecture and show how it
efficiently evaluates a probabilistic federated spatial join query.
</summary>
    <author>
      <name>Tanu Malik</name>
    </author>
    <author>
      <name>Alex S. Szalay</name>
    </author>
    <author>
      <name>Tamas Budavari</name>
    </author>
    <author>
      <name>Ani R. Thakar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 Figures, To Appear in CIDR'03, Also at
  http://www.skyquery.net</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2 ;H.3.5;H.2.8 ;H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212004v1</id>
    <updated>2002-12-05T16:23:35Z</updated>
    <published>2002-12-05T16:23:35Z</published>
    <title>Minimal-Change Integrity Maintenance Using Tuple Deletions</title>
    <summary>  We address the problem of minimal-change integrity maintenance in the context
of integrity constraints in relational databases. We assume that
integrity-restoration actions are limited to tuple deletions. We identify two
basic computational issues: repair checking (is a database instance a repair of
a given database?) and consistent query answers (is a tuple an answer to a
given query in every repair of a given database?). We study the computational
complexity of both problems, delineating the boundary between the tractable and
the intractable. We consider denial constraints, general functional and
inclusion dependencies, as well as key and foreign key constraints. Our results
shed light on the computational feasibility of minimal-change integrity
maintenance. The tractable cases should lead to practical implementations. The
intractability results highlight the inherent limitations of any integrity
enforcement mechanism, e.g., triggers or referential constraint actions, as a
way of performing minimal-change integrity maintenance.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0212004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212051v1</id>
    <updated>2002-12-23T10:26:36Z</updated>
    <published>2002-12-23T10:26:36Z</published>
    <title>ExploitingWeb Service Semantics: Taxonomies vs. Ontologies</title>
    <summary>  Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.
</summary>
    <author>
      <name>Asuman Dogac</name>
    </author>
    <author>
      <name>Gokce Laleci</name>
    </author>
    <author>
      <name>Yildiray Kabak</name>
    </author>
    <author>
      <name>Ibrahim Cingil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Data Engineering Bulletin, Vol. 25, No. 4, December 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212052v1</id>
    <updated>2002-12-25T15:15:16Z</updated>
    <published>2002-12-25T15:15:16Z</published>
    <title>Improving the Functionality of UDDI Registries through Web Service
  Semantics</title>
    <summary>  In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.
</summary>
    <author>
      <name>Asuman Dogac</name>
    </author>
    <author>
      <name>Ibrahim Cingil</name>
    </author>
    <author>
      <name>Gokce Laleci</name>
    </author>
    <author>
      <name>Yildiray Kabak</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">3rd VLDB Workshop on Technologies for E-Services (TES-02), Hong
  Kong, China, August 23-24, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301017v1</id>
    <updated>2003-01-20T00:13:18Z</updated>
    <published>2003-01-20T00:13:18Z</published>
    <title>Completeness and Decidability Properties for Functional Dependencies in
  XML</title>
    <summary>  XML is of great importance in information storage and retrieval because of
its recent emergence as a standard for data representation and interchange on
the Internet. However XML provides little semantic content and as a result
several papers have addressed the topic of how to improve the semantic
expressiveness of XML. Among the most important of these approaches has been
that of defining integrity constraints in XML. In a companion paper we defined
strong functional dependencies in XML(XFDs). We also presented a set of axioms
for reasoning about the implication of XFDs and showed that the axiom system is
sound for arbitrary XFDs. In this paper we prove that the axioms are also
complete for unary XFDs (XFDs with a single path on the l.h.s.). The second
contribution of the paper is to prove that the implication problem for unary
XFDs is decidable and to provide a linear time algorithm for it.
</summary>
    <author>
      <name>Millist W. Vincent</name>
    </author>
    <author>
      <name>Jixue Liu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0301017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305056v1</id>
    <updated>2003-05-29T21:37:47Z</updated>
    <published>2003-05-29T21:37:47Z</published>
    <title>Configuration Database for BaBar On-line</title>
    <summary>  The configuration database is one of the vital systems in the BaBar on-line
system. It provides services for the different parts of the data acquisition
system and control system, which require run-time parameters. The original
design and implementation of the configuration database played a significant
role in the successful BaBar operations since the beginning of experiment.
Recent additions to the design of the configuration database provide better
means for the management of data and add new tools to simplify main
configuration tasks. We describe the design of the configuration database, its
implementation with the Objectivity/DB object-oriented database, and our
experience collected during the years of operation.
</summary>
    <author>
      <name>R. Bartoldus</name>
    </author>
    <author>
      <name>G. Dubois-Felsmann</name>
    </author>
    <author>
      <name>Y. Kolomensky</name>
    </author>
    <author>
      <name>A. Salnikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, 4 figures, PDF. PSN MOKT004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0305056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306019v1</id>
    <updated>2003-06-04T17:38:23Z</updated>
    <published>2003-06-04T17:38:23Z</published>
    <title>Relational databases for data management in PHENIX</title>
    <summary>  PHENIX is one of the two large experiments at the Relativistic Heavy Ion
Collider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly
100TB of experimental data per year. In addition, large volumes of simulated
data are produced at multiple off-site computing centers. For any file catalog
to play a central role in data management it has to face problems associated
with the need for distributed access and updates. To be used effectively by the
hundreds of PHENIX collaborators in 12 countries the catalog must satisfy the
following requirements: 1) contain up-to-date data, 2) provide fast and
reliable access to the data, 3) have write permissions for the sites that store
portions of data. We present an analysis of several available Relational
Database Management Systems (RDBMS) to support a catalog meeting the above
requirements and discuss the PHENIX experience with building and using the
distributed file catalog.
</summary>
    <author>
      <name>I. Sourikova</name>
    </author>
    <author>
      <name>D. Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, 4 eps figures. PSN
  TUKT003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306020v2</id>
    <updated>2003-06-04T20:12:08Z</updated>
    <published>2003-06-04T19:22:52Z</published>
    <title>On the Verge of One Petabyte - the Story Behind the BaBar Database
  System</title>
    <summary>  The BaBar database has pioneered the use of a commercial ODBMS within the HEP
community. The unique object-oriented architecture of Objectivity/DB has made
it possible to manage over 700 terabytes of production data generated since
May'99, making the BaBar database the world's largest known database. The
ongoing development includes new features, addressing the ever-increasing
luminosity of the detector as well as other changing physics requirements.
Significant efforts are focused on reducing space requirements and operational
costs. The paper discusses our experience with developing a large scale
database system, emphasizing universal aspects which may be applied to any
large scale system, independently of underlying technology used.
</summary>
    <author>
      <name>Adeyemi Adesanya</name>
    </author>
    <author>
      <name>Tofigh Azemoon</name>
    </author>
    <author>
      <name>Jacek Becla</name>
    </author>
    <author>
      <name>Andrew Hanushevsky</name>
    </author>
    <author>
      <name>Adil Hasan</name>
    </author>
    <author>
      <name>Wilko Kroeger</name>
    </author>
    <author>
      <name>Artem Trunov</name>
    </author>
    <author>
      <name>Daniel Wang</name>
    </author>
    <author>
      <name>Igor Gaponenko</name>
    </author>
    <author>
      <name>Simon Patton</name>
    </author>
    <author>
      <name>David Quarrie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT010</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306023v1</id>
    <updated>2003-06-04T23:51:52Z</updated>
    <published>2003-06-04T23:51:52Z</published>
    <title>The Redesigned BaBar Event Store: Believe the Hype</title>
    <summary>  As the BaBar experiment progresses, it produces new and unforeseen
requirements and increasing demands on capacity and feature base. The current
system is being utilized well beyond its original design specifications, and
has scaled appropriately, maintaining data consistency and durability. The
persistent event storage system has remained largely unchanged since the
initial implementation, and thus includes many design features which have
become performance bottlenecks. Programming interfaces were designed before
sufficient usage information became available. Performance and efficiency were
traded off for added flexibility to cope with future demands. With significant
experience in managing actual production data under our belt, we are now in a
position to recraft the system to better suit current needs. The Event Store
redesign is intended to eliminate redundant features while adding new ones,
increase overall performance, and contain the physical storage cost of the
world's largest database.
</summary>
    <author>
      <name>Adeyemi Adesanya</name>
    </author>
    <author>
      <name>Jacek Becla</name>
    </author>
    <author>
      <name>Daniel Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), 5 pages, 2 ps figures, PSN TUKT008</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.4; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306056v1</id>
    <updated>2003-06-12T17:24:16Z</updated>
    <published>2003-06-12T17:24:16Z</published>
    <title>Twelve Ways to Build CMS Crossings from ROOT Files</title>
    <summary>  The simulation of CMS raw data requires the random selection of one hundred
and fifty pileup events from a very large set of files, to be superimposed in
memory to the signal event. The use of ROOT I/O for that purpose is quite
unusual: the events are not read sequentially but pseudo-randomly, they are not
processed one by one in memory but by bunches, and they do not contain orthodox
ROOT objects but many foreign objects and templates. In this context, we have
compared the performance of ROOT containers versus the STL vectors, and the use
of trees versus a direct storage of containers. The strategy with best
performances is by far the one using clones within trees, but it stays hard to
tune and very dependant on the exact use-case. The use of STL vectors could
bring more easily similar performances in a future ROOT release.
</summary>
    <author>
      <name>D. Chamont</name>
    </author>
    <author>
      <name>C. Charlot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, LaTeX, 1 eps figures. PSN
  TUKT004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306061v1</id>
    <updated>2003-06-13T00:40:18Z</updated>
    <published>2003-06-13T00:40:18Z</published>
    <title>Operational Aspects of Dealing with the Large BaBar Data Set</title>
    <summary>  To date, the BaBar experiment has stored over 0.7PB of data in an
Objectivity/DB database. Approximately half this data-set comprises simulated
data of which more than 70% has been produced at more than 20 collaborating
institutes outside of SLAC. The operational aspects of managing such a large
data set and providing access to the physicists in a timely manner is a
challenging and complex problem. We describe the operational aspects of
managing such a large distributed data-set as well as importing and exporting
data from geographically spread BaBar collaborators. We also describe problems
common to dealing with such large datasets.
</summary>
    <author>
      <name>Tofigh Azemoon</name>
    </author>
    <author>
      <name>Adil Hasan</name>
    </author>
    <author>
      <name>Wilko Kroeger</name>
    </author>
    <author>
      <name>Artem Trunov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented for Computing in High Energy Physics, San Diego, March 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7; E.5; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306065v1</id>
    <updated>2003-06-13T16:21:53Z</updated>
    <published>2003-06-13T16:21:53Z</published>
    <title>POOL File Catalog, Collection and Metadata Components</title>
    <summary>  The POOL project is the common persistency framework for the LHC experiments
to store petabytes of experiment data and metadata in a distributed and grid
enabled way. POOL is a hybrid event store consisting of a data streaming layer
and a relational layer. This paper describes the design of file catalog,
collection and metadata components which are not part of the data streaming
layer of POOL and outlines how POOL aims to provide transparent and efficient
data access for a wide range of environments and use cases - ranging from a
large production site down to a single disconnected laptops. The file catalog
is the central POOL component translating logical data references to physical
data files in a grid environment. POOL collections with their associated
metadata provide an abstract way of accessing experiment data via their logical
grouping into sets of related data objects.
</summary>
    <author>
      <name>C. Cioffi</name>
    </author>
    <author>
      <name>S. Eckmann</name>
    </author>
    <author>
      <name>M. Girone</name>
    </author>
    <author>
      <name>J. Hrivnac</name>
    </author>
    <author>
      <name>D. Malon</name>
    </author>
    <author>
      <name>H. Schmuecker</name>
    </author>
    <author>
      <name>A. Vaniachine</name>
    </author>
    <author>
      <name>J. Wojcieszuk</name>
    </author>
    <author>
      <name>Z. Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, 1 eps figure, PSN MOKT009</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306077v1</id>
    <updated>2003-06-13T22:10:36Z</updated>
    <published>2003-06-13T22:10:36Z</published>
    <title>The TESLA Requirements Database</title>
    <summary>  In preparation for the planned linear collider TESLA, DESY is designing the
required buildings and facilities. The accelerator and infrastructure
components have to be allocated to buildings, and their required areas for
installation, operation and maintenance have to be determined.
Interdisciplinary working groups specify the project from different viewpoints
and need to develop a common vision as a precondition for an optimal solution.
A commercial requirements database is used as a collaborative tool, enabling
concurrent requirements specification by independent working groups. The
requirements database ensures long term storage and availability of the
emerging knowledge, and it offers a central platform for communication which is
available for all project members. It is successfully operating since summer
2002 and has since then become an important tool for the design team.
</summary>
    <author>
      <name>Lars Hagge</name>
    </author>
    <author>
      <name>Jens Kreutzkamp</name>
    </author>
    <author>
      <name>Kathrin Lappe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to CHEP2003 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.1;H.4.0;K.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306079v1</id>
    <updated>2003-06-13T23:04:10Z</updated>
    <published>2003-06-13T23:04:10Z</published>
    <title>Integrated Information Management for TESLA</title>
    <summary>  Next-generation projects in High Energy Physics will reach again a new
dimension of complexity. Information management has to ensure an efficient and
economic information flow within the collaborations, offering world-wide
up-to-date information access to the collaborators as one condition for
successful projects. DESY introduces several information systems in preparation
for the planned linear collider TESLA: a Requirements Management System (RMS)
is in production for the TESLA planning group, a Product Data Management System
(PDMS) is in production since the beginning of 2002 and is supporting the
cavity preparation and the general engineering of accelerator components. A
pilot Asset Management System (AMS) is in production for supporting the
management and maintenance of the technical infrastructure, and a Facility
Management System (FMS) with a Geographic Information System (GIS) is currently
being introduced to support civil engineering. Efforts have been started to
integrate the systems with the goal that users can retrieve information through
a single point of access. The paper gives an introduction to information
management and the activities at DESY.
</summary>
    <author>
      <name>Jochen Buerger</name>
    </author>
    <author>
      <name>Lars Hagge</name>
    </author>
    <author>
      <name>Jens Kreutzkamp</name>
    </author>
    <author>
      <name>Kathrin Lappe</name>
    </author>
    <author>
      <name>Andrea Robben</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to CHEP2003 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.0;K.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307015v2</id>
    <updated>2006-06-10T15:18:44Z</updated>
    <published>2003-07-07T14:07:59Z</published>
    <title>Architecture of an Open-Sourced, Extensible Data Warehouse Builder:
  InterBase 6 Data Warehouse Builder (IB-DWB)</title>
    <summary>  We report the development of an open-sourced data warehouse builder,
InterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open
Edition Database Server. InterBase 6 is used for its low maintenance and small
footprint. IB-DWB is designed modularly and consists of 5 main components, Data
Plug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query
Supporter, bounded together by a Kernel. It is also an extensible system, made
possible by the Data Plug Platform and the Discoverer Platform. Currently,
extensions are only possible via dynamic linked-libraries (DLLs).
Multi-Dimensional Cube Builder represents a basal mean of data aggregation. The
architectural philosophy of IB-DWB centers around providing a base platform
that is extensible, which is functionally supported by expansion modules.
IB-DWB is currently being hosted by sourceforge.net (Project Unix Name:
ib-dwb), licensed under GNU General Public License, Version 2.
</summary>
    <author>
      <name>Maurice HT Ling</name>
    </author>
    <author>
      <name>Chi Wai So</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ling, Maurice HT and So, Chi Wai. 2003. Proceedings of the First
  Australian Undergraduate Students' Computing Conference. (pp. 40-45)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0307015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307032v2</id>
    <updated>2003-07-16T12:49:46Z</updated>
    <published>2003-07-12T12:35:37Z</published>
    <title>Data Management and Mining in Astrophysical Databases</title>
    <summary>  We analyse the issues involved in the management and mining of astrophysical
data. The traditional approach to data management in the astrophysical field is
not able to keep up with the increasing size of the data gathered by modern
detectors. An essential role in the astrophysical research will be assumed by
automatic tools for information extraction from large datasets, i.e. data
mining techniques, such as clustering and classification algorithms. This asks
for an approach to data management based on data warehousing, emphasizing the
efficiency and simplicity of data access; efficiency is obtained using
multidimensional access methods and simplicity is achieved by properly handling
metadata. Clustering and classification techniques, on large datasets, pose
additional requirements: computational and memory scalability with respect to
the data size, interpretability and objectivity of clustering or classification
results. In this study we address some possible solutions.
</summary>
    <author>
      <name>M. Frailis</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>V. Roberto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Latex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Ciprini, A. De Angelis, P. Lubrano and O. Mansutti (eds.):
  Proc. of ``Science with the New Generation of High Energy Gamma-ray
  Experiments'' (Perugia, Italy, May 2003). Forum, Udine 2003, p. 157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0307032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308004v1</id>
    <updated>2003-08-02T08:13:06Z</updated>
    <published>2003-08-02T08:13:06Z</published>
    <title>DPG: A Cache-Efficient Accelerator for Sorting and for Join Operators</title>
    <summary>  We present a new algorithm for fast record retrieval,
distribute-probe-gather, or DPG. DPG has important applications both in sorting
and in joins. Current main memory sorting algorithms split their work into
three phases: extraction of key-pointer pairs; sorting of the key-pointer
pairs; and copying of the original records into the destination array according
the sorted key-pointer pairs. The copying in the last phase dominates today's
sorting time. Hence, the use of DPG in the third phase provides an accelerator
for existing sorting algorithms.
  DPG also provides two new join methods for foreign key joins: DPG-move join
and DPG-sort join. The resulting join methods with DPG are faster because DPG
join is cache-efficient and at the same time DPG join avoids the need for
sorting or for hashing. The ideas presented for foreign key join can also be
extended to faster record pair retrieval for spatial and temporal databases.
</summary>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <author>
      <name>Xiaoqin Ma</name>
    </author>
    <author>
      <name>Viet Ha Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309011v1</id>
    <updated>2003-09-08T19:57:46Z</updated>
    <published>2003-09-08T19:57:46Z</published>
    <title>Indexing of Tables Referencing Complex Structures</title>
    <summary>  We introduce indexing of tables referencing complex structures such as
digraphs and spatial objects, appearing in genetics and other data intensive
analysis. The indexing is achieved by extracting dimension schemas from the
referenced structures. The schemas and their dimensionality are determined by
proper coloring algorithms and the duality between all such schemas and all
such possible proper colorings is established. This duality, in turn, provides
us with an extensive library of solutions when addressing indexing questions.
It is illustrated how to use the schemas, in connection with additional
relational database technologies, to optimize queries conditioned on the
structural information being referenced. Comparisons using bitmap indexing in
the Oracle 9.2i database, on the one hand, and multidimensional clustering in
DB2 8.1.2, on the other hand, are used to illustrate the applicability of the
indexing to different technology settings. Finally, we illustrate how the
indexing can be used to extract low dimensional schemas from a binary interval
tree in order to resolve efficiently interval and stabbing queries.
</summary>
    <author>
      <name>Agust S. Egilsson</name>
    </author>
    <author>
      <name>Hakon Gudbjartsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0309011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;H.2.8;J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310028v1</id>
    <updated>2003-10-15T16:07:56Z</updated>
    <published>2003-10-15T16:07:56Z</published>
    <title>Providing Diversity in K-Nearest Neighbor Query Results</title>
    <summary>  Given a point query Q in multi-dimensional space, K-Nearest Neighbor (KNN)
queries return the K closest answers according to given distance metric in the
database with respect to Q. In this scenario, it is possible that a majority of
the answers may be very similar to some other, especially when the data has
clusters. For a variety of applications, such homogeneous result sets may not
add value to the user. In this paper, we consider the problem of providing
diversity in the results of KNN queries, that is, to produce the closest result
set such that each answer is sufficiently different from the rest. We first
propose a user-tunable definition of diversity, and then present an algorithm,
called MOTLEY, for producing a diverse result set as per this definition.
Through a detailed experimental evaluation on real and synthetic data, we show
that MOTLEY can produce diverse result sets by reading only a small fraction of
the tuples in the database. Further, it imposes no additional overhead on the
evaluation of traditional KNN queries, thereby providing a seamless interface
between diversity and distance.
</summary>
    <author>
      <name>Anoop Jain</name>
    </author>
    <author>
      <name>Parag Sarda</name>
    </author>
    <author>
      <name>Jayant R. Haritsa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310038v1</id>
    <updated>2003-10-17T16:55:08Z</updated>
    <published>2003-10-17T16:55:08Z</published>
    <title>On Addressing Efficiency Concerns in Privacy Preserving Data Mining</title>
    <summary>  Data mining services require accurate input data for their results to be
meaningful, but privacy concerns may influence users to provide spurious
information. To encourage users to provide correct inputs, we recently proposed
a data distortion scheme for association rule mining that simultaneously
provides both privacy to the user and accuracy in the mining results. However,
mining the distorted database can be orders of magnitude more time-consuming as
compared to mining the original database. In this paper, we address this issue
and demonstrate that by (a) generalizing the distortion process to perform
symbol-specific distortion, (b) appropriately choosing the distortion
parameters, and (c) applying a variety of optimizations in the reconstruction
process, runtime efficiencies that are well within an order of magnitude of
undistorted mining can be achieved.
</summary>
    <author>
      <name>Shipra Agrawal</name>
    </author>
    <author>
      <name>Vijay Krishnan</name>
    </author>
    <author>
      <name>Jayant Haritsa</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0310038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311031v1</id>
    <updated>2003-11-21T14:44:22Z</updated>
    <published>2003-11-21T14:44:22Z</published>
    <title>Towards an Intelligent Database System Founded on the SP Theory of
  Computing and Cognition</title>
    <summary>  The SP theory of computing and cognition, described in previous publications,
is an attractive model for intelligent databases because it provides a simple
but versatile format for different kinds of knowledge, it has capabilities in
artificial intelligence, and it can also function like established database
models when that is required.
  This paper describes how the SP model can emulate other models used in
database applications and compares the SP model with those other models. The
artificial intelligence capabilities of the SP model are reviewed and its
relationship with other artificial intelligence systems is described. Also
considered are ways in which current prototypes may be translated into an
'industrial strength' working system.
</summary>
    <author>
      <name>J. Gerard Wolff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.datak.2006.04.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.datak.2006.04.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J G Wolff, Data &amp; Knowledge Engineering 60, 596-624, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0311031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311038v1</id>
    <updated>2003-11-25T09:42:59Z</updated>
    <published>2003-11-25T09:42:59Z</published>
    <title>XPath-Logic and XPathLog: A Logic-Programming Style XML Data
  Manipulation Language</title>
    <summary>  We define XPathLog as a Datalog-style extension of XPath. XPathLog provides a
clear, declarative language for querying and manipulating XML whose
perspectives are especially in XML data integration. In our characterization,
the formal semantics is defined wrt. an edge-labeled graph-based model which
covers the XML data model. We give a complete, logic-based characterization of
XML data and the main language concept for XML, XPath. XPath-Logic extends the
XPath language with variable bindings and embeds it into first-order logic.
XPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style,
rule-based language for querying and manipulating XML data. The model-theoretic
semantics of XPath-Logic serves as the base of XPathLog as a logic-programming
language, whereas also an equivalent answer-set semantics for evaluating
XPathLog queries is given. In contrast to other approaches, the XPath syntax
and semantics is also used for a declarative specification how the database
should be updated: when used in rule heads, XPath filters are interpreted as
specifications of elements and properties which should be added to the
database.
</summary>
    <author>
      <name>Wolfgang May</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0311038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; D.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312041v1</id>
    <updated>2003-12-18T17:29:05Z</updated>
    <published>2003-12-18T17:29:05Z</published>
    <title>Greedy Algorithms in Datalog</title>
    <summary>  In the design of algorithms, the greedy paradigm provides a powerful tool for
solving efficiently classical computational problems, within the framework of
procedural languages. However, expressing these algorithms within the
declarative framework of logic-based languages has proven a difficult research
challenge. In this paper, we extend the framework of Datalog-like languages to
obtain simple declarative formulations for such problems, and propose effective
implementation techniques to ensure computational complexities comparable to
those of procedural formulations. These advances are achieved through the use
of the "choice" construct, extended with preference annotations to effect the
selection of alternative stable-models and nondeterministic fixpoints. We show
that, with suitable storage structures, the differential fixpoint computation
of our programs matches the complexity of procedural algorithms in classical
search and optimization problems.
</summary>
    <author>
      <name>Sergio Greco</name>
    </author>
    <author>
      <name>Carlo Zaniolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, 1(4): 381-407, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.3.1; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312046v1</id>
    <updated>2003-12-19T15:25:53Z</updated>
    <published>2003-12-19T15:25:53Z</published>
    <title>On the Abductive or Deductive Nature of Database Schema Validation and
  Update Processing Problems</title>
    <summary>  We show that database schema validation and update processing problems such
as view updating, materialized view maintenance, integrity constraint checking,
integrity constraint maintenance or condition monitoring can be classified as
problems of either abductive or deductive nature, according to the reasoning
paradigm that inherently suites them. This is done by performing abductive and
deductive reasoning on the event rules [Oli91], a set of rules that define the
difference between consecutive database states In this way, we show that it is
possible to provide methods able to deal with all these problems as a whole. We
also show how some existing general deductive and abductive procedures may be
used to reason on the event rules. In this way, we show that these procedures
can deal with all database schema validation and update processing problems
considered in this paper.
</summary>
    <author>
      <name>Ernest Teniente</name>
    </author>
    <author>
      <name>Toni Urpi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming 3(3):287-327, may 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1;H.2.4; H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402003v1</id>
    <updated>2004-02-02T01:42:35Z</updated>
    <published>2004-02-02T01:42:35Z</published>
    <title>Semantic Optimization of Preference Queries</title>
    <summary>  The notion of preference is becoming more and more ubiquitous in present-day
information systems. Preferences are primarily used to filter and personalize
the information reaching the users of such systems. In database systems,
preferences are usually captured as preference relations that are used to build
preference queries. In our approach, preference queries are relational algebra
or SQL queries that contain occurrences of the winnow operator ("find the most
preferred tuples in a given relation").
  We present here a number of semantic optimization techniques applicable to
preference queries. The techniques make use of integrity constraints, and make
it possible to remove redundant occurrences of the winnow operator and to apply
a more efficient algorithm for the computation of winnow. We also study the
propagation of integrity constraints in the result of the winnow. We have
identified necessary and sufficient conditions for the applicability of our
techniques, and formulated those conditions as constraint satisfiability
problems.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402007v1</id>
    <updated>2004-02-02T20:12:53Z</updated>
    <published>2004-02-02T20:12:53Z</published>
    <title>An Integrated Approach for Extraction of Objects from XML and
  Transformation to Heterogeneous Object Oriented Databases</title>
    <summary>  CERN's (European Organization for Nuclear Research) WISDOM project uses XML
for the replication of data between different data repositories in a
heterogeneous operating system environment. For exchanging data from
Web-resident databases, the data needs to be transformed into XML and back to
the database format. Many different approaches are employed to do this
transformation. This paper addresses issues that make this job more efficient
and robust than existing approaches. It incorporates the World Wide Web
Consortium (W3C) XML Schema specification in the database-XML relationship.
Incorporation of the XML Schema exhibits significant improvements in XML
content usage and reduces the limitations of DTD-based database XML services.
Secondly the paper explores the possibility of database independent
transformation of data between XML and different databases. It proposes a
standard XML format that every serialized object should follow. This makes it
possible to use objects of heterogeneous database seamlessly using XML.
</summary>
    <author>
      <name>Uzair Ahmad</name>
    </author>
    <author>
      <name>Mohammad Waseem Hassan</name>
    </author>
    <author>
      <name>Arshad Ali</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Ian Willers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures. Presented at the 5th Int Conf on Enterprise
  Information Systems, ICEIS'03. Angers France April 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402008v1</id>
    <updated>2004-02-02T20:18:23Z</updated>
    <published>2004-02-02T20:18:23Z</published>
    <title>A Use-Case Driven Approach in Requirements Engineering : The Mammogrid
  Project</title>
    <summary>  We report on the application of the use-case modeling technique to identify
and specify the user requirements of the MammoGrid project in an incremental
and controlled iterative approach. Modeling has been carried out in close
collaboration with clinicians and radiologists with no prior experience of use
cases. The study reveals the advantages and limitations of applying this
technique to requirements specification in the domains of breast cancer
screening and mammography research, with implications for medical imaging more
generally. In addition, this research has shown a return on investment in
use-case modeling in shorter gaps between phases of the requirements
engineering process. The qualitative result of this analysis leads us to
propose that a use-case modeling approach may result in reducing the cycle of
the requirements engineering process for medical imaging.
</summary>
    <author>
      <name>Mohammed Odeh</name>
    </author>
    <author>
      <name>Tamas Hauer</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Tony Solomonides</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Presented at the 7th IASTED Int Conf on Software
  Engineering Applications. Marina del Rey, USA November 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402009v1</id>
    <updated>2004-02-03T14:32:39Z</updated>
    <published>2004-02-03T14:32:39Z</published>
    <title>Resolving Clinicians Queries Across a Grids Infrastructure</title>
    <summary>  The past decade has witnessed order of magnitude increases in computing
power, data storage capacity and network speed, giving birth to applications
which may handle large data volumes of increased complexity, distributed over
the Internet. Grids computing promises to resolve many of the difficulties in
facilitating medical image analysis to allow radiologists to collaborate
without having to co-locate. The EU-funded MammoGrid project aims to
investigate the feasibility of developing a Grid-enabled European database of
mammograms and provide an information infrastructure which federates multiple
mammogram databases. This will enable clinicians to develop new common,
collaborative and co-operative approaches to the analysis of mammographic data.
This paper focuses on one of the key requirements for large-scale distributed
mammogram analysis: resolving queries across a grid-connected federation of
images.
</summary>
    <author>
      <name>F Estrella</name>
    </author>
    <author>
      <name>C del Frate</name>
    </author>
    <author>
      <name>T Hauer</name>
    </author>
    <author>
      <name>R McClatchey</name>
    </author>
    <author>
      <name>M Odeh</name>
    </author>
    <author>
      <name>D Rogulin</name>
    </author>
    <author>
      <name>S R Amendolia</name>
    </author>
    <author>
      <name>D Schottlander</name>
    </author>
    <author>
      <name>T Solomonides</name>
    </author>
    <author>
      <name>R Warren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures. Presented at the 2nd Int Conf on HealthGrids
  Clermont-Ferrand, France January 2004 and accepted by Methods of Information
  in Medicine</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402023v1</id>
    <updated>2004-02-12T15:00:18Z</updated>
    <published>2004-02-12T15:00:18Z</published>
    <title>A Service-Based Approach for Managing Mammography Data</title>
    <summary>  Grid-based technologies are emerging as a potential open-source
standards-based solution for managing and collabo-rating distributed resources.
In view of these new computing solutions, the Mammogrid project is developing a
service-based and Grid-aware application which manages a Euro-pean-wide
database of mammograms. Medical conditions such as breast cancer, and
mammograms as images, are ex-tremely complex with many dimensions of
variability across the population. An effective solution for the management of
disparate mammogram data sources is a federation of autonomous multi-centre
sites which transcends national boundaries. The Mammogrid solution utilizes the
Grid tech-nologies to integrate geographically distributed data sets. The
Mammogrid application will explore the potential of the Grid to support
effective co-working among radiologists through-out the EU. This paper outlines
the Mammogrid service-based approach in managing a federation of grid-connected
mam-mography databases.
</summary>
    <author>
      <name>Florida Estrella</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Dmitry Rogulina</name>
    </author>
    <author>
      <name>Roberto Amendolia</name>
    </author>
    <author>
      <name>Tony Solomonides</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures. Accepted by the 11th World Congress on Medical
  Informatics (MedInfo'04). San Francisco, USA. September 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402025v1</id>
    <updated>2004-02-12T14:36:05Z</updated>
    <published>2004-02-12T14:36:05Z</published>
    <title>A perspective on the Healthgrid initiative</title>
    <summary>  This paper presents a perspective on the Healthgrid initiative which involves
European projects deploying pioneering applications of grid technology in the
health sector. In the last couple of years, several grid projects have been
funded on health related issues at national and European levels. A crucial
issue is to maximize their cross fertilization in the context of an environment
where data of medical interest can be stored and made easily available to the
different actors in healthcare, physicians, healthcare centres and
administrations, and of course the citizens. The Healthgrid initiative,
represented by the Healthgrid association (http://www.healthgrid.org), was
initiated to bring the necessary long term continuity, to reinforce and promote
awareness of the possibilities and advantages linked to the deployment of GRID
technologies in health. Technologies to address the specific requirements for
medical applications are under development. Results from the DataGrid and other
projects are given as examples of early applications.
</summary>
    <author>
      <name>V. Breton</name>
    </author>
    <author>
      <name>A. E. Solomonides</name>
    </author>
    <author>
      <name>R. H. McClatchey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCGrid.2004.1336598</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCGrid.2004.1336598" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure. Accepted by the Second International Workshop on
  Biomedical Computations on the Grid, at the 4th IEEE/ACM International
  Symposium on Cluster Computing and the Grid (CCGrid 2004). Chicago USA, April
  2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4,J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403018v1</id>
    <updated>2004-03-12T09:57:43Z</updated>
    <published>2004-03-12T09:57:43Z</published>
    <title>The World Wide Telescope: An Archetype for Online Science</title>
    <summary>  Most scientific data will never be directly examined by scientists; rather it
will be put into online databases where it will be analyzed and summarized by
computer programs. Scientists increasingly see their instruments through online
scientific archives and analysis tools, rather than examining the raw data.
Today this analysis is primarily driven by scientists asking queries, but
scientific archives are becoming active databases that self-organize and
recognize interesting and anomalous facts as data arrives. In some fields, data
from many different archives can be cross-correlated to produce new insights.
Astronomy presents an excellent example of these trends; and, federating
Astronomy archives presents interesting challenges for computer scientists.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, msword posted at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=590</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CACM, V. 45.11, pp. 50-54, Nov. 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0403018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403020v1</id>
    <updated>2004-03-12T10:20:23Z</updated>
    <published>2004-03-12T10:20:23Z</published>
    <title>The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte
  Astronomical Archive from Object to Relational DBMS</title>
    <summary>  The Sloan Digital Sky Survey Science Archive is the first in a series of
multi-Terabyte digital archives in Astronomy and other data-intensive sciences.
To facilitate data mining in the SDSS archive, we adapted a commercial database
engine and built specialized tools on top of it. Originally we chose an
object-oriented database management system due to its data organization
capabilities, platform independence, query performance and conceptual fit to
the data. However, after using the object database for the first couple of
years of the project, it soon began to fall short in terms of its query support
and data mining performance. This was as much due to the inability of the
database vendor to respond our demands for features and bug fixes as it was due
to their failure to keep up with the rapid improvements in hardware
performance, particularly faster RAID disk systems. In the end, we were forced
to abandon the object database and migrate our data to a relational database.
We describe below the technical issues that we faced with the object database
and how and why we migrated to relational technology.
</summary>
    <author>
      <name>Aniruddha R. Thakar</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Peter Z. Kunszt</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Comput.Sci.Eng. 5 (2003) 16-29</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0403020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403021v1</id>
    <updated>2004-03-12T10:42:25Z</updated>
    <published>2004-03-12T10:42:25Z</published>
    <title>A Quick Look at SATA Disk Performance</title>
    <summary>  We have been investigating the use of low-cost, commodity components for
multi-terabyte SQL Server databases. Dubbed storage bricks, these servers are
white box PCs containing the largest ATA drives, value-priced AMD or Intel
processors, and inexpensive ECC memory. One issue has been the wiring mess, air
flow problems, length restrictions, and connector failures created by seven or
more parallel ATA (PATA) ribbon cables and drives in]a tower or 3U rack-mount
chassis. Large capacity Serial ATA (SATA) drives have recently become widely
available for the PC environment at a reasonable price. In addition to being
faster, the SATA connectors seem more reliable, have a more reasonable length
restriction (1m) and allow better airflow. We tested two drive brands along
with two RAID controllers to evaluate SATA drive performance and reliablility.
This paper documents our results so far.
</summary>
    <author>
      <name>Tom Barclay</name>
    </author>
    <author>
      <name>Wyman Chong</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0403021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404003v1</id>
    <updated>2004-04-02T02:03:32Z</updated>
    <published>2004-04-02T02:03:32Z</published>
    <title>Enhancing the expressive power of the U-Datalog language</title>
    <summary>  U-Datalog has been developed with the aim of providing a set-oriented logical
update language, guaranteeing update parallelism in the context of a
Datalog-like language. In U-Datalog, updates are expressed by introducing
constraints (+p(X), to denote insertion, and [minus sign]p(X), to denote
deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP
program. In this framework, a set of updates (constraints) is satisfiable if it
does not represent an inconsistent theory, that is, it does not require the
insertion and the deletion of the same fact. This approach resembles a very
simple form of negation. However, on the other hand, U-Datalog does not provide
any mechanism to explicitly deal with negative information, resulting in a
language with limited expressive power. In this paper, we provide a semantics,
based on stratification, handling the use of negated atoms in U-Datalog
programs, and we show which problems arise in defining a compositional
semantics.
</summary>
    <author>
      <name>Elisa Bertino</name>
    </author>
    <author>
      <name>Barbara Catania</name>
    </author>
    <author>
      <name>Roberta Gori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in Theory and Practice of Logic Programming, vol. 1, no. 1,
  2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, vol. 1, no. 1, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405069v1</id>
    <updated>2004-05-20T14:33:08Z</updated>
    <published>2004-05-20T14:33:08Z</published>
    <title>Mining Frequent Itemsets from Secondary Memory</title>
    <summary>  Mining frequent itemsets is at the core of mining association rules, and is
by now quite well understood algorithmically. However, most algorithms for
mining frequent itemsets assume that the main memory is large enough for the
data structures used in the mining, and very few efficient algorithms deal with
the case when the database is very large or the minimum support is very low.
Mining frequent itemsets from a very large database poses new challenges, as
astronomical amounts of raw data is ubiquitously being recorded in commerce,
science and government. In this paper, we discuss approaches to mining frequent
itemsets when data structures are too large to fit in main memory. Several
divide-and-conquer algorithms are given for mining from disks. Many novel
techniques are introduced. Experimental results show that the techniques reduce
the required disk accesses by orders of magnitude, and enable truly scalable
data mining.
</summary>
    <author>
      <name>Gösta Grahne</name>
    </author>
    <author>
      <name>Jianfei Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0405069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405072v1</id>
    <updated>2004-05-21T13:24:00Z</updated>
    <published>2004-05-21T13:24:00Z</published>
    <title>Grid Databases for Shared Image Analysis in the MammoGrid Project</title>
    <summary>  The MammoGrid project aims to prove that Grid infrastructures can be used for
collaborative clinical analysis of database-resident but geographically
distributed medical images. This requires: a) the provision of a
clinician-facing front-end workstation and b) the ability to service real-world
clinician queries across a distributed and federated database. The MammoGrid
project will prove the viability of the Grid by harnessing its power to enable
radiologists from geographically dispersed hospitals to share standardized
mammograms, to compare diagnoses (with and without computer aided detection of
tumours) and to perform sophisticated epidemiological studies across national
boundaries. This paper outlines the approach taken in MammoGrid to seamlessly
connect radiologist workstations across a Grid using an "information
infrastructure" and a DICOM-compliant object model residing in multiple
distributed data stores in Italy and the UK
</summary>
    <author>
      <name>S. R. Amendolia</name>
    </author>
    <author>
      <name>F. Estrella</name>
    </author>
    <author>
      <name>T. Hauer</name>
    </author>
    <author>
      <name>D. Manset</name>
    </author>
    <author>
      <name>R. McClatchey</name>
    </author>
    <author>
      <name>M. Odeh</name>
    </author>
    <author>
      <name>T. Reading</name>
    </author>
    <author>
      <name>D. Rogulin</name>
    </author>
    <author>
      <name>D. Schottlander</name>
    </author>
    <author>
      <name>T. Solomonides</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IDEAS.2004.1319804</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IDEAS.2004.1319804" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2004 International Database Engineering and
  Applications Symposium (IDEAS'04). Coimbra Portugal. IEEE Press</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4;J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405076v1</id>
    <updated>2004-05-22T10:30:35Z</updated>
    <published>2004-05-22T10:30:35Z</published>
    <title>An Abductive Framework For Computing Knowledge Base Updates</title>
    <summary>  This paper introduces an abductive framework for updating knowledge bases
represented by extended disjunctive programs. We first provide a simple
transformation from abductive programs to update programs which are logic
programs specifying changes on abductive hypotheses. Then, extended abduction,
which was introduced by the same authors as a generalization of traditional
abduction, is computed by the answer sets of update programs. Next, different
types of updates, view updates and theory updates are characterized by
abductive programs and computed by update programs. The task of consistency
restoration is also realized as special cases of these updates. Each update
problem is comparatively assessed from the computational complexity viewpoint.
The result of this paper provides a uniform framework for different types of
knowledge base updates, and each update is computed using existing procedures
of logic programming.
</summary>
    <author>
      <name>Chiaki Sakama</name>
    </author>
    <author>
      <name>Katsumi Inoue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in Theory and Practice of Logic Programming, vol. 3, no. 6,
  2003</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, vol. 3, no. 6, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406016v1</id>
    <updated>2004-06-07T20:45:28Z</updated>
    <published>2004-06-07T20:45:28Z</published>
    <title>Schema-based Scheduling of Event Processors and Buffer Minimization for
  Queries on Structured Data Streams</title>
    <summary>  We introduce an extension of the XQuery language, FluX, that supports
event-based query processing and the conscious handling of main memory buffers.
Purely event-based queries of this language can be executed on streaming XML
data in a very direct way. We then develop an algorithm that allows to
efficiently rewrite XQueries into the event-based FluX language. This algorithm
uses order constraints from a DTD to schedule event handlers and to thus
minimize the amount of buffering required for evaluating a query. We discuss
the various technical aspects of query optimization and query evaluation within
our framework. This is complemented with an experimental evaluation of our
approach.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Stefanie Scherzinger</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <author>
      <name>Bernhard Stegmaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, to appear in Proc. 30th VLDB 2004, Toronto,
  Canada. Extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3, H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407035v1</id>
    <updated>2004-07-15T14:30:20Z</updated>
    <published>2004-07-15T14:30:20Z</published>
    <title>A Framework for High-Accuracy Privacy-Preserving Mining</title>
    <summary>  To preserve client privacy in the data mining process, a variety of
techniques based on random perturbation of data records have been proposed
recently. In this paper, we present a generalized matrix-theoretic model of
random perturbation, which facilitates a systematic approach to the design of
perturbation mechanisms for privacy-preserving mining. Specifically, we
demonstrate that (a) the prior techniques differ only in their settings for the
model parameters, and (b) through appropriate choice of parameter settings, we
can derive new perturbation techniques that provide highly accurate mining
results even under strict privacy guarantees. We also propose a novel
perturbation mechanism wherein the model parameters are themselves
characterized as random variables, and demonstrate that this feature provides
significant improvements in privacy at a very marginal cost in accuracy.
  While our model is valid for random-perturbation-based privacy-preserving
mining in general, we specifically evaluate its utility here with regard to
frequent-itemset mining on a variety of real datasets. The experimental results
indicate that our mechanisms incur substantially lower identity and support
errors as compared to the prior techniques.
</summary>
    <author>
      <name>Shipra Agrawal</name>
    </author>
    <author>
      <name>Jayant R. Haritsa</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0407035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408031v1</id>
    <updated>2004-08-14T02:40:59Z</updated>
    <published>2004-08-14T02:40:59Z</published>
    <title>There Goes the Neighborhood: Relational Algebra for Spatial Data Search</title>
    <summary>  We explored ways of doing spatial search within a relational database: (1)
hierarchical triangular mesh (a tessellation of the sphere), (2) a zoned
bucketing system, and (3) representing areas as disjunctive-normal form
constraints. Each of these approaches has merits. They all allow efficient
point-in-region queries. A relational representation for regions allows Boolean
operations among them and allows quick tests for point-in-region,
regions-containing-point, and region-overlap. The speed of these algorithms is
much improved by a zone and multi-scale zone-pyramid scheme. The approach has
the virtue that the zone mechanism works well on B-Trees native to all SQL
systems and integrates naturally with current query optimizers - rather than
requiring a new spatial access method and concomitant query optimizer
extensions. Over the last 5 years, we have used these techniques extensively in
our work on SkyServer.sdss.org, and SkyQuery.net.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Aniruddha R. Thakar</name>
    </author>
    <author>
      <name>Gyorgy Fekete</name>
    </author>
    <author>
      <name>William O'Mullane</name>
    </author>
    <author>
      <name>Maria A. Nieto-Santisteban</name>
    </author>
    <author>
      <name>Gerd Heber</name>
    </author>
    <author>
      <name>Arnold H. Rots</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=736</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0408031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408051v1</id>
    <updated>2004-08-22T03:19:19Z</updated>
    <published>2004-08-22T03:19:19Z</published>
    <title>Scalable XSLT Evaluation</title>
    <summary>  XSLT is an increasingly popular language for processing XML data. It is
widely supported by application platform software. However, little optimization
effort has been made inside the current XSLT processing engines. Evaluating a
very simple XSLT program on a large XML document with a simple schema may
result in extensive usage of memory. In this paper, we present a novel notion
of \emph{Streaming Processing Model} (\emph{SPM}) to evaluate a subset of XSLT
programs on XML documents, especially large ones. With SPM, an XSLT processor
can transform an XML source document to other formats without extra memory
buffers required. Therefore, our approach can not only tackle large source
documents, but also produce large results. We demonstrate with a performance
study the advantages of the SPM approach. Experimental results clearly confirm
that SPM improves XSLT evaluation typically 2 to 10 times better than the
existing approaches. Moreover, the SPM approach also features high scalability.
</summary>
    <author>
      <name>Zhimao Guo</name>
    </author>
    <author>
      <name>Min Li</name>
    </author>
    <author>
      <name>Xiaoling Wang</name>
    </author>
    <author>
      <name>Aoying Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">It appeared on the international conference of APWeb 04. And it
  includes 10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. of APWeb, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0408051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410001v2</id>
    <updated>2004-10-02T12:01:31Z</updated>
    <published>2004-10-01T16:55:38Z</published>
    <title>The Infati Data</title>
    <summary>  The ability to perform meaningful empirical studies is of essence in research
in spatio-temporal query processing. Such studies are often necessary to gain
detailed insight into the functional and performance characteristics of
proposals for new query processing techniques.
  We present a collection of spatio-temporal data, collected during an
intelligent speed adaptation project, termed INFATI, in which some two dozen
cars equipped with GPS receivers and logging equipment took part. We describe
how the data was collected and how it was "modified" to afford the drivers some
degree of anonymity.
  We also present the road network in which the cars were moving during data
collection.
  The GPS data is publicly available for non-commercial purposes. It is our
hope that this resource will help the spatio-temporal research community in its
efforts to develop new and better query processing techniques.
</summary>
    <author>
      <name>C. S. Jensen</name>
    </author>
    <author>
      <name>H. Lahrmann</name>
    </author>
    <author>
      <name>S. Pakalnis</name>
    </author>
    <author>
      <name>J. Runge</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0410001v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410001v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410054v1</id>
    <updated>2004-10-20T11:40:50Z</updated>
    <published>2004-10-20T11:40:50Z</published>
    <title>Paraconsistent Intuitionistic Fuzzy Relational Data Model</title>
    <summary>  In this paper, we present a generalization of the relational data model based
on paraconsistent intuitionistic fuzzy sets. Our data model is capable of
manipulating incomplete as well as inconsistent information. Fuzzy relation or
intuitionistic fuzzy relation can only handle incomplete information.
Associated with each relation are two membership functions one is called
truth-membership function $T$ which keeps track of the extent to which we
believe the tuple is in the relation, another is called false-membership
function which keeps track of the extent to which we believe that it is not in
the relation. A paraconsistent intuitionistic fuzzy relation is inconsistent if
there exists one tuple $a$ such that $T(a) + F(a) &gt; 1$. In order to handle
inconsistent situation, we propose an operator called split to transform
inconsistent paraconsistent intuitionistic fuzzy relations into
pseudo-consistent paraconsistent intuitionistic fuzzy relations and do the
set-theoretic and relation-theoretic operations on them and finally use another
operator called combine to transform the result back to paraconsistent
intuitionistic fuzzy relation. For this model, we define algebraic operators
that are generalisations of the usual operators such as union, selection, join
on fuzzy relations. Our data model can underlie any database and knowledge-base
management system that deals with incomplete and inconsistent information.
</summary>
    <author>
      <name>Rajshekhar Sunderraman</name>
    </author>
    <author>
      <name>Haibin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411035v1</id>
    <updated>2004-11-12T12:02:17Z</updated>
    <published>2004-11-12T12:02:17Z</published>
    <title>A FP-Tree Based Approach for Mining All Strongly Correlated Pairs
  without Candidate Generation</title>
    <summary>  Given a user-specified minimum correlation threshold and a transaction
database, the problem of mining all-strong correlated pairs is to find all item
pairs with Pearson's correlation coefficients above the threshold . Despite the
use of upper bound based pruning technique in the Taper algorithm [1], when the
number of items and transactions are very large, candidate pair generation and
test is still costly. To avoid the costly test of a large number of candidate
pairs, in this paper, we propose an efficient algorithm, called Tcp, based on
the well-known FP-tree data structure, for mining the complete set of
all-strong correlated item pairs. Our experimental results on both synthetic
and real world datasets show that, Tcp's performance is significantly better
than that of the previously developed Taper algorithm over practical ranges of
correlation threshold specifications.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0411035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412018v1</id>
    <updated>2004-12-04T12:28:29Z</updated>
    <published>2004-12-04T12:28:29Z</published>
    <title>Modeling Complex Higher Order Patterns</title>
    <summary>  The goal of this paper is to show that generalizing the notion of frequent
patterns can be useful in extending association analysis to more complex higher
order patterns. To that end, we describe a general framework for modeling a
complex pattern based on evaluating the interestingness of its sub-patterns. A
key goal of any framework is to allow people to more easily express, explore,
and communicate ideas, and hence, we illustrate how our framework can be used
to describe a variety of commonly used patterns, such as frequent patterns,
frequent closed patterns, indirect association patterns, hub patterns and
authority patterns. To further illustrate the usefulness of the framework, we
also present two new kinds of patterns that derived from the framework: clique
pattern and bi-clique pattern and illustrate their practical use.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412058v1</id>
    <updated>2004-12-13T06:14:20Z</updated>
    <published>2004-12-13T06:14:20Z</published>
    <title>Clustering Categorical Data Streams</title>
    <summary>  The data stream model has been defined for new classes of applications
involving massive data being generated at a fast pace. Web click stream
analysis and detection of network intrusions are two examples. Cluster analysis
on data streams becomes more difficult, because the data objects in a data
stream must be accessed in order and can be read only once or few times with
limited resources. Recently, a few clustering algorithms have been developed
for analyzing numeric data streams. However, to our knowledge to date, no
algorithm exists for clustering categorical data streams. In this paper, we
propose an efficient clustering algorithm for analyzing categorical data
streams. It has been proved that the proposed algorithm uses small memory
footprints. We provide empirical analysis on the performance of the algorithm
in clustering both synthetic and real data streams
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <author>
      <name>Joshua Zhexue Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages. To Appear in Journal of Computational Methods on Science
  and Engineering(JCMSE)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501029v1</id>
    <updated>2005-01-14T19:45:10Z</updated>
    <published>2005-01-14T19:45:10Z</published>
    <title>Estimating Range Queries using Aggregate Data with Integrity
  Constraints: a Probabilistic Approach</title>
    <summary>  The problem of recovering (count and sum) range queries over multidimensional
data only on the basis of aggregate information on such data is addressed. This
problem can be formalized as follows. Suppose that a transformation T producing
a summary from a multidimensional data set is used. Now, given a data set D, a
summary S=T(D) and a range query r on D, the problem consists of studying r by
modelling it as a random variable defined over the sample space of all the data
sets D' such that T(D) = S. The study of such a random variable, done by the
definition of its probability distribution and the computation of its mean
value and variance, represents a well-founded, theoretical probabilistic
approach for estimating the query only on the basis of the available
information (that is the summary S) without assumptions on original data.
</summary>
    <author>
      <name>Francesco Buccafurri</name>
    </author>
    <author>
      <name>Filippo Furfaro</name>
    </author>
    <author>
      <name>Domenico Sacca'</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0501029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.3; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501079v1</id>
    <updated>2005-01-27T12:13:16Z</updated>
    <published>2005-01-27T12:13:16Z</published>
    <title>Data Mining for Actionable Knowledge: A Survey</title>
    <summary>  The data mining process consists of a series of steps ranging from data
cleaning, data selection and transformation, to pattern evaluation and
visualization. One of the central problems in data mining is to make the mined
patterns or knowledge actionable. Here, the term actionable refers to the mined
patterns suggest concrete and profitable actions to the decision-maker. That
is, the user can do something to bring direct benefits (increase in profits,
reduction in cost, improvement in efficiency, etc.) to the organization's
advantage. However, there has been written no comprehensive survey available on
this topic. The goal of this paper is to fill the void.
  In this paper, we first present two frameworks for mining actionable
knowledge that are inexplicitly adopted by existing research methods. Then we
try to situate some of the research on this topic from two different
viewpoints: 1) data mining tasks and 2) adopted framework. Finally, we specify
issues that are either not addressed or insufficiently studied yet and conclude
the paper.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0501079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502008v1</id>
    <updated>2005-02-02T03:15:42Z</updated>
    <published>2005-02-02T03:15:42Z</published>
    <title>Scientific Data Management in the Coming Decade</title>
    <summary>  This is a thought piece on data-intensive science requirements for databases
and science centers. It argues that peta-scale datasets will be housed by
science centers that provide substantial storage and processing for scientists
who access the data via smart notebooks. Next-generation science instruments
and simulations will generate these peta-scale datasets. The need to publish
and share data and the need for generic analysis and visualization tools will
finally create a convergence on common metadata standards. Database systems
will be judged by their support of these metadata standards and by their
ability to manage and access peta-scale datasets. The procedural
stream-of-bytes-file-centric approach to data analysis is both too cumbersome
and too serial for such large datasets. Non-procedural query and analysis of
schematized self-describing data is both easier to use and allows much more
parallelism.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>David T. Liu</name>
    </author>
    <author>
      <name>Maria Nieto-Santisteban</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>David DeWitt</name>
    </author>
    <author>
      <name>Gerd Heber</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503012v2</id>
    <updated>2005-03-07T17:09:03Z</updated>
    <published>2005-03-04T15:34:45Z</published>
    <title>First-order Complete and Computationally Complete Query Languages for
  Spatio-Temporal Databases</title>
    <summary>  We address a fundamental question concerning spatio-temporal database
systems: ``What are exactly spatio-temporal queries?'' We define
spatio-temporal queries to be computable mappings that are also generic,
meaning that the result of a query may only depend to a limited extent on the
actual internal representation of the spatio-temporal data. Genericity is
defined as invariance under groups of geometric transformations that preserve
certain characteristics of spatio-temporal data (e.g., collinearity, distance,
velocity, acceleration, ...). These groups depend on the notions that are
relevant in particular spatio-temporal database applications.
  These transformations also have the distinctive property that they respect
the monotone and unidirectional nature of time.
  We investigate different genericity classes with respect to the constraint
database model for spatio-temporal databases and we identify sound and complete
languages for the first-order and the computable queries in these genericity
classes. We distinguish between genericity determined by time-invariant
transformations, genericity notions concerning physical quantities and
genericity determined by time-dependent transformations.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <author>
      <name>Sofie Haesevoets</name>
    </author>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Cleaned up source code</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503012v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503012v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503037v1</id>
    <updated>2005-03-17T11:01:25Z</updated>
    <published>2005-03-17T11:01:25Z</published>
    <title>Mining Top-k Approximate Frequent Patterns</title>
    <summary>  Frequent pattern (itemset) mining in transactional databases is one of the
most well-studied problems in data mining. One obstacle that limits the
practical usage of frequent pattern mining is the extremely large number of
patterns generated. Such a large size of the output collection makes it
difficult for users to understand and use in practice. Even restricting the
output to the border of the frequent itemset collection does not help much in
alleviating the problem. In this paper we address the issue of overwhelmingly
large output size by introducing and studying the following problem: mining
top-k approximate frequent patterns. The union of the power sets of these k
sets should satisfy the following conditions: (1) including itemsets with
larger support as many as possible and (2) including itemsets with smaller
support as less as possible. An integrated objective function is designed to
combine these two objectives. Consequently, we derive the upper bounds on
objective function and present an approximate branch-and-bound method for
finding the feasible solution. We give empirical evidence showing that our
formulation and approximation methods work well in practice.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503062v2</id>
    <updated>2005-03-24T00:22:53Z</updated>
    <published>2005-03-23T15:36:55Z</published>
    <title>On the Complexity of Nonrecursive XQuery and Functional Query Languages
  on Complex Values</title>
    <summary>  This paper studies the complexity of evaluating functional query languages
for complex values such as monad algebra and the recursion-free fragment of
XQuery.
  We show that monad algebra with equality restricted to atomic values is
complete for the class TA[2^{O(n)}, O(n)] of problems solvable in linear
exponential time with a linear number of alternations. The monotone fragment of
monad algebra with atomic value equality but without negation is complete for
nondeterministic exponential time. For monad algebra with deep equality, we
establish TA[2^{O(n)}, O(n)] lower and exponential-space upper bounds.
  Then we study a fragment of XQuery, Core XQuery, that seems to incorporate
all the features of a query language on complex values that are traditionally
deemed essential. A close connection between monad algebra on lists and Core
XQuery (with ``child'' as the only axis) is exhibited, and it is shown that
these languages are expressively equivalent up to representation issues. We
show that Core XQuery is just as hard as monad algebra w.r.t. combined
complexity, and that it is in TC0 if the query is assumed fixed.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long version of PODS 2005 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503062v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503062v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1, H.2.3, I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505038v1</id>
    <updated>2005-05-16T14:42:01Z</updated>
    <published>2005-05-16T14:42:01Z</published>
    <title>Efficient Management of Short-Lived Data</title>
    <summary>  Motivated by the increasing prominence of loosely-coupled systems, such as
mobile and sensor networks, which are characterised by intermittent
connectivity and volatile data, we study the tagging of data with so-called
expiration times. More specifically, when data are inserted into a database,
they may be tagged with time values indicating when they expire, i.e., when
they are regarded as stale or invalid and thus are no longer considered part of
the database. In a number of applications, expiration times are known and can
be assigned at insertion time. We present data structures and algorithms for
online management of data tagged with expiration times. The algorithms are
based on fully functional, persistent treaps, which are a combination of binary
search trees with respect to a primary attribute and heaps with respect to a
secondary attribute. The primary attribute implements primary keys, and the
secondary attribute stores expiration times in a minimum heap, thus keeping a
priority queue of tuples to expire. A detailed and comprehensive experimental
study demonstrates the well-behavedness and scalability of the approach as well
as its efficiency with respect to a number of competitors.
</summary>
    <author>
      <name>Albrecht Schmidt</name>
    </author>
    <author>
      <name>Christian S. Jensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">switched to TimeCenter latex style</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505071v1</id>
    <updated>2005-05-26T04:41:15Z</updated>
    <published>2005-05-26T04:41:15Z</published>
    <title>Summarization Techniques for Pattern Collections in Data Mining</title>
    <summary>  Discovering patterns from data is an important task in data mining. There
exist techniques to find large collections of many kinds of patterns from data
very efficiently. A collection of patterns can be regarded as a summary of the
data. A major difficulty with patterns is that pattern collections summarizing
the data well are often very large.
  In this dissertation we describe methods for summarizing pattern collections
in order to make them also more understandable. More specifically, we focus on
the following themes: 1) Quality value simplifications. 2) Pattern orderings.
3) Pattern chains and antichains. 4) Change profiles. 5) Inverse pattern
discovery.
</summary>
    <author>
      <name>Taneli Mielikäinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis, Department of Computer Science, University of Helsinki</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; F.2; H.2.8; I.2; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505074v1</id>
    <updated>2005-05-26T09:51:46Z</updated>
    <published>2005-05-26T09:51:46Z</published>
    <title>Instance-Independent View Serializability for Semistructured Databases</title>
    <summary>  Semistructured databases require tailor-made concurrency control mechanisms
since traditional solutions for the relational model have been shown to be
inadequate. Such mechanisms need to take full advantage of the hierarchical
structure of semistructured data, for instance allowing concurrent updates of
subtrees of, or even individual elements in, XML documents. We present an
approach for concurrency control which is document-independent in the sense
that two schedules of semistructured transactions are considered equivalent if
they are equivalent on all possible documents. We prove that it is decidable in
polynomial time whether two given schedules in this framework are equivalent.
This also solves the view serializability for semistructured schedules
polynomially in the size of the schedule and exponentially in the number of
transactions.
</summary>
    <author>
      <name>Stijn Dekeyser</name>
    </author>
    <author>
      <name>Jan Hidders</name>
    </author>
    <author>
      <name>Jan Paredaens</name>
    </author>
    <author>
      <name>Roel Vercammen</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0505074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506026v1</id>
    <updated>2005-06-08T22:03:49Z</updated>
    <published>2005-06-08T22:03:49Z</published>
    <title>Database Reformulation with Integrity Constraints (extended abstract)</title>
    <summary>  In this paper we study the problem of reducing the evaluation costs of
queries on finite databases in presence of integrity constraints, by designing
and materializing views. Given a database schema, a set of queries defined on
the schema, a set of integrity constraints, and a storage limit, to find a
solution to this problem means to find a set of views that satisfies the
storage limit, provides equivalent rewritings of the queries under the
constraints (this requirement is weaker than equivalence in the absence of
constraints), and reduces the total costs of evaluating the queries. This
problem, database reformulation, is important for many applications, including
data warehousing and query optimization. We give complexity results and
algorithms for database reformulation in presence of constraints, for
conjunctive queries, views, and rewritings and for several types of
constraints, including functional and inclusion dependencies. To obtain better
complexity results, we introduce an unchase technique, which reduces the
problem of query equivalence under constraints to equivalence in the absence of
constraints without increasing query size.
</summary>
    <author>
      <name>Rada Chirkova</name>
    </author>
    <author>
      <name>Michael R. Genesereth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the proceedings of the the Logic and Computational Complexity
  Workshop, in conjunction with of the Logic in Computer Science Conference
  (LICS), Chicago, June 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0507065v1</id>
    <updated>2005-07-27T02:14:02Z</updated>
    <published>2005-07-27T02:14:02Z</published>
    <title>A Fast Greedy Algorithm for Outlier Mining</title>
    <summary>  The task of outlier detection is to find small groups of data objects that
are exceptional when compared with rest large amount of data. In [38], the
problem of outlier detection in categorical data is defined as an optimization
problem and a local-search heuristic based algorithm (LSA) is presented.
However, as is the case with most iterative type algorithms, the LSA algorithm
is still very time-consuming on very large datasets. In this paper, we present
a very fast greedy algorithm for mining outliers under the same optimization
model. Experimental results on real datasets and large synthetic datasets show
that: (1) Our algorithm has comparable performance with respect to those
state-of-art outlier detection algorithms on identifying true outliers and (2)
Our algorithm can be an order of magnitude faster than LSA algorithm.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0507065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0507065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508120v1</id>
    <updated>2005-08-26T21:05:44Z</updated>
    <published>2005-08-26T21:05:44Z</published>
    <title>Iterative Algorithm for Finding Frequent Patterns in Transactional
  Databases</title>
    <summary>  A high-performance algorithm for searching for frequent patterns (FPs) in
transactional databases is presented. The search for FPs is carried out by
using an iterative sieve algorithm by computing the set of enclosed cycles. In
each inner cycle of level FPs composed of elements are generated. The assigned
number of enclosed cycles (the parameter of the problem) defines the maximum
length of the desired FPs. The efficiency of the algorithm is produced by (i)
the extremely simple logical searching scheme, (ii) the avoidance of recursive
procedures, and (iii) the usage of only one-dimensional arrays of integers.
</summary>
    <author>
      <name>Gennady P. Berman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Theoretical Division, Los Alamos National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Vyacheslav N. Gorshkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Theoretical Division, Los Alamos National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Edward P. MacKerrow</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Theoretical Division, Los Alamos National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Xidi Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Global Consumer Bank, Citigroup</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 tables and figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0508120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0509088v1</id>
    <updated>2005-09-28T08:38:15Z</updated>
    <published>2005-09-28T08:38:15Z</published>
    <title>Business intelligence systems and user's parameters: an application to a
  documents' database</title>
    <summary>  This article presents earlier results of our research works in the area of
modeling Business Intelligence Systems. The basic idea of this research area is
presented first. We then show the necessity of including certain users'
parameters in Information systems that are used in Business Intelligence
systems in order to integrate a better response from such systems. We
identified two main types of attributes that can be missing from a base and we
showed why they needed to be included. A user model that is based on a
cognitive user evolution is presented. This model when used together with a
good definition of the information needs of the user (decision maker) will
accelerate his decision making process.
</summary>
    <author>
      <name>Babajide Afolabi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Odile Thiery</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Modelling Others for Observation A workshop of IJCAI 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0509088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0510036v1</id>
    <updated>2005-10-14T13:25:34Z</updated>
    <published>2005-10-14T13:25:34Z</published>
    <title>Semantic Optimization Techniques for Preference Queries</title>
    <summary>  Preference queries are relational algebra or SQL queries that contain
occurrences of the winnow operator ("find the most preferred tuples in a given
relation"). Such queries are parameterized by specific preference relations.
Semantic optimization techniques make use of integrity constraints holding in
the database. In the context of semantic optimization of preference queries, we
identify two fundamental properties: containment of preference relations
relative to integrity constraints and satisfaction of order axioms relative to
integrity constraints. We show numerous applications of those notions to
preference query evaluation and optimization. As integrity constraints, we
consider constraint-generating dependencies, a class generalizing functional
dependencies. We demonstrate that the problems of containment and satisfaction
of order axioms can be captured as specific instances of constraint-generating
dependency entailment. This makes it possible to formulate necessary and
sufficient conditions for the applicability of our techniques as constraint
validity problems. We characterize the computational complexity of such
problems.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0510036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0510036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511106v1</id>
    <updated>2005-11-30T16:12:38Z</updated>
    <published>2005-11-30T16:12:38Z</published>
    <title>Benefits of InterSite Pre-Processing and Clustering Methods in
  E-Commerce Domain</title>
    <summary>  This paper presents our preprocessing and clustering analysis on the
clickstream dataset proposed for the ECMLPKDD 2005 Discovery Challenge. The
main contributions of this article are double. First, after presenting the
clickstream dataset, we show how we build a rich data warehouse based an
advanced preprocesing. We take into account the intersite aspects in the given
ecommerce domain, which offers an interesting data structuration. A preliminary
statistical analysis based on time period clickstreams is given, emphasing the
importance of intersite user visits in such a context. Secondly, we describe
our crossed-clustering method which is applied on data generated from our data
warehouse. Our preliminary results are interesting and promising illustrating
the benefits of our WUM methods, even if more investigations are needed on the
same dataset.
</summary>
    <author>
      <name>Sergiu Theodor Chelcea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Alzennyr Da Silva</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Yves Lechevallier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Doru Tanasa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Brigitte Trousse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Proceedings of the ECML/PKDD2005 Discovery Challenge, A
  Collaborative Effort in Knowledge Discovery from Databases</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0511106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601114v2</id>
    <updated>2009-02-11T16:07:23Z</updated>
    <published>2006-01-27T16:21:25Z</published>
    <title>Efficient Query Answering over Conceptual Schemas of Relational
  Databases : Technical Report</title>
    <summary>  We develop a query answering system, where at the core of the work there is
an idea of query answering by rewriting. For this purpose we extend the DL
DL-Lite with the ability to support n-ary relations, obtaining the DL DLR-Lite,
which is still polynomial in the size of the data. We devise a flexible way of
mapping the conceptual level to the relational level, which provides the users
an SQL-like query language over the conceptual schema. The rewriting technique
adds value to conventional query answering techniques, allowing to formulate
simpler queries, with the ability to infer additional information that was not
stated explicitly in the user query. The formalization of the conceptual schema
and the developed reasoning technique allow checking for consistency between
the database and the conceptual schema, thus improving the trustiness of the
information system.
</summary>
    <author>
      <name>Mantas Simkus</name>
    </author>
    <author>
      <name>Evaldas Taroza</name>
    </author>
    <author>
      <name>Lina Lubyte</name>
    </author>
    <author>
      <name>Daniel Trivellato</name>
    </author>
    <author>
      <name>Zivile Norkunaite</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0601114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602004v1</id>
    <updated>2006-02-02T23:28:24Z</updated>
    <published>2006-02-02T23:28:24Z</published>
    <title>Conjunctive Queries over Trees</title>
    <summary>  We study the complexity and expressive power of conjunctive queries over
unranked labeled trees represented using a variety of structure relations such
as ``child'', ``descendant'', and ``following'' as well as unary relations for
node labels. We establish a framework for characterizing structures
representing trees for which conjunctive queries can be evaluated efficiently.
Then we completely chart the tractability frontier of the problem and establish
a dichotomy theorem for our axis relations, i.e., we find all subset-maximal
sets of axes for which query evaluation is in polynomial time and show that for
all other cases, query evaluation is NP-complete. All polynomial-time results
are obtained immediately using the proof techniques from our framework.
Finally, we study the expressiveness of conjunctive queries over trees and show
that for each conjunctive query, there is an equivalent acyclic positive query
(i.e., a set of acyclic conjunctive queries), but that in general this query is
not of polynomial size.
</summary>
    <author>
      <name>Georg Gottlob</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Klaus U. Schulz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 12 figures, 2 tables, long version of PODS 2004 papers. To
  appear in Journal of the ACM 53(2), March 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1.3; F.2.2; H.2.3; H.2.4; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602006v1</id>
    <updated>2006-02-03T19:22:28Z</updated>
    <published>2006-02-03T19:22:28Z</published>
    <title>A Visual Query Language for Complex-Value Databases</title>
    <summary>  In this paper, a visual language, VCP, for queries on complex-value databases
is proposed. The main strength of the new language is that it is purely visual:
(i) It has no notion of variable, quantification, partiality, join, pattern
matching, regular expression, recursion, or any other construct proper to
logical, functional, or other database query languages and (ii) has a very
natural, strong, and intuitive design metaphor. The main operation is that of
copying and pasting in a schema tree.
  We show that despite its simplicity, VCP precisely captures complex-value
algebra without powerset, or equivalently, monad algebra with union and
difference. Thus, its expressive power is precisely that of the language that
is usually considered to play the role of relational algebra for complex-value
databases.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604002v1</id>
    <updated>2006-04-02T03:07:09Z</updated>
    <published>2006-04-02T03:07:09Z</published>
    <title>Complexity of Consistent Query Answering in Databases under
  Cardinality-Based and Incremental Repair Semantics</title>
    <summary>  Consistent Query Answering (CQA) is the problem of computing from a database
the answers to a query that are consistent with respect to certain integrity
constraints that the database, as a whole, may fail to satisfy. Consistent
answers have been characterized as those that are invariant under certain
minimal forms of restoration of the database consistency. We investigate
algorithmic and complexity theoretic issues of CQA under database repairs that
minimally depart -wrt the cardinality of the symmetric difference- from the
original database. We obtain first tight complexity bounds.
  We also address the problem of incremental complexity of CQA, that naturally
occurs when an originally consistent database becomes inconsistent after the
execution of a sequence of update operations. Tight bounds on incremental
complexity are provided for various semantics under denial constraints. Fixed
parameter tractability is also investigated in this dynamic context, where the
size of the update sequence becomes the relevant parameter.
</summary>
    <author>
      <name>Andrei Lopatenko</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604076v1</id>
    <updated>2006-04-19T22:33:33Z</updated>
    <published>2006-04-19T22:33:33Z</published>
    <title>Semantically Correct Query Answers in the Presence of Null Values</title>
    <summary>  For several reasons a database may not satisfy a given set of integrity
constraints(ICs), but most likely most of the information in it is still
consistent with those ICs; and could be retrieved when queries are answered.
Consistent answers to queries wrt a set of ICs have been characterized as
answers that can be obtained from every possible minimally repaired consistent
version of the original database. In this paper we consider databases that
contain null values and are also repaired, if necessary, using null values. For
this purpose, we propose first a precise semantics for IC satisfaction in a
database with null values that is compatible with the way null values are
treated in commercial database management systems. Next, a precise notion of
repair is introduced that privileges the introduction of null values when
repairing foreign key constraints, in such a way that these new values do not
create an infinite cycle of new inconsistencies. Finally, we analyze how to
specify this kind of repairs of a database that contains null values using
disjunctive logic programs with stable model semantics.
</summary>
    <author>
      <name>Loreto Bravo</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages. To appear in Proc. of International Workshop on
  Inconsistency and Incompleteness in Databases (IIDB 2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604112v1</id>
    <updated>2006-04-28T03:04:00Z</updated>
    <published>2006-04-28T03:04:00Z</published>
    <title>Designing a Multi-petabyte Database for LSST</title>
    <summary>  The 3.2 giga-pixel LSST camera will produce approximately half a petabyte of
archive images every month. These data need to be reduced in under a minute to
produce real-time transient alerts, and then added to the cumulative catalog
for further analysis. The catalog is expected to grow about three hundred
terabytes per year. The data volume, the real-time transient alerting
requirements of the LSST, and its spatio-temporal aspects require innovative
techniques to build an efficient data access system at reasonable cost. As
currently envisioned, the system will rely on a database for catalogs and
metadata. Several database systems are being evaluated to understand how they
perform at these data rates, data volumes, and access patterns. This paper
describes the LSST requirements, the challenges they impose, the data access
philosophy, results to date from evaluating available database technologies
against LSST requirements, and the proposed database architecture to meet the
data challenges.
</summary>
    <author>
      <name>Jacek Becla</name>
    </author>
    <author>
      <name>Andrew Hanushevsky</name>
    </author>
    <author>
      <name>Sergei Nikolaev</name>
    </author>
    <author>
      <name>Ghaleb Abdulla</name>
    </author>
    <author>
      <name>Alex Szalay</name>
    </author>
    <author>
      <name>Maria Nieto-Santisteban</name>
    </author>
    <author>
      <name>Ani Thakar</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.671721</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.671721" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. to appear in SPIE</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606065v4</id>
    <updated>2006-09-20T09:20:04Z</updated>
    <published>2006-06-14T09:49:17Z</published>
    <title>On the complexity of XPath containment in the presence of disjunction,
  DTDs, and variables</title>
    <summary>  XPath is a simple language for navigating an XML-tree and returning a set of
answer nodes. The focus in this paper is on the complexity of the containment
problem for various fragments of XPath. We restrict attention to the most
common XPath expressions which navigate along the child and/or descendant axis.
In addition to basic expressions using only node tests and simple predicates,
we also consider disjunction and variables (ranging over nodes). Further, we
investigate the containment problem relative to a given DTD. With respect to
variables we study two semantics, (1) the original semantics of XPath, where
the values of variables are given by an outer context, and (2) an existential
semantics introduced by Deutsch and Tannen, in which the values of variables
are existentially quantified. In this framework, we establish an exact
classification of the complexity of the containment problem for many XPath
fragments.
</summary>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Thomas Schwentick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2168/LMCS-2(3:1)2006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2168/LMCS-2(3:1)2006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, will appear in Logical Methods in Computer Science
  (http://www.lmcs-online.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Logical Methods in Computer Science, Volume 2, Issue 3 (July 26,
  2006) lmcs:2243</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0606065v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606065v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; I.7.2; F.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606075v2</id>
    <updated>2008-02-13T23:36:24Z</updated>
    <published>2006-06-16T14:24:34Z</published>
    <title>10^(10^6) Worlds and Beyond: Efficient Representation and Processing of
  Incomplete Information</title>
    <summary>  Current systems and formalisms for representing incomplete information
generally suffer from at least one of two weaknesses. Either they are not
strong enough for representing results of simple queries, or the handling and
processing of the data, e.g. for query evaluation, is intractable.
  In this paper, we present a decomposition-based approach to addressing this
problem. We introduce world-set decompositions (WSDs), a space-efficient
formalism for representing any finite set of possible worlds over relational
databases. WSDs are therefore a strong representation system for any relational
query language. We study the problem of efficiently evaluating relational
algebra queries on sets of worlds represented by WSDs. We also evaluate our
technique experimentally in a large census data scenario and show that it is
both scalable and efficient.
</summary>
    <author>
      <name>Lyublena Antova</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606075v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606075v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609144v1</id>
    <updated>2006-09-26T15:21:40Z</updated>
    <published>2006-09-26T15:21:40Z</published>
    <title>The Management and Integration of Biomedical Knowledge: Application in
  the Health-e-Child Project (Position Paper)</title>
    <summary>  The Health-e-Child project aims to develop an integrated healthcare platform
for European paediatrics. In order to achieve a comprehensive view of childrens
health, a complex integration of biomedical data, information, and knowledge is
necessary. Ontologies will be used to formally define this domain knowledge and
will form the basis for the medical knowledge management system. This paper
introduces an innovative methodology for the vertical integration of biomedical
knowledge. This approach will be largely clinician-centered and will enable the
definition of ontology fragments, connections between them (semantic bridges)
and enriched ontology fragments (views). The strategy for the specification and
capture of fragments, bridges and views is outlined with preliminary examples
demonstrated in the collection of biomedical information from hospital
databases, biomedical ontologies, and biomedical public databases.
</summary>
    <author>
      <name>E. Jimenez-Ruiz</name>
    </author>
    <author>
      <name>R. Berlanga</name>
    </author>
    <author>
      <name>I. Sanz</name>
    </author>
    <author>
      <name>R. McClatchey</name>
    </author>
    <author>
      <name>R. Danger</name>
    </author>
    <author>
      <name>D. Manset</name>
    </author>
    <author>
      <name>J. Paraire</name>
    </author>
    <author>
      <name>A. Rios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; 2 figures. Proceedings of the 1st International Workshop on
  Ontology content and evaluation in Enterprise</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610010v4</id>
    <updated>2014-02-04T16:15:57Z</updated>
    <published>2006-10-03T18:04:22Z</published>
    <title>One-Pass, One-Hash n-Gram Statistics Estimation</title>
    <summary>  In multimedia, text or bioinformatics databases, applications query sequences
of n consecutive symbols called n-grams. Estimating the number of distinct
n-grams is a view-size estimation problem. While view sizes can be estimated by
sampling under statistical assumptions, we desire an unassuming algorithm with
universally valid accuracy bounds. Most related work has focused on repeatedly
hashing the data, which is prohibitive for large data sources. We prove that a
one-pass one-hash algorithm is sufficient for accurate estimates if the hashing
is sufficiently independent. To reduce costs further, we investigate recursive
random hashing algorithms and show that they are sufficiently independent in
practice. We compare our running times with exact counts using suffix arrays
and show that, while we use hardly any storage, we are an order of magnitude
faster. The approach further is extended to a one-pass/one-hash computation of
n-gram entropy and iceberg counts. The experiments use a large collection of
English text from the Gutenberg Project as well as synthetic data.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed a typo</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610010v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610010v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611035v1</id>
    <updated>2006-11-08T15:03:49Z</updated>
    <published>2006-11-08T15:03:49Z</published>
    <title>The Role of Quasi-identifiers in k-Anonymity Revisited</title>
    <summary>  The concept of k-anonymity, used in the recent literature to formally
evaluate the privacy preservation of published tables, was introduced based on
the notion of quasi-identifiers (or QI for short). The process of obtaining
k-anonymity for a given private table is first to recognize the QIs in the
table, and then to anonymize the QI values, the latter being called
k-anonymization. While k-anonymization is usually rigorously validated by the
authors, the definition of QI remains mostly informal, and different authors
seem to have different interpretations of the concept of QI. The purpose of
this paper is to provide a formal underpinning of QI and examine the
correctness and incorrectness of various interpretations of QI in our formal
framework. We observe that in cases where the concept has been used correctly,
its application has been conservative; this note provides a formal
understanding of the conservative nature in such cases.
</summary>
    <author>
      <name>Claudio Bettini</name>
    </author>
    <author>
      <name>X. Sean Wang</name>
    </author>
    <author>
      <name>Sushil Jajodia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages. Submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611094v1</id>
    <updated>2006-11-20T10:12:46Z</updated>
    <published>2006-11-20T10:12:46Z</published>
    <title>Reducing Order Enforcement Cost in Complex Query Plans</title>
    <summary>  Algorithms that exploit sort orders are widely used to implement joins,
grouping, duplicate elimination and other set operations. Query optimizers
traditionally deal with sort orders by using the notion of interesting orders.
The number of interesting orders is unfortunately factorial in the number of
participating attributes. Optimizer implementations use heuristics to prune the
number of interesting orders, but the quality of the heuristics is unclear.
Increasingly complex decision support queries and increasing use of covering
indices, which provide multiple alternative sort orders for relations, motivate
us to better address the problem of optimization with interesting orders.
  We show that even a simplified version of optimization with sort orders is
NP-hard and provide principled heuristics for choosing interesting orders. We
have implemented the proposed techniques in a Volcano-style cost-based
optimizer, and our performance study shows significant improvements in
estimated cost. We also executed our plans on a widely used commercial database
system, and on PostgreSQL, and found that actual execution times for our plans
were significantly better than for plans generated by those systems in several
cases.
</summary>
    <author>
      <name>Ravindra Guravannavar</name>
    </author>
    <author>
      <name>S Sudarshan</name>
    </author>
    <author>
      <name>Ajit A Diwan</name>
    </author>
    <author>
      <name>Ch. Sobhan Babu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612086v2</id>
    <updated>2007-10-08T14:21:04Z</updated>
    <published>2006-12-18T16:22:12Z</published>
    <title>An asynchronous, decentralised commitment protocol for semantic
  optimistic replication</title>
    <summary>  We study large-scale distributed cooperative systems that use optimistic
replication. We represent a system as a graph of actions (operations) connected
by edges that reify semantic constraints between actions. Constraint types
include conflict, execution order, dependence, and atomicity. The local state
is some schedule that conforms to the constraints; because of conflicts, client
state is only tentative. For consistency, site schedules should converge; we
designed a decentralised, asynchronous commitment protocol. Each client makes a
proposal, reflecting its tentative and{\slash}or preferred schedules. Our
protocol distributes the proposals, which it decomposes into
semantically-meaningful units called candidates, and runs an election between
comparable candidates. A candidate wins when it receives a majority or a
plurality. The protocol is fully asynchronous: each site executes its tentative
schedule independently, and determines locally when a candidate has won an
election. The committed schedule is as close as possible to the preferences
expressed by clients.
</summary>
    <author>
      <name>Pierre Sutra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Shapiro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>João Pedro Barreto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INESC-ID</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0612086v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612086v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612103v1</id>
    <updated>2006-12-21T00:21:45Z</updated>
    <published>2006-12-21T00:21:45Z</published>
    <title>The Boundary Between Privacy and Utility in Data Anonymization</title>
    <summary>  We consider the privacy problem in data publishing: given a relation I
containing sensitive information 'anonymize' it to obtain a view V such that,
on one hand attackers cannot learn any sensitive information from V, and on the
other hand legitimate users can use V to compute useful statistics on I. These
are conflicting goals. We use a definition of privacy that is derived from
existing ones in the literature, which relates the a priori probability of a
given tuple t, Pr(t), with the a posteriori probability, Pr(t | V), and propose
a novel and quite practical definition for utility. Our main result is the
following. Denoting n the size of I and m the size of the domain from which I
was drawn (i.e. n &lt; m) then: when the a priori probability is Pr(t) =
Omega(n/sqrt(m)) for some t, there exists no useful anonymization algorithm,
while when Pr(t) = O(n/m) for all tuples t, then we give a concrete
anonymization algorithm that is both private and useful. Our algorithm is quite
different from the k-anonymization algorithm studied intensively in the
literature, and is based on random deletions and insertions to I.
</summary>
    <author>
      <name>Vibhor Rastogi</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <author>
      <name>Sungho Hong</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612110v1</id>
    <updated>2006-12-21T19:36:38Z</updated>
    <published>2006-12-21T19:36:38Z</published>
    <title>Architecture for Modular Data Centers</title>
    <summary>  Several factors are driving high-scale deployments of large data centers
built upon commodity components. These commodity clusters are far cheaper than
mainframe systems of the past but they bring serious heat and power density
issues. Also the high failure rate of the individual components drives
significant administrative costs. This proposal outlines an architecture for
data center design based upon 20'x8'x8' modules that substantially changes how
these systems are acquired, administered, and then later recycled.
</summary>
    <author>
      <name>James Hamilton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612112v1</id>
    <updated>2006-12-21T21:51:55Z</updated>
    <published>2006-12-21T21:51:55Z</published>
    <title>Managing Query Compilation Memory Consumption to Improve DBMS Throughput</title>
    <summary>  While there are known performance trade-offs between database page buffer
pool and query execution memory allocation policies, little has been written on
the impact of query compilation memory use on overall throughput of the
database management system (DBMS). We present a new aspect of the query
optimization problem and offer a solution implemented in Microsoft SQL Server
2005. The solution provides stable throughput for a range of workloads even
when memory requests outstrip the ability of the hardware to service those
requests.
</summary>
    <author>
      <name>Boris Baryshnikov</name>
    </author>
    <author>
      <name>Cipri Clinciu</name>
    </author>
    <author>
      <name>Conor Cunningham</name>
    </author>
    <author>
      <name>Leo Giakoumakis</name>
    </author>
    <author>
      <name>Slava Oks</name>
    </author>
    <author>
      <name>Stefano Stefani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612113v1</id>
    <updated>2006-12-21T22:08:17Z</updated>
    <published>2006-12-21T22:08:17Z</published>
    <title>Isolation Support for Service-based Applications: A Position Paper</title>
    <summary>  In this paper, we propose an approach to providing the benefits of isolation
in service-oriented applications where it is not feasible to hold traditional
locks for ACID transactions. Our technique, called "Promises", provides an
uniform view for clients which covers a wide range of implementation techniques
on the service side, all allowing the client to check a condition and then
later rely on that condition still holding.
</summary>
    <author>
      <name>Paul Greenfield</name>
    </author>
    <author>
      <name>Alan Fekete</name>
    </author>
    <author>
      <name>Julian Jang</name>
    </author>
    <author>
      <name>Dean Kuo</name>
    </author>
    <author>
      <name>Surya Nepal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612115v1</id>
    <updated>2006-12-21T22:40:33Z</updated>
    <published>2006-12-21T22:40:33Z</published>
    <title>Consistent Streaming Through Time: A Vision for Event Stream Processing</title>
    <summary>  Event processing will play an increasingly important role in constructing
enterprise applications that can immediately react to business critical events.
Various technologies have been proposed in recent years, such as event
processing, data streams and asynchronous messaging (e.g. pub/sub). We believe
these technologies share a common processing model and differ only in target
workload, including query language features and consistency requirements. We
argue that integrating these technologies is the next step in a natural
progression. In this paper, we present an overview and discuss the foundations
of CEDR, an event streaming system that embraces a temporal stream model to
unify and further enrich query language features, handle imperfections in event
delivery and define correctness guarantees. We describe specific contributions
made so far and outline next steps in developing the CEDR system.
</summary>
    <author>
      <name>Roger S. Barga</name>
    </author>
    <author>
      <name>Jonathan Goldstein</name>
    </author>
    <author>
      <name>Mohamed Ali</name>
    </author>
    <author>
      <name>Mingsheng Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612123v1</id>
    <updated>2006-12-22T13:29:27Z</updated>
    <published>2006-12-22T13:29:27Z</published>
    <title>Electronic Laboratory Notebook Assisting Reflectance Spectrometry in
  Legal Medicine</title>
    <summary>  Reflectance spectrometry is a fast and reliable method for the
characterisation of human skin if the spectra are analysed with respect to a
physical model describing the optical properties of human skin. For a field
study performed at the Institute of Legal Medicine and the Freiburg Materials
Research Center of the University of Freiburg an electronic laboratory notebook
has been developed, which assists in the recording, management, and analysis of
reflectance spectra. The core of the electronic laboratory notebook is a MySQL
database. It is filled with primary data via a web interface programmed in
Java, which also enables the user to browse the database and access the results
of data analysis. These are carried out by Matlab, Tcl and
  Python scripts, which retrieve the primary data from the electronic
laboratory notebook, perform the analysis, and store the results in the
database for further usage.
</summary>
    <author>
      <name>Lioudmila Belenkaia</name>
    </author>
    <author>
      <name>Michael Bohnert</name>
    </author>
    <author>
      <name>Andreas W. Liehr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/2211068212443960</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/2211068212443960" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612128v1</id>
    <updated>2006-12-22T20:38:58Z</updated>
    <published>2006-12-22T20:38:58Z</published>
    <title>SASE: Complex Event Processing over Streams</title>
    <summary>  RFID technology is gaining adoption on an increasing scale for tracking and
monitoring purposes. Wide deployments of RFID devices will soon generate an
unprecedented volume of data. Emerging applications require the RFID data to be
filtered and correlated for complex pattern detection and transformed to events
that provide meaningful, actionable information to end applications. In this
work, we design and develop SASE, a com-plex event processing system that
performs such data-information transformation over real-time streams. We design
a complex event language for specifying application logic for such
transformation, devise new query processing techniques to effi-ciently
implement the language, and develop a comprehensive system that collects,
cleans, and processes RFID data for deliv-ery of relevant, timely information
as well as storing necessary data for future querying. We demonstrate an
initial prototype of SASE through a real-world retail management scenario.
</summary>
    <author>
      <name>Daniel Gyllstrom</name>
    </author>
    <author>
      <name>Eugene Wu</name>
    </author>
    <author>
      <name>Hee-Jin Chae</name>
    </author>
    <author>
      <name>Yanlei Diao</name>
    </author>
    <author>
      <name>Patrick Stahlberg</name>
    </author>
    <author>
      <name>Gordon Anderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612137v1</id>
    <updated>2006-12-27T22:21:57Z</updated>
    <published>2006-12-27T22:21:57Z</published>
    <title>Turning Cluster Management into Data Management: A System Overview</title>
    <summary>  This paper introduces the CondorJ2 cluster management system. Traditionally,
cluster management systems such as Condor employ a process-oriented approach
with little or no use of modern database system technology. In contrast,
CondorJ2 employs a data-centric, 3-tier web-application architecture for all
system functions (e.g., job submission, monitoring and scheduling; node
configuration, monitoring and management, etc.) except for job execution.
Employing a data-oriented approach allows the core challenge (i.e., managing
and coordinating a large set of distributed computing resources) to be
transformed from a relatively low-level systems problem into a more abstract,
higher-level data management problem. Preliminary results suggest that
CondorJ2's use of standard 3-tier software represents a significant step
forward to the design and implementation of large clusters (1,000 to 10,000
nodes).
</summary>
    <author>
      <name>Eric Robinson</name>
    </author>
    <author>
      <name>David DeWitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701114v2</id>
    <updated>2007-03-15T22:43:08Z</updated>
    <published>2007-01-17T20:08:53Z</published>
    <title>The problem determination of Functional Dependencies between attributes
  Relation Scheme in the Relational Data Model. El problema de determinar
  Dependencias Funcionales entre atributos en los esquemas en el Modelo
  Relacional</title>
    <summary>  An alternative definition of the concept is given of functional dependence
among the attributes of the relational schema in the Relational Model, this
definition is obtained in terms of the set theory. For that which a theorem is
demonstrated that establishes equivalence and on the basis theorem an algorithm
is built for the search of the functional dependences among the attributes. The
algorithm is illustrated by a concrete example
</summary>
    <author>
      <name>Ignacio Vega-Paez</name>
    </author>
    <author>
      <name>Georgina G. Pulido</name>
    </author>
    <author>
      <name>Jose Angel Ortega</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Multidisciplinary Sciences and
  Engineering, Vol. 2, No. 5, 2011, 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701156v1</id>
    <updated>2007-01-25T22:42:28Z</updated>
    <published>2007-01-25T22:42:28Z</published>
    <title>Data Management: Past, Present, and Future</title>
    <summary>  Soon most information will be available at your fingertips, anytime,
anywhere. Rapid advances in storage, communications, and processing allow us
move all information into Cyberspace. Software to define, search, and visualize
online information is also a key to creating and accessing online information.
This article traces the evolution of data management systems and outlines
current trends. Data management systems began by automating traditional tasks:
recording transactions in business, science, and commerce. This data consisted
primarily of numbers and character strings. Today these systems provide the
infrastructure for much of our society, allowing fast, reliable, secure, and
automatic access to data distributed throughout the world. Increasingly these
systems automatically design and manage access to the data. The next steps are
to automate access to richer forms of data: images, sound, video, maps, and
other media. A second major challenge is automatically summarizing and
abstracting data in anticipation of user requests. These multi-media databases
and tools to access them will be a cornerstone of our move to Cyberspace.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computer 29(10): 38-46 (1996)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701158v1</id>
    <updated>2007-01-25T22:56:45Z</updated>
    <published>2007-01-25T22:56:45Z</published>
    <title>Queues Are Databases</title>
    <summary>  Message-oriented-middleware (MOM) has become an small industry. MOM offers
queued transaction processing as an advance over pure client-server transaction
processing. This note makes four points: Queued transaction processing is less
general than direct transaction processing. Queued systems are built on top of
direct systems. You cannot build a direct system atop a queued system. It is
difficult to build direct, conversational, or distributed transactions atop a
queued system. Queues are interesting databases with interesting concurrency
control. It is best to build these mechanisms into a standard database system
so other applications can use these interesting features. Queue systems need
DBMS functionality. Queues need security, configuration, performance
monitoring, recovery, and reorganization utilities. Database systems already
have these features. A full-function MOM system duplicates these database
features. Queue managers are simple TP-monitors managing server pools driven by
queues. Database systems are encompassing many server pool features as they
evolve to TP-lite systems.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701159v1</id>
    <updated>2007-01-25T23:02:32Z</updated>
    <published>2007-01-25T23:02:32Z</published>
    <title>Supporting Finite Element Analysis with a Relational Database Backend,
  Part I: There is Life beyond Files</title>
    <summary>  In this paper, we show how to use a Relational Database Management System in
support of Finite Element Analysis. We believe it is a new way of thinking
about data management in well-understood applications to prepare them for two
major challenges, - size and integration (globalization). Neither extreme size
nor integration (with other applications over the Web) was a design concern 30
years ago when the paradigm for FEA implementation first was formed. On the
other hand, database technology has come a long way since its inception and it
is past time to highlight its usefulness to the field of scientific computing
and computer based engineering. This series aims to widen the list of
applications for database designers and for FEA users and application
developers to reap some of the benefits of database development.
</summary>
    <author>
      <name>Gerd Heber</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701164v1</id>
    <updated>2007-01-26T00:04:12Z</updated>
    <published>2007-01-26T00:04:12Z</published>
    <title>Indexing the Sphere with the Hierarchical Triangular Mesh</title>
    <summary>  We describe a method to subdivide the surface of a sphere into spherical
triangles of similar, but not identical, shapes and sizes. The Hierarchical
Triangular Mesh (HTM) is a quad-tree that is particularly good at supporting
searches at different resolutions, from arc seconds to hemispheres. The
subdivision scheme is universal, providing the basis for addressing and for
fast lookups. The HTM provides the basis for an efficient geospatial indexing
scheme in relational databases where the data have an inherent location on
either the celestial sphere or the Earth. The HTM index is superior to
cartographical methods using coordinates with singularities at the poles. We
also describe a way to specify surface regions that efficiently represent
spherical query areas. This article presents the algorithms used to identify
the HTM triangles covering such regions.
</summary>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>George Fekete</name>
    </author>
    <author>
      <name>Peter Z. Kunszt</name>
    </author>
    <author>
      <name>Peter Kukol</name>
    </author>
    <author>
      <name>Ani Thakar</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701166v1</id>
    <updated>2007-01-26T00:29:02Z</updated>
    <published>2007-01-26T00:29:02Z</published>
    <title>Empirical Measurements of Disk Failure Rates and Error Rates</title>
    <summary>  The SATA advertised bit error rate of one error in 10 terabytes is
frightening. We moved 2 PB through low-cost hardware and saw five disk read
error events, several controller failures, and many system reboots caused by
security patches. We conclude that SATA uncorrectable read errors are not yet a
dominant system-fault source - they happen, but are rare compared to other
problems. We also conclude that UER (uncorrectable error rate) is not the
relevant metric for our needs. When an uncorrectable read error happens, there
are typically several damaged storage blocks (and many uncorrectable read
errors.) Also, some uncorrectable read errors may be masked by the operating
system. The more meaningful metric for data architects is Mean Time To Data
Loss (MTTDL.)
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Catharine van Ingen</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701167v1</id>
    <updated>2007-01-26T00:33:26Z</updated>
    <published>2007-01-26T00:33:26Z</published>
    <title>Large-Scale Query and XMatch, Entering the Parallel Zone</title>
    <summary>  Current and future astronomical surveys are producing catalogs with millions
and billions of objects. On-line access to such big datasets for data mining
and cross-correlation is usually as highly desired as unfeasible. Providing
these capabilities is becoming critical for the Virtual Observatory framework.
In this paper we present various performance tests that show how using
Relational Database Management Systems (RDBMS) and a Zoning algorithm to
partition and parallelize the computation, we can facilitate large-scale query
and cross-match.
</summary>
    <author>
      <name>Maria A. Nieto-Santisteban</name>
    </author>
    <author>
      <name>Aniruddha R. Thakar</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Astronomical Data Analysis Software and Systems XV in San Lorenzo de
  El Escorial, Madrid, Spain, October 2005, to appear in the ASP Conference
  Series</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701168v1</id>
    <updated>2007-01-26T00:54:04Z</updated>
    <published>2007-01-26T00:54:04Z</published>
    <title>To BLOB or Not To BLOB: Large Object Storage in a Database or a
  Filesystem?</title>
    <summary>  Application designers often face the question of whether to store large
objects in a filesystem or in a database. Often this decision is made for
application design simplicity. Sometimes, performance measurements are also
used. This paper looks at the question of fragmentation - one of the
operational issues that can affect the performance and/or manageability of the
system as deployed long term. As expected from the common wisdom, objects
smaller than 256KB are best stored in a database while objects larger than 1M
are best stored in the filesystem. Between 256KB and 1MB, the read:write ratio
and rate of object overwrite or replacement are important factors. We used the
notion of "storage age" or number of object overwrites as way of normalizing
wall clock time. Storage age allows our results or similar such results to be
applied across a number of read:write ratios and object replacement rates.
</summary>
    <author>
      <name>Russell Sears</name>
    </author>
    <author>
      <name>Catharine Van Ingen</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701173v1</id>
    <updated>2007-01-26T05:22:15Z</updated>
    <published>2007-01-26T05:22:15Z</published>
    <title>SkyServer Traffic Report - The First Five Years</title>
    <summary>  The SkyServer is an Internet portal to the Sloan Digital Sky Survey Catalog
Archive Server. From 2001 to 2006, there were a million visitors in 3 million
sessions generating 170 million Web hits, 16 million ad-hoc SQL queries, and 62
million page views. The site currently averages 35 thousand visitors and 400
thousand sessions per month. The Web and SQL logs are public. We analyzed
traffic and sessions by duration, usage pattern, data product, and client type
(mortal or bot) over time. The analysis shows (1) the site's popularity, (2)
the educational website that delivered nearly fifty thousand hours of
interactive instruction, (3) the relative use of interactive, programmatic, and
batch-local access, (4) the success of offering ad-hoc SQL, personal database,
and batch job access to scientists as part of the data publication, (5) the
continuing interest in "old" datasets, (6) the usage of SQL constructs, and (7)
a novel approach of using the corpus of correct SQL queries to suggest similar
but correct statements when a user presents an incorrect SQL statement.
</summary>
    <author>
      <name>Vik Singh</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Ani Thakar</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Jordan Raddick</name>
    </author>
    <author>
      <name>Bill Boroski</name>
    </author>
    <author>
      <name>Svetlana Lebedeva</name>
    </author>
    <author>
      <name>Brian Yanny</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702143v1</id>
    <updated>2007-02-24T03:11:09Z</updated>
    <published>2007-02-24T03:11:09Z</published>
    <title>Attribute Value Reordering For Efficient Hybrid OLAP</title>
    <summary>  The normalization of a data cube is the ordering of the attribute values. For
large multidimensional arrays where dense and sparse chunks are stored
differently, proper normalization can lead to improved storage efficiency. We
show that it is NP-hard to compute an optimal normalization even for 1x3
chunks, although we find an exact algorithm for 1x2 chunks. When dimensions are
nearly statistically independent, we show that dimension-wise attribute
frequency sorting is an optimal normalization and takes time O(d n log(n)) for
data cubes of size n^d. When dimensions are not independent, we propose and
evaluate several heuristics. The hybrid OLAP (HOLAP) storage mechanism is
already 19%-30% more efficient than ROLAP, but normalization can improve it
further by 9%-13% for a total gain of 29%-44% over ROLAP.
</summary>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2005.09.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2005.09.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Owen Kaser, Daniel Lemire, Attribute Value Reordering For
  Efficient Hybrid OLAP, Information Sciences, Volume 176, Issue 16, 2006,
  Pages 2304-2336</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702144v2</id>
    <updated>2008-09-15T19:57:44Z</updated>
    <published>2007-02-24T03:16:27Z</published>
    <title>Slope One Predictors for Online Rating-Based Collaborative Filtering</title>
    <summary>  Rating-based collaborative filtering is the process of predicting how a user
would rate a given item from other user ratings. We propose three related slope
one schemes with predictors of the form f(x) = x + b, which precompute the
average difference between the ratings of one item and another for users who
rated both. Slope one algorithms are easy to implement, efficient to query,
reasonably accurate, and they support both online queries and dynamic updates,
which makes them good candidates for real-world systems. The basic slope one
scheme is suggested as a new reference scheme for collaborative filtering. By
factoring in items that a user liked separately from items that a user
disliked, we achieve results competitive with slower memory-based schemes over
the standard benchmark EachMovie and Movielens data sets while better
fulfilling the desiderata of CF applications.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Anna Maclachlan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In SIAM Data Mining (SDM'05), Newport Beach, California, April 21-23,
  2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702144v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702144v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703058v3</id>
    <updated>2008-12-08T19:26:17Z</updated>
    <published>2007-03-13T17:46:11Z</published>
    <title>A Comparison of Five Probabilistic View-Size Estimation Techniques in
  OLAP</title>
    <summary>  A data warehouse cannot materialize all possible views, hence we must
estimate quickly, accurately, and reliably the size of views to determine the
best candidates for materialization. Many available techniques for view-size
estimation make particular statistical assumptions and their error can be
large. Comparatively, unassuming probabilistic techniques are slower, but they
estimate accurately and reliability very large view sizes using little memory.
We compare five unassuming hashing-based view-size estimation techniques
including Stochastic Probabilistic Counting and LogLog Probabilistic Counting.
Our experiments show that only Generalized Counting, Gibbons-Tirthapura, and
Adaptive Counting provide universally tight estimates irrespective of the size
of the view; of those, only Adaptive Counting remains constantly fast as we
increase the memory budget.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0703058v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703058v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703089v1</id>
    <updated>2007-03-16T00:39:57Z</updated>
    <published>2007-03-16T00:39:57Z</published>
    <title>Space Program Language (SPL/SQL) for the Relational Approach of the
  Spatial Databases</title>
    <summary>  In this project we are presenting a grammar which unify the design and
development of spatial databases. In order to make it, we combine nominal and
spatial information, the former is represented by the relational model and
latter by a modification of the same model. The modification lets to represent
spatial data structures (as Quadtrees, Octrees, etc.) in a integrated way. This
grammar is important because with it we can create tools to build systems that
combine spatial-nominal characteristics such as Geographical Information
Systems (GIS), Hypermedia Systems, Computed Aided Design Systems (CAD), and so
on
</summary>
    <author>
      <name>Ignacio Vega-Paez</name>
    </author>
    <author>
      <name>Feliu D. Sagols T</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings in Information Systems Analysis and Synthesis ISAS
  1995, 5th International Symposium on Systems Research, Informatics and
  Cybernetics, pp. 89-93 August 16-20, 95, Baden-Baden, Germany</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703113v1</id>
    <updated>2007-03-23T04:25:36Z</updated>
    <published>2007-03-23T04:25:36Z</published>
    <title>Automatic Selection of Bitmap Join Indexes in Data Warehouses</title>
    <summary>  The queries defined on data warehouses are complex and use several join
operations that induce an expensive computational cost. This cost becomes even
more prohibitive when queries access very large volumes of data. To improve
response time, data warehouse administrators generally use indexing techniques
such as star join indexes or bitmap join indexes. This task is nevertheless
complex and fastidious. Our solution lies in the field of data warehouse
auto-administration. In this framework, we propose an automatic index selection
strategy. We exploit a data mining technique ; more precisely frequent itemset
mining, in order to determine a set of candidate indexes from a given workload.
Then, we propose several cost models allowing to create an index configuration
composed by the indexes providing the best profit. These models evaluate the
cost of accessing data using bitmap join indexes, and the cost of updating and
storing these indexes.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Jerome Darmont</name>
    </author>
    <author>
      <name>Omar Boussaid</name>
    </author>
    <author>
      <name>Fadila Bentayeb</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0703113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703114v1</id>
    <updated>2007-03-23T04:31:35Z</updated>
    <published>2007-03-23T04:31:35Z</published>
    <title>Clustering-Based Materialized View Selection in Data Warehouses</title>
    <summary>  Materialized view selection is a non-trivial task. Hence, its complexity must
be reduced. A judicious choice of views must be cost-driven and influenced by
the workload experienced by the system. In this paper, we propose a framework
for materialized view selection that exploits a data mining technique
(clustering), in order to determine clusters of similar queries. We also
propose a view merging algorithm that builds a set of candidate views, as well
as a greedy process for selecting a set of views to materialize. This selection
is based on cost models that evaluate the cost of accessing data using views
and the cost of storing these views. To validate our strategy, we executed a
workload of decision-support queries on a test data warehouse, with and without
using our strategy. Our experimental results demonstrate its efficiency, even
when storage space is limited.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Pierre-Emmanuel Jouve</name>
    </author>
    <author>
      <name>Jerome Darmont</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0703114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3501v1</id>
    <updated>2007-04-26T09:13:04Z</updated>
    <published>2007-04-26T09:13:04Z</published>
    <title>Conception d'un banc d'essais décisionnel</title>
    <summary>  We present in this paper a new benchmark for evaluating the performances of
data warehouses. Benchmarking is useful either to system users for comparing
the performances of different systems, or to system engineers for testing the
effect of various design choices. While the TPC (Transaction Processing
Performance Council) standard benchmarks address the first point, they are not
tuneable enough to address the second one. Our Data Warehouse Engineering
Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses
and workloads. DWEB is fully parameterized. However, two levels of
parameterization keep it easy to tune. Since DWEB mainly meets engineering
benchmarking needs, it is complimentary to the TPC standard benchmarks, and not
a competitor. Finally, DWEB is implemented as a Java free software that can be
interfaced with most existing relational database management systems.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Fadila Bentayeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Boussaïd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">20\`emes Journ\'ees Bases de Donn\'ees Avanc\'ees (BDA 04),
  Montpellier (19/10/2004) 493-511</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.3501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3520v1</id>
    <updated>2007-04-26T11:47:35Z</updated>
    <published>2007-04-26T11:47:35Z</published>
    <title>Vers l'auto-administration des entrepôts de données</title>
    <summary>  With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. The aim of this study
is to search for a way of extracting useful knowledge from stored data to
automatically apply performance optimization techniques, and more particularly
indexing techniques. We have designed a tool that extracts frequent itemsets
from a given workload to compute an index configuration that helps optimizing
data access time. The experiments we performed showed that the index
configurations generated by our tool allowed performance gains of 15% to 25% on
a test database and a test data warehouse.
</summary>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version courte de 4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">XXXV\`emes Journ\'ees de Statistique, Session sp\'eciale
  Entreposage et Fouille de Donn\'ees, Lyon (02/06/2003) 105-108</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.3520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0281v1</id>
    <updated>2007-05-02T12:50:39Z</updated>
    <published>2007-05-02T12:50:39Z</published>
    <title>Dynamic Clustering in Object-Oriented Databases: An Advocacy for
  Simplicity</title>
    <summary>  We present in this paper three dynamic clustering techniques for
Object-Oriented Databases (OODBs). The first two, Dynamic, Statistical &amp;
Tunable Clustering (DSTC) and StatClust, exploit both comprehensive usage
statistics and the inter-object reference graph. They are quite elaborate.
However, they are also complex to implement and induce a high overhead. The
third clustering technique, called Detection &amp; Reclustering of Objects (DRO),
is based on the same principles, but is much simpler to implement. These three
clustering algorithm have been implemented in the Texas persistent object store
and compared in terms of clustering efficiency (i.e., overall performance
increase) and overhead using the Object Clustering Benchmark (OCB). The results
obtained showed that DRO induced a lighter overhead while still achieving
better overall performance.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Christophe Fromantin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Régnier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Le Gruenwald</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Schneider</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, Vol. 1944 (06/2000) 71-85</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0453v1</id>
    <updated>2007-05-03T12:54:30Z</updated>
    <published>2007-05-03T12:54:30Z</published>
    <title>OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented
  Database Systems</title>
    <summary>  We present in this paper a generic object-oriented benchmark (the Object
Clustering Benchmark) that has been designed to evaluate the performances of
clustering policies in object-oriented databases. OCB is generic because its
sample database may be customized to fit the databases introduced by the main
existing benchmarks (e.g., OO1). OCB's current form is clustering-oriented
because of its clustering-oriented workload, but it can be easily adapted to
other purposes. Lastly, OCB's code is compact and easily portable. OCB has been
implemented in a real system (Texas, running on a Sun workstation), in order to
test a specific clustering policy called DSTC. A few results concerning this
test are presented.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Bertrand Petit</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Schneider</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, Vol. 1377 (03/1998) 326-340</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0454v1</id>
    <updated>2007-05-03T13:02:06Z</updated>
    <published>2007-05-03T13:02:06Z</published>
    <title>Performance Evaluation for Clustering Algorithms in Object-Oriented
  Database Systems</title>
    <summary>  It is widely acknowledged that good object clustering is critical to the
performance of object-oriented databases. However, object clustering always
involves some kind of overhead for the system. The aim of this paper is to
propose a modelling methodology in order to evaluate the performances of
different clustering policies. This methodology has been used to compare the
performances of three clustering algorithms found in the literature (Cactis, CK
and ORION) that we considered representative of the current research in the
field of object clustering. The actual performance evaluation was performed
using simulation. Simulation experiments we performed showed that the Cactis
algorithm is better than the ORION algorithm and that the CK algorithm totally
outperforms both other algorithms in terms of response time and clustering
overhead.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Amar Attoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Gourgand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, Vol. 978 (09/1995) 187-196</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1453v1</id>
    <updated>2007-05-10T12:23:35Z</updated>
    <published>2007-05-10T12:23:35Z</published>
    <title>DWEB: A Data Warehouse Engineering Benchmark</title>
    <summary>  Data warehouse architectural choices and optimization techniques are critical
to decision support query performance. To facilitate these choices, the
performance of the designed data warehouse must be assessed. This is usually
done with the help of benchmarks, which can either help system users comparing
the performances of different systems, or help system engineers testing the
effect of various design choices. While the TPC standard decision support
benchmarks address the first point, they are not tuneable enough to address the
second one and fail to model different data warehouse schemas. By contrast, our
Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc
synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill
data warehouse design needs. However, two levels of parameterization keep it
relatively easy to tune. Finally, DWEB is implemented as a Java free software
that can be interfaced with most existing relational database management
systems. A sample usage of DWEB is also provided in this paper.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Fadila Bentayeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Boussaïd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, Vol. 3589 (08/2005) 85-94</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.1453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1454v1</id>
    <updated>2007-05-10T12:24:27Z</updated>
    <published>2007-05-10T12:24:27Z</published>
    <title>DOEF: A Dynamic Object Evaluation Framework</title>
    <summary>  In object-oriented or object-relational databases such as multimedia
databases or most XML databases, access patterns are not static, i.e.,
applications do not always access the same objects in the same order
repeatedly. However, this has been the way these databases and associated
optimisation techniques like clustering have been evaluated up to now. This
paper opens up research regarding this issue by proposing a dynamic object
evaluation framework (DOEF) that accomplishes access pattern change by defining
configurable styles of change. This preliminary prototype has been designed to
be open and fully extensible. To illustrate the capabilities of DOEF, we used
it to compare the performances of four state of the art dynamic clustering
algorithms. The results show that DOEF is indeed effective at determining the
adaptability of each dynamic clustering algorithm to changes in access pattern.
</summary>
    <author>
      <name>Zhen He</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, Vol. 2736 (09/2003) 662-671</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.1454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1455v1</id>
    <updated>2007-05-10T12:25:57Z</updated>
    <published>2007-05-10T12:25:57Z</published>
    <title>Decision tree modeling with relational views</title>
    <summary>  Data mining is a useful decision support technique that can be used to
discover production rules in warehouses or corporate data. Data mining research
has made much effort to apply various mining algorithms efficiently on large
databases. However, a serious problem in their practical application is the
long processing time of such algorithms. Nowadays, one of the key challenges is
to integrate data mining methods within the framework of traditional database
systems. Indeed, such implementations can take advantage of the efficiency
provided by SQL engines. In this paper, we propose an integrating approach for
decision trees within a classical database system. In other words, we try to
discover knowledge from relational databases, in the form of production rules,
via a procedure embedding SQL queries. The obtained decision tree is defined by
successive, related relational views. Each view corresponds to a given
population in the underlying decision tree. We selected the classical Induction
Decision Tree (ID3) algorithm to build the decision tree. To prove that our
implementation of ID3 works properly, we successfully compared the output of
our procedure with the output of an existing and validated data mining
software, SIPINA. Furthermore, since our approach is tuneable, it can be
generalized to any other similar decision tree-based method.
</summary>
    <author>
      <name>Fadila Bentayeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNAI, Vol. 2366 (06/2002) 423-431</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.1455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1456v1</id>
    <updated>2007-05-10T12:28:52Z</updated>
    <published>2007-05-10T12:28:52Z</published>
    <title>Warehousing Web Data</title>
    <summary>  In a data warehousing process, mastering the data preparation phase allows
substantial gains in terms of time and performance when performing
multidimensional analysis or using data mining algorithms. Furthermore, a data
warehouse can require external data. The web is a prevalent data source in this
context. In this paper, we propose a modeling process for integrating diverse
and heterogeneous (so-called multiform) data into a unified format.
Furthermore, the very schema definition provides first-rate metadata in our
data warehousing context. At the conceptual level, a complex object is
represented in UML. Our logical model is an XML schema that can be described
with a DTD or the XML-Schema language. Eventually, we have designed a Java
prototype that transforms our multiform input data into XML documents
representing our physical model. Then, the XML documents we obtain are mapped
into a relational database we view as an ODS (Operational Data Storage), whose
content will have to be re-modeled in a multidimensional way to allow its
storage in a star schema-based warehouse and, later, its analysis.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Boussaïd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Fadila Bentayeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Conference on Information Integration and
  Web-based Applications and Services (iiWAS 02) (09/2002) 148-152</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.1456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1457v1</id>
    <updated>2007-05-10T12:30:19Z</updated>
    <published>2007-05-10T12:30:19Z</published>
    <title>Web data modeling for integration in data warehouses</title>
    <summary>  In a data warehousing process, the data preparation phase is crucial.
Mastering this phase allows substantial gains in terms of time and performance
when performing a multidimensional analysis or using data mining algorithms.
Furthermore, a data warehouse can require external data. The web is a prevalent
data source in this context, but the data broadcasted on this medium are very
heterogeneous. We propose in this paper a UML conceptual model for a complex
object representing a superclass of any useful data source (databases, plain
texts, HTML and XML documents, images, sounds, video clips...). The translation
into a logical model is achieved with XML, which helps integrating all these
diverse, heterogeneous data into a unified format, and whose schema definition
provides first-rate metadata in our data warehousing context. Moreover, we
benefit from XML's flexibility, extensibility and from the richness of the
semi-structured data model, but we are still able to later map XML documents
into a database if more structuring is needed.
</summary>
    <author>
      <name>Sami Miniaoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Boussaïd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">First International Workshop on Multimedia Data and Document
  Engineering (MDDE 01) (07/2001) 88-97</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.1457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.2787v1</id>
    <updated>2007-05-19T00:12:24Z</updated>
    <published>2007-05-19T00:12:24Z</published>
    <title>Worst-Case Background Knowledge for Privacy-Preserving Data Publishing</title>
    <summary>  Recent work has shown the necessity of considering an attacker's background
knowledge when reasoning about privacy in data publishing. However, in
practice, the data publisher does not know what background knowledge the
attacker possesses. Thus, it is important to consider the worst-case. In this
paper, we initiate a formal study of worst-case background knowledge. We
propose a language that can express any background knowledge about the data. We
provide a polynomial time algorithm to measure the amount of disclosure of
sensitive information in the worst case, given that the attacker has at most a
specified number of pieces of information in this language. We also provide a
method to efficiently sanitize the data so that the amount of disclosure in the
worst case is less than a specified threshold.
</summary>
    <author>
      <name>David J. Martin</name>
    </author>
    <author>
      <name>Daniel Kifer</name>
    </author>
    <author>
      <name>Ashwin Machanavajjhala</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Joseph Y. Halpern</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.2787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.2787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.4442v2</id>
    <updated>2008-01-09T10:58:06Z</updated>
    <published>2007-05-30T17:56:06Z</published>
    <title>World-set Decompositions: Expressiveness and Efficient Algorithms</title>
    <summary>  Uncertain information is commonplace in real-world data management scenarios.
The ability to represent large sets of possible instances (worlds) while
supporting efficient storage and processing is an important challenge in this
context. The recent formalism of world-set decompositions (WSDs) provides a
space-efficient representation for uncertain data that also supports scalable
processing. WSDs are complete for finite world-sets in that they can represent
any finite set of possible worlds. For possibly infinite world-sets, we show
that a natural generalization of WSDs precisely captures the expressive power
of c-tables. We then show that several important decision problems are
efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we
give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient
algorithm for minimizing such representations.
</summary>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Lyublena Antova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 13 figures, extended version of ICDT'07 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.4442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.4442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.4676v8</id>
    <updated>2016-06-06T15:18:03Z</updated>
    <published>2007-05-31T18:41:28Z</published>
    <title>Recursive n-gram hashing is pairwise independent, at best</title>
    <summary>  Many applications use sequences of n consecutive symbols (n-grams). Hashing
these n-grams can be a performance bottleneck. For more speed, recursive hash
families compute hash values by updating previous values. We prove that
recursive hash families cannot be more than pairwise independent. While hashing
by irreducible polynomials is pairwise independent, our implementations either
run in time O(n) or use an exponential amount of memory. As a more scalable
alternative, we make hashing by cyclic polynomials pairwise independent by
ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials
is is twice as fast as hashing by irreducible polynomials. We also show that
randomized Karp-Rabin hash families are not pairwise independent.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.csl.2009.12.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.csl.2009.12.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">See software at https://github.com/lemire/rollinghashcpp</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Speech &amp; Language 24(4): 698-710 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.4676v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.4676v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3, H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0421v2</id>
    <updated>2009-06-02T16:40:37Z</updated>
    <published>2007-07-03T14:17:49Z</published>
    <title>The $k$-anonymity Problem is Hard</title>
    <summary>  The problem of publishing personal data without giving up privacy is becoming
increasingly important. An interesting formalization recently proposed is the
k-anonymity. This approach requires that the rows in a table are clustered in
sets of size at least k and that all the rows in a cluster become the same
tuple, after the suppression of some records. The natural optimization problem,
where the goal is to minimize the number of suppressed entries, is known to be
NP-hard when the values are over a ternary alphabet, k = 3 and the rows length
is unbounded. In this paper we give a lower bound on the approximation factor
that any polynomial-time algorithm can achive on two restrictions of the
problem,namely (i) when the records values are over a binary alphabet and k =
3, and (ii) when the records have length at most 8 and k = 4, showing that
these restrictions of the problem are APX-hard.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Gianluca Della Vedova</name>
    </author>
    <author>
      <name>Riccardo Dondi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, A short version of this paper has been accepted in FCT 2009
  - 17th International Symposium on Fundamentals of Computation Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.0421v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0421v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0745v1</id>
    <updated>2007-07-05T09:40:19Z</updated>
    <published>2007-07-05T09:40:19Z</published>
    <title>Semantic Information Retrieval from Distributed Heterogeneous Data
  Sources</title>
    <summary>  Information retrieval from distributed heterogeneous data sources remains a
challenging issue. As the number of data sources increases more intelligent
retrieval techniques, focusing on information content and semantics, are
required. Currently ontologies are being widely used for managing semantic
knowledge, especially in the field of bioinformatics. In this paper we describe
an ontology assisted system that allows users to query distributed
heterogeneous data sources by hiding details like location, information
structure, access pattern and semantic structure of the data. Our goal is to
provide an integrated view on biomedical information sources for the
Health-e-Child project with the aim to overcome the lack of sufficient
semantic-based reformulation techniques for querying distributed data sources.
In particular, this paper examines the problem of query reformulation across
biomedical data sources, based on merged ontologies and the underlying
heterogeneous descriptions of the respective data sources.
</summary>
    <author>
      <name>K. Munir</name>
    </author>
    <author>
      <name>M. Odeh</name>
    </author>
    <author>
      <name>R. McClatchey</name>
    </author>
    <author>
      <name>S. Khan</name>
    </author>
    <author>
      <name>I. Habib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Presented at the 4th International Workshop on
  Frontiers of Information Technology -- FIT 2006. Islamabad, Pakistan December
  2006</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.0745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0763v1</id>
    <updated>2007-07-05T11:21:39Z</updated>
    <published>2007-07-05T11:21:39Z</published>
    <title>The Requirements for Ontologies in Medical Data Integration: A Case
  Study</title>
    <summary>  Evidence-based medicine is critically dependent on three sources of
information: a medical knowledge base, the patients medical record and
knowledge of available resources, including where appropriate, clinical
protocols. Patient data is often scattered in a variety of databases and may,
in a distributed model, be held across several disparate repositories.
Consequently addressing the needs of an evidence-based medicine community
presents issues of biomedical data integration, clinical interpretation and
knowledge management. This paper outlines how the Health-e-Child project has
approached the challenge of requirements specification for (bio-) medical data
integration, from the level of cellular data, through disease to that of
patient and population. The approach is illuminated through the requirements
elicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three
diseases being studied in the EC-funded Health-e-Child project.
</summary>
    <author>
      <name>Ashiq Anjum</name>
    </author>
    <author>
      <name>Peter Bloodsworth</name>
    </author>
    <author>
      <name>Andrew Branson</name>
    </author>
    <author>
      <name>Tamas Hauer</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Kamran Munir</name>
    </author>
    <author>
      <name>Dmitry Rogulin</name>
    </author>
    <author>
      <name>Jetendr Shamdasani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure. Presented at the 11th International Database
  Engineering &amp; Applications Symposium (Ideas2007). Banff, Canada September
  2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.0763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1534v1</id>
    <updated>2007-07-10T22:01:40Z</updated>
    <published>2007-07-10T22:01:40Z</published>
    <title>An Architecture Framework for Complex Data Warehouses</title>
    <summary>  Nowadays, many decision support applications need to exploit data that are
not only numerical or symbolic, but also multimedia, multistructure,
multisource, multimodal, and/or multiversion. We term such data complex data.
Managing and analyzing complex data involves a lot of different issues
regarding their structure, storage and processing, and metadata are a key
element in all these processes. Such problems have been addressed by classical
data warehousing (i.e., applied to "simple" data). However, data warehousing
approaches need to be adapted for complex data. In this paper, we first propose
a precise, though open, definition of complex data. Then we present a general
architecture framework for warehousing complex data. This architecture heavily
relies on metadata and domain-related knowledge, and rests on the XML language,
which helps storing data, metadata and domain-specific knowledge altogether,
and facilitates communication between the various warehousing processes.
</summary>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <author>
      <name>Omar Boussaid</name>
    </author>
    <author>
      <name>Jean-Christian Ralaivao</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1548v1</id>
    <updated>2007-07-11T02:45:10Z</updated>
    <published>2007-07-11T02:45:10Z</published>
    <title>Data Mining-based Materialized View and Index Selection in Data
  Warehouses</title>
    <summary>  Materialized views and indexes are physical structures for accelerating data
access that are casually used in data warehouses. However, these data
structures generate some maintenance overhead. They also share the same storage
space. Most existing studies about materialized view and index selection
consider these structures separately. In this paper, we adopt the opposite
stance and couple materialized view and index selection to take view-index
interactions into account and achieve efficient storage space sharing.
Candidate materialized views and indexes are selected through a data mining
process. We also exploit cost models that evaluate the respective benefit of
indexing and view materialization, and help select a relevant configuration of
indexes and materialized views among the candidates. Experimental results show
that our strategy performs better than an independent selection of materialized
views and indexes.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1644v1</id>
    <updated>2007-07-11T15:13:39Z</updated>
    <published>2007-07-11T15:13:39Z</published>
    <title>Fast and Simple Relational Processing of Uncertain Data</title>
    <summary>  This paper introduces U-relations, a succinct and purely relational
representation system for uncertain databases. U-relations support
attribute-level uncertainty using vertical partitioning. If we consider
positive relational algebra extended by an operation for computing possible
answers, a query on the logical level can be translated into, and evaluated as,
a single relational algebra query on the U-relation representation. The
translation scheme essentially preserves the size of the query in terms of
number of operations and, in particular, number of joins. Standard techniques
employed in off-the-shelf relational database management systems are effective
for optimizing and processing queries on U-relations. In our experiments we
show that query evaluation on U-relations scales to large amounts of data with
high degrees of uncertainty.
</summary>
    <author>
      <name>Lyublena Antova</name>
    </author>
    <author>
      <name>Thomas Jansen</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.1644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.4304v1</id>
    <updated>2007-07-29T21:46:50Z</updated>
    <published>2007-07-29T21:46:50Z</published>
    <title>Spatial Aggregation: Data Model and Implementation</title>
    <summary>  Data aggregation in Geographic Information Systems (GIS) is only marginally
present in commercial systems nowadays, mostly through ad-hoc solutions. In
this paper, we first present a formal model for representing spatial data. This
model integrates geographic data and information contained in data warehouses
external to the GIS. We define the notion of geometric aggregation, a general
framework for aggregate queries in a GIS setting. We also identify the class of
summable queries, which can be efficiently evaluated by precomputing the
overlay of two or more of the thematic layers involved in the query. We also
sketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS
and OLAP features. In addition, we introduce Piet, an implementation of our
proposal, that makes use of overlay precomputation for answering spatial
queries (aggregate or not). Our experimental evaluation showed that for a
certain class of geometric queries with or without aggregation, overlay
precomputation outperforms R-tree-based techniques. Finally, as a particular
application of our proposal, we study topological queries.
</summary>
    <author>
      <name>Leticia Gomez</name>
    </author>
    <author>
      <name>Sofie Haesevoets</name>
    </author>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages, 28 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.4304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.4304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.0361v7</id>
    <updated>2007-09-05T20:18:51Z</updated>
    <published>2007-08-02T15:24:29Z</published>
    <title>Why the relational data model can be considered as a formal basis for
  group operations in object-oriented systems</title>
    <summary>  Relational data model defines a specification of a type "relation". However,
its simplicity does not mean that the system implementing this model must
operate with structures having the same simplicity. We consider two principles
allowing create a system which combines object-oriented paradigm (OOP) and
relational data model (RDM) in one framework. The first principle -- "complex
data in encapsulated domains" -- is well known from The Third Manifesto by Date
and Darwen. The second principle --"data complexity in names"-- is the basis
for a system where data are described as complex objects and uniquely
represented as a set of relations. Names of these relations and names of their
attributes are combinations of names entered in specifications of the complex
objects. Below, we consider the main properties of such a system.
</summary>
    <author>
      <name>Evgeniy Grigoriev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.0361v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0361v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; D.3.3; F.3.3; F.4.1; H.2.1; H.2.3; H.2.4; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2076v1</id>
    <updated>2007-08-15T18:31:48Z</updated>
    <published>2007-08-15T18:31:48Z</published>
    <title>Repairing Inconsistent XML Write-Access Control Policies</title>
    <summary>  XML access control policies involving updates may contain security flaws,
here called inconsistencies, in which a forbidden operation may be simulated by
performing a sequence of allowed operations. This paper investigates the
problem of deciding whether a policy is consistent, and if not, how its
inconsistencies can be repaired. We consider policies expressed in terms of
annotated DTDs defining which operations are allowed or denied for the XML
trees that are instances of the DTD. We show that consistency is decidable in
PTIME for such policies and that consistent partial policies can be extended to
unique "least-privilege" consistent total policies. We also consider repair
problems based on deleting privileges to restore consistency, show that finding
minimal repairs is NP-complete, and give heuristics for finding repairs.
</summary>
    <author>
      <name>Loreto Bravo</name>
    </author>
    <author>
      <name>James Cheney</name>
    </author>
    <author>
      <name>Irini Fundulaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages. To appear in Proceedings of DBPL 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2173v2</id>
    <updated>2009-12-22T15:25:13Z</updated>
    <published>2007-08-16T11:11:43Z</published>
    <title>Provenance as Dependency Analysis</title>
    <summary>  Provenance is information recording the source, derivation, or history of
some information. Provenance tracking has been studied in a variety of
settings; however, although many design points have been explored, the
mathematical or semantic foundations of data provenance have received
comparatively little attention. In this paper, we argue that dependency
analysis techniques familiar from program analysis and program slicing provide
a formal foundation for forms of provenance that are intended to show how (part
of) the output of a query depends on (parts of) its input. We introduce a
semantic characterization of such dependency provenance, show that this form of
provenance is not computable, and provide dynamic and static approximation
techniques.
</summary>
    <author>
      <name>James Cheney</name>
    </author>
    <author>
      <name>Amal Ahmed</name>
    </author>
    <author>
      <name>Umut Acar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long version of paper in 2007 Symposium on Database Programming
  Languages; revised November 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2173v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2173v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0218v2</id>
    <updated>2007-09-26T06:49:01Z</updated>
    <published>2007-09-03T13:23:33Z</published>
    <title>Inferring Neuronal Network Connectivity using Time-constrained Episodes</title>
    <summary>  Discovering frequent episodes in event sequences is an interesting data
mining task. In this paper, we argue that this framework is very effective for
analyzing multi-neuronal spike train data. Analyzing spike train data is an
important problem in neuroscience though there are no data mining approaches
reported for this. Motivated by this application, we introduce different
temporal constraints on the occurrences of episodes. We present algorithms for
discovering frequent episodes under temporal constraints. Through simulations,
we show that our method is very effective for analyzing spike train data for
unearthing underlying connectivity patterns.
</summary>
    <author>
      <name>Debprakash Patnaik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Electrical Engg. Dept., Indian Institute of Science</arxiv:affiliation>
    </author>
    <author>
      <name>P. S. Sastry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Electrical Engg. Dept., Indian Institute of Science</arxiv:affiliation>
    </author>
    <author>
      <name>K. P. Unnikrishnan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">General Motors R&amp;D</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. See also http://neural-code.cs.vt.edu/</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.0218v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0218v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0566v2</id>
    <updated>2007-09-26T06:51:41Z</updated>
    <published>2007-09-05T14:46:59Z</published>
    <title>Discovering Patterns in Multi-neuronal Spike Trains using the Frequent
  Episode Method</title>
    <summary>  Discovering the 'Neural Code' from multi-neuronal spike trains is an
important task in neuroscience. For such an analysis, it is important to
unearth interesting regularities in the spiking patterns. In this report, we
present an efficient method for automatically discovering synchrony, synfire
chains, and more general sequences of neuronal firings. We use the Frequent
Episode Discovery framework of Laxman, Sastry, and Unnikrishnan (2005), in
which the episodes are represented and recognized using finite-state automata.
Many aspects of functional connectivity between neuronal populations can be
inferred from the episodes. We demonstrate these using simulated multi-neuronal
data from a Poisson model. We also present a method to assess the statistical
significance of the discovered episodes. Since the Temporal Data Mining (TDM)
methods used in this report can analyze data from hundreds and potentially
thousands of neurons, we argue that this framework is appropriate for
discovering the `Neural Code'.
</summary>
    <author>
      <name>K. P. Unnikrishnan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">General Motors R&amp;D Center, Warren, MI</arxiv:affiliation>
    </author>
    <author>
      <name>Debprakash Patnaik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. Elecetrical Engineering, Indian Institute of Science, Bangalore</arxiv:affiliation>
    </author>
    <author>
      <name>P. S. Sastry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. Elecetrical Engineering, Indian Institute of Science, Bangalore</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also see http://neural-code.cs.vt.edu/</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.0566v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0566v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.1166v1</id>
    <updated>2007-09-07T21:18:57Z</updated>
    <published>2007-09-07T21:18:57Z</published>
    <title>An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation</title>
    <summary>  Monotonicity is a simple yet significant qualitative characteristic. We
consider the problem of segmenting a sequence in up to K segments. We want
segments to be as monotonic as possible and to alternate signs. We propose a
quality metric for this problem using the l_inf norm, and we present an optimal
linear time algorithm based on novel formalism. Moreover, given a
precomputation in time O(n log n) consisting of a labeling of all extrema, we
compute any optimal segmentation in constant time. We compare experimentally
its performance to two piecewise linear segmentation heuristics (top-down and
bottom-up). We show that our algorithm is faster and more accurate.
Applications include pattern recognition and qualitative modeling.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Martin Brooks</name>
    </author>
    <author>
      <name>Yuhong Yan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00207160701694153</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00207160701694153" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the extended version of our ICDM'05 paper (arXiv:cs/0702142)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Daniel Lemire, Martin Brooks and Yuhong Yan, An Optimal Linear
  Time Algorithm for Quasi-Monotonic Segmentation. International Journal of
  Computer Mathematics 86 (7), 2009.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.1166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.1166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3034v1</id>
    <updated>2007-09-19T15:10:05Z</updated>
    <published>2007-09-19T15:10:05Z</published>
    <title>Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,
  Complexity, and Optimizations</title>
    <summary>  In this study, we address the problem of answering queries over a
peer-to-peer system of taxonomy-based sources. A taxonomy states subsumption
relationships between negation-free DNF formulas on terms and negation-free
conjunctions of terms. To the end of laying the foundations of our study, we
first consider the centralized case, deriving the complexity of the decision
problem and of query evaluation. We conclude by presenting an algorithm that is
efficient in data complexity and is based on hypergraphs. More expressive forms
of taxonomies are also investigated, which however lead to intractability. We
then move to the distributed case, and introduce a logical model of a network
of taxonomy-based sources. On such network, a distributed version of the
centralized algorithm is then presented, based on a message passing paradigm,
and its correctness is proved. We finally discuss optimization issues, and
relate our work to the literature.
</summary>
    <author>
      <name>Carlo Meghini</name>
    </author>
    <author>
      <name>Yannis Tzitzikas</name>
    </author>
    <author>
      <name>Anastasia Analyti</name>
    </author>
    <link href="http://arxiv.org/abs/0709.3034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2083v1</id>
    <updated>2007-10-10T18:00:44Z</updated>
    <published>2007-10-10T18:00:44Z</published>
    <title>Association Rules in the Relational Calculus</title>
    <summary>  One of the most utilized data mining tasks is the search for association
rules. Association rules represent significant relationships between items in
transactions. We extend the concept of association rule to represent a much
broader class of associations, which we refer to as \emph{entity-relationship
rules.} Semantically, entity-relationship rules express associations between
properties of related objects. Syntactically, these rules are based on a broad
subclass of safe domain relational calculus queries. We propose a new
definition of support and confidence for entity-relationship rules and for the
frequency of entity-relationship queries. We prove that the definition of
frequency satisfies standard probability axioms and the Apriori property.
</summary>
    <author>
      <name>Oliver Schulte</name>
    </author>
    <author>
      <name>Flavia Moser</name>
    </author>
    <author>
      <name>Martin Ester</name>
    </author>
    <author>
      <name>Zhiyong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2156v3</id>
    <updated>2016-03-15T21:52:12Z</updated>
    <published>2007-10-11T19:48:10Z</published>
    <title>Collaborative OLAP with Tag Clouds: Web 2.0 OLAP Formalism and
  Experimental Evaluation</title>
    <summary>  Increasingly, business projects are ephemeral. New Business Intelligence
tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds
are a popular community-driven visualization technique. Hence, we investigate
tag-cloud views with support for OLAP operations such as roll-ups, slices,
dices, clustering, and drill-downs. As a case study, we implemented an
application where users can upload data and immediately navigate through its ad
hoc dimensions. To support social networking, views can be easily shared and
embedded in other Web sites. Algorithmically, our tag-cloud views are
approximate range top-k queries over spontaneous data cubes. We present
experimental evidence that iceberg cuboids provide adequate online
approximations. We benchmark several browser-oblivious tag-cloud layout
optimizations.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Robert Godin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software at https://github.com/lemire/OLAPTagCloud</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2156v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2156v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.5333v1</id>
    <updated>2007-10-29T19:46:58Z</updated>
    <published>2007-10-29T19:46:58Z</published>
    <title>Neutrosophic Relational Data Model</title>
    <summary>  In this paper, we present a generalization of the relational data model based
on interval neutrosophic set. Our data model is capable of manipulating
incomplete as well as inconsistent information. Fuzzy relation or
intuitionistic fuzzy relation can only handle incomplete information.
Associated with each relation are two membership functions one is called
truth-membership function T which keeps track of the extent to which we believe
the tuple is in the relation, another is called falsity-membership function F
which keeps track of the extent to which we believe that it is not in the
relation. A neutrosophic relation is inconsistent if there exists one tuple a
such that T(a) + F(a) &gt; 1 . In order to handle inconsistent situation, we
propose an operator called "split" to transform inconsistent neutrosophic
relations into pseudo-consistent neutrosophic relations and do the
set-theoretic and relation-theoretic operations on them and finally use another
operator called "combine" to transform the result back to neutrosophic
relation. For this data model, we define algebraic operators that are
generalizations of the usual operators such as intersection, union, selection,
join on fuzzy relations. Our data model can underlie any database and
knowledge-base management system that deals with incomplete and inconsistent
information.
</summary>
    <author>
      <name>Haibin Wang</name>
    </author>
    <author>
      <name>Rajshekhar Sunderraman</name>
    </author>
    <author>
      <name>Florentin Smarandache</name>
    </author>
    <author>
      <name>Andre Rogatko</name>
    </author>
    <link href="http://arxiv.org/abs/0710.5333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.5333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2615v1</id>
    <updated>2007-11-16T13:38:15Z</updated>
    <published>2007-11-16T13:38:15Z</published>
    <title>A Biologically Inspired Classifier</title>
    <summary>  We present a method for measuring the distance among records based on the
correlations of data stored in the corresponding database entries. The original
method (F. Bagnoli, A. Berrones and F. Franci. Physica A 332 (2004) 509-518)
was formulated in the context of opinion formation. The opinions expressed over
a set of topic originate a ``knowledge network'' among individuals, where two
individuals are nearer the more similar their expressed opinions are. Assuming
that individuals' opinions are stored in a database, the authors show that it
is possible to anticipate an opinion using the correlations in the database.
This corresponds to approximating the overlap between the tastes of two
individuals with the correlations of their expressed opinions.
  In this paper we extend this model to nonlinear matching functions, inspired
by biological problems such as microarray (probe-sample pairing). We
investigate numerically the error between the correlation and the overlap
matrix for eight sequences of reference with random probes. Results show that
this method is particularly robust for detecting similarities in the presence
of translocations.
</summary>
    <author>
      <name>Franco Bagnoli</name>
    </author>
    <author>
      <name>Francesca Di Patti</name>
    </author>
    <link href="http://arxiv.org/abs/0711.2615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3375v1</id>
    <updated>2007-11-21T13:22:15Z</updated>
    <published>2007-11-21T13:22:15Z</published>
    <title>An Inflationary Fixed Point Operator in XQuery</title>
    <summary>  We introduce a controlled form of recursion in XQuery, inflationary fixed
points, familiar in the context of relational databases. This imposes
restrictions on the expressible types of recursion, but we show that
inflationary fixed points nevertheless are sufficiently versatile to capture a
wide range of interesting use cases, including the semantics of Regular XPath
and its core transitive closure construct.
  While the optimization of general user-defined recursive functions in XQuery
appears elusive, we will describe how inflationary fixed points can be
efficiently evaluated, provided that the recursive XQuery expressions exhibit a
distributivity property. We show how distributivity can be assessed both,
syntactically and algebraically, and provide experimental evidence that XQuery
processors can substantially benefit during inflationary fixed point
evaluation.
</summary>
    <author>
      <name>Loredana Afanasiev</name>
    </author>
    <author>
      <name>Torsten Grust</name>
    </author>
    <author>
      <name>Maarten Marx</name>
    </author>
    <author>
      <name>Jan Rittinger</name>
    </author>
    <author>
      <name>Jens Teubner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.3375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.0139v1</id>
    <updated>2007-12-30T15:04:25Z</updated>
    <published>2007-12-30T15:04:25Z</published>
    <title>Principles of the Concept-Oriented Data Model</title>
    <summary>  In the paper a new approach to data representation and manipulation is
described, which is called the concept-oriented data model (CODM). It is
supposed that items represent data units, which are stored in concepts. A
concept is a combination of superconcepts, which determine the concept's
dimensionality or properties. An item is a combination of superitems taken by
one from all the superconcepts. An item stores a combination of references to
its superitems. The references implement inclusion relation or attribute-value
relation among items. A concept-oriented database is defined by its concept
structure called syntax or schema and its item structure called semantics. The
model defines formal transformations of syntax and semantics including the
canonical semantics where all concepts are merged and the data semantics is
represented by one set of items. The concept-oriented data model treats
relations as subconcepts where items are instances of the relations.
Multi-valued attributes are defined via subconcepts as a view on the database
semantics rather than as a built-in mechanism. The model includes
concept-oriented query language, which is based on collection manipulations. It
also has such mechanisms as aggregation and inference based on semantics
propagation through the database schema.
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages. Related papers: http://conceptoriented.com/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Mathematics and Computer Science, Academy of Sciences
  of Moldova, Technical Report, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.0139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.0139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1715v2</id>
    <updated>2008-02-08T21:58:35Z</updated>
    <published>2008-01-11T03:21:49Z</published>
    <title>On Breaching Enterprise Data Privacy Through Adversarial Information
  Fusion</title>
    <summary>  Data privacy is one of the key challenges faced by enterprises today.
Anonymization techniques address this problem by sanitizing sensitive data such
that individual privacy is preserved while allowing enterprises to maintain and
share sensitive data. However, existing work on this problem make inherent
assumptions about the data that are impractical in day-to-day enterprise data
management scenarios. Further, application of existing anonymization schemes on
enterprise data could lead to adversarial attacks in which an intruder could
use information fusion techniques to inflict a privacy breach. In this paper,
we shed light on the shortcomings of current anonymization schemes in the
context of enterprise data. We define and experimentally demonstrate Web-based
Information- Fusion Attack on anonymized enterprise data. We formulate the
problem of Fusion Resilient Enterprise Data Anonymization and propose a
prototype solution to address this problem.
</summary>
    <author>
      <name>Srivatsava Ranjit Ganta</name>
    </author>
    <author>
      <name>Raj Acharya</name>
    </author>
    <link href="http://arxiv.org/abs/0801.1715v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1715v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3448v1</id>
    <updated>2008-02-23T15:25:04Z</updated>
    <published>2008-02-23T15:25:04Z</published>
    <title>Sketch-Based Estimation of Subpopulation-Weight</title>
    <summary>  Summaries of massive data sets support approximate query processing over the
original data. A basic aggregate over a set of records is the weight of
subpopulations specified as a predicate over records' attributes. Bottom-k
sketches are a powerful summarization format of weighted items that includes
priority sampling and the classic weighted sampling without replacement. They
can be computed efficiently for many representations of the data including
distributed databases and data streams.
  We derive novel unbiased estimators and efficient confidence bounds for
subpopulation weight. Our estimators and bounds are tailored by distinguishing
between applications (such as data streams) where the total weight of the
sketched set can be computed by the summarization algorithm without a
significant use of additional resources, and applications (such as sketches of
network neighborhoods) where this is not the case.
  Our rigorous derivations are based on clever applications of the
Horvitz-Thompson estimator, and are complemented by efficient computational
methods. We demonstrate their benefit on a wide range of Pareto distributions.
</summary>
    <author>
      <name>Edith Cohen</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <link href="http://arxiv.org/abs/0802.3448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3582v1</id>
    <updated>2008-02-25T09:57:31Z</updated>
    <published>2008-02-25T09:57:31Z</published>
    <title>Neural Networks and Database Systems</title>
    <summary>  Object-oriented database systems proved very valuable at handling and
administrating complex objects. In the following guidelines for embedding
neural networks into such systems are presented. It is our goal to treat
networks as normal data in the database system. From the logical point of view,
a neural network is a complex data value and can be stored as a normal data
object. It is generally accepted that rule-based reasoning will play an
important role in future database applications. The knowledge base consists of
facts and rules, which are both stored and handled by the underlying database
system. Neural networks can be seen as representation of intensional knowledge
of intelligent database systems. So they are part of a rule based knowledge
pool and can be used like conventional rules. The user has a unified view about
his knowledge base regardless of the origin of the unique rules.
</summary>
    <author>
      <name>Erich Schikuta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, Festschrift Informationssysteme, in honor of G. Vinek</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">pp. 133-152, 2007, publisher Austrian Computer Society</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0954v1</id>
    <updated>2008-03-06T19:43:35Z</updated>
    <published>2008-03-06T19:43:35Z</published>
    <title>Selective association rule generation</title>
    <summary>  Mining association rules is a popular and well researched method for
discovering interesting relations between variables in large databases. A
practical problem is that at medium to low support values often a large number
of frequent itemsets and an even larger number of association rules are found
in a database. A widely used approach is to gradually increase minimum support
and minimum confidence or to filter the found rules using increasingly strict
constraints on additional measures of interestingness until the set of rules
found is reduced to a manageable size. In this paper we describe a different
approach which is based on the idea to first define a set of ``interesting''
itemsets (e.g., by a mixture of mining and expert knowledge) and then, in a
second step to selectively generate rules for only these itemsets. The main
advantage of this approach over increasing thresholds or filtering rules is
that the number of rules found is significantly reduced while at the same time
it is not necessary to increase the support and confidence thresholds which
might lead to missing important information in the database.
</summary>
    <author>
      <name>Michael Hahsler</name>
    </author>
    <author>
      <name>Christian Buchta</name>
    </author>
    <author>
      <name>Kurt Hornik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00180-007-0062-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00180-007-0062-z" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics, 2007. Online First, Published: 25 July
  2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0966v1</id>
    <updated>2008-03-06T20:17:19Z</updated>
    <published>2008-03-06T20:17:19Z</published>
    <title>New probabilistic interest measures for association rules</title>
    <summary>  Mining association rules is an important technique for discovering meaningful
patterns in transaction databases. Many different measures of interestingness
have been proposed for association rules. However, these measures fail to take
the probabilistic properties of the mined data into account. In this paper, we
start with presenting a simple probabilistic framework for transaction data
which can be used to simulate transaction data when no associations are
present. We use such data and a real-world database from a grocery outlet to
explore the behavior of confidence and lift, two popular interest measures used
for rule mining. The results show that confidence is systematically influenced
by the frequency of the items in the left hand side of rules and that lift
performs poorly to filter random noise in transaction data. Based on the
probabilistic framework we develop two new interest measures, hyper-lift and
hyper-confidence, which can be used to filter or order mined association rules.
The new measures show significantly better performance than lift for
applications where spurious rules are problematic.
</summary>
    <author>
      <name>Michael Hahsler</name>
    </author>
    <author>
      <name>Kurt Hornik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/IDA-2007-11502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/IDA-2007-11502" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Intelligent Data Analysis, 11(5):437-455, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1555v1</id>
    <updated>2008-03-11T11:18:52Z</updated>
    <published>2008-03-11T11:18:52Z</published>
    <title>Privacy Preserving ID3 over Horizontally, Vertically and Grid
  Partitioned Data</title>
    <summary>  We consider privacy preserving decision tree induction via ID3 in the case
where the training data is horizontally or vertically distributed. Furthermore,
we consider the same problem in the case where the data is both horizontally
and vertically distributed, a situation we refer to as grid partitioned data.
We give an algorithm for privacy preserving ID3 over horizontally partitioned
data involving more than two parties. For grid partitioned data, we discuss two
different evaluation methods for preserving privacy ID3, namely, first merging
horizontally and developing vertically or first merging vertically and next
developing horizontally. Next to introducing privacy preserving data mining
over grid-partitioned data, the main contribution of this paper is that we
show, by means of a complexity analysis that the former evaluation method is
the more efficient.
</summary>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <author>
      <name>Vanessa Lemmens</name>
    </author>
    <author>
      <name>Bart Moelans</name>
    </author>
    <author>
      <name>Karl Tuyls</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.1555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.3; H.2.8; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2212v2</id>
    <updated>2008-06-16T16:20:06Z</updated>
    <published>2008-03-14T17:23:34Z</published>
    <title>Conditioning Probabilistic Databases</title>
    <summary>  Past research on probabilistic databases has studied the problem of answering
queries on a static database. Application scenarios of probabilistic databases
however often involve the conditioning of a database using additional
information in the form of new evidence. The conditioning problem is thus to
transform a probabilistic database of priors into a posterior probabilistic
database which is materialized for subsequent query processing or further
refinement. It turns out that the conditioning problem is closely related to
the problem of computing exact tuple confidence values.
  It is known that exact confidence computation is an NP-hard problem. This has
led researchers to consider approximation techniques for confidence
computation. However, neither conditioning nor exact confidence computation can
be solved using such techniques.
  In this paper we present efficient techniques for both problems. We study
several problem decomposition methods and heuristics that are based on the most
successful search techniques from constraint satisfaction, such as the
Davis-Putnam algorithm. We complement this with a thorough experimental
evaluation of the algorithms proposed. Our experiments show that our exact
algorithms scale well to realistic database sizes and can in some scenarios
compete with the most efficient previous approximation algorithms.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.2212v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2212v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0747v1</id>
    <updated>2008-05-06T15:45:15Z</updated>
    <published>2008-05-06T15:45:15Z</published>
    <title>Pruning Attribute Values From Data Cubes with Diamond Dicing</title>
    <summary>  Data stored in a data warehouse are inherently multidimensional, but most
data-pruning techniques (such as iceberg and top-k queries) are unidimensional.
However, analysts need to issue multidimensional queries. For example, an
analyst may need to select not just the most profitable stores
or--separately--the most profitable products, but simultaneous sets of stores
and products fulfilling some profitability constraints. To fill this need, we
propose a new operator, the diamond dice. Because of the interaction between
dimensions, the computation of diamonds is challenging. We present the first
diamond-dicing experiments on large data sets. Experiments show that we can
compute diamond cubes over fact tables containing 100 million facts in less
than 35 minutes using a standard PC.
</summary>
    <author>
      <name>Hazel Webb</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/0805.0747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1487v1</id>
    <updated>2008-05-10T17:18:32Z</updated>
    <published>2008-05-10T17:18:32Z</published>
    <title>A Time Efficient Indexing Scheme for Complex Spatiotemporal Retrieval</title>
    <summary>  The paper is concerned with the time efficient processing of spatiotemporal
predicates, i.e. spatial predicates associated with an exact temporal
constraint. A set of such predicates forms a buffer query or a Spatio-temporal
Pattern (STP) Query with time. In the more general case of an STP query, the
temporal dimension is introduced via the relative order of the spatial
predicates (STP queries with order). Therefore, the efficient processing of a
spatiotemporal predicate is crucial for the efficient implementation of more
complex queries of practical interest. We propose an extension of a known
approach, suitable for processing spatial predicates, which has been used for
the efficient manipulation of STP queries with order. The extended method is
supported by efficient indexing structures. We also provide experimental
results that show the efficiency of the technique.
</summary>
    <author>
      <name>Lagogiannis George</name>
    </author>
    <author>
      <name>Lorentzos Nikos</name>
    </author>
    <author>
      <name>Sioutas Spyros</name>
    </author>
    <author>
      <name>Theodoridis Evaggelos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures, submitted to Sigmod Record</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.1487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3339v3</id>
    <updated>2008-08-15T00:08:42Z</updated>
    <published>2008-05-21T19:50:46Z</published>
    <title>Tri de la table de faits et compression des index bitmaps avec
  alignement sur les mots</title>
    <summary>  Bitmap indexes are frequently used to index multidimensional data. They rely
mostly on sequential input/output. Bitmaps can be compressed to reduce
input/output costs and minimize CPU usage. The most efficient compression
techniques are based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. This type of compression accelerates logical operations
(AND, OR) over the bitmaps. However, run-length encoding is sensitive to the
order of the facts. Thus, we propose to sort the fact tables. We review
lexicographic, Gray-code, and block-wise sorting. We found that a lexicographic
sort improves compression--sometimes generating indexes twice as small--and
make indexes several times faster. While sorting takes time, this is partially
offset by the fact that it is faster to index a sorted table. Column order is
significant: it is generally preferable to put the columns having more distinct
values at the beginning. A block-wise sort is much less efficient than a full
sort. Moreover, we found that Gray-code sorting is not better than
lexicographic sorting when using word-aligned compression.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at BDA'08</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.3339v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3339v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1071v1</id>
    <updated>2008-06-05T23:19:56Z</updated>
    <published>2008-06-05T23:19:56Z</published>
    <title>Histograms and Wavelets on Probabilistic Data</title>
    <summary>  There is a growing realization that uncertain information is a first-class
citizen in modern database management. As such, we need techniques to correctly
and efficiently process uncertain data in database systems. In particular, data
reduction techniques that can produce concise, accurate synopses of large
probabilistic relations are crucial. Similar to their deterministic relation
counterparts, such compact probabilistic data synopses can form the foundation
for human understanding and interactive data exploration, probabilistic query
planning and optimization, and fast approximate query processing in
probabilistic database systems.
  In this paper, we introduce definitions and algorithms for building
histogram- and wavelet-based synopses on probabilistic data. The core problem
is to choose a set of histogram bucket boundaries or wavelet coefficients to
optimize the accuracy of the approximate representation of a collection of
probabilistic tuples under a given error metric. For a variety of different
error metrics, we devise efficient algorithms that construct optimal or near
optimal B-term histogram and wavelet synopses. This requires careful analysis
of the structure of the probability distributions, and novel extensions of
known dynamic-programming-based techniques for the deterministic domain. Our
experiments show that this approach clearly outperforms simple ideas, such as
building summaries for samples drawn from the data distribution, while taking
equal or less time.
</summary>
    <author>
      <name>Graham Cormode</name>
    </author>
    <author>
      <name>Minos Garofalakis</name>
    </author>
    <link href="http://arxiv.org/abs/0806.1071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4627v2</id>
    <updated>2008-10-21T14:44:17Z</updated>
    <published>2008-06-30T15:31:26Z</published>
    <title>SP2Bench: A SPARQL Performance Benchmark</title>
    <summary>  Recently, the SPARQL query language for RDF has reached the W3C
recommendation status. In response to this emerging standard, the database
community is currently exploring efficient storage techniques for RDF data and
evaluation strategies for SPARQL queries. A meaningful analysis and comparison
of these approaches necessitates a comprehensive and universal benchmark
platform. To this end, we have developed SP^2Bench, a publicly available,
language-specific SPARQL performance benchmark. SP^2Bench is settled in the
DBLP scenario and comprises both a data generator for creating arbitrarily
large DBLP-like documents and a set of carefully designed benchmark queries.
The generated documents mirror key characteristics and social-world
distributions encountered in the original DBLP data set, while the queries
implement meaningful requests on top of this data, covering a variety of SPARQL
operator constellations and RDF access patterns. As a proof of concept, we
apply SP^2Bench to existing engines and discuss their strengths and weaknesses
that follow immediately from the benchmark results.
</summary>
    <author>
      <name>Michael Schmidt</name>
    </author>
    <author>
      <name>Thomas Hornung</name>
    </author>
    <author>
      <name>Georg Lausen</name>
    </author>
    <author>
      <name>Christoph Pinkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference paper to appear in Proc. ICDE'09</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.4627v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4627v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4703v2</id>
    <updated>2008-07-24T08:24:57Z</updated>
    <published>2008-06-28T16:24:03Z</published>
    <title>Challenging More Updates: Towards Anonymous Re-publication of Fully
  Dynamic Datasets</title>
    <summary>  Most existing anonymization work has been done on static datasets, which have
no update and need only one-time publication. Recent studies consider
anonymizing dynamic datasets with external updates: the datasets are updated
with record insertions and/or deletions. This paper addresses a new problem:
anonymous re-publication of datasets with internal updates, where the attribute
values of each record are dynamically updated. This is an important and
challenging problem for attribute values of records are updating frequently in
practice and existing methods are unable to deal with such a situation.
  We initiate a formal study of anonymous re-publication of dynamic datasets
with internal updates, and show the invalidation of existing methods. We
introduce theoretical definition and analysis of dynamic datasets, and present
a general privacy disclosure framework that is applicable to all anonymous
re-publication problems. We propose a new counterfeited generalization
principle alled m-Distinct to effectively anonymize datasets with both external
updates and internal updates. We also develop an algorithm to generalize
datasets to meet m-Distinct. The experiments conducted on real-world data
demonstrate the effectiveness of the proposed solution.
</summary>
    <author>
      <name>Feng Li</name>
    </author>
    <author>
      <name>Shuigeng Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/0806.4703v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4703v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4749v1</id>
    <updated>2008-06-29T11:38:06Z</updated>
    <published>2008-06-29T11:38:06Z</published>
    <title>Nested Ordered Sets and their Use for Data Modelling</title>
    <summary>  In this paper we present a new approach to data modelling, called the
concept-oriented model (CoM), and describe its main features and
characteristics including data semantics and operations. The distinguishing
feature of this model is that it is based on the formalism of nested ordered
sets where any element participates in two structures simultaneously:
hierarchical (nested) and multi-dimensional (ordered). An element of the model
is postulated to consist of two parts, called identity and entity, and the
whole approach can be naturally broken into two branches: identity modelling
and entity modelling. We also propose a new query language with the main
construct, called concept, defined as a pair of two classes: identity class and
entity class. We describe how its operations of projection, de-projection and
product can be used to solve typical data modelling tasks.
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.4749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4920v1</id>
    <updated>2008-06-30T15:23:20Z</updated>
    <published>2008-06-30T15:23:20Z</published>
    <title>Conception et Evaluation de XQuery dans une architecture de médiation
  "Tout-XML"</title>
    <summary>  XML has emerged as the leading language for representing and exchanging data
not only on the Web, but also in general in the enterprise. XQuery is emerging
as the standard query language for XML. Thus, tools are required to mediate
between XML queries and heterogeneous data sources to integrate data in XML.
This paper presents the XMedia mediator, a unique tool for integrating and
querying disparate heterogeneous information as unified XML views. It describes
the mediator architecture and focuses on the unique distributed query
processing technology implemented in this component. Query evaluation is based
on an original XML algebra simply extending classical operators to process
tuples of tree elements. Further, we present a set of performance evaluation on
a relational benchmark, which leads to discuss possible performance
enhancements.
</summary>
    <author>
      <name>Tuyet-Tram Dang-Ngoc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Gardarin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Revue ISI (Integration de syst\`emes d'information) : Num\'ero
  sp\'ecial sur les Bases de Donn\'ees Semi-structur\'ees 8, 5-6 (2003) 11-25</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.4920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1734v5</id>
    <updated>2008-10-07T00:45:02Z</updated>
    <published>2008-07-10T20:26:31Z</published>
    <title>Faster Sequential Search with a Two-Pass Dynamic-Time-Warping Lower
  Bound</title>
    <summary>  The Dynamic Time Warping (DTW) is a popular similarity measure between time
series. The DTW fails to satisfy the triangle inequality and its computation
requires quadratic time. Hence, to find closest neighbors quickly, we use
bounding techniques. We can avoid most DTW computations with an inexpensive
lower bound (LB_Keogh). We compare LB_Keogh with a tighter lower bound
(LB_Improved). We find that LB_Improved-based search is faster for sequential
search. As an example, our approach is 3 times faster over random-walk and
shape time series. We also review some of the mathematical properties of the
DTW. We derive a tight triangle inequality for the DTW. We show that the DTW
becomes the l_1 distance when time series are separated by a constant.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/0807.1734v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1734v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.2972v1</id>
    <updated>2008-07-18T14:12:02Z</updated>
    <published>2008-07-18T14:12:02Z</published>
    <title>DescribeX: A Framework for Exploring and Querying XML Web Collections</title>
    <summary>  This thesis introduces DescribeX, a powerful framework that is capable of
describing arbitrarily complex XML summaries of web collections, providing
support for more efficient evaluation of XPath workloads. DescribeX permits the
declarative description of document structure using all axes and language
constructs in XPath, and generalizes many of the XML indexing and summarization
approaches in the literature. DescribeX supports the construction of
heterogeneous summaries where different document elements sharing a common
structure can be declaratively defined and refined by means of path regular
expressions on axes, or axis path regular expression (AxPREs). DescribeX can
significantly help in the understanding of both the structure of complex,
heterogeneous XML collections and the behaviour of XPath queries evaluated on
them.
  Experimental results demonstrate the scalability of DescribeX summary
refinements and stabilizations (the key enablers for tailoring summaries) with
multi-gigabyte web collections. A comparative study suggests that using a
DescribeX summary created from a given workload can produce query evaluation
times orders of magnitude better than using existing summaries. DescribeX's
light-weight approach of combining summaries with a file-at-a-time XPath
processor can be a very competitive alternative, in terms of performance, to
conventional fully-fledged XML query engines that provide DB-like functionality
such as security, transaction processing, and native storage.
</summary>
    <author>
      <name>Flavio Rizzolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, University of Toronto, 2008, 163 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.2972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.2972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.0116v1</id>
    <updated>2008-08-31T12:24:38Z</updated>
    <published>2008-08-31T12:24:38Z</published>
    <title>Toward Expressive and Scalable Sponsored Search Auctions</title>
    <summary>  Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.
</summary>
    <author>
      <name>David J. Martin</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Joseph Y. Halpern</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDE.2008.4497432</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDE.2008.4497432" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 13 figures, ICDE 2008</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">David J. Martin, Johannes Gehrke, and Joseph Y. Halpern. Toward
  Expressive and Scalable Sponsored Search Auctions. In Proceedings of the 24th
  IEEE International Conference on Data Engineering, pages 237--246. April 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.0116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.0116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1551v2</id>
    <updated>2009-02-19T11:56:52Z</updated>
    <published>2008-09-09T15:04:52Z</published>
    <title>Consistent Query Answers in the Presence of Universal Constraints</title>
    <summary>  The framework of consistent query answers and repairs has been introduced to
alleviate the impact of inconsistent data on the answers to a query. A repair
is a minimally different consistent instance and an answer is consistent if it
is present in every repair. In this article we study the complexity of
consistent query answers and repair checking in the presence of universal
constraints.
  We propose an extended version of the conflict hypergraph which allows to
capture all repairs w.r.t. a set of universal constraints. We show that repair
checking is in PTIME for the class of full tuple-generating dependencies and
denial constraints, and we present a polynomial repair algorithm. This
algorithm is sound, i.e. always produces a repair, but also complete, i.e.
every repair can be constructed. Next, we present a polynomial-time algorithm
computing consistent answers to ground quantifier-free queries in the presence
of denial constraints, join dependencies, and acyclic full-tuple generating
dependencies. Finally, we show that extending the class of constraints leads to
intractability. For arbitrary full tuple-generating dependencies consistent
query answering becomes coNP-complete. For arbitrary universal constraints
consistent query answering is \Pi_2^p-complete and repair checking
coNP-complete.
</summary>
    <author>
      <name>Slawomir Staworko</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Information Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1551v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1551v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1963v1</id>
    <updated>2008-09-11T11:59:00Z</updated>
    <published>2008-09-11T11:59:00Z</published>
    <title>Materialized View Selection by Query Clustering in XML Data Warehouses</title>
    <summary>  XML data warehouses form an interesting basis for decision-support
applications that exploit complex data. However, native XML database management
systems currently bear limited performances and it is necessary to design
strategies to optimize them. In this paper, we propose an automatic strategy
for the selection of XML materialized views that exploits a data mining
technique, more precisely the clustering of the query workload. To validate our
strategy, we implemented an XML warehouse modeled along the XCube
specifications. We executed a workload of XQuery decision-support queries on
this warehouse, with and without using our strategy. Our experimental results
demonstrate its efficiency, even when queries are complex.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Multiconference on Computer Science and
  Information Technology (CSIT 06), Amman : Jordanie (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.1963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1965v1</id>
    <updated>2008-09-11T12:01:34Z</updated>
    <published>2008-09-11T12:01:34Z</published>
    <title>Dynamic index selection in data warehouses</title>
    <summary>  Analytical queries defined on data warehouses are complex and use several
join operations that are very costly, especially when run on very large data
volumes. To improve response times, data warehouse administrators casually use
indexing techniques. This task is nevertheless complex and fastidious. In this
paper, we present an automatic, dynamic index selection method for data
warehouses that is based on incremental frequent itemset mining from a given
query workload. The main advantage of this approach is that it helps update the
set of selected indexes when workload evolves instead of recreating it from
scratch. Preliminary experimental results illustrate the efficiency of this
approach, both in terms of performance enhancement and overhead.
</summary>
    <author>
      <name>Stéphane Azefack</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Conference on Innovations in Information
  Technology (Innovations 07), Dubai : \'Emirats arabes unis (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.1965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2686v1</id>
    <updated>2008-09-16T11:42:42Z</updated>
    <published>2008-09-16T11:42:42Z</published>
    <title>An MAS-Based ETL Approach for Complex Data</title>
    <summary>  In a data warehousing process, the phase of data integration is crucial. Many
methods for data integration have been published in the literature. However,
with the development of the Internet, the availability of various types of data
(images, texts, sounds, videos, databases...) has increased, and structuring
such data is a difficult task. We name these data, which may be structured or
unstructured, "complex data". In this paper, we propose a new approach for
complex data integration, based on a Multi-Agent System (MAS), in association
to a data warehousing approach. Our objective is to take advantage of the MAS
to perform the integration phase for complex data. We indeed consider the
different tasks of the data integration process as services offered by agents.
To validate this approach, we have actually developed an MAS for complex data
integration.
</summary>
    <author>
      <name>Omar Boussaïd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Fadila Bentayeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 10th ISPE International Conference on Concurrent Engineering:
  Research and Applications (CE 03), Madeira : Portugal (2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2687v1</id>
    <updated>2008-09-16T11:44:39Z</updated>
    <published>2008-09-16T11:44:39Z</published>
    <title>Frequent itemsets mining for database auto-administration</title>
    <summary>  With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The aim of auto-administrative systems is to
administrate and adapt themselves automatically without loss (or even with a
gain) in performance. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. This idea nevertheless
remains a very promising approach, notably in the field of data warehousing,
where queries are very heterogeneous and cannot be interpreted easily. The aim
of this study is to search for a way of extracting useful knowledge from stored
data themselves to automatically apply performance optimization techniques, and
more particularly indexing techniques. We have designed a tool that extracts
frequent itemsets from a given workload to compute an index configuration that
helps optimizing data access time. The experiments we performed showed that the
index configurations generated by our tool allowed performance gains of 15% to
25% on a test database and a test data warehouse.
</summary>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Le Gruenwald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 7th International Database Engineering and Application Symposium
  (IDEAS 03), Hong-Kong : Chine (2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2688v1</id>
    <updated>2008-09-16T11:47:58Z</updated>
    <published>2008-09-16T11:47:58Z</published>
    <title>A Complex Data Warehouse for Personalized, Anticipative Medicine</title>
    <summary>  With the growing use of new technologies, healthcare is nowadays undergoing
significant changes. Information-based medicine has to exploit medical
decision-support systems and requires the analysis of various, heterogeneous
data, such as patient records, medical images, biological analysis results,
etc. In this paper, we present the design of the complex data warehouse
relating to high-level athletes. It is original in two ways. First, it is aimed
at storing complex medical data. Second, it is designed to allow innovative and
quite different kinds of analyses to support: (1) personalized and anticipative
medicine (in opposition to curative medicine) for well-identified patients; (2)
broad-band statistical studies over a given population of patients.
Furthermore, the system includes data relating to several medical fields. It is
also designed to be evolutionary to take into account future advances in
medical research.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Emerson Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 17th Information Resources Management Association International
  Conference (IRMA 06), Wahsington, DC : \'Etats-Unis d'Am\'erique (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2691v1</id>
    <updated>2008-09-16T12:12:15Z</updated>
    <published>2008-09-16T12:12:15Z</published>
    <title>Expressing OLAP operators with the TAX XML algebra</title>
    <summary>  With the rise of XML as a standard for representing business data, XML data
warehouses appear as suitable solutions for Web-based decision-support
applications. In this context, it is necessary to allow OLAP analyses over XML
data cubes (XOLAP). Thus, XQuery extensions are needed. To help define a formal
framework and allow much-needed performance optimizations on analytical queries
expressed in XQuery, having an algebra at one's disposal is desirable. However,
XOLAP approaches and algebras from the literature still largely rely on the
relational model and/or only feature a small number of OLAP operators. In
opposition, we propose in this paper to express a broad set of OLAP operators
with the TAX XML algebra.
</summary>
    <author>
      <name>Marouane Hachicha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 3rd International Workshop on Database Technologies for Handling
  XML Information on the Web (DataX-EDBT 08), Nantes : France (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.4809v1</id>
    <updated>2008-10-27T13:41:48Z</updated>
    <published>2008-10-27T13:41:48Z</published>
    <title>XQuery Join Graph Isolation</title>
    <summary>  A purely relational account of the true XQuery semantics can turn any
relational database system into an XQuery processor. Compiling nested
expressions of the fully compositional XQuery language, however, yields odd
algebraic plan shapes featuring scattered distributions of join operators that
currently overwhelm commercial SQL query optimizers.
  This work rewrites such plans before submission to the relational database
back-end. Once cast into the shape of join graphs, we have found off-the-shelf
relational query optimizers--the B-tree indexing subsystem and join tree
planner, in particular--to cope and even be autonomously capable of
"reinventing" advanced processing strategies that have originally been devised
specifically for the XQuery domain, e.g., XPath step reordering, axis reversal,
and path stitching. Performance assessments provide evidence that relational
query engines are among the most versatile and efficient XQuery processors
readily available today.
</summary>
    <author>
      <name>T. Grust</name>
    </author>
    <author>
      <name>M. Mayr</name>
    </author>
    <author>
      <name>J. Rittinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of a paper published in the ICDE 2009 proceedings
  (13 pages, 13 figures, 9 tables)</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.4809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.4809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.5578v1</id>
    <updated>2008-10-30T21:12:25Z</updated>
    <published>2008-10-30T21:12:25Z</published>
    <title>Anonymizing Graphs</title>
    <summary>  Motivated by recently discovered privacy attacks on social networks, we study
the problem of anonymizing the underlying graph of interactions in a social
network. We call a graph (k,l)-anonymous if for every node in the graph there
exist at least k other nodes that share at least l of its neighbors. We
consider two combinatorial problems arising from this notion of anonymity in
graphs. More specifically, given an input graph we ask for the minimum number
of edges to be added so that the graph becomes (k,l)-anonymous. We define two
variants of this minimization problem and study their properties. We show that
for certain values of k and l the problems are polynomial-time solvable, while
for others they become NP-hard. Approximation algorithms for the latter cases
are also given.
</summary>
    <author>
      <name>Tomas Feder</name>
    </author>
    <author>
      <name>Shubha U. Nabar</name>
    </author>
    <author>
      <name>Evimaria Terzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.5578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.5578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.0741v1</id>
    <updated>2008-11-05T15:00:32Z</updated>
    <published>2008-11-05T15:00:32Z</published>
    <title>Data Mining-based Fragmentation of XML Data Warehouses</title>
    <summary>  With the multiplication of XML data sources, many XML data warehouse models
have been proposed to handle data heterogeneity and complexity in a way
relational data warehouses fail to achieve. However, XML-native database
systems currently suffer from limited performances, both in terms of manageable
data volume and response time. Fragmentation helps address both these issues.
Derived horizontal fragmentation is typically used in relational data
warehouses and can definitely be adapted to the XML context. However, the
number of fragments produced by classical algorithms is difficult to control.
In this paper, we propose the use of a k-means-based fragmentation approach
that allows to master the number of fragments through its $k$ parameter. We
experimentally compare its efficiency to classical derived horizontal
fragmentation algorithms adapted to XML data warehouses and show its
superiority.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM 11th International Workshop on Data Warehousing and OLAP
  (CIKM/DOLAP 08), Napa Valley : \'Etats-Unis d'Am\'erique (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.0741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.0741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.1083v1</id>
    <updated>2008-11-07T05:08:41Z</updated>
    <published>2008-11-07T05:08:41Z</published>
    <title>A role-free approach to indexing large RDF data sets in secondary memory
  for efficient SPARQL evaluation</title>
    <summary>  Massive RDF data sets are becoming commonplace. RDF data is typically
generated in social semantic domains (such as personal information management)
wherein a fixed schema is often not available a priori. We propose a simple
Three-way Triple Tree (TripleT) secondary-memory indexing technique to
facilitate efficient SPARQL query evaluation on such data sets. The novelty of
TripleT is that (1) the index is built over the atoms occurring in the data
set, rather than at a coarser granularity, such as whole triples occurring in
the data set; and (2) the atoms are indexed regardless of the roles (i.e.,
subjects, predicates, or objects) they play in the triples of the data set. We
show through extensive empirical evaluation that TripleT exhibits multiple
orders of magnitude improvement over the state of the art on RDF indexing, in
terms of both storage and query processing costs.
</summary>
    <author>
      <name>George H. L. Fletcher</name>
    </author>
    <author>
      <name>Peter W. Beck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.1083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.1083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2250v2</id>
    <updated>2009-06-08T19:47:55Z</updated>
    <published>2008-11-14T01:47:14Z</published>
    <title>Semantics and Evaluation of Top-k Queries in Probabilistic Databases</title>
    <summary>  We study here fundamental issues involved in top-k query evaluation in
probabilistic databases. We consider simple probabilistic databases in which
probabilities are associated with individual tuples, and general probabilistic
databases in which, additionally, exclusivity relationships between tuples can
be represented. In contrast to other recent research in this area, we do not
limit ourselves to injective scoring functions. We formulate three intuitive
postulates that the semantics of top-k queries in probabilistic databases
should satisfy, and introduce a new semantics, Global-Topk, that satisfies
those postulates to a large degree. We also show how to evaluate queries under
the Global-Topk semantics. For simple databases we design dynamic-programming
based algorithms, and for general databases we show polynomial-time reductions
to the simple cases. For example, we demonstrate that for a fixed k the time
complexity of top-k query evaluation is as low as linear, under the assumption
that probabilistic databases are simple and scoring functions are injective.
</summary>
    <author>
      <name>Xi Zhang</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">60 pages, section 4.4 added, section 6 added, typos corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.2250v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2250v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2049v1</id>
    <updated>2008-12-10T23:20:17Z</updated>
    <published>2008-12-10T23:20:17Z</published>
    <title>Consensus Answers for Queries over Probabilistic Databases</title>
    <summary>  We address the problem of finding a "best" deterministic query answer to a
query over a probabilistic database. For this purpose, we propose the notion of
a consensus world (or a consensus answer) which is a deterministic world
(answer) that minimizes the expected distance to the possible worlds (answers).
This problem can be seen as a generalization of the well-studied inconsistent
information aggregation problems (e.g. rank aggregation) to probabilistic
databases. We consider this problem for various types of queries including SPJ
queries, \Topk queries, group-by aggregate queries, and clustering. For
different distance metrics, we obtain polynomial time optimal or approximation
algorithms for computing the consensus answers (or prove NP-hardness). Most of
our results are for a general probabilistic database model, called {\em and/xor
tree model}, which significantly generalizes previous probabilistic database
models like x-tuples and block-independent disjoint models, and is of
independent interest.
</summary>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Amol Deshpande</name>
    </author>
    <link href="http://arxiv.org/abs/0812.2049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2874v1</id>
    <updated>2008-12-15T18:25:51Z</updated>
    <published>2008-12-15T18:25:51Z</published>
    <title>A Data Model for Integrating Heterogeneous Medical Data in the
  Health-e-Child Project</title>
    <summary>  There has been much research activity in recent times about providing the
data infrastructures needed for the provision of personalised healthcare. In
particular the requirement of integrating multiple, potentially distributed,
heterogeneous data sources in the medical domain for the use of clinicians has
set challenging goals for the healthgrid community. The approach advocated in
this paper surrounds the provision of an Integrated Data Model plus links
to/from ontologies to homogenize biomedical (from genomic, through cellular,
disease, patient and population-related) data in the context of the EC
Framework 6 Health-e-Child project. Clinical requirements are identified, the
design approach in constructing the model is detailed and the integrated model
described in the context of examples taken from that project. Pointers are
given to future work relating the model to medical ontologies and challenges to
the use of fully integrated models and ontologies are identified.
</summary>
    <author>
      <name>Andrew Branson</name>
    </author>
    <author>
      <name>Tamas Hauer</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Dmitry Rogulin</name>
    </author>
    <author>
      <name>Jetendr Shamdasani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 1 table. Proceedings the 6th HealthGrid Int.
  Conference (HG08)</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.2874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2879v1</id>
    <updated>2008-12-15T18:34:44Z</updated>
    <published>2008-12-15T18:34:44Z</published>
    <title>Ontology Assisted Query Reformulation Using Semantic and Assertion
  Capabilities of OWL-DL Ontologies</title>
    <summary>  End users of recent biomedical information systems are often unaware of the
storage structure and access mechanisms of the underlying data sources and can
require simplified mechanisms for writing domain specific complex queries. This
research aims to assist users and their applications in formulating queries
without requiring complete knowledge of the information structure of underlying
data sources. To achieve this, query reformulation techniques and algorithms
have been developed that can interpret ontology-based search criteria and
associated domain knowledge in order to reformulate a relational query. These
query reformulation algorithms exploit the semantic relationships and assertion
capabilities of OWL-DL based domain ontologies for query reformulation. In this
paper, this approach is applied to the integrated database schema of the EU
funded Health-e-Child (HeC) project with the aim of providing ontology assisted
query reformulation techniques to simplify the global access that is needed to
millions of medical records across the UK and Europe.
</summary>
    <author>
      <name>Kamran Munir</name>
    </author>
    <author>
      <name>Mohammed Odeh</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures. Proceedings of the 12th International Database
  Engineering &amp; Applications Symposium (Ideas2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.2879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3788v2</id>
    <updated>2009-01-26T12:52:36Z</updated>
    <published>2008-12-19T13:51:57Z</published>
    <title>Foundations of SPARQL Query Optimization</title>
    <summary>  The SPARQL query language is a recent W3C standard for processing RDF data, a
format that has been developed to encode information in a machine-readable way.
We investigate the foundations of SPARQL query optimization and (a) provide
novel complexity results for the SPARQL evaluation problem, showing that the
main source of complexity is operator OPTIONAL alone; (b) propose a
comprehensive set of algebraic query rewriting rules; (c) present a framework
for constraint-based SPARQL optimization based upon the well-known chase
procedure for Conjunctive Query minimization. In this line, we develop two
novel termination conditions for the chase. They subsume the strongest
conditions known so far and do not increase the complexity of the recognition
problem, thus making a larger class of both Conjunctive and SPARQL queries
amenable to constraint-based optimization. Our results are of immediate
practical interest and might empower any SPARQL query optimizer.
</summary>
    <author>
      <name>Michael Schmidt</name>
    </author>
    <author>
      <name>Michael Meier</name>
    </author>
    <author>
      <name>Georg Lausen</name>
    </author>
    <link href="http://arxiv.org/abs/0812.3788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.2224v4</id>
    <updated>2010-08-02T14:02:18Z</updated>
    <published>2009-01-15T10:41:28Z</published>
    <title>Concept-Oriented Model and Query Language</title>
    <summary>  We describe a new approach to data modeling, called the concept-oriented
model (COM), and a novel concept-oriented query language (COQL). The model is
based on three principles: duality principle postulates that any element is a
couple consisting of one identity and one entity, inclusion principle
postulates that any element has a super-element, and order principle assumes
that any element has a number of greater elements within a partially ordered
set. Concept-oriented query language is based on a new data modeling construct,
called concept, inclusion relation between concepts, and concept partial
ordering in which greater concepts are represented by their field types. It is
demonstrated how COM and COQL can be used to solve three general data modeling
tasks: logical navigation, multidimensional analysis and inference. Logical
navigation is based on two operations of projection and de-projection.
Multidimensional analysis uses product operation for producing a cube from
level concepts chosen along the chosen dimension paths. Inference is defined as
a two-step procedure where input constraints are first propagated downwards
using de-projection and then the constrained result is propagated upwards using
projection.
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 18 figures, Submitted to ACM Transactions on Database
  Systems (TODS)</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.2224v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.2224v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.3751v7</id>
    <updated>2016-07-29T20:24:07Z</updated>
    <published>2009-01-23T19:01:06Z</published>
    <title>Sorting improves word-aligned bitmap indexes</title>
    <summary>  Bitmap indexes must be compressed to reduce input/output costs and minimize
CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use
techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. These techniques are sensitive to the order of the rows: a
simple lexicographical sort can divide the index size by 9 and make indexes
several times faster. We investigate row-reordering heuristics. Simply
permuting the columns of the table can increase the sorting efficiency by 40%.
Secondary contributions include efficient algorithms to construct and aggregate
bitmaps. The effect of word length is also reviewed by constructing 16-bit,
32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are
slightly faster than 32-bit indexes despite being nearly twice as large.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.datak.2009.08.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.datak.2009.08.006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Data &amp; Knowledge Engineering, Volume 69, Issue 1, 2010, Pages 3-28</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.3751v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.3751v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.3984v2</id>
    <updated>2009-05-06T13:28:13Z</updated>
    <published>2009-01-26T12:09:38Z</published>
    <title>Stop the Chase</title>
    <summary>  The chase procedure, an algorithm proposed 25+ years ago to fix constraint
violations in database instances, has been successfully applied in a variety of
contexts, such as query optimization, data exchange, and data integration. Its
practicability, however, is limited by the fact that - for an arbitrary set of
constraints - it might not terminate; even worse, chase termination is an
undecidable problem in general. In response, the database community has
proposed sufficient restrictions on top of the constraints that guarantee chase
termination on any database instance. In this paper, we propose a novel
sufficient termination condition, called inductive restriction, which strictly
generalizes previous conditions, but can be checked as efficiently.
Furthermore, we motivate and study the problem of data-dependent chase
termination and, as a key result, present sufficient termination conditions
w.r.t. fixed instances. They are strictly more general than inductive
restriction and might guarantee termination although the chase does not
terminate in the general case.
</summary>
    <author>
      <name>Michael Meier</name>
    </author>
    <author>
      <name>Michael Schmidt</name>
    </author>
    <author>
      <name>Georg Lausen</name>
    </author>
    <link href="http://arxiv.org/abs/0901.3984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.3984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.2504v1</id>
    <updated>2009-02-15T18:53:19Z</updated>
    <published>2009-02-15T18:53:19Z</published>
    <title>Hyperset Approach to Semi-structured Databases and the Experimental
  Implementation of the Query Language Delta</title>
    <summary>  This thesis presents practical suggestions towards the implementation of the
hyperset approach to semi-structured databases and the associated query
language Delta. This work can be characterised as part of a top-down approach
to semi-structured databases, from theory to practice. The main original part
of this work consisted in implementation of the hyperset Delta query language
to semi-structured databases, including worked example queries. In fact, the
goal was to demonstrate the practical details of this approach and language.
The required development of an extended, practical version of the language
based on the existing theoretical version, and the corresponding operational
semantics. Here we present detailed description of the most essential steps of
the implementation. Another crucial problem for this approach was to
demonstrate how to deal in reality with the concept of the equality relation
between (hyper)sets, which is computationally realised by the bisimulation
relation. In fact, this expensive procedure, especially in the case of
distributed semi-structured data, required some additional theoretical
considerations and practical suggestions for efficient implementation. To this
end the 'local/global' strategy for computing the bisimulation relation over
distributed semi-structured data was developed and its efficiency was
experimentally confirmed.
</summary>
    <author>
      <name>Richard Molyneux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report (PhD thesis), University of Liverpool, England</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.2504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.3532v2</id>
    <updated>2009-03-23T23:34:29Z</updated>
    <published>2009-02-20T08:01:54Z</published>
    <title>Relational Lattice Foundation for Algebraic Logic</title>
    <summary>  Relational Lattice is a succinct mathematical model for Relational Algebra.
It reduces the set of six classic relational algebra operators to two: natural
join and inner union. In this paper we push relational lattice theory in two
directions. First, we uncover a pair of complementary lattice operators, and
organize the model into a bilattice of four operations and four distinguished
constants. We take a notice a peculiar way bilattice symmetry is broken. Then,
we give axiomatic introduction of unary negation operation and prove several
laws, including double negation and De Morgan. Next we reduce the model back to
two basic binary operations and twelve axioms, and exhibit a convincing
argument that the resulting system is complete in model-theoretic sense. The
final parts of the paper casts relational lattice perspective onto database
dependency theory and into cylindric algebras.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.3532v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.3532v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4042v1</id>
    <updated>2009-02-24T00:43:56Z</updated>
    <published>2009-02-24T00:43:56Z</published>
    <title>Algebraic operators for querying pattern bases</title>
    <summary>  The objectives of this research work which is intimately related to pattern
discovery and management are threefold: (i) handle the problem of pattern
manipulation by defining operations on patterns, (ii) study the problem of
enriching and updating a pattern set (e.g., concepts, rules) when changes occur
in the user's needs and the input data (e.g., object/attribute insertion or
elimination, taxonomy utilization), and (iii) approximate a "presumed" concept
using a related pattern space so that patterns can augment data with knowledge.
To conduct our work, we use formal concept analysis (FCA) as a framework for
pattern discovery and management and we take a joint database-FCA perspective
by defining operators similar in spirit to relational algebra operators,
investigating approximation in concept lattices and exploiting existing work
related to operations on contexts and lattices to formalize such operators.
</summary>
    <author>
      <name>Rokia Missaoui</name>
    </author>
    <author>
      <name>Leonard Kwuida</name>
    </author>
    <author>
      <name>Mohamed Quafafou</name>
    </author>
    <author>
      <name>Jean Vaillancourt</name>
    </author>
    <link href="http://arxiv.org/abs/0902.4042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4535v1</id>
    <updated>2009-02-26T09:06:29Z</updated>
    <published>2009-02-26T09:06:29Z</published>
    <title>Electronical Health Record's Systems. Interoperability</title>
    <summary>  Understanding the importance that the electronic medical health records
system has, with its various structural types and grades, has led to the
elaboration of a series of standards and quality control methods, meant to
control its functioning. In time, the electronic health records system has
evolved along with the medical data change of structure. Romania has not yet
managed to fully clarify this concept, various definitions still being
encountered, such as "Patient's electronic chart", "Electronic health file". A
slow change from functional interoperability (OSI level 6) to semantic
interoperability (level 7) is being aimed at the moment. This current article
will try to present the main electronic files models, from a functional
interoperability system's possibility to be created perspective.
</summary>
    <author>
      <name>Simona Angela Apostol</name>
    </author>
    <author>
      <name>Cosmin Catu</name>
    </author>
    <author>
      <name>Corina Vernic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series 6 (2008), 7-20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.4535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.0625v1</id>
    <updated>2009-03-03T21:21:02Z</updated>
    <published>2009-03-03T21:21:02Z</published>
    <title>Leveraging Discarded Samples for Tighter Estimation of Multiple-Set
  Aggregates</title>
    <summary>  Many datasets such as market basket data, text or hypertext documents, and
sensor observations recorded in different locations or time periods, are
modeled as a collection of sets over a ground set of keys. We are interested in
basic aggregates such as the weight or selectivity of keys that satisfy some
selection predicate defined over keys' attributes and membership in particular
sets. This general formulation includes basic aggregates such as the Jaccard
coefficient, Hamming distance, and association rules.
  On massive data sets, exact computation can be inefficient or infeasible.
Sketches based on coordinated random samples are classic summaries that support
approximate query processing.
  Queries are resolved by generating a sketch (sample) of the union of sets
used in the predicate from the sketches these sets and then applying an
estimator to this union-sketch.
  We derive novel tighter (unbiased) estimators that leverage sampled keys that
are present in the union of applicable sketches but excluded from the union
sketch. We establish analytically that our estimators dominate estimators
applied to the union-sketch for {\em all queries and data sets}. Empirical
evaluation on synthetic and real data reveals that on typical applications we
can expect a 25%-4 fold reduction in estimation error.
</summary>
    <author>
      <name>Edith Cohen</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.0625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.0625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.0682v1</id>
    <updated>2009-03-04T09:36:29Z</updated>
    <published>2009-03-04T09:36:29Z</published>
    <title>Preserving Individual Privacy in Serial Data Publishing</title>
    <summary>  While previous works on privacy-preserving serial data publishing consider
the scenario where sensitive values may persist over multiple data releases, we
find that no previous work has sufficient protection provided for sensitive
values that can change over time, which should be the more common case. In this
work we propose to study the privacy guarantee for such transient sensitive
values, which we call the global guarantee. We formally define the problem for
achieving this guarantee and derive some theoretical properties for this
problem. We show that the anonymized group sizes used in the data anonymization
is a key factor in protecting individual privacy in serial publication. We
propose two strategies for anonymization targeting at minimizing the average
group size and the maximum group size. Finally, we conduct experiments on a
medical dataset to show that our method is highly efficient and also produces
published data of very high utility.
</summary>
    <author>
      <name>Raymond Chi-Wing Wong</name>
    </author>
    <author>
      <name>Ada Wai-Chee Fu</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Yabo Xu</name>
    </author>
    <link href="http://arxiv.org/abs/0903.0682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.0682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1680v2</id>
    <updated>2010-10-05T09:38:02Z</updated>
    <published>2009-03-10T04:01:15Z</published>
    <title>Faceted Exploration of Emerging Resource Spaces</title>
    <summary>  Humans have the ability to regcognize the real world from different facets.
Faceted exploration is a mechanism for browsing and understanding large-scale
resources in information network by multiple facets. This paper proposes an
Emerging Resource Space Model, whose schema is a partially ordered set of
concepts with subclassOf relation and each resource is categorized by multiple
concepts. Emering Resource Space (ERS) is a class of resources characterized by
a concept set. ERSes compose a lattice (ERSL) via concept association. A series
of exploration operations is proposed to guide users to explore through ERSL
with more demanding and richer semantics than current faceted navigation. To
fulfill instant response during faceted exploration, we devise an efficient
algorithm for mining and indexing ERSL. The proposed model can effectively
support faceted exploration in various applications from personal information
management to large-scale information sharing.
</summary>
    <author>
      <name>H. Zhuge</name>
    </author>
    <author>
      <name>C. He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.1680v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1680v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.8; H.3.1; H.3.7; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1953v1</id>
    <updated>2009-03-11T11:34:20Z</updated>
    <published>2009-03-11T11:34:20Z</published>
    <title>Laconic schema mappings: computing core universal solutions by means of
  SQL queries</title>
    <summary>  We present a new method for computing core universal solutions in data
exchange settings specified by source-to-target dependencies, by means of SQL
queries. Unlike previously known algorithms, which are recursive in nature, our
method can be implemented directly on top of any DBMS. Our method is based on
the new notion of a laconic schema mapping. A laconic schema mapping is a
schema mapping for which the canonical universal solution is the core universal
solution. We give a procedure by which every schema mapping specified by FO s-t
tgds can be turned into a laconic schema mapping specified by FO s-t tgds that
may refer to a linear order on the domain of the source instance. We show that
our results are optimal, in the sense that the linear order is necessary and
the method cannot be extended to schema mapping involving target constraints.
</summary>
    <author>
      <name>Balder ten Cate</name>
    </author>
    <author>
      <name>Laura Chiticariu</name>
    </author>
    <author>
      <name>Phokion Kolaitis</name>
    </author>
    <author>
      <name>Wang-Chiew Tan</name>
    </author>
    <link href="http://arxiv.org/abs/0903.1953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3317v2</id>
    <updated>2009-06-13T03:57:09Z</updated>
    <published>2009-03-19T12:33:47Z</published>
    <title>Discovering Matching Dependencies</title>
    <summary>  The concept of matching dependencies (mds) is recently pro- posed for
specifying matching rules for object identification. Similar to the functional
dependencies (with conditions), mds can also be applied to various data quality
applications such as violation detection. In this paper, we study the problem
of discovering matching dependencies from a given database instance. First, we
formally define the measures, support and confidence, for evaluating utility of
mds in the given database instance. Then, we study the discovery of mds with
certain utility requirements of support and confidence. Exact algorithms are
developed, together with pruning strategies to improve the time performance.
Since the exact algorithm has to traverse all the data during the computation,
we propose an approximate solution which only use some of the data. A bound of
relative errors introduced by the approximation is also developed. Finally, our
experimental evaluation demonstrates the efficiency of the proposed methods.
</summary>
    <author>
      <name>Shaoxu Song</name>
    </author>
    <author>
      <name>Lei Chen</name>
    </author>
    <link href="http://arxiv.org/abs/0903.3317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.5346v1</id>
    <updated>2009-03-31T00:10:02Z</updated>
    <published>2009-03-31T00:10:02Z</published>
    <title>Cooperative Update Exchange in the Youtopia System</title>
    <summary>  Youtopia is a platform for collaborative management and integration of
relational data. At the heart of Youtopia is an update exchange abstraction:
changes to the data propagate through the system to satisfy user-specified
mappings. We present a novel change propagation model that combines a
deterministic chase with human intervention. The process is fundamentally
cooperative and gives users significant control over how mappings are repaired.
An additional advantage of our model is that mapping cycles can be permitted
without compromising correctness.
  We investigate potential harmful interference between updates in our model;
we introduce two appropriate notions of serializability that avoid such
interference if enforced. The first is very general and related to classical
final-state serializability; the second is more restrictive but highly
practical and related to conflict-serializability. We present an algorithm to
enforce the latter notion. Our algorithm is an optimistic one, and as such may
sometimes require updates to be aborted. We develop techniques for reducing the
number of aborts and we test these experimentally.
</summary>
    <author>
      <name>Lucja Kot</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <link href="http://arxiv.org/abs/0903.5346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.5346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.0682v4</id>
    <updated>2011-05-11T22:39:23Z</updated>
    <published>2009-04-04T05:49:00Z</published>
    <title>Privacy in Search Logs</title>
    <summary>  Search engine companies collect the "database of intentions", the histories
of their users' search queries. These search logs are a gold mine for
researchers. Search engine companies, however, are wary of publishing search
logs in order not to disclose sensitive information. In this paper we analyze
algorithms for publishing frequent keywords, queries and clicks of a search
log. We first show how methods that achieve variants of $k$-anonymity are
vulnerable to active attacks. We then demonstrate that the stronger guarantee
ensured by $\epsilon$-differential privacy unfortunately does not provide any
utility for this problem. We then propose an algorithm ZEALOUS and show how to
set its parameters to achieve $(\epsilon,\delta)$-probabilistic privacy. We
also contrast our analysis of ZEALOUS with an analysis by Korolova et al. [17]
that achieves $(\epsilon',\delta')$-indistinguishability. Our paper concludes
with a large experimental study using real applications where we compare
ZEALOUS and previous work that achieves $k$-anonymity in search log publishing.
Our results show that ZEALOUS yields comparable utility to $k-$anonymity while
at the same time achieving much stronger privacy guarantees.
</summary>
    <author>
      <name>Michaela Goetz</name>
    </author>
    <author>
      <name>Ashwin Machanavajjhala</name>
    </author>
    <author>
      <name>Guozhang Wang</name>
    </author>
    <author>
      <name>Xiaokui Xiao</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <link href="http://arxiv.org/abs/0904.0682v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.0682v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.0942v5</id>
    <updated>2010-07-09T01:34:32Z</updated>
    <published>2009-04-06T14:58:20Z</published>
    <title>Boosting the Accuracy of Differentially-Private Histograms Through
  Consistency</title>
    <summary>  We show that it is possible to significantly improve the accuracy of a
general class of histogram queries while satisfying differential privacy. Our
approach carefully chooses a set of queries to evaluate, and then exploits
consistency constraints that should hold over the noisy output. In a
post-processing phase, we compute the consistent input most likely to have
produced the noisy output. The final output is differentially-private and
consistent, but in addition, it is often much more accurate. We show, both
theoretically and experimentally, that these techniques can be used for
estimating the degree sequence of a graph very precisely, and for computing a
histogram that can support arbitrary range queries accurately.
</summary>
    <author>
      <name>Michael Hay</name>
    </author>
    <author>
      <name>Vibhor Rastogi</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, minor revisions to previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.0942v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.0942v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.2012v1</id>
    <updated>2009-04-13T21:20:57Z</updated>
    <published>2009-04-13T21:20:57Z</published>
    <title>Simplicial Databases</title>
    <summary>  In this paper, we define a category DB, called the category of simplicial
databases, whose objects are databases and whose morphisms are data-preserving
maps. Along the way we give a precise formulation of the category of relational
databases, and prove that it is a full subcategory of DB. We also prove that
limits and colimits always exist in DB and that they correspond to queries such
as select, join, union, etc.
  One feature of our construction is that the schema of a simplicial database
has a natural geometric structure: an underlying simplicial set. The geometry
of a schema is a way of keeping track of relationships between distinct tables,
and can be thought of as a system of foreign keys. The shape of a schema is
generally intuitive (e.g. the schema for round-trip flights is a circle
consisting of an edge from $A$ to $B$ and an edge from $B$ to $A$), and as
such, may be useful for analyzing data.
  We give several applications of our approach, as well as possible advantages
it has over the relational model. We also indicate some directions for further
research.
</summary>
    <author>
      <name>David I. Spivak</name>
    </author>
    <link href="http://arxiv.org/abs/0904.2012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.2012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3310v1</id>
    <updated>2009-04-21T18:33:04Z</updated>
    <published>2009-04-21T18:33:04Z</published>
    <title>FastLMFI: An Efficient Approach for Local Maximal Patterns Propagation
  and Maximal Patterns Superset Checking</title>
    <summary>  Maximal frequent patterns superset checking plays an important role in the
efficient mining of complete Maximal Frequent Itemsets (MFI) and maximal search
space pruning. In this paper we present a new indexing approach, FastLMFI for
local maximal frequent patterns (itemset) propagation and maximal patterns
superset checking. Experimental results on different sparse and dense datasets
show that our work is better than the previous well known progressive focusing
technique. We have also integrated our superset checking approach with an
existing state of the art maximal itemsets algorithm Mafia, and compare our
results with current best maximal itemsets algorithms afopt-max and FP
(zhu)-max. Our results outperform afopt-max and FP (zhu)-max on dense (chess
and mushroom) datasets on almost all support thresholds, which shows the
effectiveness of our approach.
</summary>
    <author>
      <name>Shariq Bashir</name>
    </author>
    <author>
      <name>Abdul Rauf Baig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AICCSA.2006.205130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AICCSA.2006.205130" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, In the proceedings of 4th ACS/IEEE International Conference
  on Computer Systems and Applications 2006, March 8, 2006, Dubai/Sharjah, UAE,
  2006, Page(s) 452-459</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.3310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3316v1</id>
    <updated>2009-04-21T18:49:13Z</updated>
    <published>2009-04-21T18:49:13Z</published>
    <title>Ramp: Fast Frequent Itemset Mining with Efficient Bit-Vector Projection
  Technique</title>
    <summary>  Mining frequent itemset using bit-vector representation approach is very
efficient for dense type datasets, but highly inefficient for sparse datasets
due to lack of any efficient bit-vector projection technique. In this paper we
present a novel efficient bit-vector projection technique, for sparse and dense
datasets. To check the efficiency of our bit-vector projection technique, we
present a new frequent itemset mining algorithm Ramp (Real Algorithm for Mining
Patterns) build upon our bit-vector projection technique. The performance of
the Ramp is compared with the current best (all, maximal and closed) frequent
itemset mining algorithms on benchmark datasets. Different experimental results
on sparse and dense datasets show that mining frequent itemset using Ramp is
faster than the current best algorithms, which show the effectiveness of our
bit-vector projection idea. We also present a new local maximal frequent
itemsets propagation and maximal itemset superset checking approach FastLMFI,
build upon our PBR bit-vector projection technique. Our different computational
experiments suggest that itemset maximality checking using FastLMFI is fast and
efficient than a previous will known progressive focusing approach.
</summary>
    <author>
      <name>Shariq Bashir</name>
    </author>
    <author>
      <name>Abdul Rauf Baig</name>
    </author>
    <link href="http://arxiv.org/abs/0904.3316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3320v1</id>
    <updated>2009-04-21T19:09:57Z</updated>
    <published>2009-04-21T19:09:57Z</published>
    <title>Using Association Rules for Better Treatment of Missing Values</title>
    <summary>  The quality of training data for knowledge discovery in databases (KDD) and
data mining depends upon many factors, but handling missing values is
considered to be a crucial factor in overall data quality. Today real world
datasets contains missing values due to human, operational error, hardware
malfunctioning and many other factors. The quality of knowledge extracted,
learning and decision problems depend directly upon the quality of training
data. By considering the importance of handling missing values in KDD and data
mining tasks, in this paper we propose a novel Hybrid Missing values Imputation
Technique (HMiT) using association rules mining and hybrid combination of
k-nearest neighbor approach. To check the effectiveness of our HMiT missing
values imputation technique, we also perform detail experimental results on
real world datasets. Our results suggest that the HMiT technique is not only
better in term of accuracy but it also take less processing time as compared to
current best missing values imputation technique based on k-nearest neighbor
approach, which shows the effectiveness of our missing values imputation
technique.
</summary>
    <author>
      <name>Shariq Bashir</name>
    </author>
    <author>
      <name>Saad Razzaq</name>
    </author>
    <author>
      <name>Umer Maqbool</name>
    </author>
    <author>
      <name>Sonya Tahir</name>
    </author>
    <author>
      <name>Abdul Rauf Baig</name>
    </author>
    <link href="http://arxiv.org/abs/0904.3320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3321v1</id>
    <updated>2009-04-21T19:16:00Z</updated>
    <published>2009-04-21T19:16:00Z</published>
    <title>Introducing Partial Matching Approach in Association Rules for Better
  Treatment of Missing Values</title>
    <summary>  Handling missing values in training datasets for constructing learning models
or extracting useful information is considered to be an important research task
in data mining and knowledge discovery in databases. In recent years, lot of
techniques are proposed for imputing missing values by considering attribute
relationships with missing value observation and other observations of training
dataset. The main deficiency of such techniques is that, they depend upon
single approach and do not combine multiple approaches, that why they are less
accurate. To improve the accuracy of missing values imputation, in this paper
we introduce a novel partial matching concept in association rules mining,
which shows better results as compared to full matching concept that we
described in our previous work. Our imputation technique combines the partial
matching concept in association rules with k-nearest neighbor approach. Since
this is a hybrid technique, therefore its accuracy is much better than as
compared to those techniques which depend upon single approach. To check the
efficiency of our technique, we also provide detail experimental results on
number of benchmark datasets which show better results as compared to previous
approaches.
</summary>
    <author>
      <name>Shariq Bashir</name>
    </author>
    <author>
      <name>Saad Razzaq</name>
    </author>
    <author>
      <name>Umer Maqbool</name>
    </author>
    <author>
      <name>Sonya Tahir</name>
    </author>
    <author>
      <name>Abdul Rauf Baig</name>
    </author>
    <link href="http://arxiv.org/abs/0904.3321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2657v2</id>
    <updated>2016-03-15T21:52:24Z</updated>
    <published>2009-05-16T05:57:06Z</published>
    <title>Web 2.0 OLAP: From Data Cubes to Tag Clouds</title>
    <summary>  Increasingly, business projects are ephemeral. New Business Intelligence
tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds
are a popular community-driven visualization technique. Hence, we investigate
tag-cloud views with support for OLAP operations such as roll-ups, slices,
dices, clustering, and drill-downs. As a case study, we implemented an
application where users can upload data and immediately navigate through its ad
hoc dimensions. To support social networking, views can be easily shared and
embedded in other Web sites. Algorithmically, our tag-cloud views are
approximate range top-k queries over spontaneous data cubes. We present
experimental evidence that iceberg cuboids provide adequate online
approximations. We benchmark several browser-oblivious tag-cloud layout
optimizations.
</summary>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Robert Godin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-01344-7_5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-01344-7_5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software at https://github.com/lemire/OLAPTagCloud. arXiv admin note:
  substantial text overlap with arXiv:0710.2156</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Business Information Processing Vol. 18, pages
  51-64, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.2657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0684v1</id>
    <updated>2009-06-03T15:13:12Z</updated>
    <published>2009-06-03T15:13:12Z</published>
    <title>New Instability Results for High Dimensional Nearest Neighbor Search</title>
    <summary>  Consider a dataset of n(d) points generated independently from R^d according
to a common p.d.f. f_d with support(f_d) = [0,1]^d and sup{f_d([0,1]^d)}
growing sub-exponentially in d. We prove that: (i) if n(d) grows
sub-exponentially in d, then, for any query point q^d in [0,1]^d and any
epsilon&gt;0, the ratio of the distance between any two dataset points and q^d is
less that 1+epsilon with probability --&gt;1 as d--&gt;infinity; (ii) if
n(d)&gt;[4(1+epsilon)]^d for large d, then for all q^d in [0,1]^d (except a small
subset) and any epsilon&gt;0, the distance ratio is less than 1+epsilon with
limiting probability strictly bounded away from one. Moreover, we provide
preliminary results along the lines of (i) when f_d=N(mu_d,Sigma_d).
</summary>
    <author>
      <name>Chris Giannella</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing Letters 109(19), 2009.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.0684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4096v2</id>
    <updated>2009-06-22T20:04:32Z</updated>
    <published>2009-06-22T19:40:12Z</published>
    <title>An Event Based Approach To Situational Representation</title>
    <summary>  Many application domains require representing interrelated real-world
activities and/or evolving physical phenomena. In the crisis response domain,
for instance, one may be interested in representing the state of the unfolding
crisis (e.g., forest fire), the progress of the response activities such as
evacuation and traffic control, and the state of the crisis site(s). Such a
situation representation can then be used to support a multitude of
applications including situation monitoring, analysis, and planning. In this
paper, we make a case for an event based representation of situations where
events are defined to be domain-specific significant occurrences in space and
time. We argue that events offer a unifying and powerful abstraction to
building situational awareness applications. We identify challenges in building
an Event Management System (EMS) for which traditional data and knowledge
management systems prove to be limited and suggest possible directions and
technologies to address the challenges.
</summary>
    <author>
      <name>Naveen Ashish</name>
    </author>
    <author>
      <name>Dmitri Kalashnikov</name>
    </author>
    <author>
      <name>Sharad Mehrotra</name>
    </author>
    <author>
      <name>Nalini Venkatasubramanian</name>
    </author>
    <link href="http://arxiv.org/abs/0906.4096v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4096v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4172v1</id>
    <updated>2009-06-23T06:24:57Z</updated>
    <published>2009-06-23T06:24:57Z</published>
    <title>Rough Set Model for Discovering Hybrid Association Rules</title>
    <summary>  In this paper, the mining of hybrid association rules with rough set approach
is investigated as the algorithm RSHAR.The RSHAR algorithm is constituted of
two steps mainly. At first, to join the participant tables into a general table
to generate the rules which is expressing the relationship between two or more
domains that belong to several different tables in a database. Then we apply
the mapping code on selected dimension, which can be added directly into the
information system as one certain attribute. To find the association rules,
frequent itemsets are generated in second step where candidate itemsets are
generated through equivalence classes and also transforming the mapping code in
to real dimensions. The searching method for candidate itemset is similar to
apriori algorithm. The analysis of the performance of algorithm has been
carried out.
</summary>
    <author>
      <name>Anjana Pandey</name>
    </author>
    <author>
      <name>K. R. Pardasani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, International Journal of Computer Science and Information
  Security</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS June 2009 Issue, Vol. 2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.4172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4560v2</id>
    <updated>2010-11-09T23:13:19Z</updated>
    <published>2009-06-24T20:09:00Z</published>
    <title>Coordinated Weighted Sampling for Estimating Aggregates Over Multiple
  Weight Assignments</title>
    <summary>  Many data sources are naturally modeled by multiple weight assignments over a
set of keys: snapshots of an evolving database at multiple points in time,
measurements collected over multiple time periods, requests for resources
served at multiple locations, and records with multiple numeric attributes.
Over such vector-weighted data we are interested in aggregates with respect to
one set of weights, such as weighted sums, and aggregates over multiple sets of
weights such as the $L_1$ difference.
  Sample-based summarization is highly effective for data sets that are too
large to be stored or manipulated. The summary facilitates approximate
processing queries that may be specified after the summary was generated.
  Current designs, however, are geared for data sets where a single {\em
scalar} weight is associated with each key.
  We develop a sampling framework based on {\em coordinated weighted samples}
that is suited for multiple weight assignments and obtain estimators that are
{\em orders of magnitude tighter} than previously possible.
  We demonstrate the power of our methods through an extensive empirical
evaluation on diverse data sets ranging from IP network to stock quotes data.
</summary>
    <author>
      <name>Edith Cohen</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <author>
      <name>Subhabrata Sen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an updated full version of the PVLDB 2009 conference version</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.4560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5040v1</id>
    <updated>2009-06-27T05:11:48Z</updated>
    <published>2009-06-27T05:11:48Z</published>
    <title>Towards the Patterns of Hard CSPs with Association Rule Mining</title>
    <summary>  The hardness of finite domain Constraint Satisfaction Problems (CSPs) is a
very important research area in Constraint Programming (CP) community. However,
this problem has not yet attracted much attention from the researchers in the
association rule mining community. As a popular data mining technique,
association rule mining has an extremely wide application area and it has
already been successfully applied to many interdisciplines. In this paper, we
study the association rule mining techniques and propose a cascaded approach to
extract the interesting patterns of the hard CSPs. As far as we know, this
problem is investigated with the data mining techniques for the first time.
Specifically, we generate the random CSPs and collect their characteristics by
solving all the CSP instances, and then apply the data mining techniques on the
data set and further to discover the interesting patterns of the hardness of
the randomly generated CSPs
</summary>
    <author>
      <name>Chendong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, submitted to ICDM'09</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.5040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5485v1</id>
    <updated>2009-06-30T14:08:19Z</updated>
    <published>2009-06-30T14:08:19Z</published>
    <title>Query Significance in Databases via Randomizations</title>
    <summary>  Many sorts of structured data are commonly stored in a multi-relational
format of interrelated tables. Under this relational model, exploratory data
analysis can be done by using relational queries. As an example, in the
Internet Movie Database (IMDb) a query can be used to check whether the average
rank of action movies is higher than the average rank of drama movies.
  We consider the problem of assessing whether the results returned by such a
query are statistically significant or just a random artifact of the structure
in the data. Our approach is based on randomizing the tables occurring in the
queries and repeating the original query on the randomized tables. It turns out
that there is no unique way of randomizing in multi-relational data. We propose
several randomization techniques, study their properties, and show how to find
out which queries or hypotheses about our data result in statistically
significant information. We give results on real and generated data and show
how the significance of some queries vary between different randomizations.
</summary>
    <author>
      <name>Markus Ojala</name>
    </author>
    <author>
      <name>Gemma C. Garriga</name>
    </author>
    <author>
      <name>Aristides Gionis</name>
    </author>
    <author>
      <name>Heikki Mannila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.5485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1632v1</id>
    <updated>2009-07-09T18:45:29Z</updated>
    <published>2009-07-09T18:45:29Z</published>
    <title>Incorporating Integrity Constraints in Uncertain Databases</title>
    <summary>  We develop an approach to incorporate additional knowledge, in the form of
general purpose integrity constraints (ICs), to reduce uncertainty in
probabilistic databases. While incorporating ICs improves data quality (and
hence quality of answers to a query), it significantly complicates query
processing. To overcome the additional complexity, we develop an approach to
map an uncertain relation U with ICs to another uncertain relation U', that
approximates the set of consistent worlds represented by U. Queries over U can
instead be evaluated over U' achieving higher quality (due to reduced
uncertainty in U') without additional complexity in query processing due to
ICs. We demonstrate the effectiveness and scalability of our approach to large
data-sets with complex constraints. We also present experimental results
demonstrating the utility of incorporating integrity constraints in uncertain
relations, in the context of an information extraction application.
</summary>
    <author>
      <name>Naveen Ashish</name>
    </author>
    <author>
      <name>Sharad Mehrotra</name>
    </author>
    <author>
      <name>Pouria Pirzadeh</name>
    </author>
    <link href="http://arxiv.org/abs/0907.1632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.2471v1</id>
    <updated>2009-07-15T00:40:06Z</updated>
    <published>2009-07-15T00:40:06Z</published>
    <title>Benchmarking Declarative Approximate Selection Predicates</title>
    <summary>  Declarative data quality has been an active research topic. The fundamental
principle behind a declarative approach to data quality is the use of
declarative statements to realize data quality primitives on top of any
relational data source. A primary advantage of such an approach is the ease of
use and integration with existing applications. Several similarity predicates
have been proposed in the past for common quality primitives (approximate
selections, joins, etc.) and have been fully expressed using declarative SQL
statements. In this thesis, new similarity predicates are proposed along with
their declarative realization, based on notions of probabilistic information
retrieval. Then, full declarative specifications of previously proposed
similarity predicates in the literature are presented, grouped into classes
according to their primary characteristics. Finally, a thorough performance and
accuracy study comparing a large number of similarity predicates for data
cleaning operations is performed.
</summary>
    <author>
      <name>Oktie Hassanzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">75 pages, 7 figures, February 2007, Masters Thesis at University of
  Toronto</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.2471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.2471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.2868v1</id>
    <updated>2009-07-16T15:25:50Z</updated>
    <published>2009-07-16T15:25:50Z</published>
    <title>Scalable Probabilistic Similarity Ranking in Uncertain Databases
  (Technical Report)</title>
    <summary>  This paper introduces a scalable approach for probabilistic top-k similarity
ranking on uncertain vector data. Each uncertain object is represented by a set
of vector instances that are assumed to be mutually-exclusive. The objective is
to rank the uncertain data according to their distance to a reference object.
We propose a framework that incrementally computes for each object instance and
ranking position, the probability of the object falling at that ranking
position. The resulting rank probability distribution can serve as input for
several state-of-the-art probabilistic ranking models. Existing approaches
compute this probability distribution by applying a dynamic programming
approach of quadratic complexity. In this paper we theoretically as well as
experimentally show that our framework reduces this to a linear-time complexity
while having the same memory requirements, facilitated by incremental accessing
of the uncertain vector instances in increasing order of their distance to the
reference object. Furthermore, we show how the output of our method can be used
to apply probabilistic top-k ranking for the objects, according to different
state-of-the-art definitions. We conduct an experimental evaluation on
synthetic and real data, which demonstrates the efficiency of our approach.
</summary>
    <author>
      <name>Thomas Bernecker</name>
    </author>
    <author>
      <name>Hans-Peter Kriegel</name>
    </author>
    <author>
      <name>Nikos Mamoulis</name>
    </author>
    <author>
      <name>Matthias Renz</name>
    </author>
    <author>
      <name>Andreas Zuefle</name>
    </author>
    <link href="http://arxiv.org/abs/0907.2868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.2868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.2951v1</id>
    <updated>2009-07-16T22:57:53Z</updated>
    <published>2009-07-16T22:57:53Z</published>
    <title>Untangling the Braid: Finding Outliers in a Set of Streams</title>
    <summary>  Monitoring the performance of large shared computing systems such as the
cloud computing infrastructure raises many challenging algorithmic problems.
One common problem is to track users with the largest deviation from the norm
(outliers), for some measure of performance. Taking a stream-computing
perspective, we can think of each user's performance profile as a stream of
numbers (such as response times), and the aggregate performance profile of the
shared infrastructure as a "braid" of these intermixed streams. The monitoring
system's goal then is to untangle this braid sufficiently to track the top k
outliers. This paper investigates the space complexity of one-pass algorithms
for approximating outliers of this kind, proves lower bounds using multi-party
communication complexity, and proposes small-memory heuristic algorithms. On
one hand, stream outliers are easily tracked for simple measures, such as max
or min, but our theoretical results rule out even good approximations for most
of the natural measures such as average, median, or the quantiles. On the other
hand, we show through simulation that our proposed heuristics perform quite
well for a variety of synthetic data.
</summary>
    <author>
      <name>Chiranjeeb Buragohain</name>
    </author>
    <author>
      <name>Luca Foschini</name>
    </author>
    <author>
      <name>Subhash Suri</name>
    </author>
    <link href="http://arxiv.org/abs/0907.2951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.2951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0411v3</id>
    <updated>2009-12-15T09:09:32Z</updated>
    <published>2009-08-04T08:53:33Z</published>
    <title>Data management in systems biology I - Overview and bibliography</title>
    <summary>  Large systems biology projects can encompass several workgroups often located
in different countries. An overview about existing data standards in systems
biology and the management, storage, exchange and integration of the generated
data in large distributed research projects is given, the pros and cons of the
different approaches are illustrated from a practical point of view, the
existing software - open source as well as commercial - and the relevant
literature is extensively overview, so that the reader should be enabled to
decide which data management approach is the best suited for his special needs.
An emphasis is laid on the use of workflow systems and of TAB-based formats.
The data in this format can be viewed and edited easily using spreadsheet
programs which are familiar to the working experimental biologists. The use of
workflows for the standardized access to data in either own or publicly
available databanks and the standardization of operation procedures is
presented. The use of ontologies and semantic web technologies for data
management will be discussed in a further paper.
</summary>
    <author>
      <name>Gerhard Mayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0411v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0411v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0464v3</id>
    <updated>2011-10-07T16:45:52Z</updated>
    <published>2009-08-04T15:10:25Z</published>
    <title>Prioritized Repairing and Consistent Query Answering in Relational
  Databases</title>
    <summary>  A consistent query answer in an inconsistent database is an answer obtained
in every (minimal) repair. The repairs are obtained by resolving all conflicts
in all possible ways. Often, however, the user is able to provide a preference
on how conflicts should be resolved. We investigate here the framework of
preferred consistent query answers, in which user preferences are used to
narrow down the set of repairs to a set of preferred repairs. We axiomatize
desirable properties of preferred repairs. We present three different families
of preferred repairs and study their mutual relationships. Finally, we
investigate the complexity of preferred repairing and computing preferred
consistent query answers.
</summary>
    <author>
      <name>Slawomir Staworko</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the special SUM'08 issue of AMAI</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0464v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0464v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0567v1</id>
    <updated>2009-08-04T23:30:46Z</updated>
    <published>2009-08-04T23:30:46Z</published>
    <title>LinkedCT: A Linked Data Space for Clinical Trials</title>
    <summary>  The Linked Clinical Trials (LinkedCT) project aims at publishing the first
open semantic web data source for clinical trials data. The database exposed by
LinkedCT is generated by (1) transforming existing data sources of clinical
trials into RDF, and (2) discovering semantic links between the records in the
trials data and several other data sources. In this paper, we discuss several
challenges involved in these two steps and present the methodology used in
LinkedCT to overcome these challenges. Our approach for semantic link discovery
involves using state-of-the-art approximate string matching techniques combined
with ontology-based semantic matching of the records, all performed in a
declarative and easy-to-use framework. We present an evaluation of the
performance of our proposed techniques in several link discovery scenarios in
LinkedCT.
</summary>
    <author>
      <name>Oktie Hassanzadeh</name>
    </author>
    <author>
      <name>Anastasios Kementsietsidis</name>
    </author>
    <author>
      <name>Lipyeow Lim</name>
    </author>
    <author>
      <name>Renee J. Miller</name>
    </author>
    <author>
      <name>Min Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.3.5; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0984v1</id>
    <updated>2009-08-07T05:36:51Z</updated>
    <published>2009-08-07T05:36:51Z</published>
    <title>An Application of Bayesian classification to Interval Encoded Temporal
  mining with prioritized items</title>
    <summary>  In real life, media information has time attributes either implicitly or
explicitly known as temporal data. This paper investigates the usefulness of
applying Bayesian classification to an interval encoded temporal database with
prioritized items. The proposed method performs temporal mining by encoding the
database with weighted items which prioritizes the items according to their
importance from the user perspective. Naive Bayesian classification helps in
making the resulting temporal rules more effective. The proposed priority based
temporal mining (PBTM) method added with classification aids in solving
problems in a well informed and systematic manner. The experimental results are
obtained from the complaints database of the telecommunications system, which
shows the feasibility of this method of classification based temporal mining.
</summary>
    <author>
      <name>C. Balasubramanian</name>
    </author>
    <author>
      <name>K. Duraiswamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS July 2009, ISSN 1947 5500, Impact Factor 0.423</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 3, No. 1, July 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.0984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.2588v1</id>
    <updated>2009-08-18T15:17:19Z</updated>
    <published>2009-08-18T15:17:19Z</published>
    <title>Wild Card Queries for Searching Resources on the Web</title>
    <summary>  We propose a domain-independent framework for searching and retrieving facts
and relationships within natural language text sources. In this framework, an
extraction task over a text collection is expressed as a query that combines
text fragments with wild cards, and the query result is a set of facts in the
form of unary, binary and general $n$-ary tuples. A significance of our
querying mechanism is that, despite being both simple and declarative, it can
be applied to a wide range of extraction tasks. A problem in querying natural
language text though is that a user-specified query may not retrieve enough
exact matches. Unlike term queries which can be relaxed by removing some of the
terms (as is done in search engines), removing terms from a wild card query
without ruining its meaning is more challenging. Also, any query expansion has
the potential to introduce false positives. In this paper, we address the
problem of query expansion, and also analyze a few ranking alternatives to
score the results and to remove false positives. We conduct experiments and
report an evaluation of the effectiveness of our querying and scoring
functions.
</summary>
    <author>
      <name>Davood Rafiei</name>
    </author>
    <author>
      <name>Haobin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.2588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.2588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3957v1</id>
    <updated>2009-08-27T09:12:08Z</updated>
    <published>2009-08-27T09:12:08Z</published>
    <title>Enhancing XML Data Warehouse Query Performance by Fragmentation</title>
    <summary>  XML data warehouses form an interesting basis for decision-support
applications that exploit heterogeneous data from multiple sources. However,
XML-native database systems currently suffer from limited performances in terms
of manageable data volume and response time for complex analytical queries.
Fragmenting and distributing XML data warehouses (e.g., on data grids) allow to
address both these issues. In this paper, we work on XML warehouse
fragmentation. In relational data warehouses, several studies recommend the use
of derived horizontal fragmentation. Hence, we propose to adapt it to the XML
context. We particularly focus on the initial horizontal fragmentation of
dimensions' XML documents and exploit two alternative algorithms. We
experimentally validate our proposal and compare these alternatives with
respect to a unified XML warehouse model we advocate for.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">24th Annual ACM Symposium on Applied Computing (SAC 09), Hawaii :
  United States (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.3957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0685v1</id>
    <updated>2009-09-03T15:26:38Z</updated>
    <published>2009-09-03T15:26:38Z</published>
    <title>In-Network Outlier Detection in Wireless Sensor Networks</title>
    <summary>  To address the problem of unsupervised outlier detection in wireless sensor
networks, we develop an approach that (1) is flexible with respect to the
outlier definition, (2) computes the result in-network to reduce both bandwidth
and energy usage,(3) only uses single hop communication thus permitting very
simple node failure detection and message reliability assurance mechanisms
(e.g., carrier-sense), and (4) seamlessly accommodates dynamic updates to data.
We examine performance using simulation with real sensor data streams. Our
results demonstrate that our approach is accurate and imposes a reasonable
communication load and level of power consumption.
</summary>
    <author>
      <name>Joel W. Branch</name>
    </author>
    <author>
      <name>Chris Giannella</name>
    </author>
    <author>
      <name>Boleslaw Szymanski</name>
    </author>
    <author>
      <name>Ran Wolff</name>
    </author>
    <author>
      <name>Hillol Kargupta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10115-011-0474-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10115-011-0474-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper appearing in the Int'l Conference on
  Distributed Computing Systems 2006</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Knowledge and Information Systems 34(1) January, 2013, pp. 23-54</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.0685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1127v1</id>
    <updated>2009-09-07T01:44:36Z</updated>
    <published>2009-09-07T01:44:36Z</published>
    <title>Anonymization with Worst-Case Distribution-Based Background Knowledge</title>
    <summary>  Background knowledge is an important factor in privacy preserving data
publishing. Distribution-based background knowledge is one of the well studied
background knowledge. However, to the best of our knowledge, there is no
existing work considering the distribution-based background knowledge in the
worst case scenario, by which we mean that the adversary has accurate knowledge
about the distribution of sensitive values according to some tuple attributes.
Considering this worst case scenario is essential because we cannot overlook
any breaching possibility. In this paper, we propose an algorithm to anonymize
dataset in order to protect individual privacy by considering this background
knowledge. We prove that the anonymized datasets generated by our proposed
algorithm protects individual privacy. Our empirical studies show that our
method preserves high utility for the published data at the same time.
</summary>
    <author>
      <name>Raymond Chi-Wing Wong</name>
    </author>
    <author>
      <name>Ada Wai-Chee Fu</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Yabo Xu</name>
    </author>
    <author>
      <name>Jian Pei</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <link href="http://arxiv.org/abs/0909.1127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1346v8</id>
    <updated>2011-02-22T16:00:56Z</updated>
    <published>2009-09-07T21:34:52Z</published>
    <title>Reordering Columns for Smaller Indexes</title>
    <summary>  Column-oriented indexes-such as projection or bitmap indexes-are compressed
by run-length encoding to reduce storage and increase speed. Sorting the tables
improves compression. On realistic data sets, permuting the columns in the
right order before sorting can reduce the number of runs by a factor of two or
more. Unfortunately, determining the best column order is NP-hard. For many
cases, we prove that the number of runs in table columns is minimized if we
sort columns by increasing cardinality. Experimentally, sorting based on
Hilbert space-filling curves is poor at minimizing the number of runs.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2011.02.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2011.02.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Information Sciences</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Daniel Lemire and Owen Kaser, Reordering Columns for Smaller
  Indexes, Information Sciences 181 (12), 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.1346v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1346v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1758v1</id>
    <updated>2009-09-09T18:00:21Z</updated>
    <published>2009-09-09T18:00:21Z</published>
    <title>Teaching an Old Elephant New Tricks</title>
    <summary>  In recent years, column stores (or C-stores for short) have emerged as a
novel approach to deal with read-mostly data warehousing applications.
Experimental evidence suggests that, for certain types of queries, the new
features of C-stores result in orders of magnitude improvement over traditional
relational engines. At the same time, some C-store proponents argue that
C-stores are fundamentally different from traditional engines, and therefore
their benefits cannot be incorporated into a relational engine short of a
complete rewrite. In this paper we challenge this claim and show that many of
the benefits of C-stores can indeed be simulated in traditional engines with no
changes whatsoever. We then identify some limitations of our ?pure-simulation?
approach for the case of more complex queries. Finally, we predict that
traditional relational engines will eventually leverage most of the benefits of
C-stores natively, as is currently happening in other domains such as XML data.
</summary>
    <author>
      <name>Nicolas Bruno</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1760v1</id>
    <updated>2009-09-09T18:05:37Z</updated>
    <published>2009-09-09T18:05:37Z</published>
    <title>LifeRaft: Data-Driven, Batch Processing for the Exploration of
  Scientific Databases</title>
    <summary>  Workloads that comb through vast amounts of data are gaining importance in
the sciences. These workloads consist of "needle in a haystack" queries that
are long running and data intensive so that query throughput limits
performance. To maximize throughput for data-intensive queries, we put forth
LifeRaft: a query processing system that batches queries with overlapping data
requirements. Rather than scheduling queries in arrival order, LifeRaft
executes queries concurrently against an ordering of the data that maximizes
data sharing among queries. This decreases I/O and increases cache utility.
However, such batch processing can increase query response time by starving
interactive workloads. LifeRaft addresses starvation using techniques inspired
by head scheduling in disk drives. Depending upon the workload saturation and
queuing times, the system adaptively and incrementally trades-off processing
queries in arrival order and data-driven batch processing. Evaluating LifeRaft
in the SkyQuery federation of astronomy databases reveals a two-fold
improvement in query throughput.
</summary>
    <author>
      <name>Xiaodan Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Johns Hopkins University</arxiv:affiliation>
    </author>
    <author>
      <name>Randal Burns</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Johns Hopkins</arxiv:affiliation>
    </author>
    <author>
      <name>Tanu Malik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Purdue University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1764v1</id>
    <updated>2009-09-09T18:09:17Z</updated>
    <published>2009-09-09T18:09:17Z</published>
    <title>Data Management for High-Throughput Genomics</title>
    <summary>  Today's sequencing technology allows sequencing an individual genome within a
few weeks for a fraction of the costs of the original Human Genome project.
Genomics labs are faced with dozens of TB of data per week that have to be
automatically processed and made available to scientists for further analysis.
This paper explores the potential and the limitations of using relational
database systems as the data processing platform for high-throughput genomics.
In particular, we are interested in the storage management for high-throughput
sequence data and in leveraging SQL and user-defined functions for data
analysis inside a database system. We give an overview of a database design for
high-throughput genomics, how we used a SQL Server database in some
unconventional ways to prototype this scenario, and we will discuss some
initial findings about the scalability and performance of such a more
database-centric approach.
</summary>
    <author>
      <name>Uwe Roehm</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Sydney</arxiv:affiliation>
    </author>
    <author>
      <name>Jose Blakeley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1766v1</id>
    <updated>2009-09-09T18:09:27Z</updated>
    <published>2009-09-09T18:09:27Z</published>
    <title>RIOT: I/O-Efficient Numerical Computing without SQL</title>
    <summary>  R is a numerical computing environment that is widely popular for statistical
data analysis. Like many such environments, R performs poorly for large
datasets whose sizes exceed that of physical memory. We present our vision of
RIOT (R with I/O Transparency), a system that makes R programs I/O-efficient in
a way transparent to the users. We describe our experience with RIOT-DB, an
initial prototype that uses a relational database system as a backend. Despite
the overhead and inadequacy of generic database systems in handling array data
and numerical computation, RIOT-DB significantly outperforms R in many
large-data scenarios, thanks to a suite of high-level, inter-operation
optimizations that integrate seamlessly into R. While many techniques in RIOT
are inspired by databases (and, for RIOT-DB, realized by a database system),
RIOT users are insulated from anything database related. Compared with previous
approaches that require users to learn new languages and rewrite their programs
to interface with a database, RIOT will, we believe, be easier to adopt by the
majority of the R users.
</summary>
    <author>
      <name>Yi Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Duke University</arxiv:affiliation>
    </author>
    <author>
      <name>Herodotos Herodotou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Duke</arxiv:affiliation>
    </author>
    <author>
      <name>Jun Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Duke</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1770v1</id>
    <updated>2009-09-09T18:09:44Z</updated>
    <published>2009-09-09T18:09:44Z</published>
    <title>From Declarative Languages to Declarative Processing in Computer Games</title>
    <summary>  Recent work has shown that we can dramatically improve the performance of
computer games and simulations through declarative processing: Character AI can
be written in an imperative scripting language which is then compiled to
relational algebra and executed by a special games engine with features similar
to a main memory database system. In this paper we lay out a challenging
research agenda built on these ideas.
  We discuss several research ideas for novel language features to support
atomic actions and reactive programming. We also explore challenges for
main-memory query processing in games and simulations including adaptive query
plan selection, support for parallel architectures, debugging simulation
scripts, and extensions for multi-player games and virtual worlds. We believe
that these research challenges will result in a dramatic change in the design
of game engines over the next decade.
</summary>
    <author>
      <name>Benjamin Sowell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Alan Demers</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Johannes Gehrke</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Nitin Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Haoyuan Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Walker White</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1771v1</id>
    <updated>2009-09-09T18:09:48Z</updated>
    <published>2009-09-09T18:09:48Z</published>
    <title>The Role of Schema Matching in Large Enterprises</title>
    <summary>  To date, the principal use case for schema matching research has been as a
precursor for code generation, i.e., constructing mappings between schema
elements with the end goal of data transfer. In this paper, we argue that
schema matching plays valuable roles independent of mapping construction,
especially as schemata grow to industrial scales. Specifically, in large
enterprises human decision makers and planners are often the immediate consumer
of information derived from schema matchers, instead of schema mapping tools.
We list a set of real application areas illustrating this role for schema
matching, and then present our experiences tackling a customer problem in one
of these areas. We describe the matcher used, where the tool was effective,
where it fell short, and our lessons learned about how well current schema
matching technology is suited for use in large enterprises. Finally, we suggest
a new agenda for schema matching research based on these experiences.
</summary>
    <author>
      <name>Ken Smith</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Morse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Mork</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>Maya Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>Arnon Rosenthal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>David Allen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>Len Seligman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Wolf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITRE</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1772v1</id>
    <updated>2009-09-09T18:09:52Z</updated>
    <published>2009-09-09T18:09:52Z</published>
    <title>Visualizing the robustness of query execution</title>
    <summary>  In database query processing, actual run-time conditions (e.g., actual
selectivities and actual available memory) very often differ from compile-time
expectations of run-time conditions (e.g., estimated predicate selectivities
and anticipated memory availability). Robustness of query processing can be
defined as the ability to handle unexpected conditions. Robustness of query
execution, specifically, can be defined as the ability to process a specific
plan efficiently in an unexpected condition. We focus on query execution
(run-time), ignoring query optimization (compile-time), in order to complement
existing research and to explore untapped potential for improved robustness in
database query processing.
  One of our initial steps has been to devise diagrams or maps that show how
well plans perform in the face of varying run-time conditions and how
gracefully a system's query architecture, operators, and their implementation
degrade in the face of adverse conditions. In this paper, we show several kinds
of diagrams with data from three real systems and report on what we have
learned both about these visualization techniques and about the three database
systems
</summary>
    <author>
      <name>Goetz Graefe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HP</arxiv:affiliation>
    </author>
    <author>
      <name>Harumi Kuno</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hewlett-Packard Co.</arxiv:affiliation>
    </author>
    <author>
      <name>Janet Wiener</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hewlett-Packard, Co.</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1773v1</id>
    <updated>2009-09-09T18:09:55Z</updated>
    <published>2009-09-09T18:09:55Z</published>
    <title>Search Driven Analysis of Heterogenous XML Data</title>
    <summary>  Analytical processing on XML repositories is usually enabled by designing
complex data transformations that shred the documents into a common data
warehousing schema. This can be very time-consuming and costly, especially if
the underlying XML data has a lot of variety in structure, and only a subset of
attributes constitutes meaningful dimensions and facts. Today, there is no tool
to explore an XML data set, discover interesting attributes, dimensions and
facts, and rapidly prototype an OLAP solution.
  In this paper, we propose a system, called SEDA that enables users to start
with simple keyword-style querying, and interactively refine the query based on
result summaries. SEDA then maps query results onto a set of known, or newly
created, facts and dimensions, and derives a star schema and its instantiation
to be fed into an off-the-shelf OLAP tool, for further analysis.
</summary>
    <author>
      <name>Andrey Balmin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Almaden Research Center</arxiv:affiliation>
    </author>
    <author>
      <name>Latha Colby</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Almaden Research Center</arxiv:affiliation>
    </author>
    <author>
      <name>Emiran Curtmola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC, San Diego</arxiv:affiliation>
    </author>
    <author>
      <name>Quanzhong Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Almaden Research Center</arxiv:affiliation>
    </author>
    <author>
      <name>Fatma Ozcan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Almaden Research Center</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1774v1</id>
    <updated>2009-09-09T18:10:01Z</updated>
    <published>2009-09-09T18:10:01Z</published>
    <title>Social Systems: Can we Do More Than Just Poke Friends?</title>
    <summary>  Social sites have become extremely popular among users but have they
attracted equal attention from the research community? Are they good only for
simple tasks, such as tagging and poking friends? Do they present any new or
interesting research challenges? In this paper, we describe the insights we
have obtained implementing CourseRank, a course evaluation and planning social
system. We argue that more attention should be given to social sites like ours
and that there are many challenges (though not the traditional DBMS ones) that
should be addressed by our community.
</summary>
    <author>
      <name>Georgia Koutrika</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Bercovitz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Ikeda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Filip Kaliszan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Henry Liou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Zahra Mohammadi Zadeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Hector Garcia-Molina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1775v1</id>
    <updated>2009-09-09T18:10:04Z</updated>
    <published>2009-09-09T18:10:04Z</published>
    <title>SCADS: Scale-Independent Storage for Social Computing Applications</title>
    <summary>  Collaborative web applications such as Facebook, Flickr and Yelp present new
challenges for storing and querying large amounts of data. As users and
developers are focused more on performance than single copy consistency or the
ability to perform ad-hoc queries, there exists an opportunity for a
highly-scalable system tailored specifically for relaxed consistency and
pre-computed queries. The Web 2.0 development model demands the ability to both
rapidly deploy new features and automatically scale with the number of users.
There have been many successful distributed key-value stores, but so far none
provide as rich a query language as SQL. We propose a new architecture, SCADS,
that allows the developer to declaratively state application specific
consistency requirements, takes advantage of utility computing to provide cost
effective scale-up and scale-down, and will use machine learning models to
introspectively anticipate performance problems and predict the resource
requirements of new queries before execution.
</summary>
    <author>
      <name>Michael Armbrust</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>Armando Fox</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>David Patterson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Lanham</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>Beth Trushkowsky</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>Jesse Trutna</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>Haruki Oh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1778v1</id>
    <updated>2009-09-09T18:10:16Z</updated>
    <published>2009-09-09T18:10:16Z</published>
    <title>A Case for A Collaborative Query Management System</title>
    <summary>  Over the past 40 years, database management systems (DBMSs) have evolved to
provide a sophisticated variety of data management capabilities. At the same
time, tools for managing queries over the data have remained relatively
primitive. One reason for this is that queries are typically issued through
applications. They are thus debugged once and re-used repeatedly. This mode of
interaction, however, is changing. As scientists (and others) store and share
increasingly large volumes of data in data centers, they need the ability to
analyze the data by issuing exploratory queries. In this paper, we argue that,
in these new settings, data management systems must provide powerful query
management capabilities, from query browsing to automatic query
recommendations. We first discuss the requirements for a collaborative query
management system. We outline an early system architecture and discuss the many
research challenges associated with building such an engine.
</summary>
    <author>
      <name>Nodira Khoussainova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Magda Balazinska</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U. Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Wolfgang Gatterbauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>YongChul Kwon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Suciu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1783v1</id>
    <updated>2009-09-09T18:10:36Z</updated>
    <published>2009-09-09T18:10:36Z</published>
    <title>The Case for a Structured Approach to Managing Unstructured Data</title>
    <summary>  The challenge of managing unstructured data represents perhaps the largest
data management opportunity for our community since managing relational data.
And yet we are risking letting this opportunity go by, ceding the playing field
to other players, ranging from communities such as AI, KDD, IR, Web, and
Semantic Web, to industrial players such as Google, Yahoo, and Microsoft. In
this essay we explore what we can do to improve upon this situation. Drawing on
the lessons learned while managing relational data, we outline a structured
approach to managing unstructured data. We conclude by discussing the potential
implications of this approach to managing other kinds of non-relational data,
and to the identify of our field.
</summary>
    <author>
      <name>AnHai Doan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ of Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Jeff Naughton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Akanksha Baid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Xiaoyong Chai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Fei Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Ting Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Chu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Pedro DeRose</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Byron Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Chaitanya Gokhale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Jiansheng Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Warren Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <author>
      <name>Ba-Quy Vuong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wisconsin</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1784v1</id>
    <updated>2009-09-09T18:10:39Z</updated>
    <published>2009-09-09T18:10:39Z</published>
    <title>Energy Efficiency: The New Holy Grail of Data Management Systems
  Research</title>
    <summary>  Energy costs are quickly rising in large-scale data centers and are soon
projected to overtake the cost of hardware. As a result, data center operators
have recently started turning into using more energy-friendly hardware. Despite
the growing body of research in power management techniques, there has been
little work to date on energy efficiency from a data management software
perspective.
  In this paper, we argue that hardware-only approaches are only part of the
solution, and that data management software will be key in optimizing for
energy efficiency. We discuss the problems arising from growing energy use in
data centers and the trends that point to an increasing set of opportunities
for software-level optimizations. Using two simple experiments, we illustrate
the potential of such optimizations, and, motivated by these examples, we
discuss general approaches for reducing energy waste. Lastly, we point out
existing places within database systems that are promising for
energy-efficiency optimizations and urge the data management systems community
to shift focus from performance-oriented research to energy-efficient
computing.
</summary>
    <author>
      <name>Stavros Harizopoulos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HP Labs</arxiv:affiliation>
    </author>
    <author>
      <name>Mehul Shah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UCLA</arxiv:affiliation>
    </author>
    <author>
      <name>Justin Meza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UCLA</arxiv:affiliation>
    </author>
    <author>
      <name>Parthasarathy Ranganathan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HP Labs</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1785v1</id>
    <updated>2009-09-09T18:10:42Z</updated>
    <published>2009-09-09T18:10:42Z</published>
    <title>Harnessing the Deep Web: Present and Future</title>
    <summary>  Over the past few years, we have built a system that has exposed large
volumes of Deep-Web content to Google.com users. The content that our system
exposes contributes to more than 1000 search queries per-second and spans over
50 languages and hundreds of domains. The Deep Web has long been acknowledged
to be a major source of structured data on the web, and hence accessing
Deep-Web content has long been a problem of interest in the data management
community. In this paper, we report on where we believe the Deep Web provides
value and where it does not. We contrast two very different approaches to
exposing Deep-Web content -- the surfacing approach that we used, and the
virtual integration approach that has often been pursued in the data management
literature. We emphasize where the values of each of the two approaches lie and
caution against potential pitfalls. We outline important areas of future
research and, in particular, emphasize the value that can be derived from
analyzing large collections of potentially disparate structured data on the
web.
</summary>
    <author>
      <name>Jayant Madhavan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Loredana Afanasiev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universiteit van Amsterdam</arxiv:affiliation>
    </author>
    <author>
      <name>Lyublena Antova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Alon Halevy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2062v1</id>
    <updated>2009-09-10T22:55:16Z</updated>
    <published>2009-09-10T22:55:16Z</published>
    <title>Inter-Operator Feedback in Data Stream Management Systems via
  Punctuation</title>
    <summary>  High-volume, high-speed data streams may overwhelm the capabilities of stream
processing systems; techniques such as data prioritization, avoidance of
unnecessary processing and on-demand result production may be necessary to
reduce processing requirements. However, the dynamic nature of data streams, in
terms of both rate and content, makes the application of such techniques
challenging. Such techniques have been addressed in the context of static and
centralized query optimization; however, they have not been fully addressed for
data stream management systems. In this work, we present a comprehensive
framework that supports prioritization, avoidance of unnecessary work, and
on-demand result production over distributed, unreliable, bursty, disordered
data sources, typical of many data streams. We propose a form of inter-operator
feedback, which flows against the stream direction, to communicate the
information needed to enable execution of these techniques. This feedback
leverages punctuations to describe the subsets of interest. We identify
potential sources of feedback information, characterize new types of
punctuation to support feedback, and describe the roles of producers,
exploiters, and relayers of feedback that query operators may implement. We
present initial experimental observations using the NiagaraST data-stream
system.
</summary>
    <author>
      <name>Rafael Fernández-Moctezuma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Portland State University</arxiv:affiliation>
    </author>
    <author>
      <name>Kristin Tufte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Portland State University</arxiv:affiliation>
    </author>
    <author>
      <name>Jin Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Portland State University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.2062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2290v1</id>
    <updated>2009-09-12T01:42:55Z</updated>
    <published>2009-09-12T01:42:55Z</published>
    <title>Slicing: A New Approach to Privacy Preserving Data Publishing</title>
    <summary>  Several anonymization techniques, such as generalization and bucketization,
have been designed for privacy preserving microdata publishing. Recent work has
shown that generalization loses considerable amount of information, especially
for high-dimensional data. Bucketization, on the other hand, does not prevent
membership disclosure and does not apply for data that do not have a clear
separation between quasi-identifying attributes and sensitive attributes.
  In this paper, we present a novel technique called slicing, which partitions
the data both horizontally and vertically. We show that slicing preserves
better data utility than generalization and can be used for membership
disclosure protection. Another important advantage of slicing is that it can
handle high-dimensional data. We show how slicing can be used for attribute
disclosure protection and develop an efficient algorithm for computing the
sliced data that obey the l-diversity requirement. Our workload experiments
confirm that slicing preserves better utility than generalization and is more
effective than bucketization in workloads involving the sensitive attribute.
Our experiments also demonstrate that slicing can be used to prevent membership
disclosure.
</summary>
    <author>
      <name>Tiancheng Li</name>
    </author>
    <author>
      <name>Ninghui Li</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Ian Molloy</name>
    </author>
    <link href="http://arxiv.org/abs/0909.2290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2623v1</id>
    <updated>2009-09-14T19:07:58Z</updated>
    <published>2009-09-14T19:07:58Z</published>
    <title>Reducing Network Traffic in Unstructured P2P Systems Using Top-k Queries</title>
    <summary>  A major problem of unstructured P2P systems is their heavy network traffic.
This is caused mainly by high numbers of query answers, many of which are
irrelevant for users. One solution to this problem is to use Top-k queries
whereby the user can specify a limited number (k) of the most relevant answers.
In this paper, we present FD, a (Fully Distributed) framework for executing
Top-k queries in unstructured P2P systems, with the objective of reducing
network traffic. FD consists of a family of algorithms that are simple but
effec-tive. FD is completely distributed, does not depend on the existence of
certain peers, and addresses the volatility of peers during query execution. We
vali-dated FD through implementation over a 64-node cluster and simulation
using the BRITE topology generator and SimJava. Our performance evaluation
shows that FD can achieve major performance gains in terms of communication and
response time.
</summary>
    <author>
      <name>Reza Akbarinia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Lina</arxiv:affiliation>
    </author>
    <author>
      <name>Esther Pacitti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Lina</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick Valduriez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Lina</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10619-006-8313-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10619-006-8313-5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Distributed and Parallel Databases 19, 2-3 (2006) 67-86</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.2623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4409v1</id>
    <updated>2009-09-24T10:52:58Z</updated>
    <published>2009-09-24T10:52:58Z</published>
    <title>Clustering with Obstacles in Spatial Databases</title>
    <summary>  Clustering large spatial databases is an important problem, which tries to
find the densely populated regions in a spatial area to be used in data mining,
knowledge discovery, or efficient information retrieval. However most
algorithms have ignored the fact that physical obstacles such as rivers, lakes,
and highways exist in the real world and could thus affect the result of the
clustering. In this paper, we propose CPO, an efficient clustering technique to
solve the problem of clustering in the presence of obstacles. The proposed
algorithm divides the spatial area into rectangular cells. Each cell is
associated with statistical information used to label the cell as dense or
non-dense. It also labels each cell as obstructed (i.e. intersects any
obstacle) or nonobstructed. For each obstructed cell, the algorithm finds a
number of non-obstructed sub-cells. Then it finds the dense regions of
non-obstructed cells or sub-cells by a breadthfirst search as the required
clusters with a center to each region.
</summary>
    <author>
      <name>Mohamed A. El-Zawawy</name>
    </author>
    <author>
      <name>Mohamed E. El-Sharkawi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. 2001 IEEE Int. Symposium of Signal Processing and
  Information Technology (ISSPIT01), pages 420-425, Cairo, Egypt, Dec. 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.4409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.5530v1</id>
    <updated>2009-09-30T07:16:38Z</updated>
    <published>2009-09-30T07:16:38Z</published>
    <title>Differential Privacy via Wavelet Transforms</title>
    <summary>  Privacy preserving data publishing has attracted considerable research
interest in recent years. Among the existing solutions, {\em
$\epsilon$-differential privacy} provides one of the strongest privacy
guarantees. Existing data publishing methods that achieve
$\epsilon$-differential privacy, however, offer little data utility. In
particular, if the output dataset is used to answer count queries, the noise in
the query answers can be proportional to the number of tuples in the data,
which renders the results useless.
  In this paper, we develop a data publishing technique that ensures
$\epsilon$-differential privacy while providing accurate answers for {\em
range-count queries}, i.e., count queries where the predicate on each attribute
is a range. The core of our solution is a framework that applies {\em wavelet
transforms} on the data before adding noise to it. We present instantiations of
the proposed framework for both ordinal and nominal data, and we provide a
theoretical analysis on their privacy and utility guarantees. In an extensive
experimental study on both real and synthetic data, we show the effectiveness
and efficiency of our solution.
</summary>
    <author>
      <name>Xiaokui Xiao</name>
    </author>
    <author>
      <name>Guozhang Wang</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <link href="http://arxiv.org/abs/0909.5530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.5530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0983v1</id>
    <updated>2009-10-06T12:09:52Z</updated>
    <published>2009-10-06T12:09:52Z</published>
    <title>On Metric Skyline Processing by PM-tree</title>
    <summary>  The task of similarity search in multimedia databases is usually accomplished
by range or k nearest neighbor queries. However, the expressing power of these
"single-example" queries fails when the user's delicate query intent is not
available as a single example. Recently, the well-known skyline operator was
reused in metric similarity search as a "multi-example" query type. When
applied on a multi-dimensional database (i.e., on a multi-attribute table), the
traditional skyline operator selects all database objects that are not
dominated by other objects. The metric skyline query adopts the skyline
operator such that the multiple attributes are represented by distances
(similarities) to multiple query examples. Hence, we can view the metric
skyline as a set of representative database objects which are as similar to all
the examples as possible and, simultaneously, are semantically distinct. In
this paper we propose a technique of processing the metric skyline query by use
of PM-tree, while we show that our technique significantly outperforms the
original M-tree based implementation in both time and space costs. In
experiments we also evaluate the partial metric skyline processing, where only
a controlled number of skyline objects is retrieved.
</summary>
    <author>
      <name>Tomas Skopal</name>
    </author>
    <author>
      <name>Jakub Lokoc</name>
    </author>
    <link href="http://arxiv.org/abs/0910.0983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.3372v4</id>
    <updated>2010-03-04T18:36:16Z</updated>
    <published>2009-10-18T13:46:36Z</published>
    <title>Composition and Inversion of Schema Mappings</title>
    <summary>  In the recent years, a lot of attention has been paid to the development of
solid foundations for the composition and inversion of schema mappings. In this
paper, we review the proposals for the semantics of these crucial operators.
For each of these proposals, we concentrate on the three following problems:
the definition of the semantics of the operator, the language needed to express
the operator, and the algorithmic issues associated to the problem of computing
the operator. It should be pointed out that we primarily consider the
formalization of schema mappings introduced in the work on data exchange. In
particular, when studying the problem of computing the composition and inverse
of a schema mapping, we will be mostly interested in computing these operators
for mappings specified by source-to-target tuple-generating dependencies.
</summary>
    <author>
      <name>Marcelo Arenas</name>
    </author>
    <author>
      <name>Jorge Perez</name>
    </author>
    <author>
      <name>Juan Reutter</name>
    </author>
    <author>
      <name>Cristian Riveros</name>
    </author>
    <link href="http://arxiv.org/abs/0910.3372v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.3372v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0781v1</id>
    <updated>2009-11-04T11:19:55Z</updated>
    <published>2009-11-04T11:19:55Z</published>
    <title>A Way to Understand Various Patterns of Data Mining Techniques for
  Selected Domains</title>
    <summary>  This has much in common with traditional work in statistics and machine
learning. However, there are important new issues which arise because of the
sheer size of the data. One of the important problem in data mining is the
Classification-rule learning which involves finding rules that partition given
data into predefined classes. In the data mining domain where millions of
records and a large number of attributes are involved, the execution time of
existing algorithms can become prohibitive, particularly in interactive
applications.
</summary>
    <author>
      <name>Kanak Saxena</name>
    </author>
    <author>
      <name>D. S Rajpoot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 1, pp. 186-191, October 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.0781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4329v2</id>
    <updated>2009-11-24T01:00:10Z</updated>
    <published>2009-11-23T06:45:37Z</published>
    <title>Structural Consistency: Enabling XML Keyword Search to Eliminate
  Spurious Results Consistently</title>
    <summary>  XML keyword search is a user-friendly way to query XML data using only
keywords. In XML keyword search, to achieve high precision without sacrificing
recall, it is important to remove spurious results not intended by the user.
Efforts to eliminate spurious results have enjoyed some success by using the
concepts of LCA or its variants, SLCA and MLCA. However, existing methods still
could find many spurious results. The fundamental cause for the occurrence of
spurious results is that the existing methods try to eliminate spurious results
locally without global examination of all the query results and, accordingly,
some spurious results are not consistently eliminated. In this paper, we
propose a novel keyword search method that removes spurious results
consistently by exploiting the new concept of structural consistency.
</summary>
    <author>
      <name>Ki-Hoon Lee</name>
    </author>
    <author>
      <name>Kyu-Young Whang</name>
    </author>
    <author>
      <name>Wook-Shin Han</name>
    </author>
    <author>
      <name>Min-Soo Kim</name>
    </author>
    <link href="http://arxiv.org/abs/0911.4329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0840v1</id>
    <updated>2009-12-04T13:04:44Z</updated>
    <published>2009-12-04T13:04:44Z</published>
    <title>Applying an XML Warehouse to Social Network Analysis, Lessons from the
  WebStand Project</title>
    <summary>  In this paper we present the state of advancement of the French ANR WebStand
project. The objective of this project is to construct a customizable XML based
warehouse platform to acquire, transform, analyze, store, query and export data
from the web, in particular mailing lists, with the final intension of using
this data to perform sociological studies focused on social groups of World
Wide Web, with a specific emphasis on the temporal aspects of this data. We are
currently using this system to analyze the standardization process of the W3C,
through its social network of standard setters.
</summary>
    <author>
      <name>Benjamin Nguyen</name>
    </author>
    <author>
      <name>Antoine Vion</name>
    </author>
    <author>
      <name>Francois-Xavier Dudouet</name>
    </author>
    <author>
      <name>Loic Saint-Ghislain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">W3C Workshop on the Future of Social Networking</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">W3C Workshop on the Future of Social Networking, electronic
  proceedings available at http://www.w3.org/2008/09/msnws/ Barcelona, Spain,
  2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.0840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1016v1</id>
    <updated>2009-12-05T13:21:57Z</updated>
    <published>2009-12-05T13:21:57Z</published>
    <title>Refactoring of a Database</title>
    <summary>  The technique of database refactoring is all about applying disciplined and
controlled techniques to change an existing database schema. The problem is to
successfully create a Database Refactoring Framework for databases. This paper
concentrates on the feasibility of adapting this concept to work as a generic
template. To retain the constraints regardless of the modifications to the
metadata, the paper proposes a MetaData Manipulation Tool to facilitate change.
The tool adopts a Template Design Pattern to make it database independent. The
paper presents a drawback of using java for constraint extraction and proposes
an alternative.
</summary>
    <author>
      <name>Ayeesha Dsousa</name>
    </author>
    <author>
      <name>Shalini Bhatia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 307-315, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1110v1</id>
    <updated>2009-12-06T15:26:43Z</updated>
    <published>2009-12-06T15:26:43Z</published>
    <title>XML Multidimensional Modelling and Querying</title>
    <summary>  As XML becomes ubiquitous and XML storage and processing becomes more
efficient, the range of use cases for these technologies widens daily. One
promising area is the integration of XML and data warehouses, where an
XML-native database stores multidimensional data and processes OLAP queries
written in the XQuery interrogation language. This paper explores issues
arising in the implementation of such a data warehouse. We first compare
approaches for multidimensional data modelling in XML, then describe how
typical OLAP queries on these models can be expressed in XQuery. We then show
how, regardless of the model, the grouping features of XQuery 1.1 improve
performance and readability of these queries. Finally, we evaluate the
performance of query evaluation in each modelling choice using the eXist
database, which we extended with a grouping clause implementation.
</summary>
    <author>
      <name>Serge Boucher</name>
    </author>
    <author>
      <name>Boris Verhaegen</name>
    </author>
    <author>
      <name>Esteban Zimányi</name>
    </author>
    <link href="http://arxiv.org/abs/0912.1110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2282v1</id>
    <updated>2009-12-11T17:04:01Z</updated>
    <published>2009-12-11T17:04:01Z</published>
    <title>Design of Intelligent layer for flexible querying in databases</title>
    <summary>  Computer-based information technologies have been extensively used to help
many organizations, private companies, and academic and education institutions
manage their processes and information systems hereby become their nervous
centre. The explosion of massive data sets created by businesses, science and
governments necessitates intelligent and more powerful computing paradigms so
that users can benefit from this data. Therefore most new-generation database
applications demand intelligent information management to enhance efficient
interactions between database and the users. Database systems support only a
Boolean query model. A selection query on SQL database returns all those tuples
that satisfy the conditions in the query.
</summary>
    <author>
      <name>Mrs. Neelu Nihalani</name>
    </author>
    <author>
      <name>Dr. Sanjay Silakari</name>
    </author>
    <author>
      <name>Dr. Mahesh Motwani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSE Volume 1 Issue 2 2009 30-39</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.2282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2548v2</id>
    <updated>2010-01-26T05:26:00Z</updated>
    <published>2009-12-13T23:30:24Z</published>
    <title>Towards Utility-driven Anonymization of Transactions</title>
    <summary>  Publishing person-specific transactions in an anonymous form is increasingly
required by organizations. Recent approaches ensure that potentially
identifying information (e.g., a set of diagnosis codes) cannot be used to link
published transactions to persons' identities, but all are limited in
application because they incorporate coarse privacy requirements (e.g.,
protecting a certain set of m diagnosis codes requires protecting all m-sized
sets), do not integrate utility requirements, and tend to explore a small
portion of the solution space. In this paper, we propose a more general
framework for anonymizing transactional data under specific privacy and utility
requirements. We model such requirements as constraints, investigate how these
constraints can be specified, and propose COAT (COnstraint-based Anonymization
of Transactions), an algorithm that anonymizes transactions using a flexible
hierarchy-free generalization scheme to meet the specified constraints.
Experiments with benchmark datasets verify that COAT significantly outperforms
the current state-of-the-art algorithm in terms of data utility, while being
comparable in terms of efficiency. The effectiveness of our approach is also
demonstrated in a real-world scenario, which requires disseminating a private,
patient-specific transactional dataset in a way that preserves both privacy and
utility in intended studies.
</summary>
    <author>
      <name>Grigorios Loukides</name>
    </author>
    <author>
      <name>Aris Gkoulalas-Divanis</name>
    </author>
    <author>
      <name>Bradley Malin</name>
    </author>
    <link href="http://arxiv.org/abs/0912.2548v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2548v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.4742v2</id>
    <updated>2010-09-06T13:24:19Z</updated>
    <published>2009-12-23T21:22:50Z</published>
    <title>Optimizing Histogram Queries under Differential Privacy</title>
    <summary>  Differential privacy is a robust privacy standard that has been successfully
applied to a range of data analysis tasks. Despite much recent work, optimal
strategies for answering a collection of correlated queries are not known.
  We study the problem of devising a set of strategy queries, to be submitted
and answered privately, that will support the answers to a given workload of
queries. We propose a general framework in which query strategies are formed
from linear combinations of counting queries, and we describe an optimal method
for deriving new query answers from the answers to the strategy queries. Using
this framework we characterize the error of strategies geometrically, and we
propose solutions to the problem of finding optimal strategies.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Michael Hay</name>
    </author>
    <author>
      <name>Vibhor Rastogi</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <author>
      <name>Andrew McGregor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.4742v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.4742v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.5426v1</id>
    <updated>2009-12-30T08:31:10Z</updated>
    <published>2009-12-30T08:31:10Z</published>
    <title>The Hardness and Approximation Algorithms for L-Diversity</title>
    <summary>  The existing solutions to privacy preserving publication can be classified
into the theoretical and heuristic categories. The former guarantees provably
low information loss, whereas the latter incurs gigantic loss in the worst
case, but is shown empirically to perform well on many real inputs. While
numerous heuristic algorithms have been developed to satisfy advanced privacy
principles such as l-diversity, t-closeness, etc., the theoretical category is
currently limited to k-anonymity which is the earliest principle known to have
severe vulnerability to privacy attacks. Motivated by this, we present the
first theoretical study on l-diversity, a popular principle that is widely
adopted in the literature. First, we show that optimal l-diverse generalization
is NP-hard even when there are only 3 distinct sensitive values in the
microdata. Then, an (l*d)-approximation algorithm is developed, where d is the
dimensionality of the underlying dataset. This is the first known algorithm
with a non-trivial bound on information loss. Extensive experiments with real
datasets validate the effectiveness and efficiency of proposed solution.
</summary>
    <author>
      <name>Xiaokui Xiao</name>
    </author>
    <author>
      <name>Ke Yi</name>
    </author>
    <author>
      <name>Yufei Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EDBT 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.5426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.5426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1276v1</id>
    <updated>2010-01-08T13:53:15Z</updated>
    <published>2010-01-08T13:53:15Z</published>
    <title>A framework to model real-time databases</title>
    <summary>  Real-time databases deal with time-constrained data and time-constrained
transactions. The design of this kind of databases requires the introduction of
new concepts to support both data structures and the dynamic behaviour of the
database. In this paper, we give an overview about different aspects of
real-time databases and we clarify requirements of their modelling. Then, we
present a framework for real-time database design and describe its fundamental
operations. A case study demonstrates the validity of the structural model and
illustrates SQL queries and Java code generated from the classes of the model
</summary>
    <author>
      <name>Nizar Idoudi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Nada louati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Claude Duvallet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Sadeg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Rafik Bouaziz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Faiez Gargouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computing and Information Sciences
  (IJCIS) 6, 1 (2008) On line</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.1276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.2270v1</id>
    <updated>2010-01-13T19:08:49Z</updated>
    <published>2010-01-13T19:08:49Z</published>
    <title>An Improved Approach to High Level Privacy Preserving Itemset Mining</title>
    <summary>  Privacy preserving association rule mining has triggered the development of
many privacy preserving data mining techniques. A large fraction of them use
randomized data distortion techniques to mask the data for preserving. This
paper proposes a new transaction randomization method which is a combination of
the fake transaction randomization method and a new per transaction
randomization method. This method distorts the items within each transaction
and ensures a higher level of data privacy in comparison to the previous
approaches. The pertransaction randomization method involves a randomization
function to replace the item by a random number guarantying privacy within the
transaction also. A tool has also been developed to implement the proposed
approach to mine frequent itemsets and association rules from the data
guaranteeing the antimonotonic property.
</summary>
    <author>
      <name>Rajesh Kumar Boora</name>
    </author>
    <author>
      <name>Ruchi Shukla</name>
    </author>
    <author>
      <name>A. K. Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS December 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 3, pp. 216-223, December 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.2270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3488v1</id>
    <updated>2010-01-20T07:49:15Z</updated>
    <published>2010-01-20T07:49:15Z</published>
    <title>A Model for Mining Multilevel Fuzzy Association Rule in Database</title>
    <summary>  The problem of developing models and algorithms for multilevel association
mining pose for new challenges for mathematics and computer science. These
problems become more challenging, when some form of uncertainty like fuzziness
is present in data or relationships in data. This paper proposes a multilevel
fuzzy association rule mining models for extracting knowledge implicit in
transactions database with different support at each level. The proposed
algorithm adopts a top-down progressively deepening approach to derive large
itemsets. This approach incorporates fuzzy boundaries instead of sharp boundary
intervals. An example is also given to demonstrate that the proposed mining
algorithm can derive the multiple-level association rules under different
supports in a simple and effective manner.
</summary>
    <author>
      <name>Pratima Gautam</name>
    </author>
    <author>
      <name>Neelu Khare</name>
    </author>
    <author>
      <name>K. R. Pardasani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Vol. 2, Issue 1, January 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3498v1</id>
    <updated>2010-01-20T08:12:52Z</updated>
    <published>2010-01-20T08:12:52Z</published>
    <title>Interestingness Measure for Mining Spatial Gene Expression Data using
  Association Rule</title>
    <summary>  The search for interesting association rules is an important topic in
knowledge discovery in spatial gene expression databases. The set of admissible
rules for the selected support and confidence thresholds can easily be
extracted by algorithms based on support and confidence, such as Apriori.
However, they may produce a large number of rules, many of them are
uninteresting. The challenge in association rule mining (ARM) essentially
becomes one of determining which rules are the most interesting. Association
rule interestingness measures are used to help select and rank association rule
patterns. Besides support and confidence, there are other interestingness
measures, which include generality reliability, peculiarity, novelty,
surprisingness, utility, and applicability. In this paper, the application of
the interesting measures entropy and variance for association pattern discovery
from spatial gene expression data has been studied. In this study the fast
mining algorithm has been used which produce candidate itemsets and it spends
less time for calculating k-supports of the itemsets with the Boolean matrix
pruned, and it scans the database only once and needs less memory space.
Experimental results show that using entropy as the measure of interest for the
spatial gene expression data has more diverse and interesting rules.
</summary>
    <author>
      <name>M. Anandhavalli</name>
    </author>
    <author>
      <name>M. K. Ghose</name>
    </author>
    <author>
      <name>K. Gauthaman</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Vol. 2, Issue 1, January 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0971v1</id>
    <updated>2010-02-04T12:09:15Z</updated>
    <published>2010-02-04T12:09:15Z</published>
    <title>The WebStand Project</title>
    <summary>  In this paper we present the state of advancement of the French ANR WebStand
project. The objective of this project is to construct a customizable XML based
warehouse platform to acquire, transform, analyze, store, query and export data
from the web, in particular mailing lists, with the final intension of using
this data to perform sociological studies focused on social groups of World
Wide Web, with a specific emphasis on the temporal aspects of this data. We are
currently using this system to analyze the standardization process of the W3C,
through its social network of standard setters.
</summary>
    <author>
      <name>Benjamin Nguyen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>François-Xavier Dudouet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LASP, IRISES</arxiv:affiliation>
    </author>
    <author>
      <name>Dario Colazzo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Vion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEST</arxiv:affiliation>
    </author>
    <author>
      <name>Ioana Manolescu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Saclay - Ile de France</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Senellart</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WebSci'09: Society On-Line Conference, Greece (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.0971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1104v1</id>
    <updated>2010-02-04T23:33:47Z</updated>
    <published>2010-02-04T23:33:47Z</published>
    <title>An Efficient Rigorous Approach for Identifying Statistically Significant
  Frequent Itemsets</title>
    <summary>  As advances in technology allow for the collection, storage, and analysis of
vast amounts of data, the task of screening and assessing the significance of
discovered patterns is becoming a major challenge in data mining applications.
In this work, we address significance in the context of frequent itemset
mining. Specifically, we develop a novel methodology to identify a meaningful
support threshold s* for a dataset, such that the number of itemsets with
support at least s* represents a substantial deviation from what would be
expected in a random dataset with the same number of transactions and the same
individual item frequencies. These itemsets can then be flagged as
statistically significant with a small false discovery rate. We present
extensive experimental results to substantiate the effectiveness of our
methodology.
</summary>
    <author>
      <name>Adam Kirsch</name>
    </author>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Andrea Pietracaprina</name>
    </author>
    <author>
      <name>Geppino Pucci</name>
    </author>
    <author>
      <name>Eli Upfal</name>
    </author>
    <author>
      <name>Fabio Vandin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this work was presented in ACM PODS 2009. 20
  pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.1104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1143v2</id>
    <updated>2010-02-08T11:39:13Z</updated>
    <published>2010-02-05T08:22:24Z</published>
    <title>A Logical Temporal Relational Data Model</title>
    <summary>  Time is one of the most difficult aspects to handle in real world
applications such as database systems. Relational database management systems
proposed by Codd offer very little built-in query language support for temporal
data management. The model itself incorporates neither the concept of time nor
any theory of temporal semantics. Many temporal extensions of the relational
model have been proposed and some of them are also implemented. This paper
offers a brief introduction to temporal database research. We propose a
conceptual model for handling time varying attributes in the relational
database model with minimal temporal attributes.
</summary>
    <author>
      <name>Nadeem Mahmood</name>
    </author>
    <author>
      <name>Aqil Burney</name>
    </author>
    <author>
      <name>Kamran Ahsan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/A-Logical-Temporal-Relational-Data-Model.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/A-Logical-Temporal-Relational-Data-Model.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1143v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1143v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1159v1</id>
    <updated>2010-02-05T09:07:54Z</updated>
    <published>2010-02-05T09:07:54Z</published>
    <title>Mining The Successful Binary Combinations: Methodology and A Simple Case
  Study</title>
    <summary>  The importance of finding the characteristics leading to either a success or
a failure is one of the driving forces of data mining. The various application
areas of finding success/failure factors cover vast variety of areas such as
credit risk evaluation and granting loans, micro array analysis, health factors
and health risk factors, and parameter combination leading to a product
success. This paper presents a new approach for making inferences about
dichotomous data. The objective is to determine rules that lead to a certain
result. The method consists of four phases: in the first phase, the data is
processed into a binary format of a truth table, in the second phase; rules are
found by utilizing an algorithm that minimizes Boolean functions. In the third
phase the rules are checked and filtered. In the fourth phase, simple rules
that involve one to two features are revealed.
</summary>
    <author>
      <name>Yuval Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Mining-The-Successful-Binary-Combinations-Methodology-and-A-Simple-Case-Study.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Mining-The-Successful-Binary-Combinations-Methodology-and-A-Simple-Case-Study.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1185v1</id>
    <updated>2010-02-05T10:30:51Z</updated>
    <published>2010-02-05T10:30:51Z</published>
    <title>Significant Interval and Frequent Pattern Discovery in Web Log Data</title>
    <summary>  There is a considerable body of work on sequence mining of Web Log Data. We
are using One Pass frequent Episode discovery (or FED) algorithm, takes a
different approach than the traditional apriori class of pattern detection
algorithms. In this approach significant intervals for each Website are
computed first (independently) and these interval used for detecting frequent
patterns/Episode and then the Analysis is performed on Significant Intervals
and frequent patterns That can be used to forecast the user's behavior using
previous trends and this can be also used for advertising purpose. This type of
applications predicts the Website interest. In this approach, time-series data
are folded over a periodicity (day, week, etc.) Which are used to form the
Interval? Significant intervals are discovered from these time points that
satisfy the criteria of minimum confidence and maximum interval length
specified by the user.
</summary>
    <author>
      <name>Kanak Saxena</name>
    </author>
    <author>
      <name>Rahul Shukla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 3, January 2010,
  http://ijcsi.org/articles/Significant-Interval-and-Frequent-Pattern-Discovery-in-Web-Log-Data.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 3, January 2010,
  http://ijcsi.org/articles/Significant-Interval-and-Frequent-Pattern-Discovery-in-Web-Log-Data.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.4315v2</id>
    <updated>2010-03-06T11:24:01Z</updated>
    <published>2010-02-23T12:45:31Z</published>
    <title>Mining Statistically Significant Substrings Based on the Chi-Square
  Measure</title>
    <summary>  Given the vast reservoirs of data stored worldwide, efficient mining of data
from a large information store has emerged as a great challenge. Many databases
like that of intrusion detection systems, web-click records, player statistics,
texts, proteins etc., store strings or sequences. Searching for an unusual
pattern within such long strings of data has emerged as a requirement for
diverse applications. Given a string, the problem then is to identify the
substrings that differs the most from the expected or normal behavior, i.e.,
the substrings that are statistically significant. In other words, these
substrings are less likely to occur due to chance alone and may point to some
interesting information or phenomenon that warrants further exploration. To
this end, we use the chi-square measure. We propose two heuristics for
retrieving the top-k substrings with the largest chi-square measure. We show
that the algorithms outperform other competing algorithms in the runtime, while
maintaining a high approximation ratio of more than 0.96.
</summary>
    <author>
      <name>Sourav Dutta</name>
    </author>
    <author>
      <name>Arnab Bhattacharya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, to appear in PAKDD 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.4315v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4315v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1179v1</id>
    <updated>2010-03-05T03:24:13Z</updated>
    <published>2010-03-05T03:24:13Z</published>
    <title>View Synthesis from Schema Mappings</title>
    <summary>  In data management, and in particular in data integration, data exchange,
query optimization, and data privacy, the notion of view plays a central role.
In several contexts, such as data integration, data mashups, and data
warehousing, the need arises of designing views starting from a set of known
correspondences between queries over different schemas. In this paper we deal
with the issue of automating such a design process. We call this novel problem
"view synthesis from schema mappings": given a set of schema mappings, each
relating a query over a source schema to a query over a target schema,
automatically synthesize for each source a view over the target schema in such
a way that for each mapping, the query over the source is a rewriting of the
query over the target wrt the synthesized views. We study view synthesis from
schema mappings both in the relational setting, where queries and views are
(unions of) conjunctive queries, and in the semistructured data setting, where
queries and views are (two-way) regular path queries, as well as unions of
conjunctions thereof. We provide techniques and complexity upper bounds for
each of these cases.
</summary>
    <author>
      <name>Diego Calvanese</name>
    </author>
    <author>
      <name>Giuseppe De Giacomo</name>
    </author>
    <author>
      <name>Maurizio Lenzerini</name>
    </author>
    <author>
      <name>Moshe Y. Vardi</name>
    </author>
    <link href="http://arxiv.org/abs/1003.1179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1500v1</id>
    <updated>2010-03-07T17:44:18Z</updated>
    <published>2010-03-07T17:44:18Z</published>
    <title>Hierarchical Approach for Online Mining--Emphasis towards Software
  Metrics</title>
    <summary>  Several multi-pass algorithms have been proposed for Association Rule Mining
from static repositories. However, such algorithms are incapable of online
processing of transaction streams. In this paper we introduce an efficient
single-pass algorithm for mining association rules, given a hierarchical
classification amongest items. Processing efficiency is achieved by utilizing
two optimizations, hierarchy aware counting and transaction reduction, which
become possible in the context of hierarchical classification. This paper
considers the problem of integrating constraints that are Boolean expression
over the presence or absence of items into the association discovery algorithm.
This paper present three integrated algorithms for mining association rules
with item constraints and discuss their tradeoffs. It is concluded that the
variation of complexity depends on the measure of DIT (Depth of Inheritance
Tree) and NOC (Number of Children) in the context of Hierarchical
Classification.
</summary>
    <author>
      <name>M . V. Vijaya Saradhi</name>
    </author>
    <author>
      <name>B. R. Sastry</name>
    </author>
    <author>
      <name>P. Satish</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.1500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
