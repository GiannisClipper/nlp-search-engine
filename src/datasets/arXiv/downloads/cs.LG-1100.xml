<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.LG%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.LG&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/5SYktSrE11RlA17J4OkPFAyjW5c</id>
  <updated>2025-03-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">211590</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0607136v1</id>
    <updated>2006-07-28T21:45:41Z</updated>
    <published>2006-07-28T21:45:41Z</published>
    <title>Competing with Markov prediction strategies</title>
    <summary>  Assuming that the loss function is convex in the prediction, we construct a
prediction strategy universal for the class of Markov prediction strategies,
not necessarily continuous. Allowing randomization, we remove the requirement
of convexity.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1258v1</id>
    <updated>2009-02-07T18:01:09Z</updated>
    <published>2009-02-07T18:01:09Z</published>
    <title>Extraction de concepts sous contraintes dans des données d'expression
  de gènes</title>
    <summary>  In this paper, we propose a technique to extract constrained formal concepts.
</summary>
    <author>
      <name>Baptiste Jeudy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>François Rioult</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GREYC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence sur l'apprentissage automatique, Nice : France (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2299v3</id>
    <updated>2013-07-08T15:17:20Z</updated>
    <published>2009-03-13T13:47:03Z</published>
    <title>Differential Contrastive Divergence</title>
    <summary>  This paper has been retracted.
</summary>
    <author>
      <name>David McAllester</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was a rediscovery of known material</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.2299v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2299v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0453v2</id>
    <updated>2009-07-20T08:59:45Z</updated>
    <published>2009-07-02T17:54:45Z</published>
    <title>Random DFAs are Efficiently PAC Learnable</title>
    <summary>  This paper has been withdrawn due to an error found by Dana Angluin and Lev
Reyzin.
</summary>
    <author>
      <name>Leonid Aryeh Kontorovich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4080v1</id>
    <updated>2011-07-20T19:34:00Z</updated>
    <published>2011-07-20T19:34:00Z</published>
    <title>On the Universality of Online Mirror Descent</title>
    <summary>  We show that for a general class of convex online learning problems, Mirror
Descent can always achieve a (nearly) optimal regret guarantee.
</summary>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <author>
      <name>Karthik Sridharan</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <link href="http://arxiv.org/abs/1107.4080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7897v1</id>
    <updated>2014-05-30T15:50:28Z</updated>
    <published>2014-05-30T15:50:28Z</published>
    <title>Flip-Flop Sublinear Models for Graphs: Proof of Theorem 1</title>
    <summary>  We prove that there is no class-dual for almost all sublinear models on
graphs.
</summary>
    <author>
      <name>Brijnesh Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material for B. Jain. Flip-Flop Sublinear Models for
  Graphs, S+SSPR 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03326v2</id>
    <updated>2016-02-01T07:33:08Z</updated>
    <published>2015-08-13T19:49:08Z</published>
    <title>A Survey on Contextual Multi-armed Bandits</title>
    <summary>  In this survey we cover a few stochastic and adversarial contextual bandit
algorithms. We analyze each algorithm's assumption and regret bound.
</summary>
    <author>
      <name>Li Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1508.03326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03300v2</id>
    <updated>2018-08-16T17:15:44Z</updated>
    <published>2018-08-09T18:43:18Z</published>
    <title>$α$-Approximation Density-based Clustering of Multi-valued Objects</title>
    <summary>  This submission has been removed by arXiv administrators due to copyright
infringement.
</summary>
    <author>
      <name>Zhilin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This submission has been removed by arXiv administrators due to
  copyright infringement</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03300v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03300v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03550v1</id>
    <updated>2019-09-08T21:49:42Z</updated>
    <published>2019-09-08T21:49:42Z</published>
    <title>Lecture Notes: Optimization for Machine Learning</title>
    <summary>  Lecture notes on optimization for machine learning, derived from a course at
Princeton University and tutorials given in MLSS, Buenos Aires, as well as
Simons Foundation, Berkeley.
</summary>
    <author>
      <name>Elad Hazan</name>
    </author>
    <link href="http://arxiv.org/abs/1909.03550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00776v1</id>
    <updated>2019-11-02T19:53:32Z</updated>
    <published>2019-11-02T19:53:32Z</published>
    <title>Ten-year Survival Prediction for Breast Cancer Patients</title>
    <summary>  This report assesses different machine learning approaches to 10-year
survival prediction of breast cancer patients.
</summary>
    <author>
      <name>Changmao Li</name>
    </author>
    <author>
      <name>Han He</name>
    </author>
    <author>
      <name>Yunze Hao</name>
    </author>
    <author>
      <name>Caleb Ziems</name>
    </author>
    <link href="http://arxiv.org/abs/1911.00776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04947v1</id>
    <updated>2018-02-14T03:57:59Z</updated>
    <published>2018-02-14T03:57:59Z</published>
    <title>Attack RMSE Leaderboard: An Introduction and Case Study</title>
    <summary>  In this manuscript, we briefly introduce several tricks to climb the
leaderboards which use RMSE for evaluation without exploiting any training
data.
</summary>
    <author>
      <name>Cong Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00403v1</id>
    <updated>2018-04-02T05:44:59Z</updated>
    <published>2018-04-02T05:44:59Z</published>
    <title>A Note on Kaldi's PLDA Implementation</title>
    <summary>  Some explanations to Kaldi's PLDA implementation to make formula derivation
easier to catch.
</summary>
    <author>
      <name>Ke Ding</name>
    </author>
    <link href="http://arxiv.org/abs/1804.00403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00297v1</id>
    <updated>2018-07-01T09:01:52Z</updated>
    <published>2018-07-01T09:01:52Z</published>
    <title>Exponential Convergence of the Deep Neural Network Approximation for
  Analytic Functions</title>
    <summary>  We prove that for analytic functions in low dimension, the convergence rate
of the deep neural network approximation is exponential.
</summary>
    <author>
      <name>Weinan E</name>
    </author>
    <author>
      <name>Qingcan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05244v1</id>
    <updated>2020-04-10T22:14:52Z</updated>
    <published>2020-04-10T22:14:52Z</published>
    <title>Efficient Sampled Softmax for Tensorflow</title>
    <summary>  This short paper discusses an efficient implementation of \emph{sampled
softmax loss} for Tensorflow. The speedup over the default implementation is
achieved due to simplification of the graph for the forward and backward
passes.
</summary>
    <author>
      <name>Maciej Skorski</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.03015v1</id>
    <updated>2021-06-06T03:03:04Z</updated>
    <published>2021-06-06T03:03:04Z</published>
    <title>Learning proofs for the classification of nilpotent semigroups</title>
    <summary>  Machine learning is applied to find proofs, with smaller or smallest numbers
of nodes, for the classification of 4-nilpotent semigroups.
</summary>
    <author>
      <name>Carlos Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/2106.03015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.03015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T15 (Primary) 20M10, 03F07, 03B35 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.06355v1</id>
    <updated>2022-01-17T11:35:33Z</updated>
    <published>2022-01-17T11:35:33Z</published>
    <title>Probabilistic Alternatives to the Gower Distance: A Note on Deodata
  Predictors</title>
    <summary>  A probabilistic alternative to the Gower distance is proposed. The
probabilistic distance enables the realization of a generic deodata predictor.
</summary>
    <author>
      <name>Cristian Alb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.26146.35522</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.26146.35522" rel="related"/>
    <link href="http://arxiv.org/abs/2201.06355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5330v1</id>
    <updated>2014-10-17T00:50:42Z</updated>
    <published>2014-10-17T00:50:42Z</published>
    <title>An Overview of General Performance Metrics of Binary Classifier Systems</title>
    <summary>  This document provides a brief overview of different metrics and terminology
that is used to measure the performance of binary classification systems.
</summary>
    <author>
      <name>Sebastian Raschka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05266v1</id>
    <updated>2018-11-13T13:02:55Z</updated>
    <published>2018-11-13T13:02:55Z</published>
    <title>A conjugate prior for the Dirichlet distribution</title>
    <summary>  This note investigates a conjugate class for the Dirichlet distribution class
in the exponential family.
</summary>
    <author>
      <name>Jean-Marc Andreoli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.05266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60E99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02322v1</id>
    <updated>2019-02-06T18:38:02Z</updated>
    <published>2019-02-06T18:38:02Z</published>
    <title>Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?</title>
    <summary>  No.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <link href="http://arxiv.org/abs/1902.02322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11544v1</id>
    <updated>2019-10-25T06:13:08Z</updated>
    <published>2019-10-25T06:13:08Z</published>
    <title>Strong Log-Concavity Does Not Imply Log-Submodularity</title>
    <summary>  We disprove a recent conjecture regarding discrete distributions and their
generating polynomials stating that strong log-concavity implies
log-submodularity.
</summary>
    <author>
      <name>Alkis Gotovos</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.12017v1</id>
    <updated>2021-01-21T21:43:39Z</updated>
    <published>2021-01-21T21:43:39Z</published>
    <title>A Fully Rigorous Proof of the Derivation of Xavier and He's
  Initialization for Deep ReLU Networks</title>
    <summary>  A fully rigorous proof of the derivation of Xavier/He's initialization for
ReLU nets is given.
</summary>
    <author>
      <name>Quynh Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/2101.12017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.03720v1</id>
    <updated>2024-05-05T20:39:15Z</updated>
    <published>2024-05-05T20:39:15Z</published>
    <title>Spatial Transfer Learning with Simple MLP</title>
    <summary>  First step to investigate the potential of transfer learning applied to the
field of spatial statistics
</summary>
    <author>
      <name>Hongjian Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.03720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.03720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.07022v1</id>
    <updated>2024-05-11T14:15:13Z</updated>
    <published>2024-05-11T14:15:13Z</published>
    <title>DTMamba : Dual Twin Mamba for Time Series Forecasting</title>
    <summary>  We utilized the Mamba model for time series data prediction tasks, and the
experimental results indicate that our model performs well.
</summary>
    <author>
      <name>Zexue Wu</name>
    </author>
    <author>
      <name>Yifeng Gong</name>
    </author>
    <author>
      <name>Aoqian Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.07022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.07022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.19370v1</id>
    <updated>2024-10-25T08:16:19Z</updated>
    <published>2024-10-25T08:16:19Z</published>
    <title>Notes on the Mathematical Structure of GPT LLM Architectures</title>
    <summary>  An exposition of the mathematics underpinning the neural network architecture
of a GPT-3-style LLM.
</summary>
    <author>
      <name>Spencer Becker-Kahn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.19370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.19370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0201009v1</id>
    <updated>2002-01-14T18:38:55Z</updated>
    <published>2002-01-14T18:38:55Z</published>
    <title>The performance of the batch learner algorithm</title>
    <summary>  We analyze completely the convergence speed of the \emph{batch learning
algorithm}, and compare its speed to that of the memoryless learning algorithm
and of learning with memory. We show that the batch learning algorithm is never
worse than the memoryless learning algorithm (at least asymptotically). Its
performance \emph{vis-a-vis} learning with full memory is less clearcut, and
depends on certain probabilistic assumptions.
</summary>
    <author>
      <name>Igor Rivin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supercedes a part of cs.LG/0107033</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0201009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0201009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411099v1</id>
    <updated>2004-11-30T08:36:59Z</updated>
    <published>2004-11-30T08:36:59Z</published>
    <title>A Note on the PAC Bayesian Theorem</title>
    <summary>  We prove general exponential moment inequalities for averages of [0,1]-valued
iid random variables and use them to tighten the PAC Bayesian Theorem. The
logarithmic dependence on the sample count in the enumerator of the PAC
Bayesian bound is halved.
</summary>
    <author>
      <name>Andreas Maurer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0411099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506057v2</id>
    <updated>2005-07-21T02:43:12Z</updated>
    <published>2005-06-14T04:00:38Z</published>
    <title>About one 3-parameter Model of Testing</title>
    <summary>  This article offers a 3-parameter model of testing, with 1) the difference
between the ability level of the examinee and item difficulty; 2) the examinee
discrimination and 3) the item discrimination as model parameters.
</summary>
    <author>
      <name>Kromer Victor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages; in Russian; Paper with changed content</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1988v1</id>
    <updated>2008-01-14T06:56:42Z</updated>
    <published>2008-01-14T06:56:42Z</published>
    <title>Online variants of the cross-entropy method</title>
    <summary>  The cross-entropy method is a simple but efficient method for global
optimization. In this paper we provide two online variants of the basic CEM,
together with a proof of convergence.
</summary>
    <author>
      <name>Istvan Szita</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.1988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4061v1</id>
    <updated>2008-01-26T07:32:48Z</updated>
    <published>2008-01-26T07:32:48Z</published>
    <title>The optimal assignment kernel is not positive definite</title>
    <summary>  We prove that the optimal assignment kernel, proposed recently as an attempt
to embed labeled graphs and more generally tuples of basic data to a Hilbert
space, is in fact not always positive definite.
</summary>
    <author>
      <name>Jean-Philippe Vert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CB</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0801.4061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2362v1</id>
    <updated>2008-05-15T17:25:03Z</updated>
    <published>2008-05-15T17:25:03Z</published>
    <title>An optimization problem on the sphere</title>
    <summary>  We prove existence and uniqueness of the minimizer for the average geodesic
distance to the points of a geodesically convex set on the sphere. This implies
a corresponding existence and uniqueness result for an optimal algorithm for
halfspace learning, when data and target functions are drawn from the uniform
distribution.
</summary>
    <author>
      <name>Andreas Maurer</name>
    </author>
    <link href="http://arxiv.org/abs/0805.2362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3664v1</id>
    <updated>2009-04-23T11:40:57Z</updated>
    <published>2009-04-23T11:40:57Z</published>
    <title>Introduction to Machine Learning: Class Notes 67577</title>
    <summary>  Introduction to Machine learning covering Statistical Inference (Bayes, EM,
ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),
and PAC learning (the Formal model, VC dimension, Double Sampling theorem).
</summary>
    <author>
      <name>Amnon Shashua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">109 pages, class notes of Machine Learning course given at the Hebrew
  University of Jerusalem</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.3664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.4144v1</id>
    <updated>2009-08-28T07:09:19Z</updated>
    <published>2009-08-28T07:09:19Z</published>
    <title>ABC-LogitBoost for Multi-class Classification</title>
    <summary>  We develop abc-logitboost, based on the prior work on abc-boost and robust
logitboost. Our extensive experiments on a variety of datasets demonstrate the
considerable improvement of abc-logitboost over logitboost and abc-mart.
</summary>
    <author>
      <name>Ping Li</name>
    </author>
    <link href="http://arxiv.org/abs/0908.4144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.4144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2899v2</id>
    <updated>2012-07-09T18:22:27Z</updated>
    <published>2010-06-15T06:55:03Z</published>
    <title>Approximated Structured Prediction for Learning Large Scale Graphical
  Models</title>
    <summary>  This manuscripts contains the proofs for "A Primal-Dual Message-Passing
Algorithm for Approximated Large Scale Structured Prediction".
</summary>
    <author>
      <name>Tamir Hazan</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2899v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2899v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2392v2</id>
    <updated>2011-10-13T19:04:19Z</updated>
    <published>2011-10-11T14:53:35Z</published>
    <title>A Variant of Azuma's Inequality for Martingales with Subgaussian Tails</title>
    <summary>  We provide a variant of Azuma's concentration inequality for martingales, in
which the standard boundedness requirement is replaced by the milder
requirement of a subgaussian tail.
</summary>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <link href="http://arxiv.org/abs/1110.2392v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2392v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3524v1</id>
    <updated>2013-01-15T22:51:40Z</updated>
    <published>2013-01-15T22:51:40Z</published>
    <title>How good is the Electricity benchmark for evaluating concept drift
  adaptation</title>
    <summary>  In this correspondence, we will point out a problem with testing adaptive
classifiers on autocorrelated data. In such a case random change alarms may
boost the accuracy figures. Hence, we cannot be sure if the adaptation is
working well.
</summary>
    <author>
      <name>Indre Zliobaite</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages of content, 1 appendix, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4917v1</id>
    <updated>2013-01-21T16:27:17Z</updated>
    <published>2013-01-21T16:27:17Z</published>
    <title>Dirichlet draws are sparse with high probability</title>
    <summary>  This note provides an elementary proof of the folklore fact that draws from a
Dirichlet distribution (with parameters less than 1) are typically sparse (most
coordinates are small).
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.4917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0208v2</id>
    <updated>2013-07-23T02:13:57Z</updated>
    <published>2013-05-01T15:45:34Z</published>
    <title>Perceptron Mistake Bounds</title>
    <summary>  We present a brief survey of existing mistake bounds and introduce novel
bounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds
generalize beyond standard margin-loss type bounds, allow for any convex and
Lipschitz loss function, and admit a very simple proof.
</summary>
    <author>
      <name>Mehryar Mohri</name>
    </author>
    <author>
      <name>Afshin Rostamizadeh</name>
    </author>
    <link href="http://arxiv.org/abs/1305.0208v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0208v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5600v1</id>
    <updated>2014-06-21T11:47:21Z</updated>
    <published>2014-06-21T11:47:21Z</published>
    <title>From conformal to probabilistic prediction</title>
    <summary>  This paper proposes a new method of probabilistic prediction, which is based
on conformal prediction. The method is applied to the standard USPS data set
and gives encouraging results.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <author>
      <name>Ivan Petej</name>
    </author>
    <author>
      <name>Valentina Fedorova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.5600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7429v1</id>
    <updated>2014-06-28T18:59:44Z</updated>
    <published>2014-06-28T18:59:44Z</published>
    <title>Comparison of SVM Optimization Techniques in the Primal</title>
    <summary>  This paper examines the efficacy of different optimization techniques in a
primal formulation of a support vector machine (SVM). Three main techniques are
compared. The dataset used to compare all three techniques was the Sentiment
Analysis on Movie Reviews dataset, from kaggle.com.
</summary>
    <author>
      <name>Jonathan Katzman</name>
    </author>
    <author>
      <name>Diane Duros</name>
    </author>
    <link href="http://arxiv.org/abs/1406.7429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6618v1</id>
    <updated>2014-08-28T03:29:06Z</updated>
    <published>2014-08-28T03:29:06Z</published>
    <title>Falsifiable implies Learnable</title>
    <summary>  The paper demonstrates that falsifiability is fundamental to learning. We
prove the following theorem for statistical learning and sequential prediction:
If a theory is falsifiable then it is learnable -- i.e. admits a strategy that
predicts optimally. An analogous result is shown for universal induction.
</summary>
    <author>
      <name>David Balduzzi</name>
    </author>
    <link href="http://arxiv.org/abs/1408.6618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04077v3</id>
    <updated>2017-01-27T09:11:59Z</updated>
    <published>2017-01-15T17:06:08Z</published>
    <title>Breeding electric zebras in the fields of Medicine</title>
    <summary>  A few notes on the use of machine learning in medicine and the related
unintended consequences.
</summary>
    <author>
      <name>Federico Cabitza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work-in-progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04077v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04077v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04849v1</id>
    <updated>2017-07-16T09:15:08Z</updated>
    <published>2017-07-16T09:15:08Z</published>
    <title>Minimax deviation strategies for machine learning and recognition with
  short learning samples</title>
    <summary>  The article is devoted to the problem of small learning samples in machine
learning. The flaws of maximum likelihood learning and minimax learning are
looked into and the concept of minimax deviation learning is introduced that is
free of those flaws.
</summary>
    <author>
      <name>Michail Schlesinger</name>
    </author>
    <author>
      <name>Evgeniy Vodolazskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1707.04849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05721v3</id>
    <updated>2017-08-03T14:32:30Z</updated>
    <published>2017-07-18T16:04:08Z</published>
    <title>Submodular Mini-Batch Training in Generative Moment Matching Networks</title>
    <summary>  This article was withdrawn because (1) it was uploaded without the
co-authors' knowledge or consent, and (2) there are allegations of plagiarism.
</summary>
    <author>
      <name>Jun Qi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been withdrawn. See the abstract for the reason</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05721v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05721v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01902v2</id>
    <updated>2017-08-30T12:06:15Z</updated>
    <published>2017-08-06T15:28:37Z</published>
    <title>Universally consistent predictive distributions</title>
    <summary>  This paper describes simple universally consistent procedures of probability
forecasting that satisfy a natural property of small-sample validity, under the
assumption that the observations are produced independently in the IID fashion.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32, 62G20 (Primary) 68M20, 68T05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08177v1</id>
    <updated>2017-08-28T03:52:40Z</updated>
    <published>2017-08-28T03:52:40Z</published>
    <title>Hyperprior on symmetric Dirichlet distribution</title>
    <summary>  In this article we introduce how to put vague hyperprior on Dirichlet
distribution, and we update the parameter of it by adaptive rejection sampling
(ARS). Finally we analyze this hyperprior in an over-fitted mixture model by
some synthetic experiments.
</summary>
    <author>
      <name>Jun Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1708.08177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03038v1</id>
    <updated>2017-11-08T16:30:56Z</updated>
    <published>2017-11-08T16:30:56Z</published>
    <title>Recency-weighted Markovian inference</title>
    <summary>  We describe a Markov latent state space (MLSS) model, where the latent state
distribution is a decaying mixture over multiple past states. We present a
simple sampling algorithm that allows to approximate such high-order MLSS with
fixed time and memory costs.
</summary>
    <author>
      <name>Kristjan Kalm</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06552v1</id>
    <updated>2017-11-15T16:52:48Z</updated>
    <published>2017-11-15T16:52:48Z</published>
    <title>Introduction to intelligent computing unit 1</title>
    <summary>  This brief note highlights some basic concepts required toward understanding
the evolution of machine learning and deep learning models. The note starts
with an overview of artificial intelligence and its relationship to biological
neuron that ultimately led to the evolution of todays intelligent models.
</summary>
    <author>
      <name>Isa Inuwa-Dutse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 Pages and 10 figures document</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08478v1</id>
    <updated>2017-11-22T19:18:52Z</updated>
    <published>2017-11-22T19:18:52Z</published>
    <title>MagNet and "Efficient Defenses Against Adversarial Attacks" are Not
  Robust to Adversarial Examples</title>
    <summary>  MagNet and "Efficient Defenses..." were recently proposed as a defense to
adversarial examples. We find that we can construct adversarial examples that
defeat these defenses with only a slight increase in distortion.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <author>
      <name>David Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00418v1</id>
    <updated>2018-08-01T17:00:43Z</updated>
    <published>2018-08-01T17:00:43Z</published>
    <title>Stock Chart Pattern recognition with Deep Learning</title>
    <summary>  This study evaluates the performances of CNN and LSTM for recognizing common
charts patterns in a stock historical data. It presents two common patterns,
the method used to build the training set, the neural networks architectures
and the accuracies obtained.
</summary>
    <author>
      <name>Marc Velay</name>
    </author>
    <author>
      <name>Fabrice Daniel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07049v1</id>
    <updated>2018-08-20T21:49:13Z</updated>
    <published>2018-08-20T21:49:13Z</published>
    <title>Catastrophic Importance of Catastrophic Forgetting</title>
    <summary>  This paper describes some of the possibilities of artificial neural networks
that open up after solving the problem of catastrophic forgetting. A simple
model and reinforcement learning applications of existing methods are also
proposed.
</summary>
    <author>
      <name>Albert Ierusalem</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07922v1</id>
    <updated>2019-09-17T16:38:13Z</updated>
    <published>2019-09-17T16:38:13Z</published>
    <title>Distributed Function Minimization in Apache Spark</title>
    <summary>  We report on an open-source implementation for distributed function
minimization on top of Apache Spark by using gradient and quasi-Newton methods.
We show-case it with an application to Optimal Transport and some scalability
tests on classification and regression problems.
</summary>
    <author>
      <name>Andrea Schioppa</name>
    </author>
    <link href="http://arxiv.org/abs/1909.07922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01032v1</id>
    <updated>2019-11-04T05:04:21Z</updated>
    <published>2019-11-04T05:04:21Z</published>
    <title>On Batch Bayesian Optimization</title>
    <summary>  We present two algorithms for Bayesian optimization in the batch feedback
setting, based on Gaussian process upper confidence bound and Thompson sampling
approaches, along with frequentist regret guarantees and numerical results.
</summary>
    <author>
      <name>Sayak Ray Chowdhury</name>
    </author>
    <author>
      <name>Aditya Gopalan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">All of Bayesian Nonparametrics workshop, Neural Information
  Processing Systems, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06057v1</id>
    <updated>2019-11-14T12:18:34Z</updated>
    <published>2019-11-14T12:18:34Z</published>
    <title>Supplementary material for Uncorrected least-squares temporal difference
  with lambda-return</title>
    <summary>  Here, we provide a supplementary material for Takayuki Osogami, "Uncorrected
least-squares temporal difference with lambda-return," which appears in {\it
Proceedings of the 34th AAAI Conference on Artificial Intelligence} (AAAI-20).
</summary>
    <author>
      <name>Takayuki Osogami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, supplementary material for an AAAI-20 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.06057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07682v1</id>
    <updated>2019-11-18T15:05:09Z</updated>
    <published>2019-11-18T15:05:09Z</published>
    <title>A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories</title>
    <summary>  Deep neural networks are vulnerable to adversarial attacks.
</summary>
    <author>
      <name>Zhaohui Che</name>
    </author>
    <author>
      <name>Ali Borji</name>
    </author>
    <author>
      <name>Guangtao Zhai</name>
    </author>
    <author>
      <name>Suiyi Ling</name>
    </author>
    <author>
      <name>Jing Li</name>
    </author>
    <author>
      <name>Patrick Le Callet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by AAAI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3095v1</id>
    <updated>2011-03-16T04:54:58Z</updated>
    <published>2011-03-16T04:54:58Z</published>
    <title>A note on active learning for smooth problems</title>
    <summary>  We show that the disagreement coefficient of certain smooth hypothesis
classes is $O(m)$, where $m$ is the dimension of the hypothesis space, thereby
answering a question posed in \cite{friedman09}.
</summary>
    <author>
      <name>Satyaki Mahalanabis</name>
    </author>
    <link href="http://arxiv.org/abs/1103.3095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32: Computational learning theory" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01632v1</id>
    <updated>2015-02-05T16:37:31Z</updated>
    <published>2015-02-05T16:37:31Z</published>
    <title>A Simple Expression for Mill's Ratio of the Student's $t$-Distribution</title>
    <summary>  I show a simple expression of the Mill's ratio of the Student's
t-Distribution. I use it to prove Conjecture 1 in P. Auer, N. Cesa-Bianchi, and
P. Fischer. Finite-time analysis of the multiarmed bandit problem. Mach.
Learn., 47(2-3):235--256, May 2002.
</summary>
    <author>
      <name>Francesco Orabona</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02704v1</id>
    <updated>2015-02-09T22:05:25Z</updated>
    <published>2015-02-09T22:05:25Z</published>
    <title>Learning Reductions that Really Work</title>
    <summary>  We provide a summary of the mathematical and computational techniques that
have enabled learning reductions to effectively address a wide class of
problems, and show that this approach to solving machine learning problems can
be broadly useful.
</summary>
    <author>
      <name>Alina Beygelzimer</name>
    </author>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Paul Mineiro</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02956v1</id>
    <updated>2018-03-08T04:01:56Z</updated>
    <published>2018-03-08T04:01:56Z</published>
    <title>Some Approximation Bounds for Deep Networks</title>
    <summary>  In this paper we introduce new bounds on the approximation of functions in
deep networks and in doing so introduce some new deep network architectures for
function approximation. These results give some theoretical insight into the
success of autoencoders and ResNets.
</summary>
    <author>
      <name>Brendan McCane</name>
    </author>
    <author>
      <name>Lech Szymanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06586v1</id>
    <updated>2018-03-17T23:39:57Z</updated>
    <published>2018-03-17T23:39:57Z</published>
    <title>Structural query-by-committee</title>
    <summary>  In this work, we describe a framework that unifies many different interactive
learning tasks. We present a generalization of the {\it query-by-committee}
active learning algorithm for this setting, and we study its consistency and
rate of convergence, both theoretically and empirically, with and without
noise.
</summary>
    <author>
      <name>Christopher Tosh</name>
    </author>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01503v1</id>
    <updated>2020-07-03T05:26:02Z</updated>
    <published>2020-07-03T05:26:02Z</published>
    <title>Mathematical Perspective of Machine Learning</title>
    <summary>  We take a closer look at some theoretical challenges of Machine Learning as a
function approximation, gradient descent as the default optimization algorithm,
limitations of fixed length and width networks and a different approach to RNNs
from a mathematical perspective.
</summary>
    <author>
      <name>Yarema Boryshchak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13185v1</id>
    <updated>2020-07-26T17:31:44Z</updated>
    <published>2020-07-26T17:31:44Z</published>
    <title>Dimensionality Reduction for $k$-means Clustering</title>
    <summary>  We present a study on how to effectively reduce the dimensions of the
$k$-means clustering problem, so that provably accurate approximations are
obtained. Four algorithms are presented, two \textit{feature selection} and two
\textit{feature extraction} based algorithms, all of which are randomized.
</summary>
    <author>
      <name>Neophytos Charalambides</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 table, expository</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01109v1</id>
    <updated>2021-03-01T16:21:40Z</updated>
    <published>2021-03-01T16:21:40Z</published>
    <title>Optimal Linear Combination of Classifiers</title>
    <summary>  The question of whether to use one classifier or a combination of classifiers
is a central topic in Machine Learning. We propose here a method for finding an
optimal linear combination of classifiers derived from a bias-variance
framework for the classification task.
</summary>
    <author>
      <name>Georgi Nalbantov</name>
    </author>
    <author>
      <name>Svetoslav Ivanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.13533v1</id>
    <updated>2021-03-25T00:09:09Z</updated>
    <published>2021-03-25T00:09:09Z</published>
    <title>Symmetry-Preserving Paths in Integrated Gradients</title>
    <summary>  We provide rigorous proofs that the Integrated Gradients (IG) attribution
method for deep networks satisfies completeness and symmetry-preserving
properties. We also study the uniqueness of IG as a path method preserving
symmetry.
</summary>
    <author>
      <name>Miguel Lerma</name>
    </author>
    <author>
      <name>Mirtha Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.13533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08669v1</id>
    <updated>2021-05-18T16:53:00Z</updated>
    <published>2021-05-18T16:53:00Z</published>
    <title>Enhancement of prediction algorithms by betting</title>
    <summary>  This note proposes a procedure for enhancing the quality of probabilistic
prediction algorithms via betting against their predictions. It is inspired by
the success of the conformal test martingales that have been developed
recently.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.08669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05 (Primary) 62G10, 60G42 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01703v1</id>
    <updated>2016-05-05T19:34:08Z</updated>
    <published>2016-05-05T19:34:08Z</published>
    <title>A note on adjusting $R^2$ for using with cross-validation</title>
    <summary>  We show how to adjust the coefficient of determination ($R^2$) when used for
measuring predictive accuracy via leave-one-out cross-validation.
</summary>
    <author>
      <name>Indre Zliobaite</name>
    </author>
    <author>
      <name>Nikolaj Tatti</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00382v1</id>
    <updated>2018-02-01T16:46:00Z</updated>
    <published>2018-02-01T16:46:00Z</published>
    <title>Classifying medical notes into standard disease codes using Machine
  Learning</title>
    <summary>  We investigate the automatic classification of patient discharge notes into
standard disease labels. We find that Convolutional Neural Networks with
Attention outperform previous algorithms used in this task, and suggest further
areas for improvement.
</summary>
    <author>
      <name>Amitabha Karmakar</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00278v2</id>
    <updated>2020-01-06T16:37:03Z</updated>
    <published>2020-01-01T23:30:00Z</published>
    <title>Motivic clustering schemes for directed graphs</title>
    <summary>  Motivated by the concept of network motifs we construct certain clustering
methods (functors) which are parametrized by a given collection of motifs (or
representers).
</summary>
    <author>
      <name>Facundo Mémoli</name>
    </author>
    <author>
      <name>Guilherme Vituri F. Pinto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00278v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00278v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07278v1</id>
    <updated>2020-01-20T23:09:32Z</updated>
    <published>2020-01-20T23:09:32Z</published>
    <title>Mixed integer programming formulation of unsupervised learning</title>
    <summary>  A novel formulation and training procedure for full Boltzmann machines in
terms of a mixed binary quadratic feasibility problem is given. As a proof of
concept, the theory is analytically and numerically tested on XOR patterns.
</summary>
    <author>
      <name>Arturo Berrones-Santos</name>
    </author>
    <link href="http://arxiv.org/abs/2001.07278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11595v1</id>
    <updated>2020-01-30T22:44:15Z</updated>
    <published>2020-01-30T22:44:15Z</published>
    <title>Concentration Inequalities for Multinoulli Random Variables</title>
    <summary>  We investigate concentration inequalities for Dirichlet and Multinomial
random variables.
</summary>
    <author>
      <name>Jian Qian</name>
    </author>
    <author>
      <name>Ronan Fruit</name>
    </author>
    <author>
      <name>Matteo Pirotta</name>
    </author>
    <author>
      <name>Alessandro Lazaric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tutorial at ALT'19 on Regret Minimization in Infinite-Horizon Finite
  Markov Decision Processes</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.11595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03253v1</id>
    <updated>2020-02-29T14:52:28Z</updated>
    <published>2020-02-29T14:52:28Z</published>
    <title>Introduction to deep learning</title>
    <summary>  Deep Learning (DL) has made a major impact on data science in the last
decade. This chapter introduces the basic concepts of this field. It includes
both the basic structures used to design deep neural networks and a brief
survey of some of its popular use cases.
</summary>
    <author>
      <name>Lihi Shiloh-Perl</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <link href="http://arxiv.org/abs/2003.03253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4295v1</id>
    <updated>2012-05-19T04:25:04Z</updated>
    <published>2012-05-19T04:25:04Z</published>
    <title>Efficient Methods for Unsupervised Learning of Probabilistic Models</title>
    <summary>  In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.
</summary>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <link href="http://arxiv.org/abs/1205.4295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13301v1</id>
    <updated>2020-10-26T03:16:54Z</updated>
    <published>2020-10-26T03:16:54Z</published>
    <title>Scalable Bayesian Optimization with Sparse Gaussian Process Models</title>
    <summary>  This thesis focuses on Bayesian optimization with the improvements coming
from two aspects:(i) the use of derivative information to accelerate the
optimization convergence; and (ii) the consideration of scalable GPs for
handling massive data.
</summary>
    <author>
      <name>Ang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.13301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14816v1</id>
    <updated>2020-10-28T08:48:23Z</updated>
    <published>2020-10-28T08:48:23Z</published>
    <title>Higher Order Linear Transformer</title>
    <summary>  Following up on the linear transformer part of the article from Katharopoulos
et al., that takes this idea from Shen et al., the trick that produces a linear
complexity for the attention mechanism is re-used and extended to a
second-order approximation of the softmax normalization.
</summary>
    <author>
      <name>Jean Mercat</name>
    </author>
    <link href="http://arxiv.org/abs/2010.14816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14978v2</id>
    <updated>2021-02-03T09:52:35Z</updated>
    <published>2020-10-28T13:40:57Z</published>
    <title>Technical Note: Game-Theoretic Interactions of Different Orders</title>
    <summary>  In this study, we define interaction components of different orders between
two input variables based on game theory. We further prove that interaction
components of different orders satisfy several desirable properties.
</summary>
    <author>
      <name>Hao Zhang</name>
    </author>
    <author>
      <name>Xu Cheng</name>
    </author>
    <author>
      <name>Yiting Chen</name>
    </author>
    <author>
      <name>Quanshi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2010.14978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15583v3</id>
    <updated>2020-11-12T16:40:17Z</updated>
    <published>2020-10-15T01:44:59Z</published>
    <title>Probabilistic Transformers</title>
    <summary>  We show that Transformers are Maximum Posterior Probability estimators for
Mixtures of Gaussian Models. This brings a probabilistic point of view to
Transformers and suggests extensions to other probabilistic cases.
</summary>
    <author>
      <name>Javier R. Movellan</name>
    </author>
    <author>
      <name>Prasad Gabbur</name>
    </author>
    <link href="http://arxiv.org/abs/2010.15583v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15583v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05878v1</id>
    <updated>2021-04-13T00:57:39Z</updated>
    <published>2021-04-13T00:57:39Z</published>
    <title>On the validity of kernel approximations for orthogonally-initialized
  neural networks</title>
    <summary>  In this note we extend kernel function approximation results for neural
networks with Gaussian-distributed weights to single-layer networks initialized
using Haar-distributed random orthogonal matrices (with possible rescaling).
This is accomplished using recent results from random matrix theory.
</summary>
    <author>
      <name>James Martens</name>
    </author>
    <link href="http://arxiv.org/abs/2104.05878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04131v2</id>
    <updated>2022-01-12T21:40:11Z</updated>
    <published>2022-01-11T18:56:08Z</published>
    <title>Optimally compressing VC classes</title>
    <summary>  Resolving a conjecture of Littlestone and Warmuth, we show that any concept
class of VC-dimension $d$ has a sample compression scheme of size $d$.
</summary>
    <author>
      <name>Zachary Chase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invalid proof of Proposition 3.2 and thus of main theorem (Theorem 1)</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.04131v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04131v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.01907v2</id>
    <updated>2022-09-06T05:51:01Z</updated>
    <published>2022-02-03T23:23:26Z</published>
    <title>A Unified Training Process for Fake News Detection based on Fine-Tuned
  BERT Model</title>
    <summary>  An efficient fake news detector becomes essential as the accessibility of
social media platforms increases rapidly.
</summary>
    <author>
      <name>Vijay Srinivas Tida</name>
    </author>
    <author>
      <name>Sonya Hsu</name>
    </author>
    <author>
      <name>Xiali Hei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.01907v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.01907v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.03865v2</id>
    <updated>2022-02-09T14:29:08Z</updated>
    <published>2022-02-05T22:06:56Z</published>
    <title>Backtrack Tie-Breaking for Decision Trees: A Note on Deodata Predictors</title>
    <summary>  A tie-breaking method is proposed for choosing the predicted class, or
outcome, in a decision tree. The method is an adaptation of a similar technique
used for deodata predictors.
</summary>
    <author>
      <name>Cristian Alb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.13630.61763</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.13630.61763" rel="related"/>
    <link href="http://arxiv.org/abs/2202.03865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.03865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05137v1</id>
    <updated>2022-02-10T16:43:55Z</updated>
    <published>2022-02-10T16:43:55Z</published>
    <title>Quantization in Layer's Input is Matter</title>
    <summary>  In this paper, we will show that the quantization in layer's input is more
important than parameters' quantization for loss function. And the algorithm
which is based on the layer's input quantization error is better than
hessian-based mixed precision layout algorithm.
</summary>
    <author>
      <name>Daning Cheng</name>
    </author>
    <author>
      <name>WenGuang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2202.05137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09701v1</id>
    <updated>2022-02-20T00:27:14Z</updated>
    <published>2022-02-20T00:27:14Z</published>
    <title>A History of Meta-gradient: Gradient Methods for Meta-learning</title>
    <summary>  The history of meta-learning methods based on gradient descent is reviewed,
focusing primarily on methods that adapt step-size (learning rate)
meta-parameters.
</summary>
    <author>
      <name>Richard S. Sutton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages of text, 54 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.09701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0406v1</id>
    <updated>2013-02-02T17:20:47Z</updated>
    <published>2013-02-02T17:20:47Z</published>
    <title>Generalization Guarantees for a Binary Classification Framework for
  Two-Stage Multiple Kernel Learning</title>
    <summary>  We present generalization bounds for the TS-MKL framework for two stage
multiple kernel learning. We also present bounds for sparse kernel learning
formulations within the TS-MKL framework.
</summary>
    <author>
      <name>Purushottam Kar</name>
    </author>
    <link href="http://arxiv.org/abs/1302.0406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00036v2</id>
    <updated>2015-04-14T22:55:08Z</updated>
    <published>2015-02-27T23:50:22Z</published>
    <title>Norm-Based Capacity Control in Neural Networks</title>
    <summary>  We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Ryota Tomioka</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00600v1</id>
    <updated>2015-03-02T16:35:02Z</updated>
    <published>2015-03-02T16:35:02Z</published>
    <title>An $\mathcal{O}(n\log n)$ projection operator for weighted $\ell_1$-norm
  regularization with sum constraint</title>
    <summary>  We provide a simple and efficient algorithm for the projection operator for
weighted $\ell_1$-norm regularization subject to a sum constraint, together
with an elementary proof. The implementation of the proposed algorithm can be
downloaded from the author's homepage.
</summary>
    <author>
      <name>Weiran Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1503.00600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01002v1</id>
    <updated>2015-03-03T16:40:17Z</updated>
    <published>2015-03-03T16:40:17Z</published>
    <title>Projection onto the capped simplex</title>
    <summary>  We provide a simple and efficient algorithm for computing the Euclidean
projection of a point onto the capped simplex---a simplex with an additional
uniform bound on each coordinate---together with an elementary proof. Both the
MATLAB and C++ implementations of the proposed algorithm can be downloaded at
https://eng.ucmerced.edu/people/wwang5.
</summary>
    <author>
      <name>Weiran Wang</name>
    </author>
    <author>
      <name>Canyi Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1503.01002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06902v1</id>
    <updated>2015-03-24T03:26:28Z</updated>
    <published>2015-03-24T03:26:28Z</published>
    <title>A Note on Information-Directed Sampling and Thompson Sampling</title>
    <summary>  This note introduce three Bayesian style Multi-armed bandit algorithms:
Information-directed sampling, Thompson Sampling and Generalized Thompson
Sampling. The goal is to give an intuitive explanation for these three
algorithms and their regret bounds, and provide some derivations that are
omitted in the original papers.
</summary>
    <author>
      <name>Li Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03164v1</id>
    <updated>2018-01-09T22:33:51Z</updated>
    <published>2018-01-09T22:33:51Z</published>
    <title>Paranom: A Parallel Anomaly Dataset Generator</title>
    <summary>  In this paper, we present Paranom, a parallel anomaly dataset generator. We
discuss its design and provide brief experimental results demonstrating its
usefulness in improving the classification correctness of LSTM-AD, a
state-of-the-art anomaly detection model.
</summary>
    <author>
      <name>Justin Gottschlich</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04928v1</id>
    <updated>2018-01-15T00:51:29Z</updated>
    <published>2018-01-15T00:51:29Z</published>
    <title>Leapfrogging for parallelism in deep neural networks</title>
    <summary>  We present a technique, which we term leapfrogging, to parallelize back-
propagation in deep neural networks. We show that this technique yields a
savings of $1-1/k$ of a dominant term in backpropagation, where k is the number
of threads (or gpus).
</summary>
    <author>
      <name>Yatin Saraiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07904v2</id>
    <updated>2018-09-24T20:08:42Z</updated>
    <published>2018-09-21T01:02:48Z</published>
    <title>Automatic Rule Learning for Autonomous Driving Using Semantic Memory</title>
    <summary>  This paper presents a novel approach for automatic rule learning applicable
to an autonomous driving system using real driving data.
</summary>
    <author>
      <name>Dmitriy Korchev</name>
    </author>
    <author>
      <name>Aruna Jammalamadaka</name>
    </author>
    <author>
      <name>Rajan Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.07904v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07904v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.09842v1</id>
    <updated>2018-11-24T14:21:50Z</updated>
    <published>2018-11-24T14:21:50Z</published>
    <title>OCLEP+: One-class Anomaly and Intrusion Detection Using Minimal Length
  of Emerging Patterns</title>
    <summary>  This paper presents a method called One-class Classification using Length
statistics of Emerging Patterns Plus (OCLEP+).
</summary>
    <author>
      <name>Guozhu Dong</name>
    </author>
    <author>
      <name>Sai Kiran Pentukar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.09842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08994v1</id>
    <updated>2019-04-18T20:15:08Z</updated>
    <published>2019-04-18T20:15:08Z</published>
    <title>From GAN to WGAN</title>
    <summary>  This paper explains the math behind a generative adversarial network (GAN)
model and why it is hard to be trained. Wasserstein GAN is intended to improve
GANs' training by adopting a smooth metric for measuring the distance between
two probability distributions.
</summary>
    <author>
      <name>Lilian Weng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.08994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03593v2</id>
    <updated>2020-02-18T11:17:58Z</updated>
    <published>2019-06-09T08:36:35Z</published>
    <title>Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound</title>
    <summary>  We improve the over-parametrization size over two beautiful results [Li and
Liang' 2018] and [Du, Zhai, Poczos and Singh' 2019] in deep learning theory.
</summary>
    <author>
      <name>Zhao Song</name>
    </author>
    <author>
      <name>Xin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1906.03593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02647v1</id>
    <updated>2019-07-03T04:18:31Z</updated>
    <published>2019-07-03T04:18:31Z</published>
    <title>Generalized Principal Component Analysis</title>
    <summary>  Generalized principal component analysis (GLM-PCA) facilitates dimension
reduction of non-normally distributed data. We provide a detailed derivation of
GLM-PCA with a focus on optimization. We also demonstrate how to incorporate
covariates, and suggest post-processing transformations to improve
interpretability of latent factors.
</summary>
    <author>
      <name>F. William Townes</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00413v2</id>
    <updated>2022-10-26T12:18:06Z</updated>
    <published>2019-09-28T05:14:43Z</published>
    <title>A Note On k-Means Probabilistic Poverty</title>
    <summary>  It is proven, by example, that the version of $k$-means with random
initialization does not have the property probabilistic k-richness.
</summary>
    <author>
      <name>Mieczysław A. Kłopotek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11080v1</id>
    <updated>2019-10-24T13:38:08Z</updated>
    <published>2019-10-24T13:38:08Z</published>
    <title>On sample complexity of neural networks</title>
    <summary>  We consider functions defined by deep neural networks as definable objects in
an o-miminal expansion of the real field, and derive an almost linear (in the
number of weights) bound on sample complexity of such networks.
</summary>
    <author>
      <name>Alexander Usvyatsov</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12650v1</id>
    <updated>2020-08-25T16:57:19Z</updated>
    <published>2020-08-25T16:57:19Z</published>
    <title>Are Deep Neural Networks "Robust"?</title>
    <summary>  Separating outliers from inliers is the definition of robustness in computer
vision. This essay delineates how deep neural networks are different than
typical robust estimators. Deep neural networks not robust by this traditional
definition.
</summary>
    <author>
      <name>Peter Meer</name>
    </author>
    <link href="http://arxiv.org/abs/2008.12650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08488v1</id>
    <updated>2021-07-18T16:46:35Z</updated>
    <published>2021-07-18T16:46:35Z</published>
    <title>A note on the article "On Exploiting Spectral Properties for Solving MDP
  with Large State Space"</title>
    <summary>  We improve a theoretical result of the article "On Exploiting Spectral
Properties for Solving MDP with Large State Space" showing that their
algorithm, which was proved to converge under some unrealistic assumptions, is
actually guaranteed to converge always.
</summary>
    <author>
      <name>D. Maran</name>
    </author>
    <link href="http://arxiv.org/abs/2107.08488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03887v1</id>
    <updated>2021-08-09T09:17:05Z</updated>
    <published>2021-08-09T09:17:05Z</published>
    <title>Collapsing the Decision Tree: the Concurrent Data Predictor</title>
    <summary>  A family of concurrent data predictors is derived from the decision tree
classifier by removing the limitation of sequentially evaluating attributes. By
evaluating attributes concurrently, the decision tree collapses into a flat
structure. Experiments indicate improvements of the prediction accuracy.
</summary>
    <author>
      <name>Cristian Alb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.33413.06880</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.33413.06880" rel="related"/>
    <link href="http://arxiv.org/abs/2108.03887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.06011v2</id>
    <updated>2021-09-25T15:08:36Z</updated>
    <published>2021-08-13T01:11:53Z</published>
    <title>Datasets for Studying Generalization from Easy to Hard Examples</title>
    <summary>  We describe new datasets for studying generalization from easy to hard
examples.
</summary>
    <author>
      <name>Avi Schwarzschild</name>
    </author>
    <author>
      <name>Eitan Borgnia</name>
    </author>
    <author>
      <name>Arjun Gupta</name>
    </author>
    <author>
      <name>Arpit Bansal</name>
    </author>
    <author>
      <name>Zeyad Emam</name>
    </author>
    <author>
      <name>Furong Huang</name>
    </author>
    <author>
      <name>Micah Goldblum</name>
    </author>
    <author>
      <name>Tom Goldstein</name>
    </author>
    <link href="http://arxiv.org/abs/2108.06011v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.06011v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.12132v1</id>
    <updated>2021-11-23T19:55:06Z</updated>
    <published>2021-11-23T19:55:06Z</published>
    <title>Robust Principal Component Analysis: A Construction Error Minimization
  Perspective</title>
    <summary>  In this paper we propose a novel optimization framework to systematically
solve robust PCA problem with rigorous theoretical guarantee, based on which we
investigate very computationally economic updating algorithms.
</summary>
    <author>
      <name>Kai Liu</name>
    </author>
    <author>
      <name>Yarui Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.12132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01069v1</id>
    <updated>2022-04-22T11:57:19Z</updated>
    <published>2022-04-22T11:57:19Z</published>
    <title>Deep Learning: From Basics to Building Deep Neural Networks with Python</title>
    <summary>  This book is intended for beginners who have no familiarity with deep
learning. Our only expectation from readers is that they already have the basic
programming skills in Python.
</summary>
    <author>
      <name>Milad Vazan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">205 pages, in Farsi</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01214v1</id>
    <updated>2022-05-02T21:16:32Z</updated>
    <published>2022-05-02T21:16:32Z</published>
    <title>An improvement to a result about graph isomorphism networks using the
  prime factorization theorem</title>
    <summary>  The unique prime factorization theorem is used to show the existence of a
function on a countable set $\mathcal{X}$ so that the sum aggregator function
is injective on all multisets of $\mathcal{X}$ of finite size.
</summary>
    <author>
      <name>Rahul Sarkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.06114v1</id>
    <updated>2022-07-13T10:39:58Z</updated>
    <published>2022-07-13T10:39:58Z</published>
    <title>Automatic Differentiation: Theory and Practice</title>
    <summary>  We present the classical coordinate-free formalism for forward and backward
mode ad in the real and complex setting. We show how to formally derive the
forward and backward formulae for a number of matrix functions starting from
basic principles.
</summary>
    <author>
      <name>Mario Lezcano-Casado</name>
    </author>
    <link href="http://arxiv.org/abs/2207.06114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.06114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.00885v2</id>
    <updated>2022-09-24T14:50:33Z</updated>
    <published>2022-09-02T08:53:11Z</published>
    <title>Regret Analysis of Dyadic Search</title>
    <summary>  We analyze the cumulative regret of the Dyadic Search algorithm of Bachoc et
al. [2022].
</summary>
    <author>
      <name>François Bachoc</name>
    </author>
    <author>
      <name>Tommaso Cesari</name>
    </author>
    <author>
      <name>Roberto Colomboni</name>
    </author>
    <author>
      <name>Andrea Paudice</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2208.06720</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.00885v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00885v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.10010v1</id>
    <updated>2022-09-20T21:38:51Z</updated>
    <published>2022-09-20T21:38:51Z</published>
    <title>Intentional Choreography with Semi-Supervised Recurrent VAEs</title>
    <summary>  We summarize the model and results of PirouNet, a semi-supervised recurrent
variational autoencoder. Given a small amount of dance sequences labeled with
qualitative choreographic annotations, PirouNet conditionally generates dance
sequences in the style of the choreographer.
</summary>
    <author>
      <name>Mathilde Papillon</name>
    </author>
    <author>
      <name>Mariel Pettee</name>
    </author>
    <author>
      <name>Nina Miolane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2207.12126</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.10010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.10010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.14005v2</id>
    <updated>2022-11-01T07:14:25Z</updated>
    <published>2022-10-25T13:21:19Z</published>
    <title>Parametric PDF for Goodness of Fit</title>
    <summary>  The goodness of fit methods for classification problems relies traditionally
on confusion matrices. This paper aims to enrich these methods with a risk
evaluation and stability analysis tools. For this purpose, we present a
parametric PDF framework.
</summary>
    <author>
      <name>Natan Katz</name>
    </author>
    <author>
      <name>Uri Itai</name>
    </author>
    <link href="http://arxiv.org/abs/2210.14005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.14005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.17036v1</id>
    <updated>2022-11-30T14:31:04Z</updated>
    <published>2022-11-30T14:31:04Z</published>
    <title>High-Dimensional Wide Gap $k$-Means Versus Clustering Axioms</title>
    <summary>  Kleinberg's axioms for distance based clustering proved to be contradictory.
  Various efforts have been made to overcome this problem.
  Here we make an attempt to handle the issue by embedding in high-dimensional
space and granting wide gaps between clusters.
</summary>
    <author>
      <name>Mieczysław A. Kłopotek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.17036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.17036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13393v1</id>
    <updated>2023-02-26T19:25:19Z</updated>
    <published>2023-02-26T19:25:19Z</published>
    <title>Autoencoders as Pattern Filters</title>
    <summary>  We discuss a simple approach to transform autoencoders into "pattern
filters". Besides filtering, we show how this simple approach can be used also
to build robust classifiers, by learning to filter only patterns of a given
class.
</summary>
    <author>
      <name>M. Andrecut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.13393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.00359v1</id>
    <updated>2023-07-01T15:09:38Z</updated>
    <published>2023-07-01T15:09:38Z</published>
    <title>When Synthetic Data Met Regulation</title>
    <summary>  In this paper, we argue that synthetic data produced by Differentially
Private generative models can be sufficiently anonymized and, therefore,
anonymous data and regulatory compliant.
</summary>
    <author>
      <name>Georgi Ganev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 1st Workshop on Generative AI and Law (GenLaw 2023),
  part of ICML 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.00359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.00359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.01170v1</id>
    <updated>2023-07-03T17:29:58Z</updated>
    <published>2023-07-03T17:29:58Z</published>
    <title>Online nearest neighbor classification</title>
    <summary>  We study an instance of online non-parametric classification in the
realizable setting. In particular, we consider the classical 1-nearest neighbor
algorithm, and show that it achieves sublinear regret - that is, a vanishing
mistake rate - against dominated or smoothed adversaries in the realizable
setting.
</summary>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <author>
      <name>Geelon So</name>
    </author>
    <link href="http://arxiv.org/abs/2307.01170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.01170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.05621v1</id>
    <updated>2023-08-10T15:10:08Z</updated>
    <published>2023-08-10T15:10:08Z</published>
    <title>Normalized Gradients for All</title>
    <summary>  In this short note, I show how to adapt to H\"{o}lder smoothness using
normalized gradients in a black-box way. Moreover, the bound will depend on a
novel notion of local H\"{o}lder smoothness. The main idea directly comes from
Levy [2017].
</summary>
    <author>
      <name>Francesco Orabona</name>
    </author>
    <link href="http://arxiv.org/abs/2308.05621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.05621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09381v1</id>
    <updated>2023-09-17T21:20:35Z</updated>
    <published>2023-09-17T21:20:35Z</published>
    <title>Federated Learning in Temporal Heterogeneity</title>
    <summary>  In this work, we explored federated learning in temporal heterogeneity across
clients. We observed that global model obtained by \texttt{FedAvg} trained with
fixed-length sequences shows faster convergence than varying-length sequences.
We proposed methods to mitigate temporal heterogeneity for efficient federated
learning based on the empirical observation.
</summary>
    <author>
      <name>Junghwan Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2309.09381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.16209v1</id>
    <updated>2023-10-24T21:53:50Z</updated>
    <published>2023-10-24T21:53:50Z</published>
    <title>ELM Ridge Regression Boosting</title>
    <summary>  We discuss a boosting approach for the Ridge Regression (RR) method, with
applications to the Extreme Learning Machine (ELM), and we show that the
proposed method significantly improves the classification performance and
robustness of ELMs.
</summary>
    <author>
      <name>M. Andrecut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.16209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.06892v1</id>
    <updated>2024-02-10T06:49:08Z</updated>
    <published>2024-02-10T06:49:08Z</published>
    <title>Understanding Test-Time Augmentation</title>
    <summary>  Test-Time Augmentation (TTA) is a very powerful heuristic that takes
advantage of data augmentation during testing to produce averaged output.
Despite the experimental effectiveness of TTA, there is insufficient discussion
of its theoretical aspects. In this paper, we aim to give theoretical
guarantees for TTA and clarify its behavior.
</summary>
    <author>
      <name>Masanari Kimura</name>
    </author>
    <link href="http://arxiv.org/abs/2402.06892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.06721v1</id>
    <updated>2024-05-10T06:03:45Z</updated>
    <published>2024-05-10T06:03:45Z</published>
    <title>Kolmogorov-Arnold Networks are Radial Basis Function Networks</title>
    <summary>  This short paper is a fast proof-of-concept that the 3-order B-splines used
in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian
radial basis functions. Doing so leads to FastKAN, a much faster implementation
of KAN which is also a radial basis function (RBF) network.
</summary>
    <author>
      <name>Ziyao Li</name>
    </author>
    <link href="http://arxiv.org/abs/2405.06721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.06721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.20538v1</id>
    <updated>2024-05-30T23:22:36Z</updated>
    <published>2024-05-30T23:22:36Z</published>
    <title>Q-learning as a monotone scheme</title>
    <summary>  Stability issues with reinforcement learning methods persist. To better
understand some of these stability and convergence issues involving deep
reinforcement learning methods, we examine a simple linear quadratic example.
We interpret the convergence criterion of exact Q-learning in the sense of a
monotone scheme and discuss consequences of function approximation on
monotonicity properties.
</summary>
    <author>
      <name>Lingyi Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.20538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.20538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.09577v1</id>
    <updated>2024-07-12T00:37:55Z</updated>
    <published>2024-07-12T00:37:55Z</published>
    <title>Flash normalization: fast RMSNorm for LLMs</title>
    <summary>  RMSNorm is used by many LLMs such as Llama, Mistral, and OpenELM. This paper
details FlashNorm, which is an exact but faster implementation of RMSNorm
followed by linear layers. See https://huggingface.co/open-machine/FlashNorm
for code and more transformer tricks.
</summary>
    <author>
      <name>Nils Graef</name>
    </author>
    <author>
      <name>Matthew Clapp</name>
    </author>
    <author>
      <name>Andrew Wasielewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.09577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.09577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.14332v1</id>
    <updated>2024-08-26T15:01:04Z</updated>
    <published>2024-08-26T15:01:04Z</published>
    <title>One-layer transformers fail to solve the induction heads task</title>
    <summary>  A simple communication complexity argument proves that no one-layer
transformer can solve the induction heads task unless its size is exponentially
larger than the size sufficient for a two-layer transformer.
</summary>
    <author>
      <name>Clayton Sanford</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <link href="http://arxiv.org/abs/2408.14332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.14332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.01930v1</id>
    <updated>2024-09-03T14:20:26Z</updated>
    <published>2024-09-03T14:20:26Z</published>
    <title>Efficient LLM Context Distillation</title>
    <summary>  This paper specifically investigates context distillation a method that
extends the utility of task-specific examples by internalizing them, thus
augmenting the example set accessible for model inference.
</summary>
    <author>
      <name>Rajesh Upadhayayaya</name>
    </author>
    <author>
      <name>Zachary Smith</name>
    </author>
    <author>
      <name>Chritopher Kottmyer</name>
    </author>
    <author>
      <name>Manish Raj Osti</name>
    </author>
    <link href="http://arxiv.org/abs/2409.01930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.01930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.10188v1</id>
    <updated>2024-09-16T11:30:39Z</updated>
    <published>2024-09-16T11:30:39Z</published>
    <title>Enhancing RL Safety with Counterfactual LLM Reasoning</title>
    <summary>  Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard
to explain. We use counterfactual large language model reasoning to enhance RL
policy safety post-training. We show that our approach improves and helps to
explain the RL policy safety.
</summary>
    <author>
      <name>Dennis Gross</name>
    </author>
    <author>
      <name>Helge Spieker</name>
    </author>
    <link href="http://arxiv.org/abs/2409.10188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.10188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19278v1</id>
    <updated>2024-09-28T07:59:45Z</updated>
    <published>2024-09-28T07:59:45Z</published>
    <title>Explicit construction of recurrent neural networks effectively
  approximating discrete dynamical systems</title>
    <summary>  We consider arbitrary bounded discrete time series originating from dynamical
system with recursivity. More precisely, we provide an explicit construction of
recurrent neural networks which effectively approximate the corresponding
discrete dynamical systems.
</summary>
    <author>
      <name>Chikara Nakayama</name>
    </author>
    <author>
      <name>Tsuyoshi Yoneda</name>
    </author>
    <link href="http://arxiv.org/abs/2409.19278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.11579v1</id>
    <updated>2024-10-15T13:11:32Z</updated>
    <published>2024-10-15T13:11:32Z</published>
    <title>Machine Learning via rough mereology</title>
    <summary>  Rough sets (RS)proved a thriving realm with successes inn many fields of ML
and AI. In this note, we expand RS to RM - rough mereology which provides a
measurable degree of uncertainty to those areas.
</summary>
    <author>
      <name>Lech T. Polkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.11579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.11579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18593v1</id>
    <updated>2024-10-24T09:42:52Z</updated>
    <published>2024-10-24T09:42:52Z</published>
    <title>Differential Informed Auto-Encoder</title>
    <summary>  In this article, an encoder was trained to obtain the inner structure of the
original data by obtain a differential equations. A decoder was trained to
resample the original data domain, to generate new data that obey the
differential structure of the original data using the physics-informed neural
network.
</summary>
    <author>
      <name>Jinrui Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.04578v1</id>
    <updated>2024-12-05T19:48:13Z</updated>
    <published>2024-12-05T19:48:13Z</published>
    <title>Loss Terms and Operator Forms of Koopman Autoencoders</title>
    <summary>  Koopman autoencoders are a prevalent architecture in operator learning. But,
the loss functions and the form of the operator vary significantly in the
literature. This paper presents a fair and systemic study of these options.
Furthermore, it introduces novel loss terms.
</summary>
    <author>
      <name>Dustin Enyeart</name>
    </author>
    <author>
      <name>Guang Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2412.04578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.04578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.09469v1</id>
    <updated>2024-12-12T17:16:41Z</updated>
    <published>2024-12-12T17:16:41Z</published>
    <title>Neural Network Symmetrisation in Concrete Settings</title>
    <summary>  Cornish (2024) recently gave a general theory of neural network
symmetrisation in the abstract context of Markov categories. We give a
high-level overview of these results, and their concrete implications for the
symmetrisation of deterministic functions and of Markov kernels.
</summary>
    <author>
      <name>Rob Cornish</name>
    </author>
    <link href="http://arxiv.org/abs/2412.09469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.09469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11950v1</id>
    <updated>2024-12-16T16:34:48Z</updated>
    <published>2024-12-16T16:34:48Z</published>
    <title>Asynchronous Distributed Gaussian Process Regression for Online Learning
  and Dynamical Systems: Complementary Document</title>
    <summary>  This is a complementary document for the paper titled "Asynchronous
Distributed Gaussian Process Regression for Online Learning and Dynamical
Systems".
</summary>
    <author>
      <name>Zewen Yang</name>
    </author>
    <author>
      <name>Xiaobing Dai</name>
    </author>
    <author>
      <name>Sandra Hirche</name>
    </author>
    <link href="http://arxiv.org/abs/2412.11950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.07981v1</id>
    <updated>2020-07-15T20:26:49Z</updated>
    <published>2020-07-15T20:26:49Z</published>
    <title>Differential Replication in Machine Learning</title>
    <summary>  When deployed in the wild, machine learning models are usually confronted
with data and requirements that constantly vary, either because of changes in
the generating distribution or because external constraints change the
environment where the model operates. To survive in such an ecosystem, machine
learning models need to adapt to new conditions by evolving over time. The idea
of model adaptability has been studied from different perspectives. In this
paper, we propose a solution based on reusing the knowledge acquired by the
already deployed machine learning models and leveraging it to train future
generations. This is the idea behind differential replication of machine
learning models.
</summary>
    <author>
      <name>Irene Unceta</name>
    </author>
    <author>
      <name>Jordi Nin</name>
    </author>
    <author>
      <name>Oriol Pujol</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 Figure, 34 References</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.07981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG, stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16328v1</id>
    <updated>2025-02-22T19:14:36Z</updated>
    <published>2025-02-22T19:14:36Z</published>
    <title>Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on
  Temporal Difference Learning</title>
    <summary>  The primary goal of reinforcement learning is to develop decision-making
policies that prioritize optimal performance, frequently without considering
risk or safety. In contrast, safe reinforcement learning seeks to reduce or
avoid unsafe states. This letter introduces a risk-averse temporal difference
algorithm that uses optimal transport theory to direct the agent toward
predictable behavior. By incorporating a risk indicator, the agent learns to
favor actions with predictable consequences. We evaluate the proposed algorithm
in several case studies and show its effectiveness in the presence of
uncertainty. The results demonstrate that our method reduces the frequency of
visits to risky states while preserving performance. A Python implementation of
the algorithm is available at https://
github.com/SAILRIT/Risk-averse-TD-Learning.
</summary>
    <author>
      <name>Zahra Shahrooei</name>
    </author>
    <author>
      <name>Ali Baheri</name>
    </author>
    <link href="http://arxiv.org/abs/2502.16328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902026v1</id>
    <updated>1999-02-15T01:52:45Z</updated>
    <published>1999-02-15T01:52:45Z</published>
    <title>Probabilistic Inductive Inference:a Survey</title>
    <summary>  Inductive inference is a recursion-theoretic theory of learning, first
developed by E. M. Gold (1967). This paper surveys developments in
probabilistic inductive inference. We mainly focus on finite inference of
recursive functions, since this simple paradigm has produced the most
interesting (and most complex) results.
</summary>
    <author>
      <name>Andris Ambainis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, to appear in Theoretical Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9902026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1., F.4.1., I.2.3., I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312004v1</id>
    <updated>2003-11-30T20:41:18Z</updated>
    <published>2003-11-30T20:41:18Z</published>
    <title>Improving spam filtering by combining Naive Bayes with simple k-nearest
  neighbor searches</title>
    <summary>  Using naive Bayes for email classification has become very popular within the
last few months. They are quite easy to implement and very efficient. In this
paper we want to present empirical results of email classification using a
combination of naive Bayes and k-nearest neighbor searches. Using this
technique we show that the accuracy of a Bayes filter can be improved slightly
for a high number of features and significantly for a small number of features.
</summary>
    <author>
      <name>Daniel Etzold</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0312004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401005v1</id>
    <updated>2004-01-08T07:50:51Z</updated>
    <published>2004-01-08T07:50:51Z</published>
    <title>About Unitary Rating Score Constructing</title>
    <summary>  It is offered to pool test points of different subjects and different aspects
of the same subject together in order to get the unitary rating score, by the
way of nonlinear transformation of indicator points in accordance with Zipf's
distribution. It is proposed to use the well-studied distribution of
Intellectuality Quotient IQ as the reference distribution for latent variable
"progress in studies".
</summary>
    <author>
      <name>Kromer Victor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="1.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506085v1</id>
    <updated>2005-06-22T21:21:13Z</updated>
    <published>2005-06-22T21:21:13Z</published>
    <title>On the Job Training</title>
    <summary>  We propose a new framework for building and evaluating machine learning
algorithms. We argue that many real-world problems require an agent which must
quickly learn to respond to demands, yet can continue to perform and respond to
new training throughout its useful life. We give a framework for how such
agents can be built, describe several metrics for evaluating them, and show
that subtle changes in system construction can significantly affect agent
performance.
</summary>
    <author>
      <name>Jason E. Holt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">BYU</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, submitted to NIPS 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508073v1</id>
    <updated>2005-08-16T16:27:25Z</updated>
    <published>2005-08-16T16:27:25Z</published>
    <title>Universal Learning of Repeated Matrix Games</title>
    <summary>  We study and compare the learning dynamics of two universal learning
algorithms, one based on Bayesian learning and the other on prediction with
expert advice. Both approaches have strong asymptotic performance guarantees.
When confronted with the task of finding good long-term strategies in repeated
2x2 matrix games, they behave quite differently.
</summary>
    <author>
      <name>Jan Poland</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 LaTeX pages, 8 eps figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 15th Annual Machine Learning Conf. of Belgium and The
  Netherlands (Benelearn 2006) pages 7-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0508073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601087v1</id>
    <updated>2006-01-20T05:40:44Z</updated>
    <published>2006-01-20T05:40:44Z</published>
    <title>Processing of Test Matrices with Guessing Correction</title>
    <summary>  It is suggested to insert into test matrix 1s for correct responses, 0s for
response refusals, and negative corrective elements for incorrect responses.
With the classical test theory approach test scores of examinees and items are
calculated traditionally as sums of matrix elements, organized in rows and
columns. Correlation coefficients are estimated using correction coefficients.
In item response theory approach examinee and item logits are estimated using
maximum likelihood method and probabilities of all matrix elements.
</summary>
    <author>
      <name>Kromer Victor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606093v1</id>
    <updated>2006-06-22T04:31:51Z</updated>
    <published>2006-06-22T04:31:51Z</published>
    <title>Predictions as statements and decisions</title>
    <summary>  Prediction is a complex notion, and different predictors (such as people,
computer programs, and probabilistic theories) can pursue very different goals.
In this paper I will review some popular kinds of prediction and argue that the
theory of competitive on-line learning can benefit from the kinds of prediction
that are now foreign to it.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606100v4</id>
    <updated>2011-10-11T10:21:45Z</updated>
    <published>2006-06-23T10:19:40Z</published>
    <title>The generating function of the polytope of transport matrices $U(r,c)$
  as a positive semidefinite kernel of the marginals $r$ and $c$</title>
    <summary>  This paper has been withdrawn by the author due to a crucial error in the
proof of Lemma 5.
</summary>
    <author>
      <name>Marco Cuturi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606100v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606100v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611150v3</id>
    <updated>2006-12-05T15:04:52Z</updated>
    <published>2006-11-29T13:59:31Z</published>
    <title>A Novel Bayesian Classifier using Copula Functions</title>
    <summary>  A useful method for representing Bayesian classifiers is through
\emph{discriminant functions}. Here, using copula functions, we propose a new
model for discriminants. This model provides a rich and generalized class of
decision boundaries. These decision boundaries significantly boost the
classification accuracy especially for high dimensional feature spaces. We
strengthen our analysis through simulation results.
</summary>
    <author>
      <name>Saket Sathe</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611150v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611150v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.1409v3</id>
    <updated>2012-06-08T14:08:19Z</updated>
    <published>2007-04-11T13:17:01Z</published>
    <title>Preconditioned Temporal Difference Learning</title>
    <summary>  This paper has been withdrawn by the author. This draft is withdrawn for its
poor quality in english, unfortunately produced by the author when he was just
starting his science route. Look at the ICML version instead:
http://icml2008.cs.helsinki.fi/papers/111.pdf
</summary>
    <author>
      <name>Yao HengShuai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author. Look at the ICML version
  instead: http://icml2008.cs.helsinki.fi/papers/111.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.1409v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.1409v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1503v1</id>
    <updated>2007-08-10T19:19:54Z</updated>
    <published>2007-08-10T19:19:54Z</published>
    <title>Defensive forecasting for optimal prediction with expert advice</title>
    <summary>  The method of defensive forecasting is applied to the problem of prediction
with expert advice for binary outcomes. It turns out that defensive forecasting
is not only competitive with the Aggregating Algorithm but also handles the
case of "second-guessing" experts, whose advice depends on the learner's
prediction; this paper assumes that the dependence on the learner's prediction
is continuous.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.1503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2848v1</id>
    <updated>2007-10-15T15:38:33Z</updated>
    <published>2007-10-15T15:38:33Z</published>
    <title>Consistency of trace norm minimization</title>
    <summary>  Regularization by the sum of singular values, also referred to as the trace
norm, is a popular technique for estimating low rank rectangular matrices. In
this paper, we extend some of the consistency results of the Lasso to provide
necessary and sufficient conditions for rank consistency of trace norm
minimization with the square loss. We also provide an adaptive version that is
rank consistent even when the necessary condition for the non adaptive version
is not fulfilled.
</summary>
    <author>
      <name>Francis Bach</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">WILLOW Project - Inria/Ens</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0710.2848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.0840v1</id>
    <updated>2007-12-05T22:25:03Z</updated>
    <published>2007-12-05T22:25:03Z</published>
    <title>A Universal Kernel for Learning Regular Languages</title>
    <summary>  We give a universal kernel that renders all the regular languages linearly
separable. We are not able to compute this kernel efficiently and conjecture
that it is intractable, but we do have an efficient $\eps$-approximation.
</summary>
    <author>
      <name> Leonid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Aryeh</arxiv:affiliation>
    </author>
    <author>
      <name> Kontorovich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Workshop on Mining and Learning with Graphs,
  2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.0840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.0840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; D.3.1; F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.1250v1</id>
    <updated>2008-11-08T23:23:08Z</updated>
    <published>2008-11-08T23:23:08Z</published>
    <title>Adaptive Base Class Boost for Multi-class Classification</title>
    <summary>  We develop the concept of ABC-Boost (Adaptive Base Class Boost) for
multi-class classification and present ABC-MART, a concrete implementation of
ABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has
been very successful in large-scale applications. For binary classification,
ABC-MART recovers MART. For multi-class classification, ABC-MART considerably
improves MART, as evaluated on several public data sets.
</summary>
    <author>
      <name>Ping Li</name>
    </author>
    <link href="http://arxiv.org/abs/0811.1250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.1250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3145v2</id>
    <updated>2008-12-16T21:05:28Z</updated>
    <published>2008-12-16T20:41:06Z</published>
    <title>Binary Classification Based on Potentials</title>
    <summary>  We introduce a simple and computationally trivial method for binary
classification based on the evaluation of potential functions. We demonstrate
that despite the conceptual and computational simplicity of the method its
performance can match or exceed that of standard Support Vector Machine
methods.
</summary>
    <author>
      <name>Erik Boczko</name>
    </author>
    <author>
      <name>Andrew DiLullo</name>
    </author>
    <author>
      <name>Todd Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. Presented at the Ohio Collaborative Conference on
  Bioinformatics (OCCBIO) June 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.3145v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3145v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.1227v1</id>
    <updated>2009-04-07T21:15:42Z</updated>
    <published>2009-04-07T21:15:42Z</published>
    <title>Learning convex bodies is hard</title>
    <summary>  We show that learning a convex body in $\RR^d$, given random samples from the
body, requires $2^{\Omega(\sqrt{d/\eps})}$ samples. By learning a convex body
we mean finding a set having at most $\eps$ relative symmetric difference with
the input body. To prove the lower bound we construct a hard to learn family of
convex bodies. Our construction of this family is very simple and based on
error correcting codes.
</summary>
    <author>
      <name>Navin Goyal</name>
    </author>
    <author>
      <name>Luis Rademacher</name>
    </author>
    <link href="http://arxiv.org/abs/0904.1227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.1227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4032v1</id>
    <updated>2009-06-22T15:25:23Z</updated>
    <published>2009-06-22T15:25:23Z</published>
    <title>Bayesian two-sample tests</title>
    <summary>  In this paper, we present two classes of Bayesian approaches to the
two-sample problem. Our first class of methods extends the Bayesian t-test to
include all parametric models in the exponential family and their conjugate
priors. Our second class of methods uses Dirichlet process mixtures (DPM) of
such conjugate-exponential distributions as flexible nonparametric priors over
the unknown distributions.
</summary>
    <author>
      <name>Karsten M. Borgwardt</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <link href="http://arxiv.org/abs/0906.4032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0783v1</id>
    <updated>2009-07-04T18:35:52Z</updated>
    <published>2009-07-04T18:35:52Z</published>
    <title>Bayesian Multitask Learning with Latent Hierarchies</title>
    <summary>  We learn multiple hypotheses for related tasks under a latent hierarchical
relationship between tasks. We exploit the intuition that for domain
adaptation, we wish to share classifier structure, but for multitask learning,
we wish to share covariance structure. Our hierarchical model is seen to
subsume several previously proposed multitask learning models and performs well
on three distinct real-world data sets.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <link href="http://arxiv.org/abs/0907.0783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0784v1</id>
    <updated>2009-07-04T18:42:01Z</updated>
    <published>2009-07-04T18:42:01Z</published>
    <title>Cross-Task Knowledge-Constrained Self Training</title>
    <summary>  We present an algorithmic framework for learning multiple related tasks. Our
framework exploits a form of prior knowledge that relates the output spaces of
these tasks. We present PAC learning results that analyze the conditions under
which such learning is possible. We present results on learning a shallow
parser and named-entity recognition system that exploits our framework, showing
consistent improvements over baseline methods.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <link href="http://arxiv.org/abs/0907.0784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1413v3</id>
    <updated>2011-06-21T17:05:53Z</updated>
    <published>2009-07-09T06:51:54Z</published>
    <title>Privacy constraints in regularized convex optimization</title>
    <summary>  This paper is withdrawn due to some errors, which are corrected in
arXiv:0912.0071v4 [cs.LG].
</summary>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <author>
      <name>Anand D. Sarwate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors due to some errors.
  Corrections have been included in arXiv:0912.0071v4</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.1413v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1413v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0570v1</id>
    <updated>2009-08-05T01:10:09Z</updated>
    <published>2009-08-05T01:10:09Z</published>
    <title>The Infinite Hierarchical Factor Regression Model</title>
    <summary>  We propose a nonparametric Bayesian factor regression model that accounts for
uncertainty in the number of factors, and the relationship between factors. To
accomplish this, we propose a sparse variant of the Indian Buffet Process and
couple this with a hierarchical model over factors, based on Kingman's
coalescent. We apply this model to two problems (factor analysis and factor
regression) in gene-expression data analysis.
</summary>
    <author>
      <name>Piyush Rai</name>
    </author>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.0570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1769v1</id>
    <updated>2009-08-12T18:27:54Z</updated>
    <published>2009-08-12T18:27:54Z</published>
    <title>Approximating the Permanent with Belief Propagation</title>
    <summary>  This work describes a method of approximating matrix permanents efficiently
using belief propagation. We formulate a probability distribution whose
partition function is exactly the permanent, then use Bethe free energy to
approximate this partition function. After deriving some speedups to standard
belief propagation, the resulting algorithm requires $(n^2)$ time per
iteration. Finally, we demonstrate the advantages of using this approximation.
</summary>
    <author>
      <name>Bert Huang</name>
    </author>
    <author>
      <name>Tony Jebara</name>
    </author>
    <link href="http://arxiv.org/abs/0908.1769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4603v1</id>
    <updated>2009-09-25T05:23:33Z</updated>
    <published>2009-09-25T05:23:33Z</published>
    <title>Scalable Inference for Latent Dirichlet Allocation</title>
    <summary>  We investigate the problem of learning a topic model - the well-known Latent
Dirichlet Allocation - in a distributed manner, using a cluster of C processors
and dividing the corpus to be learned equally among them. We propose a simple
approximated method that can be tuned, trading speed for accuracy according to
the task at hand. Our approach is asynchronous, and therefore suitable for
clusters of heterogenous machines.
</summary>
    <author>
      <name>James Petterson</name>
    </author>
    <author>
      <name>Tiberio Caetano</name>
    </author>
    <link href="http://arxiv.org/abs/0909.4603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4683v2</id>
    <updated>2010-05-10T23:01:30Z</updated>
    <published>2009-10-24T22:40:40Z</published>
    <title>Competing with Gaussian linear experts</title>
    <summary>  We study the problem of online regression. We prove a theoretical bound on
the square loss of Ridge Regression. We do not make any assumptions about input
vectors or outcomes. We also show that Bayesian Ridge Regression can be thought
of as an online algorithm competing with all the Gaussian linear experts.
</summary>
    <author>
      <name>Fedor Zhdanov</name>
    </author>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <link href="http://arxiv.org/abs/0910.4683v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4683v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0700v1</id>
    <updated>2010-01-05T13:06:21Z</updated>
    <published>2010-01-05T13:06:21Z</published>
    <title>Vandalism Detection in Wikipedia: a Bag-of-Words Classifier Approach</title>
    <summary>  A bag-of-words based probabilistic classifier is trained using regularized
logistic regression to detect vandalism in the English Wikipedia. Isotonic
regression is used to calibrate the class membership probabilities. Learning
curve, reliability, ROC, and cost analysis are performed.
</summary>
    <author>
      <name>Amit Belani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; G.3; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2780v1</id>
    <updated>2010-02-14T16:37:04Z</updated>
    <published>2010-02-14T16:37:04Z</published>
    <title>Collaborative Filtering in a Non-Uniform World: Learning with the
  Weighted Trace Norm</title>
    <summary>  We show that matrix completion with trace-norm regularization can be
significantly hurt when entries of the matrix are sampled non-uniformly. We
introduce a weighted version of the trace-norm regularizer that works well also
with non-uniform sampling. Our experimental results demonstrate that the
weighted trace-norm regularization indeed yields significant gains on the
(highly non-uniformly sampled) Netflix dataset.
</summary>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.2780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0696v1</id>
    <updated>2010-03-02T22:27:31Z</updated>
    <published>2010-03-02T22:27:31Z</published>
    <title>Exponential Family Hybrid Semi-Supervised Learning</title>
    <summary>  We present an approach to semi-supervised learning based on an exponential
family characterization. Our approach generalizes previous work on coupled
priors for hybrid generative/discriminative models. Our model is more flexible
and natural than previous approaches. Experimental results on several data sets
show that our approach also performs better in practice.
</summary>
    <author>
      <name>Arvind Agarwal</name>
    </author>
    <author>
      <name>Hal Daume III</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Twenty-First International Joint Conference on Artificial
  Intelligence 2009, pg 974-979</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.0696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.1918v2</id>
    <updated>2010-06-04T19:13:37Z</updated>
    <published>2010-05-11T19:27:35Z</published>
    <title>Prediction with Expert Advice under Discounted Loss</title>
    <summary>  We study prediction with expert advice in the setting where the losses are
accumulated with some discounting---the impact of old losses may gradually
vanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm
for Regression to this case, propose a suitable new variant of exponential
weights algorithm, and prove respective loss bounds.
</summary>
    <author>
      <name>Alexey Chernov</name>
    </author>
    <author>
      <name>Fedor Zhdanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages; expanded (2 remarks -&gt; theorems), some misprints corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.1918v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.1918v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2243v1</id>
    <updated>2010-05-13T01:59:57Z</updated>
    <published>2010-05-13T01:59:57Z</published>
    <title>Robustness and Generalization</title>
    <summary>  We derive generalization bounds for learning algorithms based on their
robustness: the property that if a testing sample is "similar" to a training
sample, then the testing error is close to the training error. This provides a
novel approach, different from the complexity or stability arguments, to study
generalization of learning algorithms. We further show that a weak notion of
robustness is both sufficient and necessary for generalizability, which implies
that robustness is a fundamental property for learning algorithms to work.
</summary>
    <author>
      <name>Huan Xu</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1005.2243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2364v2</id>
    <updated>2010-05-14T11:28:03Z</updated>
    <published>2010-05-13T15:59:01Z</published>
    <title>A Short Introduction to Model Selection, Kolmogorov Complexity and
  Minimum Description Length (MDL)</title>
    <summary>  The concept of overfitting in model selection is explained and demonstrated
with an example. After providing some background information on information
theory and Kolmogorov complexity, we provide a short explanation of Minimum
Description Length and error minimization. We conclude with a discussion of the
typical features of overfitting in model selection.
</summary>
    <author>
      <name>Volker Nannen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, Chapter 1 of The Paradox of Overfitting, Master's thesis,
  Rijksuniversiteit Groningen, 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.2364v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2364v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2588v1</id>
    <updated>2010-06-14T02:03:12Z</updated>
    <published>2010-06-14T02:03:12Z</published>
    <title>Agnostic Active Learning Without Constraints</title>
    <summary>  We present and analyze an agnostic active learning algorithm that works
without keeping a version space. This is unlike all previous approaches where a
restricted set of candidate hypotheses is maintained throughout learning, and
only hypotheses from this set are ever returned. By avoiding this version space
approach, our algorithm sheds the computational burden and brittleness
associated with maintaining version spaces, yet still allows for substantial
improvements over supervised learning for classification.
</summary>
    <author>
      <name>Alina Beygelzimer</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0660v1</id>
    <updated>2010-07-05T11:46:35Z</updated>
    <published>2010-07-05T11:46:35Z</published>
    <title>The Latent Bernoulli-Gauss Model for Data Analysis</title>
    <summary>  We present a new latent-variable model employing a Gaussian mixture
integrated with a feature selection procedure (the Bernoulli part of the model)
which together form a "Latent Bernoulli-Gauss" distribution. The model is
applied to MAP estimation, clustering, feature selection and collaborative
filtering and fares favorably with the state-of-the-art latent-variable models.
</summary>
    <author>
      <name>Amnon Shashua</name>
    </author>
    <author>
      <name>Gabi Pragier</name>
    </author>
    <link href="http://arxiv.org/abs/1007.0660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0117v1</id>
    <updated>2010-09-01T08:29:49Z</updated>
    <published>2010-09-01T08:29:49Z</published>
    <title>Exploring Language-Independent Emotional Acoustic Features via Feature
  Selection</title>
    <summary>  We propose a novel feature selection strategy to discover
language-independent acoustic features that tend to be responsible for emotions
regardless of languages, linguistics and other factors. Experimental results
suggest that the language-independent feature subset discovered yields the
performance comparable to the full feature set on various emotional speech
corpora.
</summary>
    <author>
      <name>Arslan Shaukat</name>
    </author>
    <author>
      <name>Ke Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.0117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.4050v1</id>
    <updated>2010-10-19T21:01:45Z</updated>
    <published>2010-10-19T21:01:45Z</published>
    <title>Efficient Matrix Completion with Gaussian Models</title>
    <summary>  A general framework based on Gaussian models and a MAP-EM algorithm is
introduced in this paper for solving matrix/table completion problems. The
numerical experiments with the standard and challenging movie ratings data show
that the proposed approach, based on probably one of the simplest probabilistic
models, leads to the results in the same ballpark as the state-of-the-art, at a
lower computational cost.
</summary>
    <author>
      <name>Flavien Léger</name>
    </author>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Guillermo Sapiro</name>
    </author>
    <link href="http://arxiv.org/abs/1010.4050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.4050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.5290v2</id>
    <updated>2011-03-16T05:53:38Z</updated>
    <published>2010-10-26T00:28:36Z</published>
    <title>Converged Algorithms for Orthogonal Nonnegative Matrix Factorizations</title>
    <summary>  This paper proposes uni-orthogonal and bi-orthogonal nonnegative matrix
factorization algorithms with robust convergence proofs. We design the
algorithms based on the work of Lee and Seung [1], and derive the converged
versions by utilizing ideas from the work of Lin [2]. The experimental results
confirm the theoretical guarantees of the convergences.
</summary>
    <author>
      <name>Andri Mirzal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.5290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.5290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5668v1</id>
    <updated>2010-11-25T18:52:30Z</updated>
    <published>2010-11-25T18:52:30Z</published>
    <title>On Theorem 2.3 in "Prediction, Learning, and Games" by Cesa-Bianchi and
  Lugosi</title>
    <summary>  The note presents a modified proof of a loss bound for the exponentially
weighted average forecaster with time-varying potential. The regret term of the
algorithm is upper-bounded by sqrt{n ln(N)} (uniformly in n), where N is the
number of experts and n is the number of steps.
</summary>
    <author>
      <name>Alexey Chernov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages; excerpt from arXiv:1005.1918, simplified and rewritten using
  the notation of the monograph by Cesa-Bianchi and Lugosi</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.5668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.4249v1</id>
    <updated>2010-12-20T07:36:42Z</updated>
    <published>2010-12-20T07:36:42Z</published>
    <title>Travel Time Estimation Using Floating Car Data</title>
    <summary>  This report explores the use of machine learning techniques to accurately
predict travel times in city streets and highways using floating car data
(location information of user vehicles on a road network). The aim of this
report is twofold, first we present a general architecture of solving this
problem, then present and evaluate few techniques on real floating car data
gathered over a month on a 5 Km highway in New Delhi.
</summary>
    <author>
      <name>Raffi Sevlian</name>
    </author>
    <author>
      <name>Ram Rajagopal</name>
    </author>
    <link href="http://arxiv.org/abs/1012.4249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.2467v1</id>
    <updated>2011-02-12T01:34:52Z</updated>
    <published>2011-02-12T01:34:52Z</published>
    <title>Universal Learning Theory</title>
    <summary>  This encyclopedic article gives a mini-introduction into the theory of
universal learning, founded by Ray Solomonoff in the 1960s and significantly
developed and extended in the last decade. It explains the spirit of universal
learning, but necessarily glosses over technical subtleties.
</summary>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 LaTeX pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Machine Learning (2011) pages 1001-1008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.2467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.2467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4961v1</id>
    <updated>2011-08-24T22:38:40Z</updated>
    <published>2011-08-24T22:38:40Z</published>
    <title>Non-trivial two-armed partial-monitoring games are bandits</title>
    <summary>  We consider online learning in partial-monitoring games against an oblivious
adversary. We show that when the number of actions available to the learner is
two and the game is nontrivial then it is reducible to a bandit-like game and
thus the minimax regret is $\Theta(\sqrt{T})$.
</summary>
    <author>
      <name>András Antos</name>
    </author>
    <author>
      <name>Gábor Bartók</name>
    </author>
    <author>
      <name>Csaba Szepesvári</name>
    </author>
    <link href="http://arxiv.org/abs/1108.4961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.0593v1</id>
    <updated>2011-10-04T07:34:13Z</updated>
    <published>2011-10-04T07:34:13Z</published>
    <title>Two Projection Pursuit Algorithms for Machine Learning under
  Non-Stationarity</title>
    <summary>  This thesis derives, tests and applies two linear projection algorithms for
machine learning under non-stationarity. The first finds a direction in a
linear space upon which a data set is maximally non-stationary. The second aims
to robustify two-way classification against non-stationarity. The algorithm is
tested on a key application scenario, namely Brain Computer Interfacing.
</summary>
    <author>
      <name>Duncan A. J. Blythe</name>
    </author>
    <link href="http://arxiv.org/abs/1110.0593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.1390v1</id>
    <updated>2011-12-06T20:15:37Z</updated>
    <published>2011-12-06T20:15:37Z</published>
    <title>An Identity for Kernel Ridge Regression</title>
    <summary>  This paper derives an identity connecting the square loss of ridge regression
in on-line mode with the loss of the retrospectively best regressor. Some
corollaries about the properties of the cumulative loss of on-line ridge
regression are also obtained.
</summary>
    <author>
      <name>Fedor Zhdanov</name>
    </author>
    <author>
      <name>Yuri Kalnishkan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages; extended version of ALT 2010 paper (Proceedings of ALT
  2010, LNCS 6331, Springer, 2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.1390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.1390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5598v4</id>
    <updated>2012-04-13T06:52:44Z</updated>
    <published>2012-02-25T02:10:20Z</published>
    <title>Clustering using Max-norm Constrained Optimization</title>
    <summary>  We suggest using the max-norm as a convex surrogate constraint for
clustering. We show how this yields a better exact cluster recovery guarantee
than previously suggested nuclear-norm relaxation, and study the effectiveness
of our method, and other related convex relaxations, compared to other
clustering approaches.
</summary>
    <author>
      <name>Ali Jalali</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1202.5598v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5598v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0566v2</id>
    <updated>2012-06-21T12:14:24Z</updated>
    <published>2012-04-03T00:33:53Z</published>
    <title>The Kernelized Stochastic Batch Perceptron</title>
    <summary>  We present a novel approach for training kernel Support Vector Machines,
establish learning runtime guarantees for our method that are better then those
of any other known kernelized SVM optimization approach, and show that our
method works well in practice compared to existing alternatives.
</summary>
    <author>
      <name>Andrew Cotter</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1204.0566v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0566v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4294v1</id>
    <updated>2012-04-19T09:29:10Z</updated>
    <published>2012-04-19T09:29:10Z</published>
    <title>Learning in Riemannian Orbifolds</title>
    <summary>  Learning in Riemannian orbifolds is motivated by existing machine learning
algorithms that directly operate on finite combinatorial structures such as
point patterns, trees, and graphs. These methods, however, lack statistical
justification. This contribution derives consistency results for learning
problems in structured domains and thereby generalizes learning in vector
spaces and manifolds.
</summary>
    <author>
      <name>Brijnesh J. Jain</name>
    </author>
    <author>
      <name>Klaus Obermayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1001.0921</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6446v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Agglomerative Bregman Clustering</title>
    <summary>  This manuscript develops the theory of agglomerative clustering with Bregman
divergences. Geometric smoothing techniques are developed to deal with
degenerate clusters. To allow for cluster models based on exponential families
with overcomplete representations, Bregman divergences are developed for
nondifferentiable convex functions.
</summary>
    <author>
      <name>Matus Telgarsky</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UCSD</arxiv:affiliation>
    </author>
    <author>
      <name>Sanjoy Dasgupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UCSD</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4676v2</id>
    <updated>2012-09-16T11:24:54Z</updated>
    <published>2012-07-19T14:08:22Z</published>
    <title>Proceedings of the 29th International Conference on Machine Learning
  (ICML-12)</title>
    <summary>  This is an index to the papers that appear in the Proceedings of the 29th
International Conference on Machine Learning (ICML-12). The conference was held
in Edinburgh, Scotland, June 27th - July 3rd, 2012.
</summary>
    <author>
      <name>John Langford</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Editors</arxiv:affiliation>
    </author>
    <author>
      <name>Joelle Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Editors</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 29th International Conference on Machine Learning
  (ICML-12). Editors: John Langford and Joelle Pineau. Publisher: Omnipress,
  2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4676v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4676v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0699v1</id>
    <updated>2012-10-02T08:40:46Z</updated>
    <published>2012-10-02T08:40:46Z</published>
    <title>TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data
  Classification</title>
    <summary>  We introduce semi-supervised data classification algorithms based on total
variation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine
(SVM), Cheeger cut, labeled and unlabeled data points. We design binary and
multi-class semi-supervised classification algorithms. We compare the TV-based
classification algorithms with the related Laplacian-based algorithms, and show
that TV classification perform significantly better when the number of labeled
data is small.
</summary>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <author>
      <name>Ruiliang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1210.0699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6248v2</id>
    <updated>2012-12-04T13:50:19Z</updated>
    <published>2012-11-27T09:36:22Z</published>
    <title>A simple non-parametric Topic Mixture for Authors and Documents</title>
    <summary>  This article reviews the Author-Topic Model and presents a new non-parametric
extension based on the Hierarchical Dirichlet Process. The extension is
especially suitable when no prior information about the number of components
necessary is available. A blocked Gibbs sampler is described and focus put on
staying as close as possible to the original model with only the minimum of
theoretical and implementation overhead necessary.
</summary>
    <author>
      <name>Arnim Bleier</name>
    </author>
    <link href="http://arxiv.org/abs/1211.6248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2474v1</id>
    <updated>2012-10-19T15:06:27Z</updated>
    <published>2012-10-19T15:06:27Z</published>
    <title>Learning Riemannian Metrics</title>
    <summary>  We propose a solution to the problem of estimating a Riemannian metric
associated with a given differentiable manifold. The metric learning problem is
based on minimizing the relative volume of a given set of points. We derive the
details for a family of metrics on the multinomial simplex. The resulting
metric has applications in text classification and bears some similarity to
TFIDF representation of text documents.
</summary>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4347v1</id>
    <updated>2012-12-18T13:35:38Z</updated>
    <published>2012-12-18T13:35:38Z</published>
    <title>Bayesian Group Nonnegative Matrix Factorization for EEG Analysis</title>
    <summary>  We propose a generative model of a group EEG analysis, based on appropriate
kernel assumptions on EEG data. We derive the variational inference update rule
using various approximation techniques. The proposed model outperforms the
current state-of-the-art algorithms in terms of common pattern extraction. The
validity of the proposed model is tested on the BCI competition dataset.
</summary>
    <author>
      <name>Bonggun Shin</name>
    </author>
    <author>
      <name>Alice Oh</name>
    </author>
    <link href="http://arxiv.org/abs/1212.4347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5091v1</id>
    <updated>2012-12-19T17:40:07Z</updated>
    <published>2012-12-19T17:40:07Z</published>
    <title>Maximally Informative Observables and Categorical Perception</title>
    <summary>  We formulate the problem of perception in the framework of information
theory, and prove that categorical perception is equivalent to the existence of
an observable that has the maximum possible information on the target of
perception. We call such an observable maximally informative. Regardless
whether categorical perception is real, maximally informative observables can
form the basis of a theory of perception. We conclude with the implications of
such a theory for the problem of speech perception.
</summary>
    <author>
      <name>Elaine Tsiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3849v1</id>
    <updated>2013-01-16T15:49:46Z</updated>
    <published>2013-01-16T15:49:46Z</published>
    <title>Experiments with Random Projection</title>
    <summary>  Recent theoretical work has identified random projection as a promising
dimensionality reduction technique for learning mixtures of Gausians. Here we
summarize these results and illustrate them by a wide variety of experiments on
synthetic and real data.
</summary>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2417v1</id>
    <updated>2013-03-11T03:29:35Z</updated>
    <published>2013-03-11T03:29:35Z</published>
    <title>Linear NDCG and Pair-wise Loss</title>
    <summary>  Linear NDCG is used for measuring the performance of the Web content quality
assessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will prove
that the DCG error equals a new pair-wise loss.
</summary>
    <author>
      <name>Xiao-Bo Jin</name>
    </author>
    <author>
      <name>Guang-Gang Geng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.2417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6390v2</id>
    <updated>2013-03-27T16:23:48Z</updated>
    <published>2013-03-26T06:01:34Z</published>
    <title>A Note on k-support Norm Regularized Risk Minimization</title>
    <summary>  The k-support norm has been recently introduced to perform correlated
sparsity regularization. Although Argyriou et al. only reported experiments
using squared loss, here we apply it to several other commonly used settings
resulting in novel machine learning algorithms with interesting and familiar
limit cases. Source code for the algorithms described here is available.
</summary>
    <author>
      <name>Matthew Blaschko</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Saclay - Ile de France, CVN</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1303.6390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6899v1</id>
    <updated>2013-04-25T12:59:31Z</updated>
    <published>2013-04-25T12:59:31Z</published>
    <title>An implementation of the relational k-means algorithm</title>
    <summary>  A C# implementation of a generalized k-means variant called relational
k-means is described here. Relational k-means is a generalization of the
well-known k-means clustering method which works for non-Euclidean scenarios as
well. The input is an arbitrary distance matrix, as opposed to the traditional
k-means method, where the clustered objects need to be identified with vectors.
</summary>
    <author>
      <name>Balázs Szalkai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.6899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2218v1</id>
    <updated>2013-05-09T21:31:47Z</updated>
    <published>2013-05-09T21:31:47Z</published>
    <title>Stochastic gradient descent algorithms for strongly convex functions at
  O(1/T) convergence rates</title>
    <summary>  With a weighting scheme proportional to t, a traditional stochastic gradient
descent (SGD) algorithm achieves a high probability convergence rate of
O({\kappa}/T) for strongly convex functions, instead of O({\kappa} ln(T)/T). We
also prove that an accelerated SGD algorithm also achieves a rate of
O({\kappa}/T).
</summary>
    <author>
      <name>Shenghuo Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1305.2218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6646v1</id>
    <updated>2013-05-28T22:12:59Z</updated>
    <published>2013-05-28T22:12:59Z</published>
    <title>Normalized Online Learning</title>
    <summary>  We introduce online learning algorithms which are independent of feature
scales, proving regret bounds dependent on the ratio of scales existent in the
data rather than the absolute scale. This has several useful effects: there is
no need to pre-normalize data, the test-time and test-space complexity are
reduced, and the algorithms are more robust.
</summary>
    <author>
      <name>Stephane Ross</name>
    </author>
    <author>
      <name>Paul Mineiro</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.6646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0317v1</id>
    <updated>2013-07-01T10:03:58Z</updated>
    <published>2013-07-01T10:03:58Z</published>
    <title>Algorithms of the LDA model [REPORT]</title>
    <summary>  We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them
are variational inference algorithms: Variational Bayesian inference and Online
Variational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)
algorithm -- Collapsed Gibbs sampling. We compare their time complexity and
performance. We find that online variational Bayesian inference is the fastest
algorithm and still returns reasonably good results.
</summary>
    <author>
      <name>Jaka Špeh</name>
    </author>
    <author>
      <name>Andrej Muhič</name>
    </author>
    <author>
      <name>Jan Rupnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, report</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.0317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3675v1</id>
    <updated>2013-07-13T19:38:09Z</updated>
    <published>2013-07-13T19:38:09Z</published>
    <title>Minimum Error Rate Training and the Convex Hull Semiring</title>
    <summary>  We describe the line search used in the minimum error rate training algorithm
MERT as the "inside score" of a weighted proof forest under a semiring defined
in terms of well-understood operations from computational geometry. This
conception leads to a straightforward complexity analysis of the dynamic
programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009)
and practical approaches to implementation.
</summary>
    <author>
      <name>Chris Dyer</name>
    </author>
    <link href="http://arxiv.org/abs/1307.3675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8305v1</id>
    <updated>2013-07-31T12:38:20Z</updated>
    <published>2013-07-31T12:38:20Z</published>
    <title>The Planning-ahead SMO Algorithm</title>
    <summary>  The sequential minimal optimization (SMO) algorithm and variants thereof are
the de facto standard method for solving large quadratic programs for support
vector machine (SVM) training. In this paper we propose a simple yet powerful
modification. The main emphasis is on an algorithm improving the SMO step size
by planning-ahead. The theoretical analysis ensures its convergence to the
optimum. Experiments involving a large number of datasets were carried out to
demonstrate the superiority of the new algorithm.
</summary>
    <author>
      <name>Tobias Glasmachers</name>
    </author>
    <link href="http://arxiv.org/abs/1307.8305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1541v1</id>
    <updated>2013-09-06T05:48:40Z</updated>
    <published>2013-09-06T05:48:40Z</published>
    <title>Projection onto the probability simplex: An efficient algorithm with a
  simple proof, and an application</title>
    <summary>  We provide an elementary proof of a simple, efficient algorithm for computing
the Euclidean projection of a point onto the probability simplex. We also show
an application in Laplacian K-modes clustering.
</summary>
    <author>
      <name>Weiran Wang</name>
    </author>
    <author>
      <name>Miguel Á. Carreira-Perpiñán</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.1541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.8320v1</id>
    <updated>2013-10-30T20:56:50Z</updated>
    <published>2013-10-30T20:56:50Z</published>
    <title>Safe and Efficient Screening For Sparse Support Vector Machine</title>
    <summary>  Screening is an effective technique for speeding up the training process of a
sparse learning model by removing the features that are guaranteed to be
inactive the process. In this paper, we present a efficient screening technique
for sparse support vector machine based on variational inequality. The
technique is both efficient and safe.
</summary>
    <author>
      <name>Zheng Zhao</name>
    </author>
    <author>
      <name>Jun Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1310.8320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.8320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3157v1</id>
    <updated>2013-11-12T17:25:29Z</updated>
    <published>2013-11-12T17:25:29Z</published>
    <title>Multiple Closed-Form Local Metric Learning for K-Nearest Neighbor
  Classifier</title>
    <summary>  Many researches have been devoted to learn a Mahalanobis distance metric,
which can effectively improve the performance of kNN classification. Most
approaches are iterative and computational expensive and linear rigidity still
critically limits metric learning algorithm to perform better. We proposed a
computational economical framework to learn multiple metrics in closed-form.
</summary>
    <author>
      <name>Jianbo Ye</name>
    </author>
    <link href="http://arxiv.org/abs/1311.3157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7198v1</id>
    <updated>2013-11-28T03:59:31Z</updated>
    <published>2013-11-28T03:59:31Z</published>
    <title>ADMM Algorithm for Graphical Lasso with an $\ell_{\infty}$ Element-wise
  Norm Constraint</title>
    <summary>  We consider the problem of Graphical lasso with an additional $\ell_{\infty}$
element-wise norm constraint on the precision matrix. This problem has
applications in high-dimensional covariance decomposition such as in
\citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. We
also use a continuation strategy on the penalty parameter to have a fast
implemenation of the algorithm.
</summary>
    <author>
      <name>Karthik Mohan</name>
    </author>
    <link href="http://arxiv.org/abs/1311.7198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.0048v1</id>
    <updated>2013-11-30T01:07:25Z</updated>
    <published>2013-11-30T01:07:25Z</published>
    <title>Stochastic Optimization of Smooth Loss</title>
    <summary>  In this paper, we first prove a high probability bound rather than an
expectation bound for stochastic optimization with smooth loss. Furthermore,
the existing analysis requires the knowledge of optimal classifier for tuning
the step size in order to achieve the desired bound. However, this information
is usually not accessible in advanced. We also propose a strategy to address
the limitation.
</summary>
    <author>
      <name>Rong Jin</name>
    </author>
    <link href="http://arxiv.org/abs/1312.0048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.0048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5853v4</id>
    <updated>2014-02-18T21:35:13Z</updated>
    <published>2013-12-20T08:45:07Z</published>
    <title>Multi-GPU Training of ConvNets</title>
    <summary>  In this work we evaluate different approaches to parallelize computation of
convolutional neural networks across several GPUs.
</summary>
    <author>
      <name>Omry Yadan</name>
    </author>
    <author>
      <name>Keith Adams</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning, Deep Learning, Convolutional Networks, Computer
  Vision, GPU, CUDA</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5853v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5853v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6042v4</id>
    <updated>2014-06-17T10:24:51Z</updated>
    <published>2013-12-20T17:03:50Z</published>
    <title>Learning States Representations in POMDP</title>
    <summary>  We propose to deal with sequential processes where only partial observations
are available by learning a latent representation space on which policies may
be accurately learned.
</summary>
    <author>
      <name>Gabriella Contardo</name>
    </author>
    <author>
      <name>Ludovic Denoyer</name>
    </author>
    <author>
      <name>Thierry Artieres</name>
    </author>
    <author>
      <name>Patrick Gallinari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6042v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6042v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4845v1</id>
    <updated>2014-02-19T22:59:14Z</updated>
    <published>2014-02-19T22:59:14Z</published>
    <title>Diffusion Least Mean Square: Simulations</title>
    <summary>  In this technical report we analyse the performance of diffusion strategies
applied to the Least-Mean-Square adaptive filter. We configure a network of
cooperative agents running adaptive filters and discuss their behaviour when
compared with a non-cooperative agent which represents the average of the
network. The analysis provides conditions under which diversity in the filter
parameters is beneficial in terms of convergence and stability. Simulations
drive and support the analysis.
</summary>
    <author>
      <name>Jonathan Gelati</name>
    </author>
    <author>
      <name>Sithan Kanna</name>
    </author>
    <link href="http://arxiv.org/abs/1402.4845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5766v1</id>
    <updated>2014-02-24T09:49:04Z</updated>
    <published>2014-02-24T09:49:04Z</published>
    <title>No more meta-parameter tuning in unsupervised sparse feature learning</title>
    <summary>  We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised
feature learning algorithm, which exploits a new way of optimizing for
sparsity. Experiments on STL-10 show that the method presents state-of-the-art
performance and provides discriminative features that generalize well.
</summary>
    <author>
      <name>Adriana Romero</name>
    </author>
    <author>
      <name>Petia Radeva</name>
    </author>
    <author>
      <name>Carlo Gatta</name>
    </author>
    <link href="http://arxiv.org/abs/1402.5766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6013v1</id>
    <updated>2014-02-24T23:12:42Z</updated>
    <published>2014-02-24T23:12:42Z</published>
    <title>Open science in machine learning</title>
    <summary>  We present OpenML and mldata, open science platforms that provides easy
access to machine learning data, software and results to encourage further
study and application. They go beyond the more traditional repositories for
data sets and software packages in that they allow researchers to also easily
share the results they obtained in experiments and to compare their solutions
with those of others.
</summary>
    <author>
      <name>Joaquin Vanschoren</name>
    </author>
    <author>
      <name>Mikio L. Braun</name>
    </author>
    <author>
      <name>Cheng Soon Ong</name>
    </author>
    <link href="http://arxiv.org/abs/1402.6013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0833v1</id>
    <updated>2014-05-05T09:29:17Z</updated>
    <published>2014-05-05T09:29:17Z</published>
    <title>Generalized Risk-Aversion in Stochastic Multi-Armed Bandits</title>
    <summary>  We consider the problem of minimizing the regret in stochastic multi-armed
bandit, when the measure of goodness of an arm is not the mean return, but some
general function of the mean and the variance.We characterize the conditions
under which learning is possible and present examples for which no natural
algorithm can achieve sublinear regret.
</summary>
    <author>
      <name>Alexander Zimin</name>
    </author>
    <author>
      <name>Rasmus Ibsen-Jensen</name>
    </author>
    <author>
      <name>Krishnendu Chatterjee</name>
    </author>
    <link href="http://arxiv.org/abs/1405.0833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3167v1</id>
    <updated>2014-05-13T14:36:59Z</updated>
    <published>2014-05-13T14:36:59Z</published>
    <title>Clustering, Hamming Embedding, Generalized LSH and the Max Norm</title>
    <summary>  We study the convex relaxation of clustering and hamming embedding, focusing
on the asymmetric case (co-clustering and asymmetric hamming embedding),
understanding their relationship to LSH as studied by (Charikar 2002) and to
the max-norm ball, and the differences between their symmetric and asymmetric
versions.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Yury Makarychev</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.3167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7430v1</id>
    <updated>2014-05-29T00:37:28Z</updated>
    <published>2014-05-29T00:37:28Z</published>
    <title>BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization,
  Experimental Design and Bandits</title>
    <summary>  BayesOpt is a library with state-of-the-art Bayesian optimization methods to
solve nonlinear optimization, stochastic bandits or sequential experimental
design problems. Bayesian optimization is sample efficient by building a
posterior distribution to capture the evidence and prior knowledge for the
target function. Built in standard C++, the library is extremely efficient
while being portable and flexible. It includes a common interface for C, C++,
Python, Matlab and Octave.
</summary>
    <author>
      <name>Ruben Martinez-Cantin</name>
    </author>
    <link href="http://arxiv.org/abs/1405.7430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3726v1</id>
    <updated>2014-06-14T13:08:30Z</updated>
    <published>2014-06-14T13:08:30Z</published>
    <title>Evaluation of Machine Learning Techniques for Green Energy Prediction</title>
    <summary>  We evaluate the following Machine Learning techniques for Green Energy (Wind,
Solar) Prediction: Bayesian Inference, Neural Networks, Support Vector
Machines, Clustering techniques (PCA). Our objective is to predict green energy
using weather forecasts, predict deviations from forecast green energy, find
correlation amongst different weather parameters and green energy availability,
recover lost or missing energy (/ weather) data. We use historical weather data
and weather forecasts for the same.
</summary>
    <author>
      <name>Ankur Sahai</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2065v1</id>
    <updated>2014-08-09T06:05:51Z</updated>
    <published>2014-08-09T06:05:51Z</published>
    <title>Normalized Online Learning</title>
    <summary>  We introduce online learning algorithms which are independent of feature
scales, proving regret bounds dependent on the ratio of scales existent in the
data rather than the absolute scale. This has several useful effects: there is
no need to pre-normalize data, the test-time and test-space complexity are
reduced, and the algorithms are more robust.
</summary>
    <author>
      <name>Stephane Ross</name>
    </author>
    <author>
      <name>Paul Mineiro</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.2065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4673v2</id>
    <updated>2017-08-09T12:50:11Z</updated>
    <published>2014-08-20T14:29:11Z</published>
    <title>AFP Algorithm and a Canonical Normal Form for Horn Formulas</title>
    <summary>  AFP Algorithm is a learning algorithm for Horn formulas. We show that it does
not improve the complexity of AFP Algorithm, if after each negative
counterexample more that just one refinements are performed. Moreover, a
canonical normal form for Horn formulas is presented, and it is proved that the
output formula of AFP Algorithm is in this normal form.
</summary>
    <author>
      <name>Ruhollah Majdoddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some minor corrections in the text</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.4673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32, 68T27" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5449v1</id>
    <updated>2014-08-23T01:23:23Z</updated>
    <published>2014-08-23T01:23:23Z</published>
    <title>Stretchy Polynomial Regression</title>
    <summary>  This article proposes a novel solution for stretchy polynomial regression
learning. The solution comes in primal and dual closed-forms similar to that of
ridge regression. Essentially, the proposed solution stretches the covariance
computation via a power term thereby compresses or amplifies the estimation.
Our experiments on both synthetic data and real-world data show effectiveness
of the proposed method for compressive learning.
</summary>
    <author>
      <name>Kar-Ann Toh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article created in April and revised in August 2014. Submitted to
  ICARCV 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03838v1</id>
    <updated>2015-01-15T21:59:39Z</updated>
    <published>2015-01-15T21:59:39Z</published>
    <title>PAC-Bayes with Minimax for Confidence-Rated Transduction</title>
    <summary>  We consider using an ensemble of binary classifiers for transductive
prediction, when unlabeled test data are known in advance. We derive minimax
optimal rules for confidence-rated prediction in this setting. By using
PAC-Bayes analysis on these rules, we obtain data-dependent performance
guarantees without distributional assumptions on the data. Our analysis
techniques are readily extended to a setting in which the predictor is allowed
to abstain.
</summary>
    <author>
      <name>Akshay Balsubramani</name>
    </author>
    <author>
      <name>Yoav Freund</name>
    </author>
    <link href="http://arxiv.org/abs/1501.03838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04244v1</id>
    <updated>2015-01-17T23:42:08Z</updated>
    <published>2015-01-17T23:42:08Z</published>
    <title>Generalised Random Forest Space Overview</title>
    <summary>  Assuming a view of the Random Forest as a special case of a nested ensemble
of interchangeable modules, we construct a generalisation space allowing one to
easily develop novel methods based on this algorithm. We discuss the role and
required properties of modules at each level, especially in context of some
already proposed RF generalisations.
</summary>
    <author>
      <name>Miron B. Kursa</name>
    </author>
    <link href="http://arxiv.org/abs/1501.04244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06798v1</id>
    <updated>2015-04-26T10:18:09Z</updated>
    <published>2015-04-26T10:18:09Z</published>
    <title>Overlapping Community Detection by Online Cluster Aggregation</title>
    <summary>  We present a new online algorithm for detecting overlapping communities. The
main ingredients are a modification of an online k-means algorithm and a new
approach to modelling overlap in communities. An evaluation on large benchmark
graphs shows that the quality of discovered communities compares favorably to
several methods in the recent literature, while the running time is
significantly improved.
</summary>
    <author>
      <name>Mark Kozdoba</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08215v1</id>
    <updated>2015-04-30T13:26:46Z</updated>
    <published>2015-04-30T13:26:46Z</published>
    <title>Lateral Connections in Denoising Autoencoders Support Supervised
  Learning</title>
    <summary>  We show how a deep denoising autoencoder with lateral connections can be used
as an auxiliary unsupervised learning task to support supervised learning. The
proposed model is trained to minimize simultaneously the sum of supervised and
unsupervised cost functions by back-propagation, avoiding the need for
layer-wise pretraining. It improves the state of the art significantly in the
permutation-invariant MNIST classification task.
</summary>
    <author>
      <name>Antti Rasmus</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <author>
      <name>Tapani Raiko</name>
    </author>
    <link href="http://arxiv.org/abs/1504.08215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01563v1</id>
    <updated>2015-07-06T18:53:09Z</updated>
    <published>2015-07-06T18:53:09Z</published>
    <title>A Simple Algorithm for Maximum Margin Classification, Revisited</title>
    <summary>  In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] for
computing a linear maximum margin classifier. Our presentation is self
contained, and the algorithm itself is slightly simpler than the original
algorithm. The algorithm itself is a simple Perceptron like iterative
algorithm. For more details and background, the reader is referred to the
original paper.
</summary>
    <author>
      <name>Sariel Har-Peled</name>
    </author>
    <link href="http://arxiv.org/abs/1507.01563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02084v1</id>
    <updated>2015-07-08T09:58:06Z</updated>
    <published>2015-07-08T09:58:06Z</published>
    <title>Shedding Light on the Asymmetric Learning Capability of AdaBoost</title>
    <summary>  In this paper, we propose a different insight to analyze AdaBoost. This
analysis reveals that, beyond some preconceptions, AdaBoost can be directly
used as an asymmetric learning algorithm, preserving all its theoretical
properties. A novel class-conditional description of AdaBoost, which models the
actual asymmetric behavior of the algorithm, is presented.
</summary>
    <author>
      <name>Iago Landesa-Vázquez</name>
    </author>
    <author>
      <name>José Luis Alba-Castro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2011.10.022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2011.10.022" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters 33 (2012) 247-255</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.02084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04910v1</id>
    <updated>2015-07-17T10:39:52Z</updated>
    <published>2015-07-17T10:39:52Z</published>
    <title>Lower Bounds for Multi-armed Bandit with Non-equivalent Multiple Plays</title>
    <summary>  We study the stochastic multi-armed bandit problem with non-equivalent
multiple plays where, at each step, an agent chooses not only a set of arms,
but also their order, which influences reward distribution. In several problem
formulations with different assumptions, we provide lower bounds for regret
with standard asymptotics $O(\log{t})$ but novel coefficients and provide
optimal algorithms, thus proving that these bounds cannot be improved.
</summary>
    <author>
      <name>Aleksandr Vorobev</name>
    </author>
    <author>
      <name>Gleb Gusev</name>
    </author>
    <link href="http://arxiv.org/abs/1507.04910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07147v1</id>
    <updated>2015-07-25T22:56:29Z</updated>
    <published>2015-07-25T22:56:29Z</published>
    <title>True Online Emphatic TD($λ$): Quick Reference and Implementation
  Guide</title>
    <summary>  This document is a guide to the implementation of true online emphatic
TD($\lambda$), a model-free temporal-difference algorithm for learning to make
long-term predictions which combines the emphasis idea (Sutton, Mahmood &amp; White
2015) and the true-online idea (van Seijen &amp; Sutton 2014). The setting used
here includes linear function approximation, the possibility of off-policy
training, and all the generality of general value functions, as well as the
emphasis algorithm's notion of "interest".
</summary>
    <author>
      <name>Richard S. Sutton</name>
    </author>
    <link href="http://arxiv.org/abs/1507.07147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07374v1</id>
    <updated>2015-07-27T11:50:44Z</updated>
    <published>2015-07-27T11:50:44Z</published>
    <title>A genetic algorithm for autonomous navigation in partially observable
  domain</title>
    <summary>  The problem of autonomous navigation is one of the basic problems for
robotics. Although, in general, it may be challenging when an autonomous
vehicle is placed into partially observable domain. In this paper we consider
simplistic environment model and introduce a navigation algorithm based on
Learning Classifier System.
</summary>
    <author>
      <name>Maxim Borisyak</name>
    </author>
    <author>
      <name>Andrey Ustyuzhanin</name>
    </author>
    <link href="http://arxiv.org/abs/1507.07374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08322v1</id>
    <updated>2015-07-29T21:15:31Z</updated>
    <published>2015-07-29T21:15:31Z</published>
    <title>Distributed Mini-Batch SDCA</title>
    <summary>  We present an improved analysis of mini-batched stochastic dual coordinate
ascent for regularized empirical loss minimization (i.e. SVM and SVM-type
objectives). Our analysis allows for flexible sampling schemes, including where
data is distribute across machines, and combines a dependence on the smoothness
of the loss and/or the data spread (measured through the spectral norm).
</summary>
    <author>
      <name>Martin Takáč</name>
    </author>
    <author>
      <name>Peter Richtárik</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04826v2</id>
    <updated>2015-08-26T12:59:38Z</updated>
    <published>2015-08-19T23:02:37Z</published>
    <title>Dither is Better than Dropout for Regularising Deep Neural Networks</title>
    <summary>  Regularisation of deep neural networks (DNN) during training is critical to
performance. By far the most popular method is known as dropout. Here, cast
through the prism of signal processing theory, we compare and contrast the
regularisation effects of dropout with those of dither. We illustrate some
serious inherent limitations of dropout and demonstrate that dither provides a
more effective regulariser.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04826v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04826v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01582v2</id>
    <updated>2016-05-21T12:33:05Z</updated>
    <published>2016-02-04T08:14:06Z</published>
    <title>SDCA without Duality, Regularization, and Individual Convexity</title>
    <summary>  Stochastic Dual Coordinate Ascent is a popular method for solving regularized
loss minimization for the case of convex losses. We describe variants of SDCA
that do not require explicit regularization and do not rely on duality. We
prove linear convergence rates even if individual loss functions are
non-convex, as long as the expected loss is strongly convex.
</summary>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01582v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01582v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02350v2</id>
    <updated>2016-05-26T07:42:58Z</updated>
    <published>2016-02-07T08:37:18Z</published>
    <title>Solving Ridge Regression using Sketched Preconditioned SVRG</title>
    <summary>  We develop a novel preconditioning method for ridge regression, based on
recent linear sketching methods. By equipping Stochastic Variance Reduced
Gradient (SVRG) with this preconditioning process, we obtain a significant
speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.
</summary>
    <author>
      <name>Alon Gonen</name>
    </author>
    <author>
      <name>Francesco Orabona</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <link href="http://arxiv.org/abs/1602.02350v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02350v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06183v1</id>
    <updated>2016-02-19T15:36:38Z</updated>
    <published>2016-02-19T15:36:38Z</published>
    <title>Node-By-Node Greedy Deep Learning for Interpretable Features</title>
    <summary>  Multilayer networks have seen a resurgence under the umbrella of deep
learning. Current deep learning algorithms train the layers of the network
sequentially, improving algorithmic performance as well as providing some
regularization. We present a new training algorithm for deep networks which
trains \emph{each node in the network} sequentially. Our algorithm is orders of
magnitude faster, creates more interpretable internal representations at the
node level, while not sacrificing on the ultimate out-of-sample performance.
</summary>
    <author>
      <name>Ke Wu</name>
    </author>
    <author>
      <name>Malik Magdon-Ismail</name>
    </author>
    <link href="http://arxiv.org/abs/1602.06183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02126v1</id>
    <updated>2016-08-06T16:30:47Z</updated>
    <published>2016-08-06T16:30:47Z</published>
    <title>How Much Did it Rain? Predicting Real Rainfall Totals Based on Radar
  Data</title>
    <summary>  We applied a variety of parametric and non-parametric machine learning models
to predict the probability distribution of rainfall based on 1M training
examples over a single year across several U.S. states. Our top performing
model based on a squared loss objective was a cross-validated parametric
k-nearest-neighbor predictor that took about six days to compute, and was
competitive in a world-wide competition.
</summary>
    <author>
      <name>Adam Lesnikowski</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06664v1</id>
    <updated>2016-08-23T22:44:42Z</updated>
    <published>2016-08-23T22:44:42Z</published>
    <title>Topic Grids for Homogeneous Data Visualization</title>
    <summary>  We propose the topic grids to detect anomaly and analyze the behavior based
on the access log content. Content-based behavioral risk is quantified in the
high dimensional space where the topics are generated from the log. The topics
are being projected homogeneously into a space that is perception- and
interaction-friendly to the human experts.
</summary>
    <author>
      <name>Shih-Chieh Su</name>
    </author>
    <author>
      <name>Joseph Vaughn</name>
    </author>
    <author>
      <name>Jean-Laurent Huynh</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06146v1</id>
    <updated>2016-09-18T01:08:20Z</updated>
    <published>2016-09-18T01:08:20Z</published>
    <title>mlr Tutorial</title>
    <summary>  This document provides and in-depth introduction to the mlr framework for
machine learning experiments in R.
</summary>
    <author>
      <name>Julia Schiffner</name>
    </author>
    <author>
      <name>Bernd Bischl</name>
    </author>
    <author>
      <name>Michel Lang</name>
    </author>
    <author>
      <name>Jakob Richter</name>
    </author>
    <author>
      <name>Zachary M. Jones</name>
    </author>
    <author>
      <name>Philipp Probst</name>
    </author>
    <author>
      <name>Florian Pfisterer</name>
    </author>
    <author>
      <name>Mason Gallo</name>
    </author>
    <author>
      <name>Dominik Kirchhoff</name>
    </author>
    <author>
      <name>Tobias Kühn</name>
    </author>
    <author>
      <name>Janek Thomas</name>
    </author>
    <author>
      <name>Lars Kotthoff</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09162v1</id>
    <updated>2016-09-29T00:43:03Z</updated>
    <published>2016-09-29T00:43:03Z</published>
    <title>Universum Learning for Multiclass SVM</title>
    <summary>  We introduce Universum learning for multiclass problems and propose a novel
formulation for multiclass universum SVM (MU-SVM). We also propose a span bound
for MU-SVM that can be used for model selection thereby avoiding resampling.
Empirical results demonstrate the effectiveness of MU-SVM and the proposed
bound.
</summary>
    <author>
      <name>Sauptik Dhar</name>
    </author>
    <author>
      <name>Naveen Ramakrishnan</name>
    </author>
    <author>
      <name>Vladimir Cherkassky</name>
    </author>
    <author>
      <name>Mohak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00862v1</id>
    <updated>2016-11-03T02:28:53Z</updated>
    <published>2016-11-03T02:28:53Z</published>
    <title>Quantile Reinforcement Learning</title>
    <summary>  In reinforcement learning, the standard criterion to evaluate policies in a
state is the expectation of (discounted) sum of rewards. However, this
criterion may not always be suitable, we consider an alternative criterion
based on the notion of quantiles. In the case of episodic reinforcement
learning problems, we propose an algorithm based on stochastic approximation
with two timescales. We evaluate our proposition on a simple model of the TV
show, Who wants to be a millionaire.
</summary>
    <author>
      <name>Hugo Gilbert</name>
    </author>
    <author>
      <name>Paul Weng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AWRL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01964v1</id>
    <updated>2016-11-07T10:10:43Z</updated>
    <published>2016-11-07T10:10:43Z</published>
    <title>Log-time and Log-space Extreme Classification</title>
    <summary>  We present LTLS, a technique for multiclass and multilabel prediction that
can perform training and inference in logarithmic time and space. LTLS embeds
large classification problems into simple structured prediction problems and
relies on efficient dynamic programming algorithms for inference. We train LTLS
with stochastic gradient descent on a number of multiclass and multilabel
datasets and show that despite its small memory footprint it is often
competitive with existing approaches.
</summary>
    <author>
      <name>Kalina Jasinska</name>
    </author>
    <author>
      <name>Nikos Karampatziakis</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.05955v1</id>
    <updated>2016-11-18T02:33:10Z</updated>
    <published>2016-11-18T02:33:10Z</published>
    <title>A Characterization of Prediction Errors</title>
    <summary>  Understanding prediction errors and determining how to fix them is critical
to building effective predictive systems. In this paper, we delineate four
types of prediction errors and demonstrate that these four types characterize
all prediction errors. In addition, we describe potential remedies and tools
that can be used to reduce the uncertainty when trying to determine the source
of a prediction error and when trying to take action to remove a prediction
errors.
</summary>
    <author>
      <name>Christopher Meek</name>
    </author>
    <link href="http://arxiv.org/abs/1611.05955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02377v1</id>
    <updated>2017-01-09T22:29:08Z</updated>
    <published>2017-01-09T22:29:08Z</published>
    <title>The principle of cognitive action - Preliminary experimental analysis</title>
    <summary>  In this document we shows a first implementation and some preliminary results
of a new theory, facing Machine Learning problems in the frameworks of
Classical Mechanics and Variational Calculus. We give a general formulation of
the problem and then we studies basic behaviors of the model on simple
practical implementations.
</summary>
    <author>
      <name>Marco Gori</name>
    </author>
    <author>
      <name>Marco Maggini</name>
    </author>
    <author>
      <name>Alessandro Rossi</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05487v1</id>
    <updated>2017-01-19T15:48:11Z</updated>
    <published>2017-01-19T15:48:11Z</published>
    <title>Learning first-order definable concepts over structures of small degree</title>
    <summary>  We consider a declarative framework for machine learning where concepts and
hypotheses are defined by formulas of a logic over some background structure.
We show that within this framework, concepts defined by first-order formulas
over a background structure of at most polylogarithmic degree can be learned in
polylogarithmic time in the "probably approximately correct" learning sense.
</summary>
    <author>
      <name>Martin Grohe</name>
    </author>
    <author>
      <name>Martin Ritzert</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08305v1</id>
    <updated>2017-01-28T17:45:58Z</updated>
    <published>2017-01-28T17:45:58Z</published>
    <title>Multiclass MinMax Rank Aggregation</title>
    <summary>  We introduce a new family of minmax rank aggregation problems under two
distance measures, the Kendall {\tau} and the Spearman footrule. As the
problems are NP-hard, we proceed to describe a number of constant-approximation
algorithms for solving them. We conclude with illustrative applications of the
aggregation methods on the Mallows model and genomic data.
</summary>
    <author>
      <name>Pan Li</name>
    </author>
    <author>
      <name>Olgica Milenkovic</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08511v1</id>
    <updated>2017-01-30T08:37:25Z</updated>
    <published>2017-01-30T08:37:25Z</published>
    <title>Binary adaptive embeddings from order statistics of random projections</title>
    <summary>  We use some of the largest order statistics of the random projections of a
reference signal to construct a binary embedding that is adapted to signals
correlated with such signal. The embedding is characterized from the analytical
standpoint and shown to provide improved performance on tasks such as
classification in a reduced-dimensionality space.
</summary>
    <author>
      <name>Diego Valsesia</name>
    </author>
    <author>
      <name>Enrico Magli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2016.2639036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2016.2639036" rel="related"/>
    <link href="http://arxiv.org/abs/1701.08511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05041v1</id>
    <updated>2017-04-17T17:32:05Z</updated>
    <published>2017-04-17T17:32:05Z</published>
    <title>Fast multi-output relevance vector regression</title>
    <summary>  This paper aims to decrease the time complexity of multi-output relevance
vector regression from O(VM^3) to O(V^3+M^3), where V is the number of output
dimensions, M is the number of basis functions, and V&lt;M. The experimental
results demonstrate that the proposed method is more competitive than the
existing method, with regard to computation time. MATLAB codes are available at
http://www.mathworks.com/matlabcentral/fileexchange/49131.
</summary>
    <author>
      <name>Youngmin Ha</name>
    </author>
    <link href="http://arxiv.org/abs/1704.05041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09564v2</id>
    <updated>2018-02-23T22:30:45Z</updated>
    <published>2017-07-29T22:36:35Z</published>
    <title>A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for
  Neural Networks</title>
    <summary>  We present a generalization bound for feedforward neural networks in terms of
the product of the spectral norm of the layers and the Frobenius norm of the
weights. The generalization bound is derived using a PAC-Bayes analysis.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Srinadh Bhojanapalli</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09564v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09564v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09718v1</id>
    <updated>2017-10-26T14:06:52Z</updated>
    <published>2017-10-26T14:06:52Z</published>
    <title>Learning Approximate Stochastic Transition Models</title>
    <summary>  We examine the problem of learning mappings from state to state, suitable for
use in a model-based reinforcement-learning setting, that simultaneously
generalize to novel states and can capture stochastic transitions. We show that
currently popular generative adversarial networks struggle to learn these
stochastic transition models but a modification to their loss functions results
in a powerful learning algorithm for this class of problems.
</summary>
    <author>
      <name>Yuhang Song</name>
    </author>
    <author>
      <name>Christopher Grimm</name>
    </author>
    <author>
      <name>Xianming Wang</name>
    </author>
    <author>
      <name>Michael L. Littman</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03130v1</id>
    <updated>2017-11-08T19:30:15Z</updated>
    <published>2017-11-08T19:30:15Z</published>
    <title>EnergyNet: Energy-based Adaptive Structural Learning of Artificial
  Neural Network Architectures</title>
    <summary>  We present E NERGY N ET , a new framework for analyzing and building
artificial neural network architectures. Our approach adaptively learns the
structure of the networks in an unsupervised manner. The methodology is based
upon the theoretical guarantees of the energy function of restricted Boltzmann
machines (RBM) of infinite number of nodes. We present experimental results to
show that the final network adapts to the complexity of a given problem.
</summary>
    <author>
      <name>Gus Kristiansen</name>
    </author>
    <author>
      <name>Xavi Gonzalvo</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11139v2</id>
    <updated>2018-08-23T14:27:53Z</updated>
    <published>2017-11-29T22:43:20Z</published>
    <title>Easy High-Dimensional Likelihood-Free Inference</title>
    <summary>  We introduce a framework using Generative Adversarial Networks (GANs) for
likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)
where we replace the black-box simulator model with an approximator network and
generate a rich set of summary features in a data driven fashion. On benchmark
data sets, our approach improves on others with respect to scalability, ability
to handle high dimensional data and complex probability distributions.
</summary>
    <author>
      <name>Vinay Jethava</name>
    </author>
    <author>
      <name>Devdatt Dubhashi</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00423v1</id>
    <updated>2018-08-01T17:04:48Z</updated>
    <published>2018-08-01T17:04:48Z</published>
    <title>Seq2Seq and Multi-Task Learning for joint intent and content extraction
  for domain specific interpreters</title>
    <summary>  This study evaluates the performances of an LSTM network for detecting and
extracting the intent and content of com- mands for a financial chatbot. It
presents two techniques, sequence to sequence learning and Multi-Task Learning,
which might improve on the previous task.
</summary>
    <author>
      <name>Marc Velay</name>
    </author>
    <author>
      <name>Fabrice Daniel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01048v1</id>
    <updated>2018-08-02T23:30:50Z</updated>
    <published>2018-08-02T23:30:50Z</published>
    <title>Variational Information Bottleneck on Vector Quantized Autoencoders</title>
    <summary>  In this paper, we provide an information-theoretic interpretation of the
Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss
function of the original VQ-VAE can be derived from the variational
deterministic information bottleneck (VDIB) principle. On the other hand, the
VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as
an approximation to the variational information bottleneck(VIB) principle.
</summary>
    <author>
      <name>Hanwei Wu</name>
    </author>
    <author>
      <name>Markus Flierl</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08111v1</id>
    <updated>2018-08-23T06:51:09Z</updated>
    <published>2018-08-23T06:51:09Z</published>
    <title>Multiclass Universum SVM</title>
    <summary>  We introduce Universum learning for multiclass problems and propose a novel
formulation for multiclass universum SVM (MU-SVM). We also propose an analytic
span bound for model selection with almost 2-4x faster computation times than
standard resampling techniques. We empirically demonstrate the efficacy of the
proposed MUSVM formulation on several real world datasets achieving &gt; 20%
improvement in test accuracies compared to multi-class SVM.
</summary>
    <author>
      <name>Sauptik Dhar</name>
    </author>
    <author>
      <name>Vladimir Cherkassky</name>
    </author>
    <author>
      <name>Mohak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages. arXiv admin note: text overlap with arXiv:1609.09162</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08271v2</id>
    <updated>2020-09-06T09:54:34Z</updated>
    <published>2018-08-17T02:33:55Z</published>
    <title>An elementary introduction to information geometry</title>
    <summary>  In this survey, we describe the fundamental differential-geometric structures
of information manifolds, state the fundamental theorem of information
geometry, and illustrate some use cases of these information manifolds in
information sciences. The exposition is self-contained by concisely introducing
the necessary concepts of differential geometry, but proofs are omitted for
brevity.
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e22101100</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e22101100" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages, 16 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Entropy 2020, 22(10), 1100</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08763v2</id>
    <updated>2018-08-30T00:51:38Z</updated>
    <published>2018-08-27T09:51:03Z</published>
    <title>On the convergence of optimistic policy iteration for stochastic
  shortest path problem</title>
    <summary>  In this paper, we prove some convergence results of a special case of
optimistic policy iteration algorithm for stochastic shortest path problem. We
consider both Monte Carlo and $TD(\lambda)$ methods for the policy evaluation
step under the condition that the termination state will eventually be reached
almost surely.
</summary>
    <author>
      <name>Yuanlong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.03052v1</id>
    <updated>2018-10-06T20:58:05Z</updated>
    <published>2018-10-06T20:58:05Z</published>
    <title>Deep convolutional Gaussian processes</title>
    <summary>  We propose deep convolutional Gaussian processes, a deep Gaussian process
architecture with convolutional structure. The model is a principled Bayesian
framework for detecting hierarchical combinations of local features for image
classification. We demonstrate greatly improved image classification
performance compared to current Gaussian process approaches on the MNIST and
CIFAR-10 datasets. In particular, we improve CIFAR-10 accuracy by over 10
percentage points.
</summary>
    <author>
      <name>Kenneth Blomqvist</name>
    </author>
    <author>
      <name>Samuel Kaski</name>
    </author>
    <author>
      <name>Markus Heinonen</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.07793v1</id>
    <updated>2018-10-17T20:58:32Z</updated>
    <published>2018-10-17T20:58:32Z</published>
    <title>The Wasserstein transform</title>
    <summary>  We introduce the Wasserstein transform, a method for enhancing and denoising
datasets defined on general metric spaces. The construction draws inspiration
from Optimal Transportation ideas. We establish precise connections with the
mean shift family of algorithms and establish the stability of both our method
and mean shift under data perturbation.
</summary>
    <author>
      <name>Facundo Mémoli</name>
    </author>
    <author>
      <name>Zane Smith</name>
    </author>
    <author>
      <name>Zhengchao Wan</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.10380v1</id>
    <updated>2018-10-24T13:09:03Z</updated>
    <published>2018-10-24T13:09:03Z</published>
    <title>Why every GBDT speed benchmark is wrong</title>
    <summary>  This article provides a comprehensive study of different ways to make speed
benchmarks of gradient boosted decision trees algorithm. We show main problems
of several straight forward ways to make benchmarks, explain, why a speed
benchmarking is a challenging task and provide a set of reasonable requirements
for a benchmark to be fair and useful.
</summary>
    <author>
      <name>Anna Veronika Dorogush</name>
    </author>
    <author>
      <name>Vasily Ershov</name>
    </author>
    <author>
      <name>Dmitriy Kruchinin</name>
    </author>
    <link href="http://arxiv.org/abs/1810.10380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.10770v1</id>
    <updated>2018-10-25T08:41:24Z</updated>
    <published>2018-10-25T08:41:24Z</published>
    <title>Geometry and clustering with metrics derived from separable Bregman
  divergences</title>
    <summary>  Separable Bregman divergences induce Riemannian metric spaces that are
isometric to the Euclidean space after monotone embeddings. We investigate
fixed rate quantization and its codebook Voronoi diagrams, and report on
experimental performances of partition-based, hierarchical, and soft clustering
algorithms with respect to these Riemann-Bregman distances.
</summary>
    <author>
      <name>Erika Gomes-Gonçalves</name>
    </author>
    <author>
      <name>Henryk Gzyl</name>
    </author>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.10770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.11383v2</id>
    <updated>2018-11-08T01:03:50Z</updated>
    <published>2018-10-25T02:53:14Z</published>
    <title>Some Requests for Machine Learning Research from the East African Tech
  Scene</title>
    <summary>  Based on 46 in-depth interviews with scientists, engineers, and CEOs, this
document presents a list of concrete machine research problems, progress on
which would directly benefit tech ventures in East Africa.
</summary>
    <author>
      <name>Milan Cvitkovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NIPS 2018 Workshop on Machine Learning for the
  Developing World</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.11383v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11383v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12794v2</id>
    <updated>2018-11-01T13:18:18Z</updated>
    <published>2018-10-30T17:02:55Z</published>
    <title>Divergence Network: Graphical calculation method of divergence functions</title>
    <summary>  In this paper, we introduce directed networks called `divergence network' in
order to perform graphical calculation of divergence functions. By using the
divergence networks, we can easily understand the geometric meaning of
calculation results and grasp relations among divergence functions intuitively.
</summary>
    <author>
      <name>Tomohiro Nishiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12794v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12794v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00668v1</id>
    <updated>2019-09-02T11:11:35Z</updated>
    <published>2019-09-02T11:11:35Z</published>
    <title>Logic and the $2$-Simplicial Transformer</title>
    <summary>  We introduce the $2$-simplicial Transformer, an extension of the Transformer
which includes a form of higher-dimensional attention generalising the
dot-product attention, and uses this attention to update entity representations
with tensor products of value vectors. We show that this architecture is a
useful inductive bias for logical reasoning in the context of deep
reinforcement learning.
</summary>
    <author>
      <name>James Clift</name>
    </author>
    <author>
      <name>Dmitry Doryn</name>
    </author>
    <author>
      <name>Daniel Murfet</name>
    </author>
    <author>
      <name>James Wallbridge</name>
    </author>
    <link href="http://arxiv.org/abs/1909.00668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04474v1</id>
    <updated>2019-09-10T13:38:56Z</updated>
    <published>2019-09-10T13:38:56Z</published>
    <title>Dropout Induced Noise for Co-Creative GAN Systems</title>
    <summary>  This paper demonstrates how Dropout can be used in Generative Adversarial
Networks to generate multiple different outputs to one input. This method is
thought as an alternative to latent space exploration, especially if
constraints in the input should be preserved, like in A-to-B translation tasks.
</summary>
    <author>
      <name>Sabine Wieluch</name>
    </author>
    <author>
      <name>Friedhelm Schwenker</name>
    </author>
    <link href="http://arxiv.org/abs/1909.04474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05623v1</id>
    <updated>2019-09-12T13:22:25Z</updated>
    <published>2019-09-12T13:22:25Z</published>
    <title>A Channel-Pruned and Weight-Binarized Convolutional Neural Network for
  Keyword Spotting</title>
    <summary>  We study channel number reduction in combination with weight binarization
(1-bit weight precision) to trim a convolutional neural network for a keyword
spotting (classification) task. We adopt a group-wise splitting method based on
the group Lasso penalty to achieve over 50% channel sparsity while maintaining
the network performance within 0.25% accuracy loss. We show an effective
three-stage procedure to balance accuracy and sparsity in network training.
</summary>
    <author>
      <name>Jiancheng Lyu</name>
    </author>
    <author>
      <name>Spencer Sheen</name>
    </author>
    <link href="http://arxiv.org/abs/1909.05623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09969v1</id>
    <updated>2019-09-22T09:07:21Z</updated>
    <published>2019-09-22T09:07:21Z</published>
    <title>Classification in asymmetric spaces via sample compression</title>
    <summary>  We initiate the rigorous study of classification in quasi-metric spaces.
These are point sets endowed with a distance function that is non-negative and
also satisfies the triangle inequality, but is asymmetric. We develop and
refine a learning algorithm for quasi-metrics based on sample compression and
nearest neighbor, and prove that it has favorable statistical properties.
</summary>
    <author>
      <name>Lee-Ad Gottlieb</name>
    </author>
    <author>
      <name>Shira Ozeri</name>
    </author>
    <link href="http://arxiv.org/abs/1909.09969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04916v1</id>
    <updated>2020-06-08T20:21:34Z</updated>
    <published>2020-06-08T20:21:34Z</published>
    <title>An Algorithmic Introduction to Clustering</title>
    <summary>  This paper tries to present a more unified view of clustering, by identifying
the relationships between five different clustering algorithms. Some of the
results are not new, but they are presented in a cleaner, simpler and more
concise way. To the best of my knowledge, the interpretation of DBSCAN as a
climbing procedure, which introduces a theoretical connection between DBSCAN
and Mean shift, is a novel result.
</summary>
    <author>
      <name>Bernardo A. Gonzalez-Torres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.04916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.06040v2</id>
    <updated>2020-12-03T23:22:14Z</updated>
    <published>2020-06-10T19:38:01Z</published>
    <title>Efficient Contextual Bandits with Continuous Actions</title>
    <summary>  We create a computationally tractable algorithm for contextual bandits with
continuous actions having unknown structure. Our reduction-style algorithm
composes with most supervised learning representations. We prove that it works
in a general sense and verify the new functionality with large-scale
experiments.
</summary>
    <author>
      <name>Maryam Majzoubi</name>
    </author>
    <author>
      <name>Chicheng Zhang</name>
    </author>
    <author>
      <name>Rajan Chari</name>
    </author>
    <author>
      <name>Akshay Krishnamurthy</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Aleksandrs Slivkins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.06040v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.06040v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07237v1</id>
    <updated>2020-06-12T14:40:46Z</updated>
    <published>2020-06-12T14:40:46Z</published>
    <title>Power Consumption Variation over Activation Functions</title>
    <summary>  The power that machine learning models consume when making predictions can be
affected by a model's architecture. This paper presents various estimates of
power consumption for a range of different activation functions, a core factor
in neural network model architecture design. Substantial differences in
hardware performance exist between activation functions. This difference
informs how power consumption in machine learning models can be reduced.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08768v1</id>
    <updated>2020-06-12T16:39:54Z</updated>
    <published>2020-06-12T16:39:54Z</published>
    <title>Similarity-based transfer learning of decision policies</title>
    <summary>  A problem of learning decision policy from past experience is considered.
Using the Fully Probabilistic Design (FPD) formalism, we propose a new general
approach for finding a stochastic policy from the past data.
</summary>
    <author>
      <name>Eliška Zugarová</name>
    </author>
    <author>
      <name>Tatiana V. Guy</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09046v1</id>
    <updated>2020-06-16T10:07:50Z</updated>
    <published>2020-06-16T10:07:50Z</published>
    <title>Probabilistic Decoupling of Labels in Classification</title>
    <summary>  In this paper we develop a principled, probabilistic, unified approach to
non-standard classification tasks, such as semi-supervised,
positive-unlabelled, multi-positive-unlabelled and noisy-label learning. We
train a classifier on the given labels to predict the label-distribution. We
then infer the underlying class-distributions by variationally optimizing a
model of label-class transitions.
</summary>
    <author>
      <name>Jeppe Nørregaard</name>
    </author>
    <author>
      <name>Lars Kai Hansen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICML 2020 (not accepted)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.09046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10940v1</id>
    <updated>2020-06-19T03:00:01Z</updated>
    <published>2020-06-19T03:00:01Z</published>
    <title>Open Problem: Model Selection for Contextual Bandits</title>
    <summary>  In statistical learning, algorithms for model selection allow the learner to
adapt to the complexity of the best hypothesis class in a sequence. We ask
whether similar guarantees are possible for contextual bandit learning.
</summary>
    <author>
      <name>Dylan J. Foster</name>
    </author>
    <author>
      <name>Akshay Krishnamurthy</name>
    </author>
    <author>
      <name>Haipeng Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COLT 2020 open problem</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.10940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.13716v1</id>
    <updated>2020-06-23T01:42:47Z</updated>
    <published>2020-06-23T01:42:47Z</published>
    <title>Embedding Differentiable Sparsity into Deep Neural Network</title>
    <summary>  In this paper, we propose embedding sparsity into the structure of deep
neural networks, where model parameters can be exactly zero during training
with the stochastic gradient descent. Thus, it can learn the sparsified
structure and the weights of networks simultaneously. The proposed approach can
learn structured as well as unstructured sparsity.
</summary>
    <author>
      <name>Yongjin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1910.03201</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.13716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.16840v1</id>
    <updated>2020-06-30T14:31:24Z</updated>
    <published>2020-06-30T14:31:24Z</published>
    <title>Guided Learning of Nonconvex Models through Successive Functional
  Gradient Optimization</title>
    <summary>  This paper presents a framework of successive functional gradient
optimization for training nonconvex models such as neural networks, where
training is driven by mirror descent in a function space. We provide a
theoretical analysis and empirical study of the training method derived from
this framework. It is shown that the method leads to better performance than
that of standard training techniques.
</summary>
    <author>
      <name>Rie Johnson</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.16840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08947v2</id>
    <updated>2017-07-06T17:10:40Z</updated>
    <published>2017-06-27T17:20:06Z</published>
    <title>Exploring Generalization in Deep Learning</title>
    <summary>  With a goal of understanding what drives generalization in deep networks, we
consider several recently suggested explanations, including norm-based control,
sharpness and robustness. We study how these measures can ensure
generalization, highlighting the importance of scale normalization, and making
a connection between sharpness and PAC-Bayes theory. We then investigate how
well the measures explain different observed phenomena.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Srinadh Bhojanapalli</name>
    </author>
    <author>
      <name>David McAllester</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08947v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08947v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01509v1</id>
    <updated>2017-09-05T17:55:59Z</updated>
    <published>2017-09-05T17:55:59Z</published>
    <title>Linking Generative Adversarial Learning and Binary Classification</title>
    <summary>  In this note, we point out a basic link between generative adversarial (GA)
training and binary classification -- any powerful discriminator essentially
computes an (f-)divergence between real and generated samples. The result,
repeatedly re-derived in decision theory, has implications for GA Networks
(GANs), providing an alternative perspective on training f-GANs by designing
the discriminator loss function.
</summary>
    <author>
      <name>Akshay Balsubramani</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00069v1</id>
    <updated>2019-01-01T00:52:01Z</updated>
    <published>2019-01-01T00:52:01Z</published>
    <title>Recurrent Neural Networks for Time Series Forecasting</title>
    <summary>  Time series forecasting is difficult. It is difficult even for recurrent
neural networks with their inherent ability to learn sequentiality. This
article presents a recurrent neural network based time series forecasting
framework covering feature engineering, feature importances, point and interval
predictions, and forecast evaluation. The description of the method is followed
by an empirical study using both LSTM and GRU networks.
</summary>
    <author>
      <name>Gábor Petneházi</name>
    </author>
    <link href="http://arxiv.org/abs/1901.00069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05590v1</id>
    <updated>2019-01-17T02:21:24Z</updated>
    <published>2019-01-17T02:21:24Z</published>
    <title>Disentangling Video with Independent Prediction</title>
    <summary>  We propose an unsupervised variational model for disentangling video into
independent factors, i.e. each factor's future can be predicted from its past
without considering the others. We show that our approach often learns factors
which are interpretable as objects in a scene.
</summary>
    <author>
      <name>William F. Whitney</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Learning Disentangled Representations: from
  Perception to Control workshop at NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.05590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.07417v2</id>
    <updated>2019-05-14T14:50:44Z</updated>
    <published>2019-01-22T15:34:54Z</published>
    <title>On Connected Sublevel Sets in Deep Learning</title>
    <summary>  This paper shows that every sublevel set of the loss function of a class of
deep over-parameterized neural nets with piecewise linear activation functions
is connected and unbounded. This implies that the loss has no bad local valleys
and all of its global minima are connected within a unique and potentially very
large global valley.
</summary>
    <author>
      <name>Quynh Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICML 2019. More discussions and visualizations are added</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.07417v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07417v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09531v2</id>
    <updated>2019-07-10T17:57:00Z</updated>
    <published>2019-01-28T07:06:12Z</published>
    <title>Secure multi-party linear regression at plaintext speed</title>
    <summary>  We detail distributed algorithms for scalable, secure multiparty linear
regression and feature selection at essentially the same speed as plaintext
regression. While the core geometric ideas are simple, the recognition of their
broad utility when combined is novel. Our scheme opens the door to efficient
and secure genome-wide association studies across multiple biobanks.
</summary>
    <author>
      <name>Jonathan M. Bloom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.09531v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09531v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.10267v1</id>
    <updated>2019-01-29T13:19:47Z</updated>
    <published>2019-01-29T13:19:47Z</published>
    <title>Approximation of functions by neural networks</title>
    <summary>  We study the approximation of measurable functions on the hypercube by
functions arising from affine neural networks. Our main achievement is an
approximation of any measurable function $f \colon W_n \to [-1,1]$ up to a
prescribed precision $\varepsilon&gt;0$ by a bounded number of neurons, depending
only on $\varepsilon$ and not on the function $f$ or $n \in \mathbb N$.
</summary>
    <author>
      <name>Andreas Thom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06468v1</id>
    <updated>2019-11-15T04:04:55Z</updated>
    <published>2019-11-15T04:04:55Z</published>
    <title>$\ell_{\infty}$ Vector Contraction for Rademacher Complexity</title>
    <summary>  We show that the Rademacher complexity of any $\mathbb{R}^{K}$-valued
function class composed with an $\ell_{\infty}$-Lipschitz function is bounded
by the maximum Rademacher complexity of the restriction of the function class
along each coordinate, times a factor of $\tilde{O}(\sqrt{K})$.
</summary>
    <author>
      <name>Dylan J. Foster</name>
    </author>
    <author>
      <name>Alexander Rakhlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical note</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.06468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06612v1</id>
    <updated>2019-11-12T10:49:55Z</updated>
    <published>2019-11-12T10:49:55Z</published>
    <title>Position Paper: Towards Transparent Machine Learning</title>
    <summary>  Transparent machine learning is introduced as an alternative form of machine
learning, where both the model and the learning system are represented in
source code form. The goal of this project is to enable direct human
understanding of machine learning models, giving us the ability to learn,
verify, and refine them as programs. If solved, this technology could represent
a best-case scenario for the safety and security of AI systems going forward.
</summary>
    <author>
      <name>Dustin Juliano</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07891v2</id>
    <updated>2019-12-10T18:28:58Z</updated>
    <published>2019-11-18T19:32:04Z</published>
    <title>Basic Principles of Clustering Methods</title>
    <summary>  Clustering methods group a set of data points into a few coherent groups or
clusters of similar data points. As an example, consider clustering pixels in
an image (or video) if they belong to the same object. Different clustering
methods are obtained by using different notions of similarity and different
representations of data points.
</summary>
    <author>
      <name>Alexander Jung</name>
    </author>
    <author>
      <name>Ivan Baranov</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07891v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07891v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07910v1</id>
    <updated>2019-11-18T20:15:15Z</updated>
    <published>2019-11-18T20:15:15Z</published>
    <title>Comments on the Du-Kakade-Wang-Yang Lower Bounds</title>
    <summary>  Du, Kakade, Wang, and Yang recently established intriguing lower bounds on
sample complexity, which suggest that reinforcement learning with a
misspecified representation is intractable. Another line of work, which centers
around a statistic called the eluder dimension, establishes tractability of
problems similar to those considered in the Du-Kakade-Wang-Yang paper. We
compare these results and reconcile interpretations.
</summary>
    <author>
      <name>Benjamin Van Roy</name>
    </author>
    <author>
      <name>Shi Dong</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08024v1</id>
    <updated>2019-11-19T00:59:54Z</updated>
    <published>2019-11-19T00:59:54Z</published>
    <title>A Bias Trick for Centered Robust Principal Component Analysis</title>
    <summary>  Outlier based Robust Principal Component Analysis (RPCA) requires centering
of the non-outliers. We show a "bias trick" that automatically centers these
non-outliers. Using this bias trick we obtain the first RPCA algorithm that is
optimal with respect to centering.
</summary>
    <author>
      <name>Baokun He</name>
    </author>
    <author>
      <name>Guihong Wan</name>
    </author>
    <author>
      <name>Haim Schweitzer</name>
    </author>
    <link href="http://arxiv.org/abs/1911.08024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10875v1</id>
    <updated>2019-11-25T12:48:08Z</updated>
    <published>2019-11-25T12:48:08Z</published>
    <title>Adversarial Attack with Pattern Replacement</title>
    <summary>  We propose a generative model for adversarial attack. The model generates
subtle but predictive patterns from the input. To perform an attack, it
replaces the patterns of the input with those generated based on examples from
some other class. We demonstrate our model by attacking CNN on MNIST.
</summary>
    <author>
      <name>Ziang Dong</name>
    </author>
    <author>
      <name>Liang Mao</name>
    </author>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11444v2</id>
    <updated>2019-11-27T11:16:25Z</updated>
    <published>2019-11-26T10:40:30Z</published>
    <title>Control-Tutored Reinforcement Learning: an application to the Herding
  Problem</title>
    <summary>  In this extended abstract we introduce a novel control-tutored Q-learning
approach (CTQL) as part of the ongoing effort in developing model-based and
safe RL for continuous state spaces. We validate our approach by applying it to
a challenging multi-agent herding control problem.
</summary>
    <author>
      <name>Francesco De Lellis</name>
    </author>
    <author>
      <name>Fabrizia Auletta</name>
    </author>
    <author>
      <name>Giovanni Russo</name>
    </author>
    <author>
      <name>Mario di Bernardo</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11444v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11444v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12463v2</id>
    <updated>2019-12-11T23:03:00Z</updated>
    <published>2019-11-27T23:38:17Z</published>
    <title>Information-Geometric Set Embeddings (IGSE): From Sets to Probability
  Distributions</title>
    <summary>  This letter introduces an abstract learning problem called the "set
embedding": The objective is to map sets into probability distributions so as
to lose less information. We relate set union and intersection operations with
corresponding interpolations of probability distributions. We also demonstrate
a preliminary solution with experimental results on toy set embedding examples.
</summary>
    <author>
      <name>Ke Sun</name>
    </author>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at Sets &amp; Partitions (NeurIPS 2019 workshop)</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.12463v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12463v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5341v2</id>
    <updated>2015-06-08T19:05:44Z</updated>
    <published>2014-03-21T01:42:53Z</published>
    <title>An Information-Theoretic Analysis of Thompson Sampling</title>
    <summary>  We provide an information-theoretic analysis of Thompson sampling that
applies across a broad range of online optimization problems in which a
decision-maker must learn from partial feedback. This analysis inherits the
simplicity and elegance of information theory and leads to regret bounds that
scale with the entropy of the optimal-action distribution. This strengthens
preexisting results and yields new insight into how information improves
performance.
</summary>
    <author>
      <name>Daniel Russo</name>
    </author>
    <author>
      <name>Benjamin Van Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1403.5341v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5341v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7746v1</id>
    <updated>2014-03-30T12:22:36Z</updated>
    <published>2014-03-30T12:22:36Z</published>
    <title>Multi-label Ferns for Efficient Recognition of Musical Instruments in
  Recordings</title>
    <summary>  In this paper we introduce multi-label ferns, and apply this technique for
automatic classification of musical instruments in audio recordings. We compare
the performance of our proposed method to a set of binary random ferns, using
jazz recordings as input data. Our main result is obtaining much faster
classification and higher F-score. We also achieve substantial reduction of the
model size.
</summary>
    <author>
      <name>Miron B. Kursa</name>
    </author>
    <author>
      <name>Alicja A. Wieczorkowska</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2635v1</id>
    <updated>2014-11-10T21:41:32Z</updated>
    <published>2014-11-10T21:41:32Z</published>
    <title>A chain rule for the expected suprema of Gaussian processes</title>
    <summary>  The expected supremum of a Gaussian process indexed by the image of an index
set under a function class is bounded in terms of separate properties of the
index set and the function class. The bound is relevant to the estimation of
nonlinear transformations or the analysis of learning algorithms whenever
hypotheses are chosen from composite classes, as is the case for multi-layer
models.
</summary>
    <author>
      <name>Andreas Maurer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-11662-4_18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-11662-4_18" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science Volume 8776, 2014, pp 245-259</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.2635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2919v1</id>
    <updated>2014-11-11T18:55:35Z</updated>
    <published>2014-11-11T18:55:35Z</published>
    <title>Bounded Regret for Finite-Armed Structured Bandits</title>
    <summary>  We study a new type of K-armed bandit problem where the expected return of
one arm may depend on the returns of other arms. We present a new algorithm for
this general class of problems and show that under certain circumstances it is
possible to achieve finite expected cumulative regret. We also give
problem-dependent lower bounds on the cumulative regret showing that at least
in special cases the new algorithm is nearly optimal.
</summary>
    <author>
      <name>Tor Lattimore</name>
    </author>
    <author>
      <name>Remi Munos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3276v1</id>
    <updated>2014-12-10T12:28:34Z</updated>
    <published>2014-12-10T12:28:34Z</published>
    <title>Generalised Entropy MDPs and Minimax Regret</title>
    <summary>  Bayesian methods suffer from the problem of how to specify prior beliefs. One
interesting idea is to consider worst-case priors. This requires solving a
stochastic zero-sum game. In this paper, we extend well-known results from
bandit theory in order to discover minimax-Bayes policies and discuss when they
are practical.
</summary>
    <author>
      <name>Emmanouil G. Androulakis</name>
    </author>
    <author>
      <name>Christos Dimitrakakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, NIPS workshop "From bad models to good policies"</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04137v1</id>
    <updated>2015-02-13T21:32:12Z</updated>
    <published>2015-02-13T21:32:12Z</published>
    <title>Non-Adaptive Learning a Hidden Hipergraph</title>
    <summary>  We give a new deterministic algorithm that non-adaptively learns a hidden
hypergraph from edge-detecting queries. All previous non-adaptive algorithms
either run in exponential time or have non-optimal query complexity. We give
the first polynomial time non-adaptive learning algorithm for learning
hypergraph that asks almost optimal number of queries.
</summary>
    <author>
      <name>Hasan Abasi</name>
    </author>
    <author>
      <name>Nader H. Bshouty</name>
    </author>
    <author>
      <name>Hanna Mazzawi</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05675v2</id>
    <updated>2015-02-20T13:00:17Z</updated>
    <published>2015-02-19T19:30:46Z</published>
    <title>NP-Hardness and Inapproximability of Sparse PCA</title>
    <summary>  We give a reduction from {\sc clique} to establish that sparse PCA is
NP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse
PCA (unless P=NP). Under weaker complexity assumptions, we also exclude
polynomial constant-factor approximation algorithms.
</summary>
    <author>
      <name>Malik Magdon-Ismail</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06177v1</id>
    <updated>2015-02-22T04:42:01Z</updated>
    <published>2015-02-22T04:42:01Z</published>
    <title>SDCA without Duality</title>
    <summary>  Stochastic Dual Coordinate Ascent is a popular method for solving regularized
loss minimization for the case of convex losses. In this paper we show how a
variant of SDCA can be applied for non-convex losses. We prove linear
convergence rate even if individual loss functions are non-convex as long as
the expected loss is convex.
</summary>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <link href="http://arxiv.org/abs/1502.06177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07073v3</id>
    <updated>2015-06-19T07:31:45Z</updated>
    <published>2015-02-25T07:24:40Z</published>
    <title>Strongly Adaptive Online Learning</title>
    <summary>  Strongly adaptive algorithms are algorithms whose performance on every time
interval is close to optimal. We present a reduction that can transform
standard low-regret algorithms to strongly adaptive. As a consequence, we
derive simple, yet efficient, strongly adaptive algorithms for a handful of
problems.
</summary>
    <author>
      <name>Amit Daniely</name>
    </author>
    <author>
      <name>Alon Gonen</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <link href="http://arxiv.org/abs/1502.07073v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07073v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02510v1</id>
    <updated>2015-06-08T14:00:32Z</updated>
    <published>2015-06-08T14:00:32Z</published>
    <title>Learning Mixtures of Ising Models using Pseudolikelihood</title>
    <summary>  Maximum pseudolikelihood method has been among the most important methods for
learning parameters of statistical physics models, such as Ising models. In
this paper, we study how pseudolikelihood can be derived for learning
parameters of a mixture of Ising models. The performance of the proposed
approach is demonstrated for Ising and Potts models on both synthetic and real
data.
</summary>
    <author>
      <name>Onur Dikmen</name>
    </author>
    <link href="http://arxiv.org/abs/1506.02510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04217v2</id>
    <updated>2015-06-22T03:28:45Z</updated>
    <published>2015-06-13T03:52:44Z</published>
    <title>On the Equivalence of CoCoA+ and DisDCA</title>
    <summary>  In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015)
under the setting used in their experiments, which is also the best setting
suggested by the authors that proposed this algorithm, is equivalent to the
practical variant of DisDCA (Yang, NIPS, 2013).
</summary>
    <author>
      <name>Ching-pei Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is withdrawn by the author because this is actually a
  known fact</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05790v2</id>
    <updated>2015-11-11T01:06:07Z</updated>
    <published>2015-06-18T19:53:12Z</published>
    <title>Scalable Semi-Supervised Aggregation of Classifiers</title>
    <summary>  We present and empirically evaluate an efficient algorithm that learns to
aggregate the predictions of an ensemble of binary classifiers. The algorithm
uses the structure of the ensemble predictions on unlabeled data to yield
significant performance improvements. It does this without making assumptions
on the structure or origin of the ensemble, without parameters, and as scalably
as linear learning. We empirically demonstrate these performance gains with
random forests.
</summary>
    <author>
      <name>Akshay Balsubramani</name>
    </author>
    <author>
      <name>Yoav Freund</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05790v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05790v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06573v1</id>
    <updated>2015-06-22T12:47:07Z</updated>
    <published>2015-06-22T12:47:07Z</published>
    <title>PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures</title>
    <summary>  We give tight concentration bounds for mixtures of martingales that are
simultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;
and (b) all finite times. These bounds are proved in terms of the martingale
variance, extending classical Bernstein inequalities, and sharpening and
simplifying prior work.
</summary>
    <author>
      <name>Akshay Balsubramani</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01926v1</id>
    <updated>2015-12-07T06:37:49Z</updated>
    <published>2015-12-07T06:37:49Z</published>
    <title>Thinking Required</title>
    <summary>  There exists a theory of a single general-purpose learning algorithm which
could explain the principles its operation. It assumes the initial rough
architecture, a small library of simple innate circuits which are prewired at
birth. and proposes that all significant mental algorithms are learned. Given
current understanding and observations, this paper reviews and lists the
ingredients of such an algorithm from architectural and functional
perspectives.
</summary>
    <author>
      <name>Kamil Rocki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03375v1</id>
    <updated>2015-12-10T19:32:48Z</updated>
    <published>2015-12-10T19:32:48Z</published>
    <title>Convolutional Monte Carlo Rollouts in Go</title>
    <summary>  In this work, we present a MCTS-based Go-playing program which uses
convolutional networks in all parts. Our method performs MCTS in batches,
explores the Monte Carlo search tree using Thompson sampling and a
convolutional network, and evaluates convnet-based rollouts on the GPU. We
achieve strong win rates against open source Go programs and attain competitive
results against state of the art convolutional net-based Go-playing programs.
</summary>
    <author>
      <name>Peter H. Jin</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <link href="http://arxiv.org/abs/1512.03375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06927v4</id>
    <updated>2016-04-12T17:34:29Z</updated>
    <published>2015-12-22T01:27:23Z</published>
    <title>A C++ library for Multimodal Deep Learning</title>
    <summary>  MDL, Multimodal Deep Learning Library, is a deep learning framework that
supports multiple models, and this document explains its philosophy and
functionality. MDL runs on Linux, Mac, and Unix platforms. It depends on
OpenCV.
</summary>
    <author>
      <name>Jian Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.06927v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06927v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00772v2</id>
    <updated>2023-03-10T09:45:23Z</updated>
    <published>2016-04-04T08:16:12Z</published>
    <title>The CMA Evolution Strategy: A Tutorial</title>
    <summary>  This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands
for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized,
method for real-parameter (continuous domain) optimization of non-linear,
non-convex functions. We try to motivate and derive the algorithm from
intuitive concepts and from requirements of non-linear, non-convex search in
continuous domain.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ArXiv e-prints, arXiv:1604.00772, 2016, pp.1-39</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00772v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00772v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07078v1</id>
    <updated>2016-04-24T20:32:18Z</updated>
    <published>2016-04-24T20:32:18Z</published>
    <title>Unsupervised Representation Learning of Structured Radio Communication
  Signals</title>
    <summary>  We explore unsupervised representation learning of radio communication
signals in raw sampled time series representation. We demonstrate that we can
learn modulation basis functions using convolutional autoencoders and visually
recognize their relationship to the analytic bases used in digital
communications. We also propose and evaluate quantitative met- rics for quality
of encoding using domain relevant performance metrics.
</summary>
    <author>
      <name>Timothy J. O'Shea</name>
    </author>
    <author>
      <name>Johnathan Corgan</name>
    </author>
    <author>
      <name>T. Charles Clancy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 9 figures, currently under conference submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00398v1</id>
    <updated>2016-06-01T18:57:54Z</updated>
    <published>2016-06-01T18:57:54Z</published>
    <title>Short Communication on QUIST: A Quick Clustering Algorithm</title>
    <summary>  In this short communication we introduce the quick clustering algorithm
(QUIST), an efficient hierarchical clustering algorithm based on sorting. QUIST
is a poly-logarithmic divisive clustering algorithm that does not assume the
number of clusters, and/or the cluster size to be known ahead of time. It is
also insensitive to the original ordering of the input.
</summary>
    <author>
      <name>Sherenaz W. Al-Haj Baddar</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01540v1</id>
    <updated>2016-06-05T17:54:48Z</updated>
    <published>2016-06-05T17:54:48Z</published>
    <title>OpenAI Gym</title>
    <summary>  OpenAI Gym is a toolkit for reinforcement learning research. It includes a
growing collection of benchmark problems that expose a common interface, and a
website where people can share their results and compare the performance of
algorithms. This whitepaper discusses the components of OpenAI Gym and the
design decisions that went into the software.
</summary>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Vicki Cheung</name>
    </author>
    <author>
      <name>Ludwig Pettersson</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05664v1</id>
    <updated>2016-05-31T09:54:46Z</updated>
    <published>2016-05-31T09:54:46Z</published>
    <title>Linear Classification of data with Support Vector Machines and
  Generalized Support Vector Machines</title>
    <summary>  In this paper, we study the support vector machine and introduced the notion
of generalized support vector machine for classification of data. We show that
the problem of generalized support vector machine is equivalent to the problem
of generalized variational inequality and establish various results for the
existence of solutions. Moreover, we provide various examples to support our
results.
</summary>
    <author>
      <name>Xiaomin Qi</name>
    </author>
    <author>
      <name>Sergei Silvestrov</name>
    </author>
    <author>
      <name>Talat Nazir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4972718</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4972718" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09122v1</id>
    <updated>2016-12-29T12:29:20Z</updated>
    <published>2016-12-29T12:29:20Z</published>
    <title>Modeling documents with Generative Adversarial Networks</title>
    <summary>  This paper describes a method for using Generative Adversarial Networks to
learn distributed representations of natural language documents. We propose a
model that is based on the recently proposed Energy-Based GAN, but instead uses
a Denoising Autoencoder as the discriminator network. Document representations
are extracted from the hidden layer of the discriminator and evaluated both
quantitatively and qualitatively.
</summary>
    <author>
      <name>John Glover</name>
    </author>
    <link href="http://arxiv.org/abs/1612.09122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09147v2</id>
    <updated>2017-01-26T16:44:56Z</updated>
    <published>2016-12-29T14:02:31Z</published>
    <title>Linear Learning with Sparse Data</title>
    <summary>  Linear predictors are especially useful when the data is high-dimensional and
sparse. One of the standard techniques used to train a linear predictor is the
Averaged Stochastic Gradient Descent (ASGD) algorithm. We present an efficient
implementation of ASGD that avoids dense vector operations. We also describe a
translation invariant extension called Centered Averaged Stochastic Gradient
Descent (CASGD).
</summary>
    <author>
      <name>Ofer Dekel</name>
    </author>
    <link href="http://arxiv.org/abs/1612.09147v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09147v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01830v1</id>
    <updated>2017-03-06T12:06:58Z</updated>
    <published>2017-03-06T12:06:58Z</published>
    <title>Decomposable Submodular Function Minimization: Discrete and Continuous</title>
    <summary>  This paper investigates connections between discrete and continuous
approaches for decomposable submodular function minimization. We provide
improved running time estimates for the state-of-the-art continuous algorithms
for the problem using combinatorial arguments. We also provide a systematic
experimental comparison of the two types of methods, based on a clear
distinction between level-0 and level-1 algorithms.
</summary>
    <author>
      <name>Alina Ene</name>
    </author>
    <author>
      <name>Huy L. Nguyen</name>
    </author>
    <author>
      <name>László A. Végh</name>
    </author>
    <link href="http://arxiv.org/abs/1703.01830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06485v1</id>
    <updated>2017-03-19T18:25:52Z</updated>
    <published>2017-03-19T18:25:52Z</published>
    <title>Near Optimal Hamiltonian-Control and Learning via Chattering</title>
    <summary>  Many applications require solving non-linear control problems that are
classically not well behaved. This paper develops a simple and efficient
chattering algorithm that learns near optimal decision policies through an
open-loop feedback strategy. The optimal control problem reduces to a series of
linear optimization programs that can be easily solved to recover a relaxed
optimal trajectory. This algorithm is implemented on a real-time enterprise
scheduling and control process.
</summary>
    <author>
      <name>Peeyush Kumar</name>
    </author>
    <author>
      <name>Wolf Kohn</name>
    </author>
    <author>
      <name>Zelda B. Zabinsky</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02388v1</id>
    <updated>2018-03-06T19:16:08Z</updated>
    <published>2018-03-06T19:16:08Z</published>
    <title>Learning SMaLL Predictors</title>
    <summary>  We present a new machine learning technique for training small
resource-constrained predictors. Our algorithm, the Sparse Multiprototype
Linear Learner (SMaLL), is inspired by the classic machine learning problem of
learning $k$-DNF Boolean formulae. We present a formal derivation of our
algorithm and demonstrate the benefits of our approach with a detailed
empirical study.
</summary>
    <author>
      <name>Vikas K. Garg</name>
    </author>
    <author>
      <name>Ofer Dekel</name>
    </author>
    <author>
      <name>Lin Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/1803.02388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03684v1</id>
    <updated>2018-03-09T20:29:18Z</updated>
    <published>2018-03-09T20:29:18Z</published>
    <title>Scoring Formulation for Multi-Condition Joint PLDA</title>
    <summary>  The joint PLDA model, is a generalization of PLDA where the nuisance variable
is no longer considered independent across samples, but potentially shared
(tied) across samples that correspond to the same nuisance condition. The
original work considered a single nuisance condition, deriving the EM and
scoring formulas for this scenario. In this document, we show how to obtain
likelihood ratios for scoring when multiple nuisance conditions are allowed in
the model.
</summary>
    <author>
      <name>Luciana Ferrer</name>
    </author>
    <link href="http://arxiv.org/abs/1803.03684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07980v2</id>
    <updated>2018-03-22T02:36:59Z</updated>
    <published>2018-03-21T16:03:29Z</published>
    <title>Information Theoretic Interpretation of Deep learning</title>
    <summary>  We interpret part of the experimental results of Shwartz-Ziv and Tishby
[2017]. Inspired by these results, we established a conjecture of the dynamics
of the machinary of deep neural network. This conjecture can be used to explain
the counterpart result by Saxe et al. [2018].
</summary>
    <author>
      <name>Tianchen Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07980v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07980v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08101v1</id>
    <updated>2018-03-21T19:38:27Z</updated>
    <published>2018-03-21T19:38:27Z</published>
    <title>Clustering to Reduce Spatial Data Set Size</title>
    <summary>  Traditionally it had been a problem that researchers did not have access to
enough spatial data to answer pressing research questions or build compelling
visualizations. Today, however, the problem is often that we have too much
data. Spatially redundant or approximately redundant points may refer to a
single feature (plus noise) rather than many distinct spatial features. We use
a machine learning approach with density-based clustering to compress such
spatial data into a set of representative features.
</summary>
    <author>
      <name>Geoff Boeing</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09791v3</id>
    <updated>2018-10-03T13:44:05Z</updated>
    <published>2018-03-26T18:54:36Z</published>
    <title>A Common Framework for Natural Gradient and Taylor based Optimisation
  using Manifold Theory</title>
    <summary>  This technical report constructs a theoretical framework to relate standard
Taylor approximation based optimisation methods with Natural Gradient (NG), a
method which is Fisher efficient with probabilistic models. Such a framework
will be shown to also provide mathematical justification to combine higher
order methods with the method of NG.
</summary>
    <author>
      <name>Adnan Haider</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09791v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09791v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01907v2</id>
    <updated>2018-06-21T16:57:32Z</updated>
    <published>2018-05-04T18:07:21Z</published>
    <title>Exploration by Distributional Reinforcement Learning</title>
    <summary>  We propose a framework based on distributional reinforcement learning and
recent attempts to combine Bayesian parameter updates with deep reinforcement
learning. We show that our proposed framework conceptually unifies multiple
previous methods in exploration. We also derive a practical algorithm that
achieves efficient exploration on challenging control tasks.
</summary>
    <author>
      <name>Yunhao Tang</name>
    </author>
    <author>
      <name>Shipra Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01907v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01907v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04272v2</id>
    <updated>2018-08-15T16:24:39Z</updated>
    <published>2018-05-11T08:28:55Z</published>
    <title>An $O(N)$ Sorting Algorithm: Machine Learning Sort</title>
    <summary>  We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,
which shows a huge potential sorting big data. This sorting algorithm can be
applied to parallel sorting and is suitable for GPU or TPU acceleration.
Furthermore, we discuss the application of this algorithm to sparse hash table.
</summary>
    <author>
      <name>Hanqing Zhao</name>
    </author>
    <author>
      <name>Yuehan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05773v1</id>
    <updated>2018-05-11T18:12:19Z</updated>
    <published>2018-05-11T18:12:19Z</published>
    <title>Online Bandit Linear Optimization: A Study</title>
    <summary>  This article introduces the concepts around Online Bandit Linear Optimization
and explores an efficient setup called SCRiBLe (Self-Concordant Regularization
in Bandit Learning) created by Abernethy et. al.\cite{abernethy}. The SCRiBLe
setup and algorithm yield a $O(\sqrt{T})$ regret bound and polynomial run time
complexity bound on the dimension of the input space. In this article we build
up to the bandit linear optimization case and study SCRiBLe.
</summary>
    <author>
      <name>Vikram Mullachery</name>
    </author>
    <author>
      <name>Samarth Tiwari</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00468v2</id>
    <updated>2019-01-11T02:51:38Z</updated>
    <published>2018-06-01T17:58:58Z</published>
    <title>Implicit Bias of Gradient Descent on Linear Convolutional Networks</title>
    <summary>  We show that gradient descent on full-width linear convolutional networks of
depth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridge
penalty in the frequency domain. This is in contrast to linearly fully
connected networks, where gradient descent converges to the hard margin linear
support vector machine solution, regardless of depth.
</summary>
    <author>
      <name>Suriya Gunasekar</name>
    </author>
    <author>
      <name>Jason Lee</name>
    </author>
    <author>
      <name>Daniel Soudry</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00468v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00468v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02460v1</id>
    <updated>2018-06-06T23:34:06Z</updated>
    <published>2018-06-06T23:34:06Z</published>
    <title>The effect of the choice of neural network depth and breadth on the size
  of its hypothesis space</title>
    <summary>  We show that the number of unique function mappings in a neural network
hypothesis space is inversely proportional to $\prod_lU_l!$, where $U_{l}$ is
the number of neurons in the hidden layer $l$.
</summary>
    <author>
      <name>Lech Szymanski</name>
    </author>
    <author>
      <name>Brendan McCane</name>
    </author>
    <author>
      <name>Michael Albert</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04169v1</id>
    <updated>2018-06-11T18:22:45Z</updated>
    <published>2018-06-11T18:22:45Z</published>
    <title>Defense Against the Dark Arts: An overview of adversarial example
  security research and future research directions</title>
    <summary>  This article presents a summary of a keynote lecture at the Deep Learning
Security workshop at IEEE Security and Privacy 2018. This lecture summarizes
the state of the art in defenses against adversarial examples and provides
recommendations for future research directions on this topic.
</summary>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05403v1</id>
    <updated>2018-06-14T08:00:45Z</updated>
    <published>2018-06-14T08:00:45Z</published>
    <title>On the Perceptron's Compression</title>
    <summary>  We study and provide exposition to several phenomena that are related to the
perceptron's compression. One theme concerns modifications of the perceptron
algorithm that yield better guarantees on the margin of the hyperplane it
outputs. These modifications can be useful in training neural networks as well,
and we demonstrate them with some experimental data. In a second theme, we
deduce conclusions from the perceptron's compression in various contexts.
</summary>
    <author>
      <name>Shay Moran</name>
    </author>
    <author>
      <name>Ido Nachum</name>
    </author>
    <author>
      <name>Itai Panasoff</name>
    </author>
    <author>
      <name>Amir Yehudayoff</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06798v2</id>
    <updated>2019-02-03T16:26:40Z</updated>
    <published>2018-06-10T08:24:36Z</published>
    <title>Implicit Policy for Reinforcement Learning</title>
    <summary>  We introduce Implicit Policy, a general class of expressive policies that can
flexibly represent complex action distributions in reinforcement learning, with
efficient algorithms to compute entropy regularized policy gradients. We
empirically show that, despite its simplicity in implementation, entropy
regularization combined with a rich policy class can attain desirable
properties displayed under maximum entropy reinforcement learning framework,
such as robustness and multi-modality.
</summary>
    <author>
      <name>Yunhao Tang</name>
    </author>
    <author>
      <name>Shipra Agrawal</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06798v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06798v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08049v1</id>
    <updated>2018-06-21T02:33:44Z</updated>
    <published>2018-06-21T02:33:44Z</published>
    <title>On the Robustness of Interpretability Methods</title>
    <summary>  We argue that robustness of explanations---i.e., that similar inputs should
give rise to similar explanations---is a key desideratum for interpretability.
We introduce metrics to quantify robustness and demonstrate that current
methods do not perform well according to these metrics. Finally, we propose
ways that robustness can be enforced on existing interpretability approaches.
</summary>
    <author>
      <name>David Alvarez-Melis</name>
    </author>
    <author>
      <name>Tommi S. Jaakkola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented at 2018 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2018), Stockholm, Sweden</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09211v1</id>
    <updated>2018-06-24T21:08:16Z</updated>
    <published>2018-06-24T21:08:16Z</published>
    <title>Equalizing Financial Impact in Supervised Learning</title>
    <summary>  Notions of "fair classification" that have arisen in computer science
generally revolve around equalizing certain statistics across protected groups.
This approach has been criticized as ignoring societal issues, including how
errors can hurt certain groups disproportionately. We pose a modification of
one of the fairness criteria from Hardt, Price, and Srebro [NIPS, 2016] that
makes a small step towards addressing this issue in the case of financial
decisions like giving loans. We call this new notion "equalized financial
impact."
</summary>
    <author>
      <name>Govind Ramnarayan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04982v1</id>
    <updated>2020-06-12T14:27:56Z</updated>
    <published>2020-06-12T14:27:56Z</published>
    <title>Recursion and evolution: Part II</title>
    <summary>  We examine the question of whether it is possible for a diagonalizing system,
to learn to use environmental reward and punishment as an information, in order
to appropriately adapt. More specifically, we study the possiblity of such a
system, to learn to use diagonalization on the basis of a rewarding function.
Relevant phenomena regarding memory are also investigated.
</summary>
    <author>
      <name>Alexandros Arvanitakis</name>
    </author>
    <link href="http://arxiv.org/abs/2007.04982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.05456v1</id>
    <updated>2020-07-10T15:52:21Z</updated>
    <published>2020-07-10T15:52:21Z</published>
    <title>Improved Analysis of UCRL2 with Empirical Bernstein Inequality</title>
    <summary>  We consider the problem of exploration-exploitation in communicating Markov
Decision Processes. We provide an analysis of UCRL2 with Empirical Bernstein
inequalities (UCRL2B). For any MDP with $S$ states, $A$ actions, $\Gamma \leq
S$ next states and diameter $D$, the regret of UCRL2B is bounded as
$\widetilde{O}(\sqrt{D\Gamma S A T})$.
</summary>
    <author>
      <name>Ronan Fruit</name>
    </author>
    <author>
      <name>Matteo Pirotta</name>
    </author>
    <author>
      <name>Alessandro Lazaric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Document in support of the tutorial at ALT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.05456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.05758v1</id>
    <updated>2020-07-11T12:17:40Z</updated>
    <published>2020-07-11T12:17:40Z</published>
    <title>Feature Interactions in XGBoost</title>
    <summary>  In this paper, we investigate how feature interactions can be identified to
be used as constraints in the gradient boosting tree models using XGBoost's
implementation. Our results show that accurate identification of these
constraints can help improve the performance of baseline XGBoost model
significantly. Further, the improvement in the model structure can also lead to
better interpretability.
</summary>
    <author>
      <name>Kshitij Goyal</name>
    </author>
    <author>
      <name>Sebastijan Dumancic</name>
    </author>
    <author>
      <name>Hendrik Blockeel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.05758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08703v1</id>
    <updated>2020-07-17T00:46:30Z</updated>
    <published>2020-07-17T00:46:30Z</published>
    <title>Bandits for BMO Functions</title>
    <summary>  We study the bandit problem where the underlying expected reward is a Bounded
Mean Oscillation (BMO) function. BMO functions are allowed to be discontinuous
and unbounded, and are useful in modeling signals with infinities in the
do-main. We develop a toolset for BMO bandits, and provide an algorithm that
can achieve poly-log $\delta$-regret -- a regret measured against an arm that
is optimal after removing a $\delta$-sized portion of the arm space.
</summary>
    <author>
      <name>Tianyu Wang</name>
    </author>
    <author>
      <name>Cynthia Rudin</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09668v1</id>
    <updated>2020-07-19T13:25:38Z</updated>
    <published>2020-07-19T13:25:38Z</published>
    <title>Improving the Long-Range Performance of Gated Graph Neural Networks</title>
    <summary>  Many popular variants of graph neural networks (GNNs) that are capable of
handling multi-relational graphs may suffer from vanishing gradients. In this
work, we propose a novel GNN architecture based on the Gated Graph Neural
Network with an improved ability to handle long-range dependencies in
multi-relational graphs. An experimental analysis on different synthetic tasks
demonstrates that the proposed architecture outperforms several popular GNN
models.
</summary>
    <author>
      <name>Denis Lukovnikov</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <author>
      <name>Asja Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/2007.09668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09982v1</id>
    <updated>2020-07-20T10:10:13Z</updated>
    <published>2020-07-20T10:10:13Z</published>
    <title>MKLpy: a python-based framework for Multiple Kernel Learning</title>
    <summary>  Multiple Kernel Learning is a recent and powerful paradigm to learn the
kernel function from data. In this paper, we introduce MKLpy, a python-based
framework for Multiple Kernel Learning. The library provides Multiple Kernel
Learning algorithms for classification tasks, mechanisms to compute kernel
functions for different data types, and evaluation strategies. The library is
meant to maximize the usability and to simplify the development of novel
solutions.
</summary>
    <author>
      <name>Ivano Lauriola</name>
    </author>
    <author>
      <name>Fabio Aiolli</name>
    </author>
    <link href="http://arxiv.org/abs/2007.09982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10182v1</id>
    <updated>2020-07-20T15:15:27Z</updated>
    <published>2020-07-20T15:15:27Z</published>
    <title>Time Series Source Separation with Slow Flows</title>
    <summary>  In this paper, we show that slow feature analysis (SFA), a common time series
decomposition method, naturally fits into the flow-based models (FBM)
framework, a type of invertible neural latent variable models. Building upon
recent advances on blind source separation, we show that such a fit makes the
time series decomposition identifiable.
</summary>
    <author>
      <name>Edouard Pineau</name>
    </author>
    <author>
      <name>Sébastien Razakarivony</name>
    </author>
    <author>
      <name>Thomas Bonald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INNF+ Workshop, ICML 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11975v1</id>
    <updated>2020-07-23T12:40:57Z</updated>
    <published>2020-07-23T12:40:57Z</published>
    <title>Online Boosting with Bandit Feedback</title>
    <summary>  We consider the problem of online boosting for regression tasks, when only
limited information is available to the learner. We give an efficient regret
minimization method that has two implications: an online boosting algorithm
with noisy multi-point bandit feedback, and a new projection-free online convex
optimization algorithm with stochastic gradient, that improves state-of-the-art
guarantees in terms of efficiency.
</summary>
    <author>
      <name>Nataly Brukhim</name>
    </author>
    <author>
      <name>Elad Hazan</name>
    </author>
    <link href="http://arxiv.org/abs/2007.11975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00707v2</id>
    <updated>2020-10-28T16:43:10Z</updated>
    <published>2020-08-26T17:39:53Z</published>
    <title>Pursuing a Prospective Perspective</title>
    <summary>  Retrospective testing of predictive models does not consider the real-world
context in which models are deployed. Prospective validation, on the other
hand, enables meaningful comparisons between data generation processes by
incorporating trained models and considering the subjective decisions that
affect reproducibility. Prospective experiments are essential for consistent
progress in modeling.
</summary>
    <author>
      <name>Steven Kearnes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.trechm.2020.10.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.trechm.2020.10.012" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Trends in Chemistry (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00707v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00707v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09155v1</id>
    <updated>2020-09-19T03:37:44Z</updated>
    <published>2020-09-19T03:37:44Z</published>
    <title>SecDD: Efficient and Secure Method for Remotely Training Neural Networks</title>
    <summary>  We leverage what are typically considered the worst qualities of deep
learning algorithms - high computational cost, requirement for large data, no
explainability, high dependence on hyper-parameter choice, overfitting, and
vulnerability to adversarial perturbations - in order to create a method for
the secure and efficient training of remotely deployed neural networks over
unsecured channels.
</summary>
    <author>
      <name>Ilia Sucholutsky</name>
    </author>
    <author>
      <name>Matthias Schonlau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09719v2</id>
    <updated>2020-09-28T13:27:14Z</updated>
    <published>2020-09-21T09:41:54Z</published>
    <title>A Survey on Machine Learning Applied to Dynamic Physical Systems</title>
    <summary>  This survey is on recent advancements in the intersection of physical
modeling and machine learning. We focus on the modeling of nonlinear systems
which are closer to electric motors. Survey on motor control and fault
detection in operation of electric motors has been done.
</summary>
    <author>
      <name>Sagar Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text overlap with external source</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09719v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09719v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10312v1</id>
    <updated>2020-09-22T04:38:49Z</updated>
    <published>2020-09-22T04:38:49Z</published>
    <title>Stacked Generalization for Human Activity Recognition</title>
    <summary>  This short paper aims to discuss the effectiveness and performance of
classical machine learning approaches for Human Activity Recognition (HAR). It
proposes two important models - Extra Trees and Stacked Classifier with the
emphasize on the best practices, heuristics and measures that are required to
maximize the performance of those models.
</summary>
    <author>
      <name>Ambareesh Ravi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14075v2</id>
    <updated>2021-04-14T16:01:12Z</updated>
    <published>2020-09-29T15:04:40Z</published>
    <title>Backpropagating through Fréchet Inception Distance</title>
    <summary>  The Fr\'echet Inception Distance (FID) has been used to evaluate hundreds of
generative models. We introduce FastFID, which can efficiently train generative
models with FID as a loss function. Using FID as an additional loss for
Generative Adversarial Networks improves their FID.
</summary>
    <author>
      <name>Alexander Mathiasen</name>
    </author>
    <author>
      <name>Frederik Hvilshøj</name>
    </author>
    <link href="http://arxiv.org/abs/2009.14075v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14075v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14554v1</id>
    <updated>2020-09-30T10:52:27Z</updated>
    <published>2020-09-30T10:52:27Z</published>
    <title>One Reflection Suffice</title>
    <summary>  Orthogonal weight matrices are used in many areas of deep learning. Much
previous work attempt to alleviate the additional computational resources it
requires to constrain weight matrices to be orthogonal. One popular approach
utilizes *many* Householder reflections. The only practical drawback is that
many reflections cause low GPU utilization. We mitigate this final drawback by
proving that *one* reflection is sufficient, if the reflection is computed by
an auxiliary neural network.
</summary>
    <author>
      <name>Alexander Mathiasen</name>
    </author>
    <author>
      <name>Frederik Hvilshøj</name>
    </author>
    <link href="http://arxiv.org/abs/2009.14554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01030v1</id>
    <updated>2021-02-25T16:51:15Z</updated>
    <published>2021-02-25T16:51:15Z</published>
    <title>An Easy to Interpret Diagnostic for Approximate Inference: Symmetric
  Divergence Over Simulations</title>
    <summary>  It is important to estimate the errors of probabilistic inference algorithms.
Existing diagnostics for Markov chain Monte Carlo methods assume inference is
asymptotically exact, and are not appropriate for approximate methods like
variational inference or Laplace's method. This paper introduces a diagnostic
based on repeatedly simulating datasets from the prior and performing inference
on each. The central observation is that it is possible to estimate a symmetric
KL-divergence defined over these simulations.
</summary>
    <author>
      <name>Justin Domke</name>
    </author>
    <link href="http://arxiv.org/abs/2103.01030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.07287v2</id>
    <updated>2024-11-12T13:51:57Z</updated>
    <published>2021-01-31T10:44:38Z</published>
    <title>Neural Networks with Complex-Valued Weights Have No Spurious Local
  Minima</title>
    <summary>  We study the benefits of complex-valued weights for neural networks. We prove
that shallow complex neural networks with quadratic activations have no
spurious local minima. In contrast, shallow real neural networks with quadratic
activations have infinitely many spurious local minima under the same
conditions. In addition, we provide specific examples to demonstrate that
complex-valued weights turn poor local minima into saddle points.
</summary>
    <author>
      <name>Xingtu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2103.07287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.07287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.07802v1</id>
    <updated>2021-03-13T22:01:47Z</updated>
    <published>2021-03-13T22:01:47Z</published>
    <title>Hybrid computer approach to train a machine learning system</title>
    <summary>  This book chapter describes a novel approach to training machine learning
systems by means of a hybrid computer setup i.e. a digital computer tightly
coupled with an analog computer. As an example a reinforcement learning system
is trained to balance an inverted pendulum which is simulated on an analog
computer, thus demonstrating a solution to the major challenge of adequately
simulating the environment for reinforcement learning.
</summary>
    <author>
      <name>Mirko Holzer</name>
    </author>
    <author>
      <name>Bernd Ulmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Book chapter</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.07802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.07802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; B.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.16082v1</id>
    <updated>2021-03-30T05:21:12Z</updated>
    <published>2021-03-30T05:21:12Z</published>
    <title>Optimal Stochastic Nonconvex Optimization with Bandit Feedback</title>
    <summary>  In this paper, we analyze the continuous armed bandit problems for nonconvex
cost functions under certain smoothness and sublevel set assumptions. We first
derive an upper bound on the expected cumulative regret of a simple bin
splitting method. We then propose an adaptive bin splitting method, which can
significantly improve the performance. Furthermore, a minimax lower bound is
derived, which shows that our new adaptive method achieves locally minimax
optimal expected cumulative regret.
</summary>
    <author>
      <name>Puning Zhao</name>
    </author>
    <author>
      <name>Lifeng Lai</name>
    </author>
    <link href="http://arxiv.org/abs/2103.16082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.16082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00357v1</id>
    <updated>2021-05-01T23:48:58Z</updated>
    <published>2021-05-01T23:48:58Z</published>
    <title>RotLSTM: Rotating Memories in Recurrent Neural Networks</title>
    <summary>  Long Short-Term Memory (LSTM) units have the ability to memorise and use
long-term dependencies between inputs to generate predictions on time series
data. We introduce the concept of modifying the cell state (memory) of LSTMs
using rotation matrices parametrised by a new set of trainable weights. This
addition shows significant increases of performance on some of the tasks from
the bAbI dataset.
</summary>
    <author>
      <name>Vlad Velici</name>
    </author>
    <author>
      <name>Adam Prügel-Bennett</name>
    </author>
    <link href="http://arxiv.org/abs/2105.00357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01404v1</id>
    <updated>2021-05-04T10:36:29Z</updated>
    <published>2021-05-04T10:36:29Z</published>
    <title>TimeGym: Debugging for Time Series Modeling in Python</title>
    <summary>  We introduce the TimeGym Forecasting Debugging Toolkit, a Python library for
testing and debugging time series forecasting pipelines. TimeGym simplifies the
testing forecasting pipeline by providing generic tests for forecasting
pipelines fresh out of the box. These tests are based on common modeling
challenges of time series. Our library enables forecasters to apply a
Test-Driven Development approach to forecast modeling, using specified oracles
to generate artificial data with noise.
</summary>
    <author>
      <name>Diogo Seca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper and python package ongoing active development</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01755v1</id>
    <updated>2021-05-04T20:51:54Z</updated>
    <published>2021-05-04T20:51:54Z</published>
    <title>Reinforcement Learning for Scalable Logic Optimization with Graph Neural
  Networks</title>
    <summary>  Logic optimization is an NP-hard problem commonly approached through
hand-engineered heuristics. We propose to combine graph convolutional networks
with reinforcement learning and a novel, scalable node embedding method to
learn which local transforms should be applied to the logic graph. We show that
this method achieves a similar size reduction as ABC on smaller circuits and
outperforms it by 1.5-1.75x on larger random graphs.
</summary>
    <author>
      <name>Xavier Timoneda</name>
    </author>
    <author>
      <name>Lukas Cavigelli</name>
    </author>
    <link href="http://arxiv.org/abs/2105.01755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06060v1</id>
    <updated>2021-05-13T03:25:32Z</updated>
    <published>2021-05-13T03:25:32Z</published>
    <title>House Price Prediction using Satellite Imagery</title>
    <summary>  In this paper we show how using satellite images can improve the accuracy of
housing price estimation models. Using Los Angeles County's property assessment
dataset, by transferring learning from an Inception-v3 model pretrained on
ImageNet, we could achieve an improvement of ~10% in R-squared score compared
to two baseline models that only use non-image features of the house.
</summary>
    <author>
      <name>Sina Jandaghi Semnani</name>
    </author>
    <author>
      <name>Hoormazd Rezaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford CS230 Deep Learning, Winter 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.06060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06095v1</id>
    <updated>2015-09-21T02:28:44Z</updated>
    <published>2015-09-21T02:28:44Z</published>
    <title>Multilayer bootstrap network for unsupervised speaker recognition</title>
    <summary>  We apply multilayer bootstrap network (MBN), a recent proposed unsupervised
learning method, to unsupervised speaker recognition. The proposed method first
extracts supervectors from an unsupervised universal background model, then
reduces the dimension of the high-dimensional supervectors by multilayer
bootstrap network, and finally conducts unsupervised speaker recognition by
clustering the low-dimensional data. The comparison results with 2 unsupervised
and 1 supervised speaker recognition techniques demonstrate the effectiveness
and robustness of the proposed method.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02010v1</id>
    <updated>2016-03-07T11:23:57Z</updated>
    <published>2016-03-07T11:23:57Z</published>
    <title>Differentially Private Policy Evaluation</title>
    <summary>  We present the first differentially private algorithms for reinforcement
learning, which apply to the task of evaluating a fixed policy. We establish
two approaches for achieving differential privacy, provide a theoretical
analysis of the privacy and utility of the two algorithms, and show promising
results on simple empirical examples.
</summary>
    <author>
      <name>Borja Balle</name>
    </author>
    <author>
      <name>Maziar Gomrokchi</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <link href="http://arxiv.org/abs/1603.02010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02185v1</id>
    <updated>2016-03-07T18:11:54Z</updated>
    <published>2016-03-07T18:11:54Z</published>
    <title>Distributed Multi-Task Learning with Shared Representation</title>
    <summary>  We study the problem of distributed multi-task learning with shared
representation, where each machine aims to learn a separate, but related, task
in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix
has low rank. We consider a setting where each task is handled by a different
machine, with samples for the task available locally on the machine, and study
communication-efficient methods for exploiting the shared structure.
</summary>
    <author>
      <name>Jialei Wang</name>
    </author>
    <author>
      <name>Mladen Kolar</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1603.02185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02578v1</id>
    <updated>2016-03-08T16:36:31Z</updated>
    <published>2016-03-08T16:36:31Z</published>
    <title>Batched Lazy Decision Trees</title>
    <summary>  We introduce a batched lazy algorithm for supervised classification using
decision trees. It avoids unnecessary visits to irrelevant nodes when it is
used to make predictions with either eagerly or lazily trained decision trees.
A set of experiments demonstrate that the proposed algorithm can outperform
both the conventional and lazy decision tree algorithms in terms of computation
time as well as memory consumption, without compromising accuracy.
</summary>
    <author>
      <name>Mathieu Guillame-Bert</name>
    </author>
    <author>
      <name>Artur Dubrawski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, 3 tables, 3 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.02578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06653v1</id>
    <updated>2016-03-22T01:05:47Z</updated>
    <published>2016-03-22T01:05:47Z</published>
    <title>Information Theoretic-Learning Auto-Encoder</title>
    <summary>  We propose Information Theoretic-Learning (ITL) divergence measures for
variational regularization of neural networks. We also explore ITL-regularized
autoencoders as an alternative to variational autoencoding bayes, adversarial
autoencoders and generative adversarial networks for randomly generating sample
data without explicitly defining a partition function. This paper also
formalizes, generative moment matching networks under the ITL framework.
</summary>
    <author>
      <name>Eder Santana</name>
    </author>
    <author>
      <name>Matthew Emigh</name>
    </author>
    <author>
      <name>Jose C Principe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00251v1</id>
    <updated>2016-05-01T13:19:57Z</updated>
    <published>2016-05-01T13:19:57Z</published>
    <title>A vector-contraction inequality for Rademacher complexities</title>
    <summary>  The contraction inequality for Rademacher averages is extended to Lipschitz
functions with vector-valued domains, and it is also shown that in the bounding
expression the Rademacher variables can be replaced by arbitrary iid symmetric
and sub-gaussian variables. Example applications are given for multi-category
learning, K-means clustering and learning-to-learn.
</summary>
    <author>
      <name>Andreas Maurer</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00404v1</id>
    <updated>2016-05-02T09:33:46Z</updated>
    <published>2016-05-02T09:33:46Z</published>
    <title>Simple2Complex: Global Optimization by Gradient Descent</title>
    <summary>  A method named simple2complex for modeling and training deep neural networks
is proposed. Simple2complex train deep neural networks by smoothly adding more
and more layers to the shallow networks, as the learning procedure going on,
the network is just like growing. Compared with learning by end2end,
simple2complex is with less possibility trapping into local minimal, namely,
owning ability for global optimization. Cifar10 is used for verifying the
superiority of simple2complex.
</summary>
    <author>
      <name>Ming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02099v1</id>
    <updated>2016-05-06T20:52:26Z</updated>
    <published>2016-05-06T20:52:26Z</published>
    <title>Some Simulation Results for Emphatic Temporal-Difference Learning
  Algorithms</title>
    <summary>  This is a companion note to our recent study of the weak convergence
properties of constrained emphatic temporal-difference learning (ETD)
algorithms from a theoretic perspective. It supplements the latter analysis
with simulation results and illustrates the behavior of some of the ETD
algorithms using three example problems.
</summary>
    <author>
      <name>Huizhen Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A companion note to the article arxiv:1511.07471; 30 pages; 34
  figures, best viewed on screen</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08497v1</id>
    <updated>2016-05-27T03:11:41Z</updated>
    <published>2016-05-27T03:11:41Z</published>
    <title>Universum Learning for SVM Regression</title>
    <summary>  This paper extends the idea of Universum learning [18, 19] to regression
problems. We propose new Universum-SVM formulation for regression problems that
incorporates a priori knowledge in the form of additional data samples. These
additional data samples or Universum belong to the same application domain as
the training samples, but they follow a different distribution. Several
empirical comparisons are presented to illustrate the utility of the proposed
approach.
</summary>
    <author>
      <name>Sauptik Dhar</name>
    </author>
    <author>
      <name>Vladimir Cherkassky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,11 figures, Thesis:
  http://conservancy.umn.edu/handle/11299/162636</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08833v1</id>
    <updated>2016-05-28T02:39:24Z</updated>
    <published>2016-05-28T02:39:24Z</published>
    <title>Muffled Semi-Supervised Learning</title>
    <summary>  We explore a novel approach to semi-supervised learning. This approach is
contrary to the common approach in that the unlabeled examples serve to
"muffle," rather than enhance, the guidance provided by the labeled examples.
We provide several variants of the basic algorithm and show experimentally that
they can achieve significantly higher AUC than boosted trees, random forests
and logistic regression when unlabeled examples are available.
</summary>
    <author>
      <name>Akshay Balsubramani</name>
    </author>
    <author>
      <name>Yoav Freund</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00466v1</id>
    <updated>2016-07-02T04:48:59Z</updated>
    <published>2016-07-02T04:48:59Z</published>
    <title>Outlier absorbing based on a Bayesian approach</title>
    <summary>  The presence of outliers is prevalent in machine learning applications and
may produce misleading results. In this paper a new method for dealing with
outliers and anomal samples is proposed. To overcome the outlier issue, the
proposed method combines the global and local views of the samples. By
combination of these views, our algorithm performs in a robust manner. The
experimental results show the capabilities of the proposed method.
</summary>
    <author>
      <name>Parsa Bagherzadeh</name>
    </author>
    <author>
      <name>Hadi Sadoghi Yazdi</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03559v1</id>
    <updated>2016-07-13T01:22:04Z</updated>
    <published>2016-07-13T01:22:04Z</published>
    <title>Fast Sampling for Strongly Rayleigh Measures with Application to
  Determinantal Point Processes</title>
    <summary>  In this note we consider sampling from (non-homogeneous) strongly Rayleigh
probability measures. As an important corollary, we obtain a fast mixing Markov
Chain sampler for Determinantal Point Processes.
</summary>
    <author>
      <name>Chengtao Li</name>
    </author>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <author>
      <name>Suvrit Sra</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03626v1</id>
    <updated>2016-07-13T08:03:35Z</updated>
    <published>2016-07-13T08:03:35Z</published>
    <title>San Francisco Crime Classification</title>
    <summary>  San Francisco Crime Classification is an online competition administered by
Kaggle Inc. The competition aims at predicting the future crimes based on a
given set of geographical and time-based features. In this paper, I achieved a
an accuracy that ranks at top %18, as of May 19th, 2016. I will explore the
data, and explain in details the tools I used to achieve that result.
</summary>
    <author>
      <name>Yehya Abouelnaga</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03849v2</id>
    <updated>2016-08-02T15:34:40Z</updated>
    <published>2016-07-13T18:15:52Z</published>
    <title>Fitting a Simplicial Complex using a Variation of k-means</title>
    <summary>  We give a simple and effective two stage algorithm for approximating a point
cloud $\mathcal{S}\subset\mathbb{R}^m$ by a simplicial complex $K$. The first
stage is an iterative fitting procedure that generalizes k-means clustering,
while the second stage involves deleting redundant simplices. A form of
dimension reduction of $\mathcal{S}$ is obtained as a consequence.
</summary>
    <author>
      <name>Piotr Beben</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03849v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03849v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03613v1</id>
    <updated>2017-05-10T06:19:04Z</updated>
    <published>2017-05-10T06:19:04Z</published>
    <title>An initialization method for the k-means using the concept of useful
  nearest centers</title>
    <summary>  The aim of the k-means is to minimize squared sum of Euclidean distance from
the mean (SSEDM) of each cluster. The k-means can effectively optimize this
function, but it is too sensitive for initial centers (seeds). This paper
proposed a method for initialization of the k-means using the concept of useful
nearest center for each data point.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05264v1</id>
    <updated>2017-05-15T14:25:15Z</updated>
    <published>2017-05-15T14:25:15Z</published>
    <title>Extending Defensive Distillation</title>
    <summary>  Machine learning is vulnerable to adversarial examples: inputs carefully
modified to force misclassification. Designing defenses against such inputs
remains largely an open problem. In this work, we revisit defensive
distillation---which is one of the mechanisms proposed to mitigate adversarial
examples---to address its limitations. We view our results not only as an
effective way of addressing some of the recently discovered attacks but also as
reinforcing the importance of improved training techniques.
</summary>
    <author>
      <name>Nicolas Papernot</name>
    </author>
    <author>
      <name>Patrick McDaniel</name>
    </author>
    <link href="http://arxiv.org/abs/1705.05264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07250v3</id>
    <updated>2017-06-02T14:58:53Z</updated>
    <published>2017-05-20T02:54:52Z</published>
    <title>Speedup from a different parametrization within the Neural Network
  algorithm</title>
    <summary>  A different parametrization of the hyperplanes is used in the neural network
algorithm. As demonstrated on several autoencoder examples it significantly
outperforms the usual parametrization, reaching lower training error values
with only a fraction of the number of epochs. It's argued that it makes it
easier to understand and initialize the parameters.
</summary>
    <author>
      <name>Michael F. Zimmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07250v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07250v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08076v4</id>
    <updated>2018-04-09T20:33:39Z</updated>
    <published>2017-05-23T05:07:52Z</published>
    <title>Learning from partial correction</title>
    <summary>  We introduce a new model of interactive learning in which an expert examines
the predictions of a learner and partially fixes them if they are wrong.
Although this kind of feedback is not i.i.d., we show statistical
generalization bounds on the quality of the learned model.
</summary>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <author>
      <name>Michael Luby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08076v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08076v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04581v3</id>
    <updated>2019-06-02T19:36:25Z</updated>
    <published>2017-12-13T01:10:13Z</published>
    <title>Potential-Function Proofs for First-Order Methods</title>
    <summary>  This note discusses proofs for convergence of first-order methods based on
simple potential-function arguments. We cover methods like gradient descent
(for both smooth and non-smooth settings), mirror descent, and some accelerated
variants.
</summary>
    <author>
      <name>Nikhil Bansal</name>
    </author>
    <author>
      <name>Anupam Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04581v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04581v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04741v1</id>
    <updated>2017-12-13T12:44:46Z</updated>
    <published>2017-12-13T12:44:46Z</published>
    <title>Mathematics of Deep Learning</title>
    <summary>  Recently there has been a dramatic increase in the performance of recognition
systems due to the introduction of deep architectures for representation
learning and classification. However, the mathematical reasons for this success
remain elusive. This tutorial will review recent work that aims to provide a
mathematical justification for several properties of deep networks, such as
global optimality, geometric stability, and invariance of the learned
representations.
</summary>
    <author>
      <name>Rene Vidal</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06428v1</id>
    <updated>2017-12-18T14:40:23Z</updated>
    <published>2017-12-18T14:40:23Z</published>
    <title>A Shapelet Transform for Multivariate Time Series Classification</title>
    <summary>  Shapelets are phase independent subsequences designed for time series
classification. We propose three adaptations to the Shapelet Transform (ST) to
capture multivariate features in multivariate time series classification. We
create a unified set of data to benchmark our work on, and compare with three
other algorithms. We demonstrate that multivariate shapelets are not
significantly worse than other state-of-the-art algorithms.
</summary>
    <author>
      <name>Aaron Bostrom</name>
    </author>
    <author>
      <name>Anthony Bagnall</name>
    </author>
    <link href="http://arxiv.org/abs/1712.06428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08597v2</id>
    <updated>2017-12-25T01:34:19Z</updated>
    <published>2017-12-22T17:55:28Z</published>
    <title>Learning the Kernel for Classification and Regression</title>
    <summary>  We investigate a series of learning kernel problems with polynomial
combinations of base kernels, which will help us solve regression and
classification problems. We also perform some numerical experiments of
polynomial kernels with regression and classification tasks on different
datasets.
</summary>
    <author>
      <name>Chen Li</name>
    </author>
    <author>
      <name>Luca Venturi</name>
    </author>
    <author>
      <name>Ruitu Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03486v1</id>
    <updated>2018-02-10T00:44:20Z</updated>
    <published>2018-02-10T00:44:20Z</published>
    <title>An LSTM Recurrent Network for Step Counting</title>
    <summary>  Smartphones with sensors such as accelerometer and gyroscope can be used as
pedometers and navigators. In this paper, we propose to use an LSTM recurrent
network for counting the number of steps taken by both blind and sighted users,
based on an annotated smartphone sensor dataset, WeAllWork. The models were
trained separately for sighted people, blind people with a long cane or a guide
dog for Leave-One-Out training modality. It achieved 5% overcount and
undercount rate.
</summary>
    <author>
      <name>Ziyi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07833v2</id>
    <updated>2018-03-25T23:57:28Z</updated>
    <published>2018-02-21T22:18:20Z</published>
    <title>Variational Inference for Policy Gradient</title>
    <summary>  Inspired by the seminal work on Stein Variational Inference and Stein
Variational Policy Gradient, we derived a method to generate samples from the
posterior variational parameter distribution by \textit{explicitly} minimizing
the KL divergence to match the target distribution in an amortize fashion.
Consequently, we applied this varational inference technique into vanilla
policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations
for reinforcement learning problems.
</summary>
    <author>
      <name>Tianbing Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04435v1</id>
    <updated>2018-04-12T11:36:42Z</updated>
    <published>2018-04-12T11:36:42Z</published>
    <title>Variational Composite Autoencoders</title>
    <summary>  Learning in the latent variable model is challenging in the presence of the
complex data structure or the intractable latent variable. Previous variational
autoencoders can be low effective due to the straightforward encoder-decoder
structure. In this paper, we propose a variational composite autoencoder to
sidestep this issue by amortizing on top of the hierarchical latent variable
model. The experimental results confirm the advantages of our model.
</summary>
    <author>
      <name>Jiangchao Yao</name>
    </author>
    <author>
      <name>Ivor Tsang</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05834v1</id>
    <updated>2018-04-14T23:17:07Z</updated>
    <published>2018-04-14T23:17:07Z</published>
    <title>CytonRL: an Efficient Reinforcement Learning Open-source Toolkit
  Implemented in C++</title>
    <summary>  This paper presents an open-source enforcement learning toolkit named CytonRL
(https://github.com/arthurxlw/cytonRL). The toolkit implements four recent
advanced deep Q-learning algorithms from scratch using C++ and NVIDIA's
GPU-accelerated libraries. The code is simple and elegant, owing to an
open-source general-purpose neural network library named CytonLib. Benchmark
shows that the toolkit achieves competitive performances on the popular Atari
game of Breakout.
</summary>
    <author>
      <name>Xiaolin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00906v1</id>
    <updated>2018-07-02T21:49:32Z</updated>
    <published>2018-07-02T21:49:32Z</published>
    <title>Uncertainty in the Variational Information Bottleneck</title>
    <summary>  We present a simple case study, demonstrating that Variational Information
Bottleneck (VIB) can improve a network's classification calibration as well as
its ability to detect out-of-distribution data. Without explicitly being
designed to do so, VIB gives two natural metrics for handling and quantifying
uncertainty.
</summary>
    <author>
      <name>Alexander A. Alemi</name>
    </author>
    <author>
      <name>Ian Fischer</name>
    </author>
    <author>
      <name>Joshua V. Dillon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures. Accepted to UAI 2018 - Uncertainty in Deep
  Learning Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01280v2</id>
    <updated>2019-02-06T09:33:18Z</updated>
    <published>2018-07-03T16:56:14Z</published>
    <title>On the Computational Power of Online Gradient Descent</title>
    <summary>  We prove that the evolution of weight vectors in online gradient descent can
encode arbitrary polynomial-space computations, even in very simple learning
settings. Our results imply that, under weak complexity-theoretic assumptions,
it is impossible to reason efficiently about the fine-grained behavior of
online gradient descent.
</summary>
    <author>
      <name>Vaggos Chatziafratis</name>
    </author>
    <author>
      <name>Tim Roughgarden</name>
    </author>
    <author>
      <name>Joshua R. Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added results, linear regression, neural nets. Fixed typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02999v2</id>
    <updated>2018-12-12T06:48:21Z</updated>
    <published>2018-07-09T09:19:46Z</published>
    <title>Decreasing the size of the Restricted Boltzmann machine</title>
    <summary>  We propose a method to decrease the number of hidden units of the restricted
Boltzmann machine while avoiding decrease of the performance measured by the
Kullback-Leibler divergence. Then, we demonstrate our algorithm by using
numerical simulations.
</summary>
    <author>
      <name>Yohei Saito</name>
    </author>
    <author>
      <name>Takuya Kato</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02999v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02999v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04162v3</id>
    <updated>2018-10-04T20:26:11Z</updated>
    <published>2018-07-11T14:39:17Z</published>
    <title>TherML: Thermodynamics of Machine Learning</title>
    <summary>  In this work we offer a framework for reasoning about a wide class of
existing objectives in machine learning. We develop a formal correspondence
between this work and thermodynamics and discuss its implications.
</summary>
    <author>
      <name>Alexander A. Alemi</name>
    </author>
    <author>
      <name>Ian Fischer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the ICML 2018 workshop on Theoretical Foundations and
  Applications of Deep Generative Models</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04162v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04162v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06054v1</id>
    <updated>2018-07-16T18:41:16Z</updated>
    <published>2018-07-16T18:41:16Z</published>
    <title>On the Information Theoretic Distance Measures and Bidirectional
  Helmholtz Machines</title>
    <summary>  By establishing a connection between bi-directional Helmholtz machines and
information theory, we propose a generalized Helmholtz machine. Theoretical and
experimental results show that given \textit{shallow} architectures, the
generalized model outperforms the previous ones substantially.
</summary>
    <author>
      <name>Mahdi Azarafrooz</name>
    </author>
    <author>
      <name>Xuan Zhao</name>
    </author>
    <author>
      <name>Sepehr Akhavan-Masouleh</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06540v1</id>
    <updated>2018-07-17T16:35:51Z</updated>
    <published>2018-07-17T16:35:51Z</published>
    <title>Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try
  After Deep Learning</title>
    <summary>  We found an easy and quick post-learning method named "Icing on the Cake" to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
</summary>
    <author>
      <name>Tomohiko Konno</name>
    </author>
    <author>
      <name>Michiaki Iwazume</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00813v1</id>
    <updated>2019-12-31T02:29:49Z</updated>
    <published>2019-12-31T02:29:49Z</published>
    <title>Computing L1 Straight-Line Fits to Data (Part 1)</title>
    <summary>  The initial remarks in this technical report are primarily for those not
familiar with the properties of L1 approximation, but the remainder of the
report should also interest readers who are already acquainted with the inner
workings of L1 algorithms.
</summary>
    <author>
      <name>Ian Barrodale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The report is 25 pages long and contains more than two dozen figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04942v2</id>
    <updated>2020-02-24T19:13:28Z</updated>
    <published>2020-01-14T17:56:16Z</published>
    <title>Private Machine Learning via Randomised Response</title>
    <summary>  We introduce a general learning framework for private machine learning based
on randomised response. Our assumption is that all actors are potentially
adversarial and as such we trust only to release a single noisy version of an
individual's datapoint. We discuss a general approach that forms a consistent
way to estimate the true underlying machine learning model and demonstrate this
in the case of logistic regression.
</summary>
    <author>
      <name>David Barber</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04942v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04942v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09005v1</id>
    <updated>2020-01-24T13:43:39Z</updated>
    <published>2020-01-24T13:43:39Z</published>
    <title>Theoretically Expressive and Edge-aware Graph Learning</title>
    <summary>  We propose a new Graph Neural Network that combines recent advancements in
the field. We give theoretical contributions by proving that the model is
strictly more general than the Graph Isomorphism Network and the Gated Graph
Neural Network, as it can approximate the same functions and deal with
arbitrary edge values. Then, we show how a single node information can flow
through the graph unchanged.
</summary>
    <author>
      <name>Federico Errica</name>
    </author>
    <author>
      <name>Davide Bacciu</name>
    </author>
    <author>
      <name>Alessio Micheli</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09608v1</id>
    <updated>2020-01-27T07:26:12Z</updated>
    <published>2020-01-27T07:26:12Z</published>
    <title>Some Insights into Lifelong Reinforcement Learning Systems</title>
    <summary>  A lifelong reinforcement learning system is a learning system that has the
ability to learn through trail-and-error interaction with the environment over
its lifetime. In this paper, I give some arguments to show that the traditional
reinforcement learning paradigm fails to model this type of learning system.
Some insights into lifelong reinforcement learning are provided, along with a
simplistic prototype lifelong reinforcement learning system.
</summary>
    <author>
      <name>Changjian Li</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09771v1</id>
    <updated>2020-01-07T15:31:16Z</updated>
    <published>2020-01-07T15:31:16Z</published>
    <title>Moment-Matching Conditions for Exponential Families with Conditioning or
  Hidden Data</title>
    <summary>  Maximum likelihood learning with exponential families leads to
moment-matching of the sufficient statistics, a classic result. This can be
generalized to conditional exponential families and/or when there are hidden
data. This document gives a first-principles explanation of these generalized
moment-matching conditions, along with a self-contained derivation.
</summary>
    <author>
      <name>Justin Domke</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01599v1</id>
    <updated>2020-03-03T15:34:54Z</updated>
    <published>2020-03-03T15:34:54Z</published>
    <title>VQ-DRAW: A Sequential Discrete VAE</title>
    <summary>  In this paper, I present VQ-DRAW, an algorithm for learning compact discrete
representations of data. VQ-DRAW leverages a vector quantization effect to
adapt the sequential generation scheme of DRAW to discrete latent variables. I
show that VQ-DRAW can effectively learn to compress images from a variety of
common datasets, as well as generate realistic samples from these datasets with
no help from an autoregressive prior.
</summary>
    <author>
      <name>Alex Nichol</name>
    </author>
    <link href="http://arxiv.org/abs/2003.01599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06259v1</id>
    <updated>2020-03-13T13:14:19Z</updated>
    <published>2020-03-13T13:14:19Z</published>
    <title>Taylor Expansion Policy Optimization</title>
    <summary>  In this work, we investigate the application of Taylor expansions in
reinforcement learning. In particular, we propose Taylor expansion policy
optimization, a policy optimization formalism that generalizes prior work
(e.g., TRPO) as a first-order special case. We also show that Taylor expansions
intimately relate to off-policy evaluation. Finally, we show that this new
formulation entails modifications which improve the performance of several
state-of-the-art distributed algorithms.
</summary>
    <author>
      <name>Yunhao Tang</name>
    </author>
    <author>
      <name>Michal Valko</name>
    </author>
    <author>
      <name>Rémi Munos</name>
    </author>
    <link href="http://arxiv.org/abs/2003.06259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06868v2</id>
    <updated>2020-05-25T04:24:18Z</updated>
    <published>2020-03-15T17:00:37Z</published>
    <title>Causality-based Explanation of Classification Outcomes</title>
    <summary>  We propose a simple definition of an explanation for the outcome of a
classifier based on concepts from causality. We compare it with previously
proposed notions of explanation, and study their complexity. We conduct an
experimental evaluation with two real datasets from the financial domain.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Jordan Li</name>
    </author>
    <author>
      <name>Maximilian Schleich</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <author>
      <name>Zografoula Vagena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08051v3</id>
    <updated>2021-12-24T11:32:40Z</updated>
    <published>2020-03-18T05:27:28Z</published>
    <title>Unsupervised Domain Adaptation Through Transferring both the
  Source-Knowledge and Target-Relatedness Simultaneously</title>
    <summary>  Unsupervised domain adaptation (UDA) is an emerging research topic in the
field of machine learning and pattern recognition, which aims to help the
learning of unlabeled target domain by transferring knowledge from the source
domain.
</summary>
    <author>
      <name>Qing Tian</name>
    </author>
    <author>
      <name>Yanan Zhu</name>
    </author>
    <author>
      <name>Chuang Ma</name>
    </author>
    <author>
      <name>Meng Cao</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08051v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08051v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11958v2</id>
    <updated>2020-06-11T11:51:53Z</updated>
    <published>2020-03-26T14:55:16Z</published>
    <title>StrokeCoder: Path-Based Image Generation from Single Examples using
  Transformers</title>
    <summary>  This paper demonstrates how a Transformer Neural Network can be used to learn
a Generative Model from a single path-based example image. We further show how
a data set can be generated from the example image and how the model can be
used to generate a large set of deviated images, which still represent the
original image's style and concept.
</summary>
    <author>
      <name>Sabine Wieluch</name>
    </author>
    <author>
      <name>Friedhelm Schwenker</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12895v1</id>
    <updated>2020-03-28T21:45:42Z</updated>
    <published>2020-03-28T21:45:42Z</published>
    <title>Memorizing Gaussians with no over-parameterizaion via gradient decent on
  neural networks</title>
    <summary>  We prove that a single step of gradient decent over depth two network, with
$q$ hidden neurons, starting from orthogonal initialization, can memorize
$\Omega\left(\frac{dq}{\log^4(d)}\right)$ independent and randomly labeled
Gaussians in $\mathbb{R}^d$. The result is valid for a large class of
activation functions, which includes the absolute value.
</summary>
    <author>
      <name>Amit Daniely</name>
    </author>
    <link href="http://arxiv.org/abs/2003.12895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.06037v1</id>
    <updated>2020-03-29T00:45:01Z</updated>
    <published>2020-03-29T00:45:01Z</published>
    <title>Prediction of properties of steel alloys</title>
    <summary>  We present a study of possible predictors based on four supervised machine
learning models for the prediction of four mechanical properties of the main
industrially used steels. The results were obtained from an experimental
database available in the literature which were used as input to train and
evaluate the models.
</summary>
    <author>
      <name>Ciro Javier Diaz Penedo</name>
    </author>
    <author>
      <name>Lucas Leonardo Silveira Costa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Portuguese</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.06037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05276v2</id>
    <updated>2021-09-13T13:37:31Z</updated>
    <published>2020-05-11T17:21:23Z</published>
    <title>CupNet -- Pruning a network for geometric data</title>
    <summary>  Using data from a simulated cup drawing process, we demonstrate how the
inherent geometrical structure of cup meshes can be used to effectively prune
an artificial neural network in a straightforward way.
</summary>
    <author>
      <name>Raoul Heese</name>
    </author>
    <author>
      <name>Lukas Morand</name>
    </author>
    <author>
      <name>Dirk Helm</name>
    </author>
    <author>
      <name>Michael Bortz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-86380-7_3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-86380-7_3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning - ICANN 2021, pp
  29-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.05276v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05276v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07652v1</id>
    <updated>2020-05-15T17:13:54Z</updated>
    <published>2020-05-15T17:13:54Z</published>
    <title>Efficiently Learning Adversarially Robust Halfspaces with Noise</title>
    <summary>  We study the problem of learning adversarially robust halfspaces in the
distribution-independent setting. In the realizable setting, we provide
necessary and sufficient conditions on the adversarial perturbation sets under
which halfspaces are efficiently robustly learnable. In the presence of random
label noise, we give a simple computationally efficient algorithm for this
problem with respect to any $\ell_p$-perturbation.
</summary>
    <author>
      <name>Omar Montasser</name>
    </author>
    <author>
      <name>Surbhi Goel</name>
    </author>
    <author>
      <name>Ilias Diakonikolas</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12186v1</id>
    <updated>2020-05-25T15:57:22Z</updated>
    <published>2020-05-25T15:57:22Z</published>
    <title>Learnability of Timescale Graphical Event Models</title>
    <summary>  This technical report tries to fill a gap in current literature on Timescale
Graphical Event Models. I propose and evaluate different heuristics to
determine hyper-parameters during the structure learning algorithm and refine
an existing distance measure. A comprehensive benchmark on synthetic data will
be conducted allowing conclusions about the applicability of the different
heuristics.
</summary>
    <author>
      <name>Philipp Behrendt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.00697v1</id>
    <updated>2020-11-02T03:09:23Z</updated>
    <published>2020-11-02T03:09:23Z</published>
    <title>Time Series Forecasting with Stacked Long Short-Term Memory Networks</title>
    <summary>  Long Short-Term Memory (LSTM) networks are often used to capture temporal
dependency patterns. By stacking multi-layer LSTM networks, it can capture even
more complex patterns. This paper explores the effectiveness of applying
stacked LSTM networks in the time series prediction domain, specifically, the
traffic volume forecasting. Being able to predict traffic volume more
accurately can result in better planning, thus greatly reduce the operation
cost and improve overall efficiency.
</summary>
    <author>
      <name>Frank Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.00697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.00697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09083v1</id>
    <updated>2020-11-18T04:36:03Z</updated>
    <published>2020-11-18T04:36:03Z</published>
    <title>Weighted Entropy Modification for Soft Actor-Critic</title>
    <summary>  We generalize the existing principle of the maximum Shannon entropy in
reinforcement learning (RL) to weighted entropy by characterizing the
state-action pairs with some qualitative weights, which can be connected with
prior knowledge, experience replay, and evolution process of the policy. We
propose an algorithm motivated for self-balancing exploration with the
introduced weight function, which leads to state-of-the-art performance on
Mujoco tasks despite its simplicity in implementation.
</summary>
    <author>
      <name>Yizhou Zhao</name>
    </author>
    <author>
      <name>Song-Chun Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2011.09083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.11554v1</id>
    <updated>2020-11-19T22:06:18Z</updated>
    <published>2020-11-19T22:06:18Z</published>
    <title>ML4H Abstract Track 2020</title>
    <summary>  A collection of the accepted abstracts for the Machine Learning for Health
(ML4H) workshop at NeurIPS 2020. This index is not complete, as some accepted
abstracts chose to opt-out of inclusion.
</summary>
    <author>
      <name>Emily Alsentzer</name>
    </author>
    <author>
      <name>Matthew B. A. McDermott</name>
    </author>
    <author>
      <name>Fabian Falck</name>
    </author>
    <author>
      <name>Suproteem K. Sarkar</name>
    </author>
    <author>
      <name>Subhrajit Roy</name>
    </author>
    <author>
      <name>Stephanie L. Hyland</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.11619v1</id>
    <updated>2020-11-23T18:49:36Z</updated>
    <published>2020-11-23T18:49:36Z</published>
    <title>Neural collapse with unconstrained features</title>
    <summary>  Neural collapse is an emergent phenomenon in deep learning that was recently
discovered by Papyan, Han and Donoho. We propose a simple "unconstrained
features model" in which neural collapse also emerges empirically. By studying
this model, we provide some explanation for the emergence of neural collapse in
terms of the landscape of empirical risk.
</summary>
    <author>
      <name>Dustin G. Mixon</name>
    </author>
    <author>
      <name>Hans Parshall</name>
    </author>
    <author>
      <name>Jianzong Pi</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.14251v2</id>
    <updated>2021-06-05T20:38:20Z</updated>
    <published>2020-11-29T01:37:58Z</published>
    <title>Importance Weight Estimation and Generalization in Domain Adaptation
  under Label Shift</title>
    <summary>  We study generalization under labeled shift for categorical and general
normed label spaces. We propose a series of methods to estimate the importance
weights from labeled source to unlabeled target domain and provide confidence
bounds for these estimators. We deploy these estimators and provide
generalization bounds in the unlabeled target domain.
</summary>
    <author>
      <name>Kamyar Azizzadenesheli</name>
    </author>
    <link href="http://arxiv.org/abs/2011.14251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00995v2</id>
    <updated>2022-11-30T06:56:38Z</updated>
    <published>2022-03-02T10:01:55Z</published>
    <title>Learning Efficiently Function Approximation for Contextual MDP</title>
    <summary>  We study learning contextual MDPs using a function approximation for both the
rewards and the dynamics. We consider both the case that the dynamics dependent
or independent of the context. For both models we derive polynomial sample and
time complexity (assuming an efficient ERM oracle). Our methodology gives a
general reduction from learning contextual MDP to supervised learning.
</summary>
    <author>
      <name>Orin Levy</name>
    </author>
    <author>
      <name>Yishay Mansour</name>
    </author>
    <link href="http://arxiv.org/abs/2203.00995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01603v1</id>
    <updated>2022-03-03T09:57:50Z</updated>
    <published>2022-03-03T09:57:50Z</published>
    <title>AdaFamily: A family of Adam-like adaptive gradient methods</title>
    <summary>  We propose AdaFamily, a novel method for training deep neural networks. It is
a family of adaptive gradient methods and can be interpreted as sort of a blend
of the optimization algorithms Adam, AdaBelief and AdaMomentum. We perform
experiments on standard datasets for image classification, demonstrating that
our proposed method outperforms these algorithms.
</summary>
    <author>
      <name>Hannes Fassold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted for ISPR 2022 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.01603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03189v1</id>
    <updated>2022-03-07T07:57:18Z</updated>
    <published>2022-03-07T07:57:18Z</published>
    <title>A comparative study of several ADPCM schemes with linear and nonlinear
  prediction</title>
    <summary>  In this paper we compare several ADPCM schemes with nonlinear prediction
based on neural nets with the classical ADPCM schemes based on several linear
prediction schemes. Main studied variations of the ADPCM scheme with adaptive
quantization (2 to 5 bits) are: -forward vs backward -sample adaptive vs block
adaptive
</summary>
    <author>
      <name>Oscar Oliva</name>
    </author>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6 European Conference on Speech Communication and Technology
  EUROSPEECH 99, Budapest, Hungary, September 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.03189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13503v1</id>
    <updated>2022-03-25T08:39:51Z</updated>
    <published>2022-03-25T08:39:51Z</published>
    <title>Supplemental Material: Lifelong Generative Modelling Using Dynamic
  Expansion Graph Model</title>
    <summary>  In this article, we provide the appendix for Lifelong Generative Modelling
Using Dynamic Expansion Graph Model. This appendix includes additional visual
results as well as the numerical results on the challenging datasets. In
addition, we also provide detailed proofs for the proposed theoretical analysis
framework. The source code can be found in
https://github.com/dtuzi123/Expansion-Graph-Model.
</summary>
    <author>
      <name>Fei Ye</name>
    </author>
    <author>
      <name>Adrian G. Bors</name>
    </author>
    <link href="http://arxiv.org/abs/2203.13503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4234v2</id>
    <updated>2012-05-25T10:24:35Z</updated>
    <published>2012-05-19T08:16:21Z</published>
    <title>Visualization of features of a series of measurements with
  one-dimensional cellular structure</title>
    <summary>  This paper describes the method of visualization of periodic constituents and
instability areas in series of measurements, being based on the algorithm of
smoothing out and concept of one-dimensional cellular automata. A method can be
used at the analysis of temporal series, related to the volumes of thematic
publications in web-space.
</summary>
    <author>
      <name>D. V. Lande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, russian language</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4234v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4234v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03370v1</id>
    <updated>2015-10-12T17:14:44Z</updated>
    <published>2015-10-12T17:14:44Z</published>
    <title>Asymptotic Logical Uncertainty and The Benford Test</title>
    <summary>  We give an algorithm A which assigns probabilities to logical sentences. For
any simple infinite sequence of sentences whose truth-values appear
indistinguishable from a biased coin that outputs "true" with probability p, we
have that the sequence of probabilities that A assigns to these sentences
converges to p.
</summary>
    <author>
      <name>Scott Garrabrant</name>
    </author>
    <author>
      <name>Siddharth Bhaskar</name>
    </author>
    <author>
      <name>Abram Demski</name>
    </author>
    <author>
      <name>Joanna Garrabrant</name>
    </author>
    <author>
      <name>George Koleszarik</name>
    </author>
    <author>
      <name>Evan Lloyd</name>
    </author>
    <link href="http://arxiv.org/abs/1510.03370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.06567v1</id>
    <updated>2015-10-22T10:19:52Z</updated>
    <published>2015-10-22T10:19:52Z</published>
    <title>Generalized conditional gradient: analysis of convergence and
  applications</title>
    <summary>  The objectives of this technical report is to provide additional results on
the generalized conditional gradient methods introduced by Bredies et al.
[BLM05]. Indeed , when the objective function is smooth, we provide a novel
certificate of optimality and we show that the algorithm has a linear
convergence rate. Applications of this algorithm are also discussed.
</summary>
    <author>
      <name>Alain Rakotomamonjy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Rémi Flamary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGRANGE, OCA</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Courty</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">OBELIX</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1510.06567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07303v1</id>
    <updated>2015-10-25T21:04:12Z</updated>
    <published>2015-10-25T21:04:12Z</published>
    <title>A Framework for Distributed Deep Learning Layer Design in Python</title>
    <summary>  In this paper, a framework for testing Deep Neural Network (DNN) design in
Python is presented. First, big data, machine learning (ML), and Artificial
Neural Networks (ANNs) are discussed to familiarize the reader with the
importance of such a system. Next, the benefits and detriments of implementing
such a system in Python are presented. Lastly, the specifics of the system are
explained, and some experimental results are presented to prove the
effectiveness of the system.
</summary>
    <author>
      <name>Clay McLeod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07641v2</id>
    <updated>2017-03-21T21:28:55Z</updated>
    <published>2015-10-26T20:18:56Z</published>
    <title>Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks</title>
    <summary>  We present a novel application of LSTM recurrent neural networks to
multilabel classification of diagnoses given variable-length time series of
clinical measurements. Our method outperforms a strong baseline on a variety of
metrics.
</summary>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <author>
      <name>David C. Kale</name>
    </author>
    <author>
      <name>Randall C. Wetzel</name>
    </author>
    <link href="http://arxiv.org/abs/1510.07641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06120v1</id>
    <updated>2017-02-20T10:09:02Z</updated>
    <published>2017-02-20T10:09:02Z</published>
    <title>On the Consistency of $k$-means++ algorithm</title>
    <summary>  We prove in this paper that the expected value of the objective function of
the $k$-means++ algorithm for samples converges to population expected value.
As $k$-means++, for samples, provides with constant factor approximation for
$k$-means objectives, such an approximation can be achieved for the population
with increase of the sample size.
  This result is of potential practical relevance when one is considering using
subsampling when clustering large data sets (large data bases).
</summary>
    <author>
      <name>Mieczysław A. Kłopotek</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06914v3</id>
    <updated>2017-04-08T00:40:26Z</updated>
    <published>2017-02-22T17:54:03Z</published>
    <title>Training a Subsampling Mechanism in Expectation</title>
    <summary>  We describe a mechanism for subsampling sequences and show how to compute its
expected output so that it can be trained with standard backpropagation. We
test this approach on a simple toy problem and discuss its shortcomings.
</summary>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Dieterich Lawson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera-ready version. Includes additional figures in an appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06914v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06914v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08398v2</id>
    <updated>2017-06-08T23:45:25Z</updated>
    <published>2017-02-27T17:46:30Z</published>
    <title>McGan: Mean and Covariance Feature Matching GAN</title>
    <summary>  We introduce new families of Integral Probability Metrics (IPM) for training
Generative Adversarial Networks (GAN). Our IPMs are based on matching
statistics of distributions embedded in a finite dimensional feature space.
Mean and covariance feature matching IPMs allow for stable training of GANs,
which we will call McGan. McGan minimizes a meaningful loss between
distributions.
</summary>
    <author>
      <name>Youssef Mroueh</name>
    </author>
    <author>
      <name>Tom Sercu</name>
    </author>
    <author>
      <name>Vaibhava Goel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; published at ICML 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08398v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08398v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08553v2</id>
    <updated>2017-06-09T00:39:57Z</updated>
    <published>2017-02-27T21:59:24Z</published>
    <title>Diameter-Based Active Learning</title>
    <summary>  To date, the tightest upper and lower-bounds for the active learning of
general concept classes have been in terms of a parameter of the learning
problem called the splitting index. We provide, for the first time, an
efficient algorithm that is able to realize this upper bound, and we
empirically demonstrate its good performance.
</summary>
    <author>
      <name>Christopher Tosh</name>
    </author>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08553v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08553v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03946v1</id>
    <updated>2019-05-10T05:26:03Z</updated>
    <published>2019-05-10T05:26:03Z</published>
    <title>Credit Scoring for Micro-Loans</title>
    <summary>  Credit Scores are ubiquitous and instrumental for loan providers and
regulators. In this paper we showcase how micro-loan credit system can be
developed in real setting. We show what challenges arise and discuss solutions.
Particularly, we are concerned about model interpretability and data quality.
In the final section, we introduce semi-supervised algorithm that aids model
development and evaluate its performance
</summary>
    <author>
      <name>Nikolay Dubina</name>
    </author>
    <author>
      <name>Dasom Kang</name>
    </author>
    <author>
      <name>Alex Suh</name>
    </author>
    <link href="http://arxiv.org/abs/1905.03946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07187v1</id>
    <updated>2019-05-17T10:28:06Z</updated>
    <published>2019-05-17T10:28:06Z</published>
    <title>An Essay on Optimization Mystery of Deep Learning</title>
    <summary>  Despite the huge empirical success of deep learning, theoretical
understanding of neural networks learning process is still lacking. This is the
reason, why some of its features seem "mysterious". We emphasize two mysteries
of deep learning: generalization mystery, and optimization mystery. In this
essay we review and draw connections between several selected works concerning
the latter.
</summary>
    <author>
      <name>Eugene Golikov</name>
    </author>
    <link href="http://arxiv.org/abs/1905.07187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01394v1</id>
    <updated>2019-08-04T20:29:28Z</updated>
    <published>2019-08-04T20:29:28Z</published>
    <title>Learning to Transport with Neural Networks</title>
    <summary>  We compare several approaches to learn an Optimal Map, represented as a
neural network, between probability distributions. The approaches fall into two
categories: ``Heuristics'' and approaches with a more sound mathematical
justification, motivated by the dual of the Kantorovitch problem. Among the
algorithms we consider a novel approach involving dynamic flows and reductions
of Optimal Transport to supervised learning.
</summary>
    <author>
      <name>Andrea Schioppa</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06973v1</id>
    <updated>2019-08-19T09:47:22Z</updated>
    <published>2019-08-19T09:47:22Z</published>
    <title>Reinforcement Learning Applications</title>
    <summary>  We start with a brief introduction to reinforcement learning (RL), about its
successful stories, basics, an example, issues, the ICML 2019 Workshop on RL
for Real Life, how to use it, study material and an outlook. Then we discuss a
selection of RL applications, including recommender systems, computer systems,
energy, finance, healthcare, robotics, and transportation.
</summary>
    <author>
      <name>Yuxi Li</name>
    </author>
    <link href="http://arxiv.org/abs/1908.06973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04679v1</id>
    <updated>2020-02-11T21:00:03Z</updated>
    <published>2020-02-11T21:00:03Z</published>
    <title>IPBoost -- Non-Convex Boosting via Integer Programming</title>
    <summary>  Recently non-convex optimization approaches for solving machine learning
problems have gained significant attention. In this paper we explore non-convex
boosting in classification by means of integer programming and demonstrate
real-world practicability of the approach while circumventing shortcomings of
convex boosting approaches. We report results that are comparable to or better
than the current state-of-the-art.
</summary>
    <author>
      <name>Marc E. Pfetsch</name>
    </author>
    <author>
      <name>Sebastian Pokutta</name>
    </author>
    <link href="http://arxiv.org/abs/2002.04679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05202v1</id>
    <updated>2020-02-12T19:57:13Z</updated>
    <published>2020-02-12T19:57:13Z</published>
    <title>GLU Variants Improve Transformer</title>
    <summary>  Gated Linear Units (arXiv:1612.08083) consist of the component-wise product
of two linear projections, one of which is first passed through a sigmoid
function. Variations on GLU are possible, using different nonlinear (or even
linear) functions in place of sigmoid. We test these variants in the
feed-forward sublayers of the Transformer (arXiv:1706.03762)
sequence-to-sequence model, and find that some of them yield quality
improvements over the typically-used ReLU or GELU activations.
</summary>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05380v1</id>
    <updated>2020-02-13T07:49:22Z</updated>
    <published>2020-02-13T07:49:22Z</published>
    <title>CEB Improves Model Robustness</title>
    <summary>  We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve
model robustness. CEB is an easy strategy to implement and works in tandem with
data augmentation procedures. We report results of a large scale adversarial
robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions
Benchmark, ImageNet-A, and PGD attacks.
</summary>
    <author>
      <name>Ian Fischer</name>
    </author>
    <author>
      <name>Alexander A. Alemi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e22101081</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e22101081" rel="related"/>
    <link href="http://arxiv.org/abs/2002.05380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09814v1</id>
    <updated>2020-02-23T03:24:03Z</updated>
    <published>2020-02-23T03:24:03Z</published>
    <title>Survey Bandits with Regret Guarantees</title>
    <summary>  We consider a variant of the contextual bandit problem. In standard
contextual bandits, when a user arrives we get the user's complete feature
vector and then assign a treatment (arm) to that user. In a number of
applications (like healthcare), collecting features from users can be costly.
To address this issue, we propose algorithms that avoid needless feature
collection while maintaining strong regret guarantees.
</summary>
    <author>
      <name>Sanath Kumar Krishnamurthy</name>
    </author>
    <author>
      <name>Susan Athey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.09814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12446v1</id>
    <updated>2020-02-27T21:18:06Z</updated>
    <published>2020-02-27T21:18:06Z</published>
    <title>Provably Efficient Third-Person Imitation from Offline Observation</title>
    <summary>  Domain adaptation in imitation learning represents an essential step towards
improving generalizability. However, even in the restricted setting of
third-person imitation where transfer is between isomorphic Markov Decision
Processes, there are no strong guarantees on the performance of transferred
policies. We present problem-dependent, statistical learning guarantees for
third-person imitation from observation in an offline setting, and a lower
bound on performance in the online setting.
</summary>
    <author>
      <name>Aaron Zweig</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <link href="http://arxiv.org/abs/2002.12446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01950v3</id>
    <updated>2021-02-19T15:42:38Z</updated>
    <published>2020-09-24T01:34:42Z</published>
    <title>Torchattacks: A PyTorch Repository for Adversarial Attacks</title>
    <summary>  Torchattacks is a PyTorch library that contains adversarial attacks to
generate adversarial examples and to verify the robustness of deep learning
models. The code can be found at
https://github.com/Harry24k/adversarial-attacks-pytorch.
</summary>
    <author>
      <name>Hoki Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01950v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01950v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.05636v2</id>
    <updated>2021-01-12T14:17:57Z</updated>
    <published>2020-10-12T12:15:54Z</published>
    <title>k-simplex2vec: a simplicial extension of node2vec</title>
    <summary>  We present a novel method of associating Euclidean features to simplicial
complexes, providing a way to use them as input to statistical and machine
learning tools. This method extends the node2vec algorithm to simplices of
higher dimensions, providing insight into the structure of a simplicial
complex, or into the higher-order interactions in a graph.
</summary>
    <author>
      <name>Celia Hacker</name>
    </author>
    <link href="http://arxiv.org/abs/2010.05636v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05636v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09495v1</id>
    <updated>2020-10-15T19:40:35Z</updated>
    <published>2020-10-15T19:40:35Z</published>
    <title>Blending Search and Discovery: Tag-Based Query Refinement with
  Contextual Reinforcement Learning</title>
    <summary>  We tackle tag-based query refinement as a mobile-friendly alternative to
standard facet search. We approach the inference challenge with reinforcement
learning, and propose a deep contextual bandit that can be efficiently scaled
in a multi-tenant SaaS scenario.
</summary>
    <author>
      <name>Bingqing Yu</name>
    </author>
    <author>
      <name>Jacopo Tagliabue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at EComNLP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09995v2</id>
    <updated>2021-05-11T11:22:13Z</updated>
    <published>2020-10-20T04:02:47Z</published>
    <title>POND: Pessimistic-Optimistic oNline Dispatching</title>
    <summary>  This paper considers constrained online dispatching with unknown arrival,
reward and constraint distributions. We propose a novel online dispatching
algorithm, named POND, standing for Pessimistic-Optimistic oNline Dispatching,
which achieves $O(\sqrt{T})$ regret and $O(1)$ constraint violation. Both
bounds are sharp. Our experiments on synthetic and real datasets show that POND
achieves low regret with minimal constraint violations.
</summary>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Bin Li</name>
    </author>
    <author>
      <name>Pengyi Shi</name>
    </author>
    <author>
      <name>Lei Ying</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12711v1</id>
    <updated>2020-10-23T23:41:03Z</updated>
    <published>2020-10-23T23:41:03Z</published>
    <title>On Convergence and Generalization of Dropout Training</title>
    <summary>  We study dropout in two-layer neural networks with rectified linear unit
(ReLU) activations. Under mild overparametrization and assuming that the
limiting kernel can separate the data distribution with a positive margin, we
show that dropout training with logistic loss achieves $\epsilon$-suboptimality
in test error in $O(1/\epsilon)$ iterations.
</summary>
    <author>
      <name>Poorya Mianjy</name>
    </author>
    <author>
      <name>Raman Arora</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS), 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.12711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15552v1</id>
    <updated>2020-10-08T15:57:20Z</updated>
    <published>2020-10-08T15:57:20Z</published>
    <title>Successive Halving Top-k Operator</title>
    <summary>  We propose a differentiable successive halving method of relaxing the top-k
operator, rendering gradient-based optimization possible. The need to perform
softmax iteratively on the entire vector of scores is avoided by using a
tournament-style selection. As a result, a much better approximation of top-k
with lower computational cost is achieved compared to the previous approach.
</summary>
    <author>
      <name>Michał Pietruszka</name>
    </author>
    <author>
      <name>Łukasz Borchmann</name>
    </author>
    <author>
      <name>Filip Graliński</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01681v1</id>
    <updated>2021-04-04T20:09:20Z</updated>
    <published>2021-04-04T20:09:20Z</published>
    <title>Faster Convolution Inference Through Using Pre-Calculated Lookup Tables</title>
    <summary>  Low-cardinality activations permit an algorithm based on fetching the
inference values from pre-calculated lookup tables instead of calculating them
every time. This algorithm can have extensions, some of which offer abilities
beyond those of the currently used algorithms. It also allows for a simpler and
more effective CNN-specialized hardware.
</summary>
    <author>
      <name>Grigor Gatchev</name>
    </author>
    <author>
      <name>Valentin Mollov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.01681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02452v1</id>
    <updated>2021-04-06T12:16:53Z</updated>
    <published>2021-04-06T12:16:53Z</published>
    <title>A Latent space solver for PDE generalization</title>
    <summary>  In this work we propose a hybrid solver to solve partial differential
equation (PDE)s in the latent space. The solver uses an iterative inferencing
strategy combined with solution initialization to improve generalization of PDE
solutions. The solver is tested on an engineering case and the results show
that it can generalize well to several PDE conditions.
</summary>
    <author>
      <name>Rishikesh Ranade</name>
    </author>
    <author>
      <name>Chris Hill</name>
    </author>
    <author>
      <name>Haiyang He</name>
    </author>
    <author>
      <name>Amir Maleki</name>
    </author>
    <author>
      <name>Jay Pathak</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04812v2</id>
    <updated>2021-06-22T19:59:54Z</updated>
    <published>2021-06-09T05:11:33Z</published>
    <title>Phase Retrieval using Single-Instance Deep Generative Prior</title>
    <summary>  Several deep learning methods for phase retrieval exist, but most of them
fail on realistic data without precise support information. We propose a novel
method based on single-instance deep generative prior that works well on
complex-valued crystal data.
</summary>
    <author>
      <name>Kshitij Tayal</name>
    </author>
    <author>
      <name>Raunak Manekar</name>
    </author>
    <author>
      <name>Zhong Zhuang</name>
    </author>
    <author>
      <name>David Yang</name>
    </author>
    <author>
      <name>Vipin Kumar</name>
    </author>
    <author>
      <name>Felix Hofmann</name>
    </author>
    <author>
      <name>Ju Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2106.04812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.05840v1</id>
    <updated>2021-06-08T04:20:00Z</updated>
    <published>2021-06-08T04:20:00Z</published>
    <title>A Bagging and Boosting Based Convexly Combined Optimum Mixture
  Probabilistic Model</title>
    <summary>  Unlike previous studies on mixture distributions, a bagging and boosting
based convexly combined mixture probabilistic model has been suggested. This
model is a result of iteratively searching for obtaining the optimum
probabilistic model that provides the maximum p value.
</summary>
    <author>
      <name>Mian Arif Shams Adnan</name>
    </author>
    <author>
      <name>H. M. Miraz Mahmud</name>
    </author>
    <link href="http://arxiv.org/abs/2106.05840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06536v1</id>
    <updated>2021-06-10T11:53:38Z</updated>
    <published>2021-06-10T11:53:38Z</published>
    <title>Unsupervised Neural Hidden Markov Models with a Continuous latent state
  space</title>
    <summary>  We introduce a new procedure to neuralize unsupervised Hidden Markov Models
in the continuous case. This provides higher flexibility to solve problems with
underlying latent variables. This approach is evaluated on both synthetic and
real data. On top of generating likely model parameters with comparable
performances to off-the-shelf neural architecture (LSTMs, GRUs,..), the
obtained results are easily interpretable.
</summary>
    <author>
      <name>Firas Jarboui</name>
    </author>
    <author>
      <name>Vianney Perchet</name>
    </author>
    <link href="http://arxiv.org/abs/2106.06536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07032v1</id>
    <updated>2021-06-13T15:58:13Z</updated>
    <published>2021-06-13T15:58:13Z</published>
    <title>Category Theory in Machine Learning</title>
    <summary>  Over the past two decades machine learning has permeated almost every realm
of technology. At the same time, many researchers have begun using category
theory as a unifying language, facilitating communication between different
scientific disciplines. It is therefore unsurprising that there is a burgeoning
interest in applying category theory to machine learning. We aim to document
the motivations, goals and common themes across these applications. We touch on
gradient-based learning, probability, and equivariant learning.
</summary>
    <author>
      <name>Dan Shiebler</name>
    </author>
    <author>
      <name>Bruno Gavranović</name>
    </author>
    <author>
      <name>Paul Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/2106.07032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08206v1</id>
    <updated>2021-06-15T15:10:24Z</updated>
    <published>2021-06-15T15:10:24Z</published>
    <title>Hypergraph Dissimilarity Measures</title>
    <summary>  In this paper, we propose two novel approaches for hypergraph comparison. The
first approach transforms the hypergraph into a graph representation for use of
standard graph dissimilarity measures. The second approach exploits the
mathematics of tensors to intrinsically capture multi-way relations. For each
approach, we present measures that assess hypergraph dissimilarity at a
specific scale or provide a more holistic multi-scale comparison. We test these
measures on synthetic hypergraphs and apply them to biological datasets.
</summary>
    <author>
      <name>Amit Surana</name>
    </author>
    <author>
      <name>Can Chen</name>
    </author>
    <author>
      <name>Indika Rajapakse</name>
    </author>
    <link href="http://arxiv.org/abs/2106.08206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.10202v1</id>
    <updated>2021-06-18T16:10:23Z</updated>
    <published>2021-06-18T16:10:23Z</published>
    <title>An Investigation into Mini-Batch Rule Learning</title>
    <summary>  We investigate whether it is possible to learn rule sets efficiently in a
network structure with a single hidden layer using iterative refinements over
mini-batches of examples. A first rudimentary version shows an acceptable
performance on all but one dataset, even though it does not yet reach the
performance levels of Ripper.
</summary>
    <author>
      <name>Florian Beck</name>
    </author>
    <author>
      <name>Johannes Fürnkranz</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2nd Workshop on Deep Continuous-Discrete Machine Learning
  (DeCoDeML), ECML-PKDD 2020, Ghent, Belgium</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.10202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12417v2</id>
    <updated>2021-12-13T10:30:13Z</updated>
    <published>2021-06-23T14:11:06Z</published>
    <title>False perfection in machine prediction: Detecting and assessing
  circularity problems in machine learning</title>
    <summary>  This paper is an excerpt of an early version of Chapter 2 of the book
"Validity, Reliability, and Significance. Empirical Methods for NLP and Data
Science", by Stefan Riezler and Michael Hagmann, published in December 2021 by
Morgan &amp; Claypool. Please see the book's homepage at
https://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1688
for a more recent and comprehensive discussion.
</summary>
    <author>
      <name>Michael Hagmann</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <link href="http://arxiv.org/abs/2106.12417v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12417v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01107v1</id>
    <updated>2021-10-03T21:43:02Z</updated>
    <published>2021-10-03T21:43:02Z</published>
    <title>TinyFedTL: Federated Transfer Learning on Tiny Devices</title>
    <summary>  TinyML has rose to popularity in an era where data is everywhere. However,
the data that is in most demand is subject to strict privacy and security
guarantees. In addition, the deployment of TinyML hardware in the real world
has significant memory and communication constraints that traditional ML fails
to address. In light of these challenges, we present TinyFedTL, the first
implementation of federated transfer learning on a resource-constrained
microcontroller.
</summary>
    <author>
      <name>Kavya Kopparapu</name>
    </author>
    <author>
      <name>Eric Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2110.01107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.13649v1</id>
    <updated>2021-10-25T00:59:44Z</updated>
    <published>2021-10-25T00:59:44Z</published>
    <title>An algorithm for the computation of joint Hawkes moments with
  exponential kernel</title>
    <summary>  The purpose of this paper is to present a recursive algorithm and its
implementation in Maple and Mathematica for the computation of joint moments
and cumulants of Hawkes processes with exponential kernels. Numerical results
and computation times are also discussed. Obtaining closed form expressions can
be computationally intensive, as joint fifth cumulant and moment formulas can
be respectively expanded into up to 3,288 and 27,116 summands.
</summary>
    <author>
      <name>Nicolas Privault</name>
    </author>
    <link href="http://arxiv.org/abs/2110.13649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.14945v1</id>
    <updated>2021-10-28T08:32:27Z</updated>
    <published>2021-10-28T08:32:27Z</published>
    <title>Preventing posterior collapse in variational autoencoders for text
  generation via decoder regularization</title>
    <summary>  Variational autoencoders trained to minimize the reconstruction error are
sensitive to the posterior collapse problem, that is the proposal posterior
distribution is always equal to the prior. We propose a novel regularization
method based on fraternal dropout to prevent posterior collapse. We evaluate
our approach using several metrics and observe improvements in all the tested
configurations.
</summary>
    <author>
      <name>Alban Petit</name>
    </author>
    <author>
      <name>Caio Corro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2021 Workshop DGMs Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.14945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.14945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.00945v1</id>
    <updated>2022-01-04T02:45:57Z</updated>
    <published>2022-01-04T02:45:57Z</published>
    <title>An unfeasability view of neural network learning</title>
    <summary>  We define the notion of a continuously differentiable perfect learning
algorithm for multilayer neural network architectures and show that such
algorithms don't exist provided that the length of the data set exceeds the
number of involved parameters and the activation functions are logistic, tanh
or sin.
</summary>
    <author>
      <name>Joos Heintz</name>
    </author>
    <author>
      <name>Hvara Ocar</name>
    </author>
    <author>
      <name>Luis Miguel Pardo</name>
    </author>
    <author>
      <name>Andres Rojas Paredes</name>
    </author>
    <author>
      <name>Enrique Carlos Segura</name>
    </author>
    <link href="http://arxiv.org/abs/2201.00945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07, 68Q32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.06523v1</id>
    <updated>2022-01-17T16:57:53Z</updated>
    <published>2022-01-17T16:57:53Z</published>
    <title>Patterns of near-crash events in a naturalistic driving dataset:
  applying rules mining</title>
    <summary>  This study aims to explore the associations between near-crash events and
road geometry and trip features by investigating a naturalistic driving dataset
and a corresponding roadway inventory dataset using an association rule mining
method.
</summary>
    <author>
      <name>Xiaoqiang Kong</name>
    </author>
    <author>
      <name>Subasish Das</name>
    </author>
    <author>
      <name>Hongmin Zhou</name>
    </author>
    <author>
      <name>Yunlong Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.aap.2021.106346</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.aap.2021.106346" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Accident Analysis &amp; Prevention (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.06523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07933v1</id>
    <updated>2021-12-08T08:54:55Z</updated>
    <published>2021-12-08T08:54:55Z</published>
    <title>One-Step Abductive Multi-Target Learning with Diverse Noisy Label
  Samples</title>
    <summary>  One-step abductive multi-target learning (OSAMTL) was proposed to handle
complex noisy labels. In this paper, giving definition of diverse noisy label
samples (DNLS), we propose one-step abductive multi-target learning with DNLS
(OSAMTL-DNLS) to expand the methodology of original OSAMTL to better handle
complex noisy labels.
</summary>
    <author>
      <name>Yongquan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5pages. arXiv admin note: substantial text overlap with
  arXiv:2110.10325</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.07933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09746v1</id>
    <updated>2022-01-19T15:54:39Z</updated>
    <published>2022-01-19T15:54:39Z</published>
    <title>Reinforcement Learning Textbook</title>
    <summary>  This textbook covers principles behind main modern deep reinforcement
learning algorithms that achieved breakthrough results in many domains from
game AI to robotics. All required theory is explained with proofs using unified
notation and emphasize on the differences between different types of algorithms
and the reasons why they are constructed the way they are.
</summary>
    <author>
      <name>Sergey Ivanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The text is in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.09746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.12830v2</id>
    <updated>2022-02-01T03:31:19Z</updated>
    <published>2022-01-30T14:31:42Z</published>
    <title>Over-smoothing Effect of Graph Convolutional Networks</title>
    <summary>  Over-smoothing is a severe problem which limits the depth of Graph
Convolutional Networks. This article gives a comprehensive analysis of the
mechanism behind Graph Convolutional Networks and the over-smoothing effect.
The article proposes an upper bound for the occurrence of over-smoothing, which
offers insight into the key factors behind over-smoothing. The results
presented in this article successfully explain the feasibility of several
algorithms that alleviate over-smoothing.
</summary>
    <author>
      <name>Fang Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.12830v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.12830v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02354v1</id>
    <updated>2022-02-04T19:25:01Z</updated>
    <published>2022-02-04T19:25:01Z</published>
    <title>A note on the complex and bicomplex valued neural networks</title>
    <summary>  In this paper we first write a proof of the perceptron convergence algorithm
for the complex multivalued neural networks (CMVNNs). Our primary goal is to
formulate and prove the perceptron convergence algorithm for the bicomplex
multivalued neural networks (BMVNNs) and other important results in the theory
of neural networks based on a bicomplex algebra.
</summary>
    <author>
      <name>Daniel Alpay</name>
    </author>
    <author>
      <name>Kamal Diki</name>
    </author>
    <author>
      <name>Mihaela Vajiac</name>
    </author>
    <link href="http://arxiv.org/abs/2202.02354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="30G35, 68Q32, 68T07, 47A57" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.03289v3</id>
    <updated>2022-03-04T06:53:58Z</updated>
    <published>2022-02-07T15:23:37Z</published>
    <title>Approximation error of single hidden layer neural networks with fixed
  weights</title>
    <summary>  This paper provides an explicit formula for the approximation error of single
hidden layer neural networks with two fixed weights.
</summary>
    <author>
      <name>Vugar Ismailov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, an example added, a typo corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.03289v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.03289v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A30, 41A63, 68T05, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05163v2</id>
    <updated>2022-02-15T05:45:50Z</updated>
    <published>2022-02-03T08:30:02Z</published>
    <title>Machine Learning and Data Science: Foundations, Concepts, Algorithms,
  and Tools</title>
    <summary>  Today, data is a fuel for businesses to gain important insights and improve
their performance. There is no industry in the world today that does not use
data. But who will get this insight? Who processes all the raw data? Everything
is done by a data analyst or a data scientist.
</summary>
    <author>
      <name>Milad Vazan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Persian language</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.05163v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05163v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12064v1</id>
    <updated>2022-02-24T12:41:20Z</updated>
    <published>2022-02-24T12:41:20Z</published>
    <title>Interfering Paths in Decision Trees: A Note on Deodata Predictors</title>
    <summary>  A technique for improving the prediction accuracy of decision trees is
proposed. It consists in evaluating the tree's branches in parallel over
multiple paths. The technique enables predictions that are more aligned with
the ones generated by the nearest neighborhood variant of the deodata
algorithms. The technique also enables the hybridization of the decision tree
algorithm with the nearest neighborhood variant.
</summary>
    <author>
      <name>Cristian Alb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.12510.10565</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.12510.10565" rel="related"/>
    <link href="http://arxiv.org/abs/2202.12064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.13866v1</id>
    <updated>2022-02-24T09:09:28Z</updated>
    <published>2022-02-24T09:09:28Z</published>
    <title>N-dimensional nonlinear prediction with MLP</title>
    <summary>  In this paper we propose a Non-Linear Predictive Vector quantizer (PVQ) for
speech coding, based on Multi-Layer Perceptrons. With this scheme we have
improved the results of our previous ADPCM coder with nonlinear prediction, and
we have reduced the bit rate up to 1 bit per sample.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2002 11th European Signal Processing Conference, 2002, pp. 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.13866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.2550v1</id>
    <updated>2013-02-11T17:44:10Z</updated>
    <published>2013-02-11T17:44:10Z</published>
    <title>Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</title>
    <summary>  We derive sublinear regret bounds for undiscounted reinforcement learning in
continuous state space. The proposed algorithm combines state aggregation with
the use of upper confidence bounds for implementing optimism in the face of
uncertainty. Beside the existence of an optimal policy which satisfies the
Poisson equation, the only assumptions made are Holder continuity of rewards
and transition probabilities.
</summary>
    <author>
      <name>Ronald Ortner</name>
    </author>
    <author>
      <name>Daniil Ryabko</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in proceedings of NIPS 2012, pp. 1772--1780</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.2550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.2550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1433v3</id>
    <updated>2013-11-11T04:43:10Z</updated>
    <published>2013-06-06T15:15:07Z</published>
    <title>Tight Lower Bound on the Probability of a Binomial Exceeding its
  Expectation</title>
    <summary>  We give the proof of a tight lower bound on the probability that a binomial
random variable exceeds its expected value. The inequality plays an important
role in a variety of contexts, including the analysis of relative deviation
bounds in learning theory and generalization bounds for unbounded loss
functions.
</summary>
    <author>
      <name>Spencer Greenberg</name>
    </author>
    <author>
      <name>Mehryar Mohri</name>
    </author>
    <link href="http://arxiv.org/abs/1306.1433v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1433v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0440v1</id>
    <updated>2014-10-02T02:28:04Z</updated>
    <published>2014-10-02T02:28:04Z</published>
    <title>Scalable Nonlinear Learning with Adaptive Polynomial Expansions</title>
    <summary>  Can we effectively learn a nonlinear representation in time comparable to
linear learning? We describe a new algorithm that explicitly and adaptively
expands higher-order interaction features over base linear representations. The
algorithm is designed for extreme computational efficiency, and an extensive
experimental study shows that its computation/prediction tradeoff ability
compares very favorably against strong baselines.
</summary>
    <author>
      <name>Alekh Agarwal</name>
    </author>
    <author>
      <name>Alina Beygelzimer</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in NIPS 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.0440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4604v1</id>
    <updated>2014-10-16T23:30:08Z</updated>
    <published>2014-10-16T23:30:08Z</published>
    <title>Domain-Independent Optimistic Initialization for Reinforcement Learning</title>
    <summary>  In Reinforcement Learning (RL), it is common to use optimistic initialization
of value functions to encourage exploration. However, such an approach
generally depends on the domain, viz., the scale of the rewards must be known,
and the feature representation must have a constant norm. We present a simple
approach that performs optimistic initialization with less dependence on the
domain.
</summary>
    <author>
      <name>Marlos C. Machado</name>
    </author>
    <author>
      <name>Sriram Srinivasan</name>
    </author>
    <author>
      <name>Michael Bowling</name>
    </author>
    <link href="http://arxiv.org/abs/1410.4604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05938v1</id>
    <updated>2015-03-19T20:30:46Z</updated>
    <published>2015-03-19T20:30:46Z</published>
    <title>On Invariance and Selectivity in Representation Learning</title>
    <summary>  We discuss data representation which can be learned automatically from data,
are invariant to transformations, and at the same time selective, in the sense
that two points have the same representation only if they are one the
transformation of the other. The mathematical results here sharpen some of the
key claims of i-theory -- a recent theory of feedforward processing in sensory
cortex.
</summary>
    <author>
      <name>Fabio Anselmi</name>
    </author>
    <author>
      <name>Lorenzo Rosasco</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07211v1</id>
    <updated>2015-03-24T21:38:59Z</updated>
    <published>2015-03-24T21:38:59Z</published>
    <title>Universal Approximation of Markov Kernels by Shallow Stochastic
  Feedforward Networks</title>
    <summary>  We establish upper bounds for the minimal number of hidden units for which a
binary stochastic feedforward network with sigmoid activation probabilities and
a single hidden layer is a universal approximator of Markov kernels. We show
that each possible probabilistic assignment of the states of $n$ output units,
given the states of $k\geq1$ input units, can be approximated arbitrarily well
by a network with $2^{k-1}(2^{n-1}-1)$ hidden units.
</summary>
    <author>
      <name>Guido Montufar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.07211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82C32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05240v4</id>
    <updated>2024-05-02T13:32:25Z</updated>
    <published>2015-11-17T01:14:51Z</published>
    <title>An extension of McDiarmid's inequality</title>
    <summary>  We generalize McDiarmid's inequality for functions with bounded differences
on a high probability set, using an extension argument. Those functions
concentrate around their conditional expectations. We further extend the
results to concentration in general metric spaces.
</summary>
    <author>
      <name>Richard Combes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05240v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05240v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00564v1</id>
    <updated>2016-10-03T14:22:19Z</updated>
    <published>2016-10-03T14:22:19Z</published>
    <title>End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural
  Networks</title>
    <summary>  We investigate sequence machine learning techniques on raw radio signal
time-series data. By applying deep recurrent neural networks we learn to
discriminate between several application layer traffic types on top of a
constant envelope modulation without using an expert demodulation algorithm. We
show that complex protocol sequences can be learned and used for both
classification and generation tasks using this approach.
</summary>
    <author>
      <name>Timothy J. O'Shea</name>
    </author>
    <author>
      <name>Seth Hitefield</name>
    </author>
    <author>
      <name>Johnathan Corgan</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03793v2</id>
    <updated>2017-09-28T11:28:26Z</updated>
    <published>2016-10-12T17:18:01Z</published>
    <title>Introduction to the "Industrial Benchmark"</title>
    <summary>  A novel reinforcement learning benchmark, called Industrial Benchmark, is
introduced. The Industrial Benchmark aims at being be realistic in the sense,
that it includes a variety of aspects that we found to be vital in industrial
applications. It is not designed to be an approximation of any real system, but
to pose the same hardness and complexity.
</summary>
    <author>
      <name>Daniel Hein</name>
    </author>
    <author>
      <name>Alexander Hentschel</name>
    </author>
    <author>
      <name>Volkmar Sterzing</name>
    </author>
    <author>
      <name>Michel Tokic</name>
    </author>
    <author>
      <name>Steffen Udluft</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03793v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03793v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00171v2</id>
    <updated>2018-04-20T09:27:47Z</updated>
    <published>2017-12-30T18:11:59Z</published>
    <title>PAC-Bayesian Margin Bounds for Convolutional Neural Networks</title>
    <summary>  Recently the generalization error of deep neural networks has been analyzed
through the PAC-Bayesian framework, for the case of fully connected layers. We
adapt this approach to the convolutional setting.
</summary>
    <author>
      <name>Konstantinos Pitas</name>
    </author>
    <author>
      <name>Mike Davies</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1707.09564 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.00171v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00171v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02710v1</id>
    <updated>2018-01-08T23:04:01Z</updated>
    <published>2018-01-08T23:04:01Z</published>
    <title>Modeling urbanization patterns with generative adversarial networks</title>
    <summary>  In this study we propose a new method to simulate hyper-realistic urban
patterns using Generative Adversarial Networks trained with a global urban
land-use inventory. We generated a synthetic urban "universe" that
qualitatively reproduces the complex spatial organization observed in global
urban patterns, while being able to quantitatively recover certain key
high-level urban spatial metrics.
</summary>
    <author>
      <name>Adrian Albert</name>
    </author>
    <author>
      <name>Emanuele Strano</name>
    </author>
    <author>
      <name>Jasleen Kaur</name>
    </author>
    <author>
      <name>Marta Gonzalez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.02710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02788v1</id>
    <updated>2018-01-09T04:13:11Z</updated>
    <published>2018-01-09T04:13:11Z</published>
    <title>Sequential Preference-Based Optimization</title>
    <summary>  Many real-world engineering problems rely on human preferences to guide their
design and optimization. We present PrefOpt, an open source package to simplify
sequential optimization tasks that incorporate human preference feedback. Our
approach extends an existing latent variable model for binary preferences to
allow for observations of equivalent preference from users.
</summary>
    <author>
      <name>Ian Dewancker</name>
    </author>
    <author>
      <name>Jakob Bauer</name>
    </author>
    <author>
      <name>Michael McCourt</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00593v1</id>
    <updated>2018-09-03T13:21:28Z</updated>
    <published>2018-09-03T13:21:28Z</published>
    <title>IoU is not submodular</title>
    <summary>  This short article aims at demonstrate that the Intersection over Union (or
Jaccard index) is not a submodular function. This mistake has been made in an
article which is cited and used as a foundation in another article. The
Intersection of Union is widely used in machine learning as a cost function
especially for imbalance data and semantic segmentation.
</summary>
    <author>
      <name>Tanguy Kerdoncuff</name>
    </author>
    <author>
      <name>Rémi Emonet</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01369v1</id>
    <updated>2018-09-05T07:56:11Z</updated>
    <published>2018-09-05T07:56:11Z</published>
    <title>Towards quantitative methods to assess network generative models</title>
    <summary>  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
</summary>
    <author>
      <name>Vahid Mostofi</name>
    </author>
    <author>
      <name>Sadegh Aliakbary</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04684v1</id>
    <updated>2018-09-12T21:29:20Z</updated>
    <published>2018-09-12T21:29:20Z</published>
    <title>Fair lending needs explainable models for responsible recommendation</title>
    <summary>  The financial services industry has unique explainability and fairness
challenges arising from compliance and ethical considerations in credit
decisioning. These challenges complicate the use of model machine learning and
artificial intelligence methods in business decision processes.
</summary>
    <author>
      <name>Jiahao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, position paper accepted for FATREC 2018 conference at ACM
  RecSys</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.04684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91G40, 68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.1; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05861v1</id>
    <updated>2018-09-16T12:23:09Z</updated>
    <published>2018-09-16T12:23:09Z</published>
    <title>f-VAEs: Improve VAEs with Conditional Flows</title>
    <summary>  In this paper, we integrate VAEs and flow-based generative models
successfully and get f-VAEs. Compared with VAEs, f-VAEs generate more vivid
images, solved the blurred-image problem of VAEs. Compared with flow-based
models such as Glow, f-VAE is more lightweight and converges faster, achieving
the same performance under smaller-size architecture.
</summary>
    <author>
      <name>Jianlin Su</name>
    </author>
    <author>
      <name>Guang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.05861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00200v1</id>
    <updated>2018-11-01T03:12:26Z</updated>
    <published>2018-11-01T03:12:26Z</published>
    <title>Online Learning Algorithms for Statistical Arbitrage</title>
    <summary>  Statistical arbitrage is a class of financial trading strategies using mean
reversion models. The corresponding techniques rely on a number of assumptions
which may not hold for general non-stationary stochastic processes. This paper
presents an alternative technique for statistical arbitrage based on online
learning which does not require such assumptions and which benefits from strong
learning guarantees.
</summary>
    <author>
      <name>Christopher Mohri</name>
    </author>
    <link href="http://arxiv.org/abs/1811.00200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00246v1</id>
    <updated>2018-11-01T05:45:43Z</updated>
    <published>2018-11-01T05:45:43Z</published>
    <title>SARN: Relational Reasoning through Sequential Attention</title>
    <summary>  This paper proposes an attention module augmented relational network called
SARN(Sequential Attention Relational Network) that can carry out relational
reasoning by extracting reference objects and making efficient pairing between
objects. SARN greatly reduces the computational and memory requirements of the
relational network, which computes all object pairs. It also shows high
accuracy on the Sort-of-CLEVR dataset compared to other models, especially on
relational questions.
</summary>
    <author>
      <name>Jinwon An</name>
    </author>
    <author>
      <name>Sungwon Lyu</name>
    </author>
    <author>
      <name>Sungzoon Cho</name>
    </author>
    <link href="http://arxiv.org/abs/1811.00246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00852v1</id>
    <updated>2018-10-31T22:05:11Z</updated>
    <published>2018-10-31T22:05:11Z</published>
    <title>Understanding Deep Neural Networks Using Topological Data Analysis</title>
    <summary>  Deep neural networks (DNN) are black box algorithms. They are trained using a
gradient descent back propagation technique which trains weights in each layer
for the sole goal of minimizing training error. Hence, the resulting weights
cannot be directly explained. Using Topological Data Analysis (TDA) we can get
an insight on how the neural network is thinking, specifically by analyzing the
activation values of validation images as they pass through each layer.
</summary>
    <author>
      <name>Daniel Goldfarb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.00852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="55U99, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.01225v1</id>
    <updated>2018-11-03T14:18:40Z</updated>
    <published>2018-11-03T14:18:40Z</published>
    <title>CAAD 2018: Powerful None-Access Black-Box Attack Based on Adversarial
  Transformation Network</title>
    <summary>  In this paper, we propose an improvement of Adversarial Transformation
Networks(ATN) to generate adversarial examples, which can fool white-box models
and black-box models with a state of the art performance and won the 2rd place
in the non-target task in CAAD 2018.
</summary>
    <author>
      <name>Xiaoyi Dong</name>
    </author>
    <author>
      <name>Weiming Zhang</name>
    </author>
    <author>
      <name>Nenghai Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04422v1</id>
    <updated>2018-11-11T14:28:34Z</updated>
    <published>2018-11-11T14:28:34Z</published>
    <title>An Optimal Control View of Adversarial Machine Learning</title>
    <summary>  I describe an optimal control view of adversarial machine learning, where the
dynamical system is the machine learner, the input are adversarial actions, and
the control costs are defined by the adversary's goals to do harm and be hard
to detect. This view encompasses many types of adversarial machine learning,
including test-item attacks, training-data poisoning, and adversarial reward
shaping. The view encourages adversarial machine learning researcher to utilize
advances in control theory and reinforcement learning.
</summary>
    <author>
      <name>Xiaojin Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1811.04422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06471v2</id>
    <updated>2018-11-30T21:16:03Z</updated>
    <published>2018-11-15T17:03:59Z</published>
    <title>Towards Explainable Deep Learning for Credit Lending: A Case Study</title>
    <summary>  Deep learning adoption in the financial services industry has been limited
due to a lack of model interpretability. However, several techniques have been
proposed to explain predictions made by a neural network. We provide an initial
investigation into these techniques for the assessment of credit risk with
neural networks.
</summary>
    <author>
      <name>Ceena Modarres</name>
    </author>
    <author>
      <name>Mark Ibrahim</name>
    </author>
    <author>
      <name>Melissa Louie</name>
    </author>
    <author>
      <name>John Paisley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted NIPS 2018 FEAP</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.06471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06622v1</id>
    <updated>2018-11-15T23:13:26Z</updated>
    <published>2018-11-15T23:13:26Z</published>
    <title>Concept-Oriented Deep Learning: Generative Concept Representations</title>
    <summary>  Generative concept representations have three major advantages over
discriminative ones: they can represent uncertainty, they support integration
of learning and reasoning, and they are good for unsupervised and
semi-supervised learning. We discuss probabilistic and generative deep
learning, which generative concept representations are based on, and the use of
variational autoencoders and generative adversarial networks for learning
generative concept representations, particularly for concepts whose data are
sequences, structured data or graphs.
</summary>
    <author>
      <name>Daniel T. Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1811.06622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.11669v1</id>
    <updated>2018-11-28T16:49:37Z</updated>
    <published>2018-11-28T16:49:37Z</published>
    <title>Towards Identifying and Managing Sources of Uncertainty in AI and
  Machine Learning Models - An Overview</title>
    <summary>  Quantifying and managing uncertainties that occur when data-driven models
such as those provided by AI and machine learning methods are applied is
crucial. This whitepaper provides a brief motivation and first overview of the
state of the art in identifying and quantifying sources of uncertainty for
data-driven components as well as means for analyzing their impact.
</summary>
    <author>
      <name>Michael Kläs</name>
    </author>
    <link href="http://arxiv.org/abs/1811.11669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00249v1</id>
    <updated>2018-12-01T19:30:55Z</updated>
    <published>2018-12-01T19:30:55Z</published>
    <title>On Compressing U-net Using Knowledge Distillation</title>
    <summary>  We study the use of knowledge distillation to compress the U-net
architecture. We show that, while standard distillation is not sufficient to
reliably train a compressed U-net, introducing other regularization methods,
such as batch normalization and class re-weighting, in knowledge distillation
significantly improves the training process. This allows us to compress a U-net
by over 1000x, i.e., to 0.1% of its original number of parameters, at a
negligible decrease in performance.
</summary>
    <author>
      <name>Karttikeya Mangalam</name>
    </author>
    <author>
      <name>Mathieu Salzamann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.01410v1</id>
    <updated>2018-12-04T13:50:11Z</updated>
    <published>2018-12-04T13:50:11Z</published>
    <title>Compressive Classification (Machine Learning without learning)</title>
    <summary>  Compressive learning is a framework where (so far unsupervised) learning
tasks use not the entire dataset but a compressed summary (sketch) of it. We
propose a compressive learning classification method, and a novel sketch
function for images.
</summary>
    <author>
      <name>Vincent Schellekens</name>
    </author>
    <author>
      <name>Laurent Jacques</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of iTWIST'18, Paper-ID: 8, Marseille, France,
  November, 21-23, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.01410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.02682v1</id>
    <updated>2018-12-06T17:35:19Z</updated>
    <published>2018-12-06T17:35:19Z</published>
    <title>$β$-VAEs can retain label information even at high compression</title>
    <summary>  In this paper, we investigate the degree to which the encoding of a
$\beta$-VAE captures label information across multiple architectures on Binary
Static MNIST and Omniglot. Even though they are trained in a completely
unsupervised manner, we demonstrate that a $\beta$-VAE can retain a large
amount of label information, even when asked to learn a highly compressed
representation.
</summary>
    <author>
      <name>Emily Fertig</name>
    </author>
    <author>
      <name>Aryan Arbabi</name>
    </author>
    <author>
      <name>Alexander A. Alemi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS2018, BDL workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.02682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03049v1</id>
    <updated>2018-12-07T14:53:03Z</updated>
    <published>2018-12-07T14:53:03Z</published>
    <title>On Batch Orthogonalization Layers</title>
    <summary>  Batch normalization has become ubiquitous in many state-of-the-art nets. It
accelerates training and yields good performance results. However, there are
various other alternatives to normalization, e.g. orthonormalization. The
objective of this paper is to explore the possible alternatives to channel
normalization with orthonormalization layers. The performance of the algorithms
are compared together with BN with prescribed performance measures.
</summary>
    <author>
      <name> Blanchette</name>
    </author>
    <author>
      <name> Laganière</name>
    </author>
    <link href="http://arxiv.org/abs/1812.03049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04529v2</id>
    <updated>2019-11-20T16:13:56Z</updated>
    <published>2018-12-11T16:40:59Z</published>
    <title>On the Ineffectiveness of Variance Reduced Optimization for Deep
  Learning</title>
    <summary>  The application of stochastic variance reduction to optimization has shown
remarkable recent theoretical and practical success. The applicability of these
techniques to the hard non-convex optimization problems encountered during
training of modern deep neural networks is an open problem. We show that naive
application of the SVRG technique and related approaches fail, and explore why.
</summary>
    <author>
      <name>Aaron Defazio</name>
    </author>
    <author>
      <name>Léon Bottou</name>
    </author>
    <link href="http://arxiv.org/abs/1812.04529v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04529v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04549v2</id>
    <updated>2019-05-10T14:19:14Z</updated>
    <published>2018-12-11T17:12:50Z</published>
    <title>Controlling Covariate Shift using Balanced Normalization of Weights</title>
    <summary>  We introduce a new normalization technique that exhibits the fast convergence
properties of batch normalization using a transformation of layer weights
instead of layer outputs. The proposed technique keeps the contribution of
positive and negative weights to the layer output balanced. We validate our
method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC
2012 ImageNet.
</summary>
    <author>
      <name>Aaron Defazio</name>
    </author>
    <author>
      <name>Léon Bottou</name>
    </author>
    <link href="http://arxiv.org/abs/1812.04549v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04549v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04650v1</id>
    <updated>2018-12-11T19:05:23Z</updated>
    <published>2018-12-11T19:05:23Z</published>
    <title>Reproduction Report on "Learn to Pay Attention"</title>
    <summary>  We have successfully implemented the "Learn to Pay Attention" model of
attention mechanism in convolutional neural networks, and have replicated the
results of the original paper in the categories of image classification and
fine-grained recognition.
</summary>
    <author>
      <name>Levan Shugliashvili</name>
    </author>
    <author>
      <name>Davit Soselia</name>
    </author>
    <author>
      <name>Shota Amashukeli</name>
    </author>
    <author>
      <name>Irakli Koberidze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 tables, originally made for the ICLR 2018 Reproducibility
  Challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.04650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04622v1</id>
    <updated>2019-02-12T20:28:09Z</updated>
    <published>2019-02-12T20:28:09Z</published>
    <title>Learning Theory and Support Vector Machines - a primer</title>
    <summary>  The main goal of statistical learning theory is to provide a fundamental
framework for the problem of decision making and model construction based on
sets of data. Here, we present a brief introduction to the fundamentals of
statistical learning theory, in particular the difference between empirical and
structural risk minimization, including one of its most prominent
implementations, i.e. the Support Vector Machine.
</summary>
    <author>
      <name>Michael Banf</name>
    </author>
    <link href="http://arxiv.org/abs/1902.04622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.05017v1</id>
    <updated>2019-02-13T17:24:35Z</updated>
    <published>2019-02-13T17:24:35Z</published>
    <title>Differentially Private Learning of Geometric Concepts</title>
    <summary>  We present differentially private efficient algorithms for learning union of
polygons in the plane (which are not necessarily convex). Our algorithms
achieve $(\alpha,\beta)$-PAC learning and $(\epsilon,\delta)$-differential
privacy using a sample of size $\tilde{O}\left(\frac{1}{\alpha\epsilon}k\log
d\right)$, where the domain is $[d]\times[d]$ and $k$ is the number of edges in
the union of polygons.
</summary>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <author>
      <name>Yishay Mansour</name>
    </author>
    <author>
      <name>Yossi Matias</name>
    </author>
    <author>
      <name>Uri Stemmer</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.06223v2</id>
    <updated>2019-02-23T09:53:27Z</updated>
    <published>2019-02-17T08:54:05Z</published>
    <title>Learning Linear-Quadratic Regulators Efficiently with only $\sqrt{T}$
  Regret</title>
    <summary>  We present the first computationally-efficient algorithm with $\widetilde
O(\sqrt{T})$ regret for learning in Linear Quadratic Control systems with
unknown dynamics. By that, we resolve an open question of Abbasi-Yadkori and
Szepesv\'ari (2011) and Dean, Mania, Matni, Recht, and Tu (2018).
</summary>
    <author>
      <name>Alon Cohen</name>
    </author>
    <author>
      <name>Tomer Koren</name>
    </author>
    <author>
      <name>Yishay Mansour</name>
    </author>
    <link href="http://arxiv.org/abs/1902.06223v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.06223v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.08647v2</id>
    <updated>2019-03-28T05:42:35Z</updated>
    <published>2019-02-22T19:28:06Z</published>
    <title>Better Algorithms for Stochastic Bandits with Adversarial Corruptions</title>
    <summary>  We study the stochastic multi-armed bandits problem in the presence of
adversarial corruption. We present a new algorithm for this problem whose
regret is nearly optimal, substantially improving upon previous work. Our
algorithm is agnostic to the level of adversarial contamination and can
tolerate a significant amount of corruption with virtually no degradation in
performance.
</summary>
    <author>
      <name>Anupam Gupta</name>
    </author>
    <author>
      <name>Tomer Koren</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <link href="http://arxiv.org/abs/1902.08647v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.08647v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.05347v3</id>
    <updated>2023-01-18T18:10:10Z</updated>
    <published>2019-03-13T07:54:58Z</published>
    <title>What relations are reliably embeddable in Euclidean space?</title>
    <summary>  We consider the problem of embedding a relation, represented as a directed
graph, into Euclidean space. For three types of embeddings motivated by the
recent literature on knowledge graphs, we obtain characterizations of which
relations they are able to capture, as well as bounds on the minimal
dimensionality and precision needed.
</summary>
    <author>
      <name>Robi Bhattacharjee</name>
    </author>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ALT 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.05347v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05347v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07303v1</id>
    <updated>2019-03-18T08:45:27Z</updated>
    <published>2019-03-18T08:45:27Z</published>
    <title>M$^2$VAE - Derivation of a Multi-Modal Variational Autoencoder Objective
  from the Marginal Joint Log-Likelihood</title>
    <summary>  This work gives an in-depth derivation of the trainable evidence lower bound
obtained from the marginal joint log-Likelihood with the goal of training a
Multi-Modal Variational Autoencoder (M$^2$VAE).
</summary>
    <author>
      <name>Timo Korthals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appendix for the IEEE FUSION 2019 submission on multi-modal
  variational Autoencoders for sensor fusion</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07722v1</id>
    <updated>2019-03-18T21:10:16Z</updated>
    <published>2019-03-18T21:10:16Z</published>
    <title>Discovering Heterogeneous Subsequences for Trajectory Classification</title>
    <summary>  In this paper we propose a new parameter-free method for trajectory
classification which finds the best trajectory partition and dimension
combination for robust trajectory classification. Preliminary experiments show
that our approach is very promising.
</summary>
    <author>
      <name>Carlos Andres Ferrero</name>
    </author>
    <author>
      <name>Lucas May Petry</name>
    </author>
    <author>
      <name>Luis Otavio Alvares</name>
    </author>
    <author>
      <name>Willian Zalewski</name>
    </author>
    <author>
      <name>Vania Bogorny</name>
    </author>
    <link href="http://arxiv.org/abs/1903.07722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07977v1</id>
    <updated>2019-03-15T02:21:25Z</updated>
    <published>2019-03-15T02:21:25Z</published>
    <title>Tackling Initial Centroid of K-Means with Distance Part (DP-KMeans)</title>
    <summary>  The initial centroid is a fairly challenging problem in the k-means method
because it can affect the clustering results. In addition, choosing the
starting centroid of the cluster is not always appropriate, especially, when
the number of groups increases.
</summary>
    <author>
      <name>Ahmad Ilham</name>
    </author>
    <author>
      <name>Danny Ibrahim</name>
    </author>
    <author>
      <name>Luqman Assaffat</name>
    </author>
    <author>
      <name>Achmad Solichan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was presented at Proceeding of 2018 International
  Symposium on Advanced Intelligent Informatics (SAIN), 29-30 August 2018,
  Yogyakarta, Indonesia</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10025v1</id>
    <updated>2019-03-24T17:34:29Z</updated>
    <published>2019-03-24T17:34:29Z</published>
    <title>Generalization of k-means Related Algorithms</title>
    <summary>  This article briefly introduced Arthur and Vassilvitshii's work on
\textbf{k-means++} algorithm and further generalized the center initialization
process. It is found that choosing the most distant sample point from the
nearest center as new center can mostly have the same effect as the center
initialization process in the \textbf{k-means++} algorithm.
</summary>
    <author>
      <name>Yiwei Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06513v2</id>
    <updated>2019-06-14T00:30:27Z</updated>
    <published>2019-04-13T09:53:27Z</published>
    <title>An Integrated Autoencoder-Based Filter for Sparse Big Data</title>
    <summary>  We propose a novel filter for sparse big data, called an integrated
autoencoder (IAE), which utilizes auxiliary information to mitigate data
sparsity. The proposed model achieves an appropriate balance between prediction
accuracy, convergence speed, and complexity. We implement experiments on a GPS
trajectory dataset, and the results demonstrate that the IAE is more accurate
and robust than some state-of-the-art methods.
</summary>
    <author>
      <name>Baogui Xin</name>
    </author>
    <author>
      <name>Wei Peng</name>
    </author>
    <link href="http://arxiv.org/abs/1904.06513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09031v1</id>
    <updated>2019-04-17T07:55:35Z</updated>
    <published>2019-04-17T07:55:35Z</published>
    <title>Predict Future Sales using Ensembled Random Forests</title>
    <summary>  This is a method report for the Kaggle data competition 'Predict future
sales'. In this paper, we propose a rather simple approach to future sales
predicting based on feature engineering, Random Forest Regressor and ensemble
learning. Its performance turned out to exceed many of the conventional methods
and get final score 0.88186, representing root mean squared error. As of this
writing, our model ranked 5th on the leaderboard. (till 8.5.2018)
</summary>
    <author>
      <name>Yuwei Zhang</name>
    </author>
    <author>
      <name>Xin Wu</name>
    </author>
    <author>
      <name>Chenyang Gu</name>
    </author>
    <author>
      <name>Yueqi Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1904.09031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09948v1</id>
    <updated>2019-04-22T16:50:04Z</updated>
    <published>2019-04-22T16:50:04Z</published>
    <title>PLUME: Polyhedral Learning Using Mixture of Experts</title>
    <summary>  In this paper, we propose a novel mixture of expert architecture for learning
polyhedral classifiers. We learn the parameters of the classifierusing an
expectation maximization algorithm. Wederive the generalization bounds of the
proposedapproach. Through an extensive simulation study, we show that the
proposed method performs comparably to other state-of-the-art approaches.
</summary>
    <author>
      <name>Kulin Shah</name>
    </author>
    <author>
      <name>P. S. Sastry</name>
    </author>
    <author>
      <name>Naresh Manwani</name>
    </author>
    <link href="http://arxiv.org/abs/1904.09948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00748v1</id>
    <updated>2019-05-21T07:09:12Z</updated>
    <published>2019-05-21T07:09:12Z</published>
    <title>Improving Minimal Gated Unit for Sequential Data</title>
    <summary>  In order to obtain a model which can process sequential data related to
machine translation and speech recognition faster and more accurately, we
propose adopting Chrono Initializer as the initialization method of Minimal
Gated Unit. We evaluated the method with two tasks: adding task and copy task.
As a result of the experiment, the effectiveness of the proposed method was
confirmed.
</summary>
    <author>
      <name>Kazuki Takamura</name>
    </author>
    <author>
      <name>Satoshi Yamane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02101v2</id>
    <updated>2020-03-12T16:40:04Z</updated>
    <published>2019-06-05T16:18:40Z</published>
    <title>Diameter-based Interactive Structure Discovery</title>
    <summary>  We introduce interactive structure discovery, a generic framework that
encompasses many interactive learning settings, including active learning,
top-k item identification, interactive drug discovery, and others. We adapt a
recently developed active learning algorithm of Tosh and Dasgupta (2017) for
interactive structure discovery, and show that the new algorithm can be made
noise-tolerant and enjoys favorable query complexity bounds.
</summary>
    <author>
      <name>Christopher Tosh</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02101v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02101v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02568v1</id>
    <updated>2019-06-06T13:18:03Z</updated>
    <published>2019-06-06T13:18:03Z</published>
    <title>Localizing Catastrophic Forgetting in Neural Networks</title>
    <summary>  Artificial neural networks (ANNs) suffer from catastrophic forgetting when
trained on a sequence of tasks. While this phenomenon was studied in the past,
there is only very limited recent research on this phenomenon. We propose a
method for determining the contribution of individual parameters in an ANN to
catastrophic forgetting. The method is used to analyze an ANNs response to
three different continual learning scenarios.
</summary>
    <author>
      <name>Felix Wiewel</name>
    </author>
    <author>
      <name>Bin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02691v3</id>
    <updated>2019-12-11T17:33:13Z</updated>
    <published>2019-06-06T16:35:38Z</published>
    <title>An Introduction to Variational Autoencoders</title>
    <summary>  Variational autoencoders provide a principled framework for learning deep
latent-variable models and corresponding inference models. In this work, we
provide an introduction to variational autoencoders and some important
extensions.
</summary>
    <author>
      <name>Diederik P. Kingma</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1561/2200000056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1561/2200000056" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Foundations and Trends in Machine Learning: Vol. 12 (2019): No. 4,
  pp 307-392</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.02691v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02691v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02872v1</id>
    <updated>2019-06-07T02:48:56Z</updated>
    <published>2019-06-07T02:48:56Z</published>
    <title>Mixed Strategy Game Model Against Data Poisoning Attacks</title>
    <summary>  In this paper we use game theory to model poisoning attack scenarios. We
prove the non-existence of pure strategy Nash Equilibrium in the attacker and
defender game. We then propose a mixed extension of our game model and an
algorithm to approximate the Nash Equilibrium strategy for the defender. We
then demonstrate the effectiveness of the mixed defence strategy generated by
the algorithm, in an experiment.
</summary>
    <author>
      <name>Yifan Ou</name>
    </author>
    <author>
      <name>Reza Samavi</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03063v1</id>
    <updated>2019-06-06T16:09:01Z</updated>
    <published>2019-06-06T16:09:01Z</published>
    <title>Classical Policy Gradient: Preserving Bellman's Principle of Optimality</title>
    <summary>  We propose a new objective function for finite-horizon episodic Markov
decision processes that better captures Bellman's principle of optimality, and
provide an expression for the gradient of the objective.
</summary>
    <author>
      <name>Philip S. Thomas</name>
    </author>
    <author>
      <name>Scott M. Jordan</name>
    </author>
    <author>
      <name>Yash Chandak</name>
    </author>
    <author>
      <name>Chris Nota</name>
    </author>
    <author>
      <name>James Kostas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 page, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03612v1</id>
    <updated>2019-06-09T10:22:52Z</updated>
    <published>2019-06-09T10:22:52Z</published>
    <title>On the Vulnerability of Capsule Networks to Adversarial Attacks</title>
    <summary>  This paper extensively evaluates the vulnerability of capsule networks to
different adversarial attacks. Recent work suggests that these architectures
are more robust towards adversarial attacks than other neural networks.
However, our experiments show that capsule networks can be fooled as easily as
convolutional neural networks.
</summary>
    <author>
      <name>Felix Michels</name>
    </author>
    <author>
      <name>Tobias Uelwer</name>
    </author>
    <author>
      <name>Eric Upschulte</name>
    </author>
    <author>
      <name>Stefan Harmeling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03708v1</id>
    <updated>2019-06-09T21:08:35Z</updated>
    <published>2019-06-09T21:08:35Z</published>
    <title>Note on the bias and variance of variational inference</title>
    <summary>  In this note, we study the relationship between the variational gap and the
variance of the (log) likelihood ratio. We show that the gap can be upper
bounded by some form of dispersion measure of the likelihood ratio, which
suggests the bias of variational inference can be reduced by making the
distribution of the likelihood ratio more concentrated, such as via averaging
and variance reduction.
</summary>
    <author>
      <name>Chin-Wei Huang</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07172v4</id>
    <updated>2020-03-22T04:28:55Z</updated>
    <published>2019-06-16T23:26:03Z</published>
    <title>Equivariant neural networks and equivarification</title>
    <summary>  We provide a process to modify a neural network to an equivariant one, which
we call equivarification. As an illustration, we build an equivariant neural
network for image classification by equivarifying a convolutional neural
network.
</summary>
    <author>
      <name>Erkao Bao</name>
    </author>
    <author>
      <name>Linqi Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">More explanations added</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07172v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07172v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08068v1</id>
    <updated>2019-06-15T06:48:59Z</updated>
    <published>2019-06-15T06:48:59Z</published>
    <title>Online Heterogeneous Mixture Learning for Big Data</title>
    <summary>  We propose the online machine learning for big data analysis with
heterogeneity. We performed an experiment to compare the accuracy of each
iteration between batch one and online one. It is possible to converge quickly
with the same accuracy as the batch one.
</summary>
    <author>
      <name>Kazuki Seshimo</name>
    </author>
    <author>
      <name>Ota Akira</name>
    </author>
    <author>
      <name>Nishio Daichi</name>
    </author>
    <author>
      <name>Yamane Satoshi</name>
    </author>
    <link href="http://arxiv.org/abs/1906.08068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08720v2</id>
    <updated>2020-02-23T18:32:12Z</updated>
    <published>2019-06-20T16:05:23Z</published>
    <title>Boosting for Control of Dynamical Systems</title>
    <summary>  We study the question of how to aggregate controllers for dynamical systems
in order to improve their performance. To this end, we propose a framework of
boosting for online control. Our main result is an efficient boosting algorithm
that combines weak controllers into a provably more accurate one. Empirical
evaluation on a host of control settings supports our theoretical findings.
</summary>
    <author>
      <name>Naman Agarwal</name>
    </author>
    <author>
      <name>Nataly Brukhim</name>
    </author>
    <author>
      <name>Elad Hazan</name>
    </author>
    <author>
      <name>Zhou Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1906.08720v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08720v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10025v2</id>
    <updated>2019-07-06T18:30:45Z</updated>
    <published>2019-06-24T15:27:51Z</published>
    <title>Modern Deep Reinforcement Learning Algorithms</title>
    <summary>  Recent advances in Reinforcement Learning, grounded on combining classical
theoretical results with Deep Learning paradigm, led to breakthroughs in many
artificial intelligence tasks and gave birth to Deep Reinforcement Learning
(DRL) as a field of research. In this work latest DRL algorithms are reviewed
with a focus on their theoretical justification, practical limitations and
observed empirical properties.
</summary>
    <author>
      <name>Sergey Ivanov</name>
    </author>
    <author>
      <name>Alexander D'yakonov</name>
    </author>
    <link href="http://arxiv.org/abs/1906.10025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10242v1</id>
    <updated>2019-06-24T21:36:19Z</updated>
    <published>2019-06-24T21:36:19Z</published>
    <title>Multi-label Classification with Optimal Thresholding for
  Multi-composition Spectroscopic Analysis</title>
    <summary>  In this paper, we implement multi-label neural networks with optimal
thresholding to identify gas species among a multi gas mixture in a cluttered
environment. Using infrared absorption spectroscopy and tested on synthesized
spectral datasets, our approach outperforms conventional binary relevance -
partial least squares discriminant analysis when signal-to-noise ratio and
training sample size are sufficient.
</summary>
    <author>
      <name>Luyun Gan</name>
    </author>
    <author>
      <name>Brosnan Yuen</name>
    </author>
    <author>
      <name>Tao Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11755v1</id>
    <updated>2019-06-27T15:58:50Z</updated>
    <published>2019-06-27T15:58:50Z</published>
    <title>Singular Value Decomposition and Neural Networks</title>
    <summary>  Singular Value Decomposition (SVD) constitutes a bridge between the linear
algebra concepts and multi-layer neural networks---it is their linear analogy.
Besides of this insight, it can be used as a good initial guess for the network
parameters, leading to substantially better optimization results.
</summary>
    <author>
      <name>Bernhard Bermeitinger</name>
    </author>
    <author>
      <name>Tomas Hrycej</name>
    </author>
    <author>
      <name>Siegfried Handschuh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-30484-3_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-30484-3_13" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICANN 2019: Artificial Neural Networks and Machine Learning - Deep
  Learning</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.11755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00762v1</id>
    <updated>2019-07-01T13:37:34Z</updated>
    <published>2019-07-01T13:37:34Z</published>
    <title>Open Problem: The Oracle Complexity of Convex Optimization with Limited
  Memory</title>
    <summary>  We note that known methods achieving the optimal oracle complexity for first
order convex optimization require quadratic memory, and ask whether this is
necessary, and more broadly seek to characterize the minimax number of first
order queries required to optimize a convex Lipschitz function subject to a
memory constraint.
</summary>
    <author>
      <name>Blake Woodworth</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02015v1</id>
    <updated>2019-07-03T16:24:10Z</updated>
    <published>2019-07-03T16:24:10Z</published>
    <title>libconform v0.1.0: a Python library for conformal prediction</title>
    <summary>  This paper introduces libconform v0.1.0, a Python library for the conformal
prediction framework, licensed under the MIT-license. libconform is not yet
stable. This paper describes the main algorithms implemented and documents the
API of libconform. Also some details about the implementation and changes in
future versions are described.
</summary>
    <author>
      <name>Jonas Fassbender</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07746v1</id>
    <updated>2019-07-17T20:26:21Z</updated>
    <published>2019-07-17T20:26:21Z</published>
    <title>Deep Invertible Networks for EEG-based brain-signal decoding</title>
    <summary>  In this manuscript, we investigate deep invertible networks for EEG-based
brain signal decoding and find them to generate realistic EEG signals as well
as classify novel signals above chance. Further ideas for their regularization
towards better decoding accuracies are discussed.
</summary>
    <author>
      <name>Robin Tibor Schirrmeister</name>
    </author>
    <author>
      <name>Tonio Ball</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00768v1</id>
    <updated>2019-10-02T03:58:12Z</updated>
    <published>2019-10-02T03:58:12Z</published>
    <title>Contextual Local Explanation for Black Box Classifiers</title>
    <summary>  We introduce a new model-agnostic explanation technique which explains the
prediction of any classifier called CLE. CLE gives an faithful and
interpretable explanation to the prediction, by approximating the model locally
using an interpretable model. We demonstrate the flexibility of CLE by
explaining different models for text, tabular and image classification, and the
fidelity of it by doing simulated user experiments.
</summary>
    <author>
      <name>Zijian Zhang</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Haofan Wang</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1910.00768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05591v1</id>
    <updated>2019-10-12T16:23:04Z</updated>
    <published>2019-10-12T16:23:04Z</published>
    <title>Measuring Unfairness through Game-Theoretic Interpretability</title>
    <summary>  One often finds in the literature connections between measures of fairness
and measures of feature importance employed to interpret trained classifiers.
However, there seems to be no study that compares fairness measures and feature
importance measures. In this paper we propose ways to evaluate and compare such
measures. We focus in particular on SHAP, a game-theoretic measure of feature
importance; we present results for a number of unfairness-prone datasets.
</summary>
    <author>
      <name>Juliana Cesaro</name>
    </author>
    <author>
      <name>Fabio G. Cozman</name>
    </author>
    <link href="http://arxiv.org/abs/1910.05591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10122v1</id>
    <updated>2019-10-22T17:13:45Z</updated>
    <published>2019-10-22T17:13:45Z</published>
    <title>Class Mean Vectors, Self Monitoring and Self Learning for Neural
  Classifiers</title>
    <summary>  In this paper we explore the role of sample mean in building a neural network
for classification. This role is surprisingly extensive and includes: direct
computation of weights without training, performance monitoring for samples
without known classification, and self-training for unlabeled data.
Experimental computation on a CIFAR-10 data set provides promising empirical
evidence on the efficacy of a simple and widely applicable approach to some
difficult problems.
</summary>
    <author>
      <name>Eugene Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10831v1</id>
    <updated>2019-10-23T22:48:28Z</updated>
    <published>2019-10-23T22:48:28Z</published>
    <title>Variational Predictive Information Bottleneck</title>
    <summary>  In classic papers, Zellner demonstrated that Bayesian inference could be
derived as the solution to an information theoretic functional. Below we derive
a generalized form of this functional as a variational lower bound of a
predictive information bottleneck objective. This generalized functional
encompasses most modern inference procedures and suggests novel ones.
</summary>
    <author>
      <name>Alexander A. Alemi</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12387v2</id>
    <updated>2019-10-30T07:59:02Z</updated>
    <published>2019-10-25T17:33:33Z</published>
    <title>Components of Machine Learning: Binding Bits and FLOPS</title>
    <summary>  Many machine learning problems and methods are combinations of three
components: data, hypothesis space and loss function. Different machine
learning methods are obtained as combinations of different choices for the
representation of data, hypothesis space and loss function. After reviewing the
mathematical structure of these three components, we discuss intrinsic
trade-offs between statistical and computational properties of machine learning
methods.
</summary>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08198v1</id>
    <updated>2019-12-17T20:46:32Z</updated>
    <published>2019-12-17T20:46:32Z</published>
    <title>srlearn: A Python Library for Gradient-Boosted Statistical Relational
  Models</title>
    <summary>  We present srlearn, a Python library for boosted statistical relational
models. We adapt the scikit-learn interface to this setting and provide
examples for how this can be used to express learning and inference problems.
</summary>
    <author>
      <name>Alexander L. Hayes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ninth International Workshop on Statistical Relational AI (StarAI
  2020). Software online at https://github.com/hayesall/srlearn</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.08198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09336v1</id>
    <updated>2019-12-19T16:18:34Z</updated>
    <published>2019-12-19T16:18:34Z</published>
    <title>VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets</title>
    <summary>  We present a visualization tool to exhaustively search and browse through a
set of large-scale machine learning datasets. Built on the top of the VizWiz
dataset, our dataset browser tool has the potential to support and enable a
variety of qualitative and quantitative research, and open new directions for
visualizing and researching with multimodal information. The tool is publicly
available at https://vizwiz.org/browse.
</summary>
    <author>
      <name>Nilavra Bhattacharya</name>
    </author>
    <author>
      <name>Danna Gurari</name>
    </author>
    <link href="http://arxiv.org/abs/1912.09336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.02076v1</id>
    <updated>2020-07-27T14:00:31Z</updated>
    <published>2020-07-27T14:00:31Z</published>
    <title>Attacking and Defending Machine Learning Applications of Public Cloud</title>
    <summary>  Adversarial attack breaks the boundaries of traditional security defense. For
adversarial attack and the characteristics of cloud services, we propose
Security Development Lifecycle for Machine Learning applications, e.g., SDL for
ML. The SDL for ML helps developers build more secure software by reducing the
number and severity of vulnerabilities in ML-as-a-service, while reducing
development cost.
</summary>
    <author>
      <name>Dou Goodman</name>
    </author>
    <author>
      <name>Hao Xin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1704.05051 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.02076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03368v1</id>
    <updated>2020-08-07T20:32:34Z</updated>
    <published>2020-08-07T20:32:34Z</published>
    <title>Clustering, multicollinearity, and singular vectors</title>
    <summary>  Let $A$ be a matrix with its pseudo-matrix $A^{\dagger}$ and set
$S=I-A^{\dagger}A$. We prove that, after re-ordering the columns of $A$, the
matrix $S$ has a block-diagonal form where each block corresponds to a set of
linearly dependent columns. This allows us to identify redundant columns in
$A$. We explore some applications in supervised and unsupervised learning,
specially feature selection, clustering, and sensitivity of solutions of least
squares solutions.
</summary>
    <author>
      <name>Hamid Usefi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.03368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04105v1</id>
    <updated>2020-12-07T23:10:51Z</updated>
    <published>2020-12-07T23:10:51Z</published>
    <title>The Tribes of Machine Learning and the Realm of Computer Architecture</title>
    <summary>  Machine learning techniques have influenced the field of computer
architecture like many other fields. This paper studies how the fundamental
machine learning techniques can be applied towards computer architecture
problems. We also provide a detailed survey of computer architecture research
that employs different machine learning methods. Finally, we present some
future opportunities and the outstanding challenges that need to be overcome to
exploit full potential of machine learning for computer architecture.
</summary>
    <author>
      <name>Ayaz Akram</name>
    </author>
    <author>
      <name>Jason Lowe-Power</name>
    </author>
    <link href="http://arxiv.org/abs/2012.04105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05760v1</id>
    <updated>2020-12-10T15:44:18Z</updated>
    <published>2020-12-10T15:44:18Z</published>
    <title>Notes on Deep Learning Theory</title>
    <summary>  These are the notes for the lectures that I was giving during Fall 2020 at
the Moscow Institute of Physics and Technology (MIPT) and at the Yandex School
of Data Analysis (YSDA). The notes cover some aspects of initialization, loss
landscape, generalization, and a neural tangent kernel theory. While many other
topics (e.g. expressivity, a mean-field theory, a double descent phenomenon)
are missing in the current version, we plan to add them in future revisions.
</summary>
    <author>
      <name>Eugene A. Golikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">68 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.05760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08037v2</id>
    <updated>2021-02-16T10:57:20Z</updated>
    <published>2020-12-15T01:44:21Z</published>
    <title>Proofs and additional experiments on Second order techniques for
  learning time-series with structural breaks</title>
    <summary>  We provide complete proofs of the lemmas about the properties of the
regularized loss function that is used in the second order techniques for
learning time-series with structural breaks in Osogami (2021). In addition, we
show experimental results that support the validity of the techniques.
</summary>
    <author>
      <name>Takayuki Osogami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09725v1</id>
    <updated>2020-12-16T04:00:21Z</updated>
    <published>2020-12-16T04:00:21Z</published>
    <title>A Note on Optimizing the Ratio of Monotone Supermodular Functions</title>
    <summary>  We show that for the problem of minimizing (or maximizing) the ratio of two
supermodular functions, no bounded approximation ratio can be achieved via
polynomial number of queries, if the two supermodular functions are both
monotone non-decreasing or non-increasing.
</summary>
    <author>
      <name>Wenxin Li</name>
    </author>
    <link href="http://arxiv.org/abs/2012.09725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.11775v1</id>
    <updated>2020-12-22T01:14:28Z</updated>
    <published>2020-12-22T01:14:28Z</published>
    <title>MailLeak: Obfuscation-Robust Character Extraction Using Transfer
  Learning</title>
    <summary>  The following work presents a new algorithm for character recognition from
obfuscated images. The presented method is an example of a potential threat to
current postal services. This paper both analyzes the efficiency of the given
algorithm and suggests countermeasures to prevent such threats from occurring.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Emily Sallenback</name>
    </author>
    <author>
      <name>Zeyu Ning</name>
    </author>
    <author>
      <name>Hugues Nelson Iradukunda</name>
    </author>
    <author>
      <name>Wenxi Lu</name>
    </author>
    <author>
      <name>Qingquan Zhang</name>
    </author>
    <author>
      <name>Ting Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2012.11775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.11775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13021v1</id>
    <updated>2020-12-23T23:10:44Z</updated>
    <published>2020-12-23T23:10:44Z</published>
    <title>K-Means Kernel Classifier</title>
    <summary>  We combine K-means clustering with the least-squares kernel classification
method. K-means clustering is used to extract a set of representative vectors
for each class. The least-squares kernel method uses these representative
vectors as a training set for the classification task. We show that this
combination of unsupervised and supervised learning algorithms performs very
well, and we illustrate this approach using the MNIST dataset
</summary>
    <author>
      <name>M. Andrecut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.13021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00029v1</id>
    <updated>2020-12-31T19:00:31Z</updated>
    <published>2020-12-31T19:00:31Z</published>
    <title>Random Embeddings with Optimal Accuracy</title>
    <summary>  This work constructs Jonson-Lindenstrauss embeddings with best accuracy, as
measured by variance, mean-squared error and exponential concentration of the
length distortion. Lower bounds for any data and embedding dimensions are
determined, and accompanied by matching and efficiently samplable constructions
(built on orthogonal matrices). Novel techniques: a unit sphere
parametrization, the use of singular-value latent variables and Schur-convexity
are of independent interest.
</summary>
    <author>
      <name>Maciej Skorski</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00583v1</id>
    <updated>2021-01-03T08:36:45Z</updated>
    <published>2021-01-03T08:36:45Z</published>
    <title>Multi-label Ranking: Mining Multi-label and Label Ranking Data</title>
    <summary>  We survey multi-label ranking tasks, specifically multi-label classification
and label ranking classification. We highlight the unique challenges, and
re-categorize the methods, as they no longer fit into the traditional
categories of transformation and adaptation. We survey developments in the last
demi-decade, with a special focus on state-of-the-art methods in deep learning
multi-label mining, extreme multi-label classification and label ranking. We
conclude by offering a few future research directions.
</summary>
    <author>
      <name>Lihi Dery</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-24628-9_23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-24628-9_23" rel="related"/>
    <link href="http://arxiv.org/abs/2101.00583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00693v1</id>
    <updated>2021-01-03T19:57:45Z</updated>
    <published>2021-01-03T19:57:45Z</published>
    <title>Neural Networks for Keyword Spotting on IoT Devices</title>
    <summary>  We explore Neural Networks (NNs) for keyword spotting (KWS) on IoT devices
like smart speakers and wearables. Since we target to execute our NN on a
constrained memory and computation footprint, we propose a CNN design that. (i)
uses a limited number of multiplies. (ii) uses a limited number of model
parameters.
</summary>
    <author>
      <name>Rakesh Dhakshinamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.03219v1</id>
    <updated>2021-01-08T20:50:33Z</updated>
    <published>2021-01-08T20:50:33Z</published>
    <title>Benchmarking Machine Learning: How Fast Can Your Algorithms Go?</title>
    <summary>  This paper is focused on evaluating the effect of some different techniques
in machine learning speed-up, including vector caches, parallel execution, and
so on. The following content will include some review of the previous
approaches and our own experimental results.
</summary>
    <author>
      <name>Zeyu Ning</name>
    </author>
    <author>
      <name>Hugues Nelson Iradukunda</name>
    </author>
    <author>
      <name>Qingquan Zhang</name>
    </author>
    <author>
      <name>Ting Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2101.03219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.04347v1</id>
    <updated>2021-01-12T08:35:54Z</updated>
    <published>2021-01-12T08:35:54Z</published>
    <title>Proceedings of the NeurIPS 2020 Workshop on Machine Learning for the
  Developing World: Improving Resilience</title>
    <summary>  These are the proceedings of the 4th workshop on Machine Learning for the
Developing World (ML4D), held as part of the Thirty-fourth Conference on Neural
Information Processing Systems (NeurIPS) on Saturday, December 12th 2020.
</summary>
    <author>
      <name>Tejumade Afonja</name>
    </author>
    <author>
      <name>Konstantin Klemmer</name>
    </author>
    <author>
      <name>Aya Salama</name>
    </author>
    <author>
      <name>Paula Rodriguez Diaz</name>
    </author>
    <author>
      <name>Niveditha Kalavakonda</name>
    </author>
    <author>
      <name>Oluwafemi Azeez</name>
    </author>
    <link href="http://arxiv.org/abs/2101.04347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.04347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08576v1</id>
    <updated>2021-01-21T12:43:26Z</updated>
    <published>2021-01-21T12:43:26Z</published>
    <title>A Note on Connectivity of Sublevel Sets in Deep Learning</title>
    <summary>  It is shown that for deep neural networks, a single wide layer of width $N+1$
($N$ being the number of training samples) suffices to prove the connectivity
of sublevel sets of the training loss function. In the two-layer setting, the
same property may not hold even if one has just one neuron less (i.e. width $N$
can lead to disconnected sublevel sets).
</summary>
    <author>
      <name>Quynh Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/2101.08576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.02088v1</id>
    <updated>2021-02-03T14:42:58Z</updated>
    <published>2021-02-03T14:42:58Z</published>
    <title>Investigating Critical Risk Factors in Liver Cancer Prediction</title>
    <summary>  We exploit liver cancer prediction model using machine learning algorithms
based on epidemiological data of over 55 thousand peoples from 2014 to the
present. The best performance is an AUC of 0.71. We analyzed model parameters
to investigate critical risk factors that contribute the most to prediction.
</summary>
    <author>
      <name>Jinpeng Li</name>
    </author>
    <author>
      <name>Yaling Tao</name>
    </author>
    <author>
      <name>Ting Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.02088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.02088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06759v1</id>
    <updated>2021-02-12T20:22:56Z</updated>
    <published>2021-02-12T20:22:56Z</published>
    <title>Stochastic Gradient Langevin Dynamics with Variance Reduction</title>
    <summary>  Stochastic gradient Langevin dynamics (SGLD) has gained the attention of
optimization researchers due to its global optimization properties. This paper
proves an improved convergence property to local minimizers of nonconvex
objective functions using SGLD accelerated by variance reductions. Moreover, we
prove an ergodicity property of the SGLD scheme, which gives insights on its
potential to find global minimizers of nonconvex objectives.
</summary>
    <author>
      <name>Zhishen Huang</name>
    </author>
    <author>
      <name>Stephen Becker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN52387.2021.9533646</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN52387.2021.9533646" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCNN2021 (International Joint Conference on Neural Networks)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.06759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08012v1</id>
    <updated>2021-02-16T08:18:22Z</updated>
    <published>2021-02-16T08:18:22Z</published>
    <title>Training Stacked Denoising Autoencoders for Representation Learning</title>
    <summary>  We implement stacked denoising autoencoders, a class of neural networks that
are capable of learning powerful representations of high dimensional data. We
describe stochastic gradient descent for unsupervised training of autoencoders,
as well as a novel genetic algorithm based approach that makes use of gradient
information. We analyze the performance of both optimization algorithms and
also the representation learning ability of the autoencoder when it is trained
on standard image classification datasets.
</summary>
    <author>
      <name>Jason Liang</name>
    </author>
    <author>
      <name>Keith Kelly</name>
    </author>
    <link href="http://arxiv.org/abs/2102.08012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07009v1</id>
    <updated>2021-07-01T14:46:10Z</updated>
    <published>2021-07-01T14:46:10Z</published>
    <title>Free-Text Keystroke Dynamics for User Authentication</title>
    <summary>  In this research, we consider the problem of verifying user identity based on
keystroke dynamics obtained from free-text. We employ a novel feature
engineering method that generates image-like transition matrices. For this
image-like feature, a convolution neural network (CNN) with cutout achieves the
best results. A hybrid model consisting of a CNN and a recurrent neural network
(RNN) is also shown to outperform previous research in this field.
</summary>
    <author>
      <name>Jianwei Li</name>
    </author>
    <author>
      <name>Han-Chih Chang</name>
    </author>
    <author>
      <name>Mark Stamp</name>
    </author>
    <link href="http://arxiv.org/abs/2107.07009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07384v1</id>
    <updated>2021-07-07T07:45:15Z</updated>
    <published>2021-07-07T07:45:15Z</published>
    <title>A Fixed Version of Quadratic Program in Gradient Episodic Memory</title>
    <summary>  Gradient Episodic Memory is indeed a novel method for continual learning,
which solves new problems quickly without forgetting previously acquired
knowledge. However, in the process of studying the paper, we found there were
some problems in the proof of the dual problem of Quadratic Program, so here we
give our fixed version for this problem.
</summary>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Yiying Li</name>
    </author>
    <link href="http://arxiv.org/abs/2107.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08687v2</id>
    <updated>2021-08-17T11:03:15Z</updated>
    <published>2021-07-19T08:46:22Z</published>
    <title>Long-term series forecasting with Query Selector -- efficient model of
  sparse attention</title>
    <summary>  Various modifications of TRANSFORMER were recently used to solve time-series
forecasting problem. We propose Query Selector - an efficient, deterministic
algorithm for sparse attention matrix. Experiments show it achieves
state-of-the art results on ETT, Helpdesk and BPI'12 datasets.
</summary>
    <author>
      <name>Jacek Klimek</name>
    </author>
    <author>
      <name>Jakub Klimek</name>
    </author>
    <author>
      <name>Witold Kraskiewicz</name>
    </author>
    <author>
      <name>Mateusz Topolewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.08687v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08687v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.02814v1</id>
    <updated>2021-08-05T18:58:56Z</updated>
    <published>2021-08-05T18:58:56Z</published>
    <title>Potential Applications of Artificial Intelligence and Machine Learning
  in Radiochemistry and Radiochemical Engineering</title>
    <summary>  Artificial intelligence and machine learning are poised to disrupt PET
imaging from bench to clinic. In this perspective we offer insights into how
the technology could be applied to improve the design and synthesis of new
radiopharmaceuticals for PET imaging, including identification of an optimal
labeling approach as well as strategies for radiolabeling reaction
optimization.
</summary>
    <author>
      <name>E. William Webb</name>
    </author>
    <author>
      <name>Peter J. H. Scott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03491v1</id>
    <updated>2021-08-07T17:38:05Z</updated>
    <published>2021-08-07T17:38:05Z</published>
    <title>Approximate Last Iterate Convergence in Overparameterized GANs</title>
    <summary>  In this work, we showed that the Implicit Update and Predictive Methods
dynamics introduced in prior work satisfy last iterate convergence to a
neighborhood around the optimum in overparameterized GANs, where the size of
the neighborhood shrinks with the width of the neural network. This is in
contrast to prior results, which only guaranteed average iterate convergence.
</summary>
    <author>
      <name>Elbert Du</name>
    </author>
    <link href="http://arxiv.org/abs/2108.03491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05280v1</id>
    <updated>2021-08-11T15:27:55Z</updated>
    <published>2021-08-11T15:27:55Z</published>
    <title>Putting RDF2vec in Order</title>
    <summary>  The RDF2vec method for creating node embeddings on knowledge graphs is based
on word2vec, which, in turn, is agnostic towards the position of context words.
In this paper, we argue that this might be a shortcoming when training RDF2vec,
and show that using a word2vec variant which respects order yields considerable
performance gains especially on tasks where entities of different classes are
involved.
</summary>
    <author>
      <name>Jan Portisch</name>
    </author>
    <author>
      <name>Heiko Paulheim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ISWC 2021 posters and demos track</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.05280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.06812v1</id>
    <updated>2021-08-15T20:47:46Z</updated>
    <published>2021-08-15T20:47:46Z</published>
    <title>Batched Thompson Sampling for Multi-Armed Bandits</title>
    <summary>  We study Thompson Sampling algorithms for stochastic multi-armed bandits in
the batched setting, in which we want to minimize the regret over a sequence of
arm pulls using a small number of policy changes (or, batches). We propose two
algorithms and demonstrate their effectiveness by experiments on both synthetic
and real datasets. We also analyze the proposed algorithms from the theoretical
aspect and obtain almost tight regret-batches tradeoffs for the two-arm case.
</summary>
    <author>
      <name>Nikolai Karpov</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.06812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.06812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.07046v2</id>
    <updated>2021-08-19T15:11:38Z</updated>
    <published>2021-08-16T12:30:08Z</published>
    <title>WiseR: An end-to-end structure learning and deployment framework for
  causal graphical models</title>
    <summary>  Structure learning offers an expressive, versatile and explainable approach
to causal and mechanistic modeling of complex biological data. We present
wiseR, an open source application for learning, evaluating and deploying robust
causal graphical models using graph neural networks and Bayesian networks. We
demonstrate the utility of this application through application on for
biomarker discovery in a COVID-19 clinical dataset.
</summary>
    <author>
      <name>Shubham Maheshwari</name>
    </author>
    <author>
      <name>Khushbu Pahwa</name>
    </author>
    <author>
      <name>Tavpritesh Sethi</name>
    </author>
    <link href="http://arxiv.org/abs/2108.07046v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07046v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.09932v1</id>
    <updated>2021-08-23T04:59:16Z</updated>
    <published>2021-08-23T04:59:16Z</published>
    <title>Federated Learning Meets Fairness and Differential Privacy</title>
    <summary>  Deep learning's unprecedented success raises several ethical concerns ranging
from biased predictions to data privacy. Researchers tackle these issues by
introducing fairness metrics, or federated learning, or differential privacy. A
first, this work presents an ethical federated learning model, incorporating
all three measures simultaneously. Experiments on the Adult, Bank and Dutch
datasets highlight the resulting ``empirical interplay" between accuracy,
fairness, and privacy.
</summary>
    <author>
      <name>Manisha Padala</name>
    </author>
    <author>
      <name>Sankarshan Damle</name>
    </author>
    <author>
      <name>Sujit Gujar</name>
    </author>
    <link href="http://arxiv.org/abs/2108.09932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.09932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.10744v1</id>
    <updated>2021-08-24T13:56:15Z</updated>
    <published>2021-08-24T13:56:15Z</published>
    <title>Interpretable deep-learning models to help achieve the Sustainable
  Development Goals</title>
    <summary>  We discuss our insights into interpretable artificial-intelligence (AI)
models, and how they are essential in the context of developing ethical AI
systems, as well as data-driven solutions compliant with the Sustainable
Development Goals (SDGs). We highlight the potential of extracting
truly-interpretable models from deep-learning methods, for instance via
symbolic models obtained through inductive biases, to ensure a sustainable
development of AI.
</summary>
    <author>
      <name>Ricardo Vinuesa</name>
    </author>
    <author>
      <name>Beril Sirmacek</name>
    </author>
    <link href="http://arxiv.org/abs/2108.10744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.06458v3</id>
    <updated>2024-08-27T09:09:05Z</updated>
    <published>2021-09-14T05:54:29Z</published>
    <title>A Note on Knowledge Distillation Loss Function for Object Classification</title>
    <summary>  This research note provides a quick introduction to the knowledge
distillation loss function used in object classification. In particular, we
discuss its connection to a previously proposed logits matching loss function.
We further treat knowledge distillation as a specific form of output
regularization and demonstrate its connection to label smoothing and
entropy-based regularization.
</summary>
    <author>
      <name>Defang Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Research Note, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.06458v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.06458v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08628v1</id>
    <updated>2021-09-17T16:16:51Z</updated>
    <published>2021-09-17T16:16:51Z</published>
    <title>Autonomous Vision-based UAV Landing with Collision Avoidance using Deep
  Learning</title>
    <summary>  There is a risk of collision when multiple UAVs land simultaneously without
communication on the same platform. This work accomplishes vision-based
autonomous landing and uses a deep-learning-based method to realize collision
avoidance during the landing process.
</summary>
    <author>
      <name>Tianpei Liao</name>
    </author>
    <author>
      <name>Amal Haridevan</name>
    </author>
    <author>
      <name>Yibo Liu</name>
    </author>
    <author>
      <name>Jinjun Shan</name>
    </author>
    <link href="http://arxiv.org/abs/2109.08628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09541v1</id>
    <updated>2021-09-20T13:52:19Z</updated>
    <published>2021-09-20T13:52:19Z</published>
    <title>Scaling TensorFlow to 300 million predictions per second</title>
    <summary>  We present the process of transitioning machine learning models to the
TensorFlow framework at a large scale in an online advertising ecosystem. In
this talk we address the key challenges we faced and describe how we
successfully tackled them; notably, implementing the models in TF and serving
them efficiently with low latency using various optimization techniques.
</summary>
    <author>
      <name>Jan Hartman</name>
    </author>
    <author>
      <name>Davorin Kopič</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3460231.3474605</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3460231.3474605" rel="related"/>
    <link href="http://arxiv.org/abs/2109.09541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10162v3</id>
    <updated>2022-03-09T12:31:50Z</updated>
    <published>2021-09-21T13:19:04Z</published>
    <title>Learning low-degree functions from a logarithmic number of random
  queries</title>
    <summary>  We prove that every bounded function $f:\{-1,1\}^n\to[-1,1]$ of degree at
most $d$ can be learned with $L_2$-accuracy $\varepsilon$ and confidence
$1-\delta$ from $\log(\tfrac{n}{\delta})\,\varepsilon^{-d-1}
C^{d^{3/2}\sqrt{\log d}}$ random queries, where $C&gt;1$ is a universal finite
constant.
</summary>
    <author>
      <name>Alexandros Eskenazis</name>
    </author>
    <author>
      <name>Paata Ivanisvili</name>
    </author>
    <link href="http://arxiv.org/abs/2109.10162v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10162v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10317v2</id>
    <updated>2021-10-04T18:38:55Z</updated>
    <published>2021-09-21T16:57:01Z</published>
    <title>Introduction to Neural Network Verification</title>
    <summary>  Deep learning has transformed the way we think of software and what it can
do. But deep neural networks are fragile and their behaviors are often
surprising. In many settings, we need to provide formal guarantees on the
safety, security, correctness, or robustness of neural networks. This book
covers foundational ideas from formal verification and their adaptation to
reasoning about neural networks and deep learning.
</summary>
    <author>
      <name>Aws Albarghouthi</name>
    </author>
    <link href="http://arxiv.org/abs/2109.10317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04981v1</id>
    <updated>2021-11-09T08:03:51Z</updated>
    <published>2021-11-09T08:03:51Z</published>
    <title>Wasserstein Adversarially Regularized Graph Autoencoder</title>
    <summary>  This paper introduces Wasserstein Adversarially Regularized Graph Autoencoder
(WARGA), an implicit generative algorithm that directly regularizes the latent
distribution of node embedding to a target distribution via the Wasserstein
metric. The proposed method has been validated in tasks of link prediction and
node clustering on real-world graphs, in which WARGA generally outperforms
state-of-the-art models based on Kullback-Leibler (KL) divergence and typical
adversarial framework.
</summary>
    <author>
      <name>Huidong Liang</name>
    </author>
    <author>
      <name>Junbin Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. 2021 NeurIPS OTML Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.04981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06736v1</id>
    <updated>2021-11-11T13:45:11Z</updated>
    <published>2021-11-11T13:45:11Z</published>
    <title>The Science of Rejection: A Research Area for Human Computation</title>
    <summary>  We motivate why the science of learning to reject model predictions is
central to ML, and why human computation has a lead role in this effort.
</summary>
    <author>
      <name>Burcu Sayin</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Andrea Passerini</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of The 9th AAAI Conference on Human
  Computation and Crowdsourcing (HCOMP 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="machine learning, human in the loop" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.11848v1</id>
    <updated>2021-11-22T12:49:37Z</updated>
    <published>2021-11-22T12:49:37Z</published>
    <title>Time Series Prediction about Air Quality using LSTM-Based Models: A
  Systematic Mapping</title>
    <summary>  This systematic mapping study investigates the use of Long short-term memory
networks to predict time series data about air quality, trying to understand
the reasons, characteristics and methods available in the scientific
literature, identify gaps in the researched area and potential approaches that
can be exploited on later studies.
</summary>
    <author>
      <name>Lucas L. S. Sachetti</name>
    </author>
    <author>
      <name>Vinicius F. S. Mota</name>
    </author>
    <link href="http://arxiv.org/abs/2111.11848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.11848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.15258v1</id>
    <updated>2021-11-30T10:17:58Z</updated>
    <published>2021-11-30T10:17:58Z</published>
    <title>DeepAL: Deep Active Learning in Python</title>
    <summary>  We present DeepAL, a Python library that implements several common strategies
for active learning, with a particular emphasis on deep active learning. DeepAL
provides a simple and unified framework based on PyTorch that allows users to
easily load custom datasets, build custom data handlers, and design custom
strategies without much modification of codes. DeepAL is open-source on Github
and welcome any contribution.
</summary>
    <author>
      <name>Kuan-Hao Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">project url: https://github.com/ej0cl6/deep-active-learning</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.15258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00179v1</id>
    <updated>2021-11-30T23:53:22Z</updated>
    <published>2021-11-30T23:53:22Z</published>
    <title>A collection of the accepted abstracts for the Machine Learning for
  Health (ML4H) symposium 2021</title>
    <summary>  A collection of the accepted abstracts for the Machine Learning for Health
(ML4H) symposium 2021. This index is not complete, as some accepted abstracts
chose to opt-out of inclusion.
</summary>
    <author>
      <name>Fabian Falck</name>
    </author>
    <author>
      <name>Yuyin Zhou</name>
    </author>
    <author>
      <name>Emma Rocheteau</name>
    </author>
    <author>
      <name>Liyue Shen</name>
    </author>
    <author>
      <name>Luis Oala</name>
    </author>
    <author>
      <name>Girmaw Abebe</name>
    </author>
    <author>
      <name>Subhrajit Roy</name>
    </author>
    <author>
      <name>Stephen Pfohl</name>
    </author>
    <author>
      <name>Emily Alsentzer</name>
    </author>
    <author>
      <name>Matthew B. A. McDermott</name>
    </author>
    <link href="http://arxiv.org/abs/2112.00179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.03595v1</id>
    <updated>2021-12-07T09:42:57Z</updated>
    <published>2021-12-07T09:42:57Z</published>
    <title>State-of-the-art predictive and prescriptive analytics for IEEE CIS 3rd
  Technical Challenge</title>
    <summary>  In this paper, we describe our proposed methodology to approach the
predict+optimise challenge introduced in the IEEE CIS 3rd Technical Challenge.
The predictive model employs an ensemble of LightGBM models and the
prescriptive analysis employs mathematical optimisation to efficiently
prescribe solutions that minimise the average cost over multiple scenarios. Our
solutions ranked 1st in the optimisation and 2nd in the prediction challenge of
the competition.
</summary>
    <author>
      <name>Mahdi Abolghasemi</name>
    </author>
    <author>
      <name>Rasul Esmaeilbeigi</name>
    </author>
    <link href="http://arxiv.org/abs/2112.03595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.03595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.09305v1</id>
    <updated>2021-12-17T03:59:15Z</updated>
    <published>2021-12-17T03:59:15Z</published>
    <title>Gaussian RBF Centered Kernel Alignment (CKA) in the Large Bandwidth
  Limit</title>
    <summary>  We prove that Centered Kernel Alignment (CKA) based on a Gaussian RBF kernel
converges to linear CKA in the large-bandwidth limit. We show that convergence
onset is sensitive to the geometry of the feature representations, and that
representation eccentricity bounds the range of bandwidths for which Gaussian
CKA behaves nonlinearly.
</summary>
    <author>
      <name>Sergio A. Alvarez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston College, Chestnut Hill, MA, USA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.09305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.09305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.0; I.5.2; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11687v1</id>
    <updated>2021-12-22T06:20:27Z</updated>
    <published>2021-12-22T06:20:27Z</published>
    <title>Squareplus: A Softplus-Like Algebraic Rectifier</title>
    <summary>  We present squareplus, an activation function that resembles softplus, but
which can be computed using only algebraic operations: addition,
multiplication, and square-root. Because squareplus is ~6x faster to evaluate
than softplus on a CPU and does not require access to transcendental functions,
it may have practical value in resource-limited deep learning applications.
</summary>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://github.com/jonbarron/squareplus</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.12181v2</id>
    <updated>2024-06-14T19:39:04Z</updated>
    <published>2021-12-22T19:16:24Z</published>
    <title>Simple and near-optimal algorithms for hidden stratification and
  multi-group learning</title>
    <summary>  Multi-group agnostic learning is a formal learning criterion that is
concerned with the conditional risks of predictors within subgroups of a
population. The criterion addresses recent practical concerns such as subgroup
fairness and hidden stratification. This paper studies the structure of
solutions to the multi-group learning problem, and provides simple and
near-optimal algorithms for the learning problem.
</summary>
    <author>
      <name>Christopher Tosh</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/2112.12181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.15026v2</id>
    <updated>2022-05-09T05:43:36Z</updated>
    <published>2021-12-30T12:23:50Z</published>
    <title>Two Instances of Interpretable Neural Network for Universal
  Approximations</title>
    <summary>  This paper proposes two bottom-up interpretable neural network (NN)
constructions for universal approximation, namely Triangularly-constructed NN
(TNN) and Semi-Quantized Activation NN (SQANN). Further notable properties are
(1) resistance to catastrophic forgetting (2) existence of proof for
arbitrarily high accuracies (3) the ability to identify samples that are
out-of-distribution through interpretable activation "fingerprints".
</summary>
    <author>
      <name>Erico Tjoa</name>
    </author>
    <author>
      <name>Guan Cuntai</name>
    </author>
    <link href="http://arxiv.org/abs/2112.15026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.15026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.15568v1</id>
    <updated>2021-12-31T17:42:45Z</updated>
    <published>2021-12-31T17:42:45Z</published>
    <title>Actor Loss of Soft Actor Critic Explained</title>
    <summary>  This technical report is devoted to explaining how the actor loss of soft
actor critic is obtained, as well as the associated gradient estimate. It gives
the necessary mathematical background to derive all the presented equations,
from the theoretical actor loss to the one implemented in practice. This
necessitates a comparison of the reparameterization trick used in soft actor
critic with the nabla log trick, which leads to open questions regarding the
most efficient method to use.
</summary>
    <author>
      <name>Thibault Lahire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.15568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.15568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07697v1</id>
    <updated>2022-04-16T02:08:50Z</updated>
    <published>2022-04-16T02:08:50Z</published>
    <title>Theory of Graph Neural Networks: Representation and Learning</title>
    <summary>  Graph Neural Networks (GNNs), neural network architectures targeted to
learning representations of graphs, have become a popular learning model for
prediction tasks on nodes, graphs and configurations of points, with wide
success in practice. This article summarizes a selection of the emerging
theoretical results on approximation and learning properties of widely used
message passing GNNs and higher-order GNNs, focusing on representation,
generalization and extrapolation. Along the way, it summarizes mathematical
connections.
</summary>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <link href="http://arxiv.org/abs/2204.07697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07987v1</id>
    <updated>2022-04-17T12:08:08Z</updated>
    <published>2022-04-17T12:08:08Z</published>
    <title>Fair Classification under Covariate Shift and Missing Protected
  Attribute -- an Investigation using Related Features</title>
    <summary>  This study investigated the problem of fair classification under Covariate
Shift and missing protected attribute using a simple approach based on the use
of importance-weights to handle covariate-shift and, Related Features
arXiv:2104.14537 to handle missing protected attribute.
</summary>
    <author>
      <name>Manan Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.07987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.02752v1</id>
    <updated>2022-03-28T05:51:44Z</updated>
    <published>2022-03-28T05:51:44Z</published>
    <title>A collection of invited non-archival papers for the Conference on
  Health, Inference, and Learning (CHIL) 2022</title>
    <summary>  A collection of invited non-archival papers for the Conference on Health,
Inference, and Learning (CHIL) 2022. This index is incomplete as some authors
of invited non-archival presentations opted not to include their papers in this
index.
</summary>
    <author>
      <name>Gerardo Flores</name>
    </author>
    <author>
      <name>George H. Chen</name>
    </author>
    <author>
      <name>Tom Pollard</name>
    </author>
    <author>
      <name>Joyce C. Ho</name>
    </author>
    <author>
      <name>Tristan Naumann</name>
    </author>
    <link href="http://arxiv.org/abs/2205.02752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03712v1</id>
    <updated>2022-05-07T20:02:43Z</updated>
    <published>2022-05-07T20:02:43Z</published>
    <title>Accuracy Convergent Field Predictors</title>
    <summary>  Several predictive algorithms are described. Highlighted are variants that
make predictions by superposing fields associated to the training data
instances. They operate seamlessly with categorical, continuous, and mixed
data. Predictive accuracy convergence is also discussed as a criteria for
evaluating predictive algorithms. Methods are described on how to adapt
algorithms in order to make them achieve predictive accuracy convergence.
</summary>
    <author>
      <name>Cristian Alb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.30987.16165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.30987.16165" rel="related"/>
    <link href="http://arxiv.org/abs/2205.03712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06969v1</id>
    <updated>2022-05-14T05:05:37Z</updated>
    <published>2022-05-14T05:05:37Z</published>
    <title>Mask CycleGAN: Unpaired Multi-modal Domain Translation with
  Interpretable Latent Variable</title>
    <summary>  We propose Mask CycleGAN, a novel architecture for unpaired image domain
translation built based on CycleGAN, with an aim to address two issues: 1)
unimodality in image translation and 2) lack of interpretability of latent
variables. Our innovation in the technical approach is comprised of three key
components: masking scheme, generator and objective. Experimental results
demonstrate that this architecture is capable of bringing variations to
generated images in a controllable manner and is reasonably robust to different
masks.
</summary>
    <author>
      <name>Minfa Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2205.06969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.08978v2</id>
    <updated>2022-05-27T19:26:41Z</updated>
    <published>2022-05-18T15:02:01Z</published>
    <title>Fast Neural Network based Solving of Partial Differential Equations</title>
    <summary>  We present a novel method for using Neural Networks (NNs) for finding
solutions to a class of Partial Differential Equations (PDEs). Our method
builds on recent advances in Neural Radiance Field research (NeRFs) and allows
for a NN to converge to a PDE solution much faster than classic Physically
Informed Neural Network (PINNs) approaches.
</summary>
    <author>
      <name>Jaroslaw Rzepecki</name>
    </author>
    <author>
      <name>Daniel Bates</name>
    </author>
    <author>
      <name>Chris Doran</name>
    </author>
    <link href="http://arxiv.org/abs/2205.08978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.08978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11257v1</id>
    <updated>2022-05-19T23:56:07Z</updated>
    <published>2022-05-19T23:56:07Z</published>
    <title>Manifold-aligned Neighbor Embedding</title>
    <summary>  In this paper, we introduce a neighbor embedding framework for manifold
alignment. We demonstrate the efficacy of the framework using a
manifold-aligned version of the uniform manifold approximation and projection
algorithm. We show that our algorithm can learn an aligned manifold that is
visually competitive to embedding of the whole dataset.
</summary>
    <author>
      <name>Mohammad Tariqul Islam</name>
    </author>
    <author>
      <name>Jason W. Fleischer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ICLR 2022 Workshop on Geometrical and Topological
  Representation Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.11257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.12752v1</id>
    <updated>2022-05-25T13:01:12Z</updated>
    <published>2022-05-25T13:01:12Z</published>
    <title>NECA: Network-Embedded Deep Representation Learning for Categorical Data</title>
    <summary>  We propose NECA, a deep representation learning method for categorical data.
Built upon the foundations of network embedding and deep unsupervised
representation learning, NECA deeply embeds the intrinsic relationship among
attribute values and explicitly expresses data objects with numeric vector
representations. Designed specifically for categorical data, NECA can support
important downstream data mining tasks, such as clustering. Extensive
experimental analysis demonstrated the effectiveness of NECA.
</summary>
    <author>
      <name>Xiaonan Gao</name>
    </author>
    <author>
      <name>Sen Wu</name>
    </author>
    <author>
      <name>Wenjun Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2205.12752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.12752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.14236v1</id>
    <updated>2022-05-27T21:05:52Z</updated>
    <published>2022-05-27T21:05:52Z</published>
    <title>FedControl: When Control Theory Meets Federated Learning</title>
    <summary>  To date, the most popular federated learning algorithms use coordinate-wise
averaging of the model parameters. We depart from this approach by
differentiating client contributions according to the performance of local
learning and its evolution. The technique is inspired from control theory and
its classification performance is evaluated extensively in IID framework and
compared with FedAvg.
</summary>
    <author>
      <name>Adnan Ben Mansour</name>
    </author>
    <author>
      <name>Gaia Carenini</name>
    </author>
    <author>
      <name>Alexandre Duplessis</name>
    </author>
    <author>
      <name>David Naccache</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2205.10864</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.14236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00255v1</id>
    <updated>2022-06-01T06:34:35Z</updated>
    <published>2022-06-01T06:34:35Z</published>
    <title>Star algorithm for NN ensembling</title>
    <summary>  Neural network ensembling is a common and robust way to increase model
efficiency. In this paper, we propose a new neural network ensemble algorithm
based on Audibert's empirical star algorithm. We provide optimal theoretical
minimax bound on the excess squared risk. Additionally, we empirically study
this algorithm on regression and classification tasks and compare it to most
popular ensembling methods.
</summary>
    <author>
      <name>Sergey Zinchenko</name>
    </author>
    <author>
      <name>Dmitry Lishudi</name>
    </author>
    <link href="http://arxiv.org/abs/2206.00255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03328v2</id>
    <updated>2022-06-12T14:05:14Z</updated>
    <published>2022-06-07T14:12:21Z</published>
    <title>Concentration bounds for SSP Q-learning for average cost MDPs</title>
    <summary>  We derive a concentration bound for a Q-learning algorithm for average cost
Markov decision processes based on an equivalent shortest path problem, and
compare it numerically with the alternative scheme based on relative value
iteration.
</summary>
    <author>
      <name>Shaan Ul Haque</name>
    </author>
    <author>
      <name>Vivek Borkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.03328v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03328v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.05928v2</id>
    <updated>2022-06-28T09:39:14Z</updated>
    <published>2022-06-13T06:54:05Z</published>
    <title>Compressive Clustering with an Optical Processing Unit</title>
    <summary>  We explore the use of Optical Processing Units (OPU) to compute random
Fourier features for sketching, and adapt the overall compressive clustering
pipeline to this setting. We also propose some tools to help tuning a critical
hyper-parameter of compressive clustering.
</summary>
    <author>
      <name>Luc Giffon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DANTE</arxiv:affiliation>
    </author>
    <author>
      <name>Rémi Gribonval</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DANTE</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GRETSI 2022 - XXVIII{\`e}me Colloque Francophone de Traitement du
  Signal et des Images, Sep 2022, Nancy, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.05928v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05928v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07658v1</id>
    <updated>2022-05-16T13:55:50Z</updated>
    <published>2022-05-16T13:55:50Z</published>
    <title>Experimental Validation of Spectral-Spatial Power Evolution Design Using
  Raman Amplifiers</title>
    <summary>  We experimentally validate a machine learning-enabled Raman amplification
framework, capable of jointly shaping the signal power evolution in two
domains: frequency and fiber distance. The proposed experiment addresses the
amplification in the whole C-band, by optimizing four first-order
counter-propagating Raman pumps.
</summary>
    <author>
      <name>Mehran Soltani</name>
    </author>
    <author>
      <name>Francesco Da Ros</name>
    </author>
    <author>
      <name>Andrea Carena</name>
    </author>
    <author>
      <name>Darko Zibar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.07658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12416v1</id>
    <updated>2022-06-24T02:07:54Z</updated>
    <published>2022-06-24T02:07:54Z</published>
    <title>A Grey-box Launch-profile Aware Model for C+L Band Raman Amplification</title>
    <summary>  Based on the physical features of Raman amplification, we propose a
three-step modelling scheme based on neural networks (NN) and linear
regression. Higher accuracy, less data requirements and lower computational
complexity are demonstrated through simulations compared with the pure NN-based
method.
</summary>
    <author>
      <name>Yihao Zhang</name>
    </author>
    <author>
      <name>Xiaomin Liu</name>
    </author>
    <author>
      <name>Yichen Liu</name>
    </author>
    <author>
      <name>Lilin Yi</name>
    </author>
    <author>
      <name>Weisheng Hu</name>
    </author>
    <author>
      <name>Qunbi Zhuge</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13446v1</id>
    <updated>2022-06-27T16:53:18Z</updated>
    <published>2022-06-27T16:53:18Z</published>
    <title>Pen and Paper Exercises in Machine Learning</title>
    <summary>  This is a collection of (mostly) pen-and-paper exercises in machine learning.
The exercises are on the following topics: linear algebra, optimisation,
directed graphical models, undirected graphical models, expressive power of
graphical models, factor graphs and message passing, inference for hidden
Markov models, model-based learning (including ICA and unnormalised models),
sampling and Monte-Carlo integration, and variational inference.
</summary>
    <author>
      <name>Michael U. Gutmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The associated github page is
  https://github.com/michaelgutmann/ml-pen-and-paper-exercises</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.13446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02847v1</id>
    <updated>2022-07-05T08:31:24Z</updated>
    <published>2022-07-05T08:31:24Z</published>
    <title>Scoring Rules for Performative Binary Prediction</title>
    <summary>  We construct a model of expert prediction where predictions can influence the
state of the world. Under this model, we show through theoretical and numerical
results that proper scoring rules can incentivize experts to manipulate the
world with their predictions. We also construct a simple class of scoring rules
that avoids this problem.
</summary>
    <author>
      <name>Alan Chan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted to workshop on Strategic Machine Learning at
  NeurIPS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.08950v1</id>
    <updated>2022-07-18T21:30:03Z</updated>
    <published>2022-07-18T21:30:03Z</published>
    <title>Adversarial Training Improves Joint Energy-Based Generative Modelling</title>
    <summary>  We propose the novel framework for generative modelling using hybrid
energy-based models. In our method we combine the interpretable input gradients
of the robust classifier and Langevin Dynamics for sampling. Using the
adversarial training we improve not only the training stability, but robustness
and generative modelling of the joint energy-based models.
</summary>
    <author>
      <name>Rostislav Korst</name>
    </author>
    <author>
      <name>Arip Asadulaev</name>
    </author>
    <link href="http://arxiv.org/abs/2207.08950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.01185v1</id>
    <updated>2022-08-02T01:05:46Z</updated>
    <published>2022-08-02T01:05:46Z</published>
    <title>A Note on Zeroth-Order Optimization on the Simplex</title>
    <summary>  We construct a zeroth-order gradient estimator for a smooth function defined
on the probability simplex. The proposed estimator queries the simplex only. We
prove that projected gradient descent and the exponential weights algorithm,
when run with this estimator instead of exact gradients, converge at a
$\mathcal O(T^{-1/4})$ rate.
</summary>
    <author>
      <name>Tijana Zrnic</name>
    </author>
    <author>
      <name>Eric Mazumdar</name>
    </author>
    <link href="http://arxiv.org/abs/2208.01185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.01951v1</id>
    <updated>2022-08-03T09:59:11Z</updated>
    <published>2022-08-03T09:59:11Z</published>
    <title>Exploration with Model Uncertainty at Extreme Scale in Real-Time Bidding</title>
    <summary>  In this work, we present a scalable and efficient system for exploring the
supply landscape in real-time bidding. The system directs exploration based on
the predictive uncertainty of models used for click-through rate prediction and
works in a high-throughput, low-latency environment. Through online A/B
testing, we demonstrate that exploration with model uncertainty has a positive
impact on model performance and business KPIs.
</summary>
    <author>
      <name>Jan Hartman</name>
    </author>
    <author>
      <name>Davorin Kopič</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3523227.3547383</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3523227.3547383" rel="related"/>
    <link href="http://arxiv.org/abs/2208.01951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04828v1</id>
    <updated>2022-08-09T15:12:43Z</updated>
    <published>2022-08-09T15:12:43Z</published>
    <title>Global Evaluation for Decision Tree Learning</title>
    <summary>  We transfer distances on clusterings to the building process of decision
trees, and as a consequence extend the classical ID3 algorithm to perform
modifications based on the global distance of the tree to the ground
truth--instead of considering single leaves. Next, we evaluate this idea in
comparison with the original version and discuss occurring problems, but also
strengths of the global approach. On this basis, we finish by identifying other
scenarios where global evaluations are worthwhile.
</summary>
    <author>
      <name>Fabian Spaeh</name>
    </author>
    <author>
      <name>Sven Kosub</name>
    </author>
    <link href="http://arxiv.org/abs/2208.04828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09643v1</id>
    <updated>2022-08-20T09:13:23Z</updated>
    <published>2022-08-20T09:13:23Z</published>
    <title>The computational complexity of some explainable clustering problems</title>
    <summary>  We study the computational complexity of some explainable clustering problems
in the framework proposed by [Dasgupta et al., ICML 2020], where explainability
is achieved via axis-aligned decision trees. We consider the $k$-means,
$k$-medians, $k$-centers and the spacing cost functions. We prove that the
first three are hard to optimize while the latter can be optimized in
polynomial time.
</summary>
    <author>
      <name>Eduardo Sany Laber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages and 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.09643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10463v1</id>
    <updated>2022-08-22T17:33:59Z</updated>
    <published>2022-08-22T17:33:59Z</published>
    <title>Survey of Machine Learning Techniques To Predict Heartbeat Arrhythmias</title>
    <summary>  Many works in biomedical computer science research use machine learning
techniques to give accurate results. However, these techniques may not be
feasible for real-time analysis of data pulled from live hospital feeds. In
this project, different machine learning techniques are compared from various
sources to find one that provides not only high accuracy but also low latency
and memory overhead to be used in real-world health care systems.
</summary>
    <author>
      <name>Samuel Armstrong</name>
    </author>
    <link href="http://arxiv.org/abs/2208.10463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11236v4</id>
    <updated>2022-09-08T15:56:19Z</updated>
    <published>2022-08-23T23:57:40Z</published>
    <title>Psychophysical Machine Learning</title>
    <summary>  The Weber Fechner Law of psychophysics observes that human perception is
logarithmic in the stimulus. We present an algorithm for incorporating the
Weber Fechner law into loss functions for machine learning, and use the
algorithm to enhance the performance of deep learning networks.
</summary>
    <author>
      <name>B. N. Kausik</name>
    </author>
    <link href="http://arxiv.org/abs/2208.11236v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11236v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11337v1</id>
    <updated>2022-08-24T07:36:05Z</updated>
    <published>2022-08-24T07:36:05Z</published>
    <title>A Bayesian Variational principle for dynamic Self Organizing Maps</title>
    <summary>  We propose organisation conditions that yield a method for training SOM with
adaptative neighborhood radius in a variational Bayesian framework. This method
is validated on a non-stationary setting and compared in an high-dimensional
setting with an other adaptative method.
</summary>
    <author>
      <name>Anthony Fillion</name>
    </author>
    <author>
      <name>Thibaut Kulak</name>
    </author>
    <author>
      <name>François Blayo</name>
    </author>
    <link href="http://arxiv.org/abs/2208.11337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11792v1</id>
    <updated>2022-08-24T23:00:32Z</updated>
    <published>2022-08-24T23:00:32Z</published>
    <title>A Survey of Open Source Automation Tools for Data Science Predictions</title>
    <summary>  We present an expository overview of technical and cultural challenges to the
development and adoption of automation at various stages in the data science
prediction lifecycle, restricting focus to supervised learning with structured
datasets. In addition, we review popular open source Python tools implementing
common solution patterns for the automation challenges and highlight gaps where
we feel progress still demands to be made.
</summary>
    <author>
      <name>Nicholas Hoell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages, 1 figure, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.11792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.12813v1</id>
    <updated>2022-08-26T06:10:37Z</updated>
    <published>2022-08-26T06:10:37Z</published>
    <title>Abnormal Local Clustering in Federated Learning</title>
    <summary>  Federated learning is a model for privacy without revealing private data by
transfer models instead of personal and private data from local client devices.
While, in the global model, it's crucial to recognize each local data is
normal. This paper suggests one method to separate normal locals and abnormal
locals by Euclidean similarity clustering of vectors extracted by inputting
dummy data in local models. In a federated classification model, this method
divided locals into normal and abnormal.
</summary>
    <author>
      <name>Jihwan Won</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.12813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01820v1</id>
    <updated>2022-09-05T08:06:29Z</updated>
    <published>2022-09-05T08:06:29Z</published>
    <title>Natural Policy Gradients In Reinforcement Learning Explained</title>
    <summary>  Traditional policy gradient methods are fundamentally flawed. Natural
gradients converge quicker and better, forming the foundation of contemporary
Reinforcement Learning such as Trust Region Policy Optimization (TRPO) and
Proximal Policy Optimization (PPO). This lecture note aims to clarify the
intuition behind natural policy gradients, focusing on the thought process and
the key mathematical constructs.
</summary>
    <author>
      <name>W. J. A. van Heeswijk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.01820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.05084v1</id>
    <updated>2022-09-12T08:36:35Z</updated>
    <published>2022-09-12T08:36:35Z</published>
    <title>Explaining Predictions from Machine Learning Models: Algorithms, Users,
  and Pedagogy</title>
    <summary>  Model explainability has become an important problem in machine learning (ML)
due to the increased effect that algorithmic predictions have on humans.
Explanations can help users understand not only why ML models make certain
predictions, but also how these predictions can be changed. In this thesis, we
examine the explainability of ML models from three vantage points: algorithms,
users, and pedagogy, and contribute several novel solutions to the
explainability problem.
</summary>
    <author>
      <name>Ana Lucic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.05084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.05084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.06113v2</id>
    <updated>2022-12-23T02:45:47Z</updated>
    <published>2022-09-12T03:21:14Z</published>
    <title>Generate synthetic samples from tabular data</title>
    <summary>  Generating new samples from data sets can mitigate extra expensive
operations, increased invasive procedures, and mitigate privacy issues. These
novel samples that are statistically robust can be used as a temporary and
intermediate replacement when privacy is a concern. This method can enable
better data sharing practices without problems relating to identification
issues or biases that are flaws for an adversarial attack.
</summary>
    <author>
      <name>David Banh</name>
    </author>
    <author>
      <name>Alan Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2209.06113v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06113v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.06302v1</id>
    <updated>2022-09-13T21:09:34Z</updated>
    <published>2022-09-13T21:09:34Z</published>
    <title>Optimization without Backpropagation</title>
    <summary>  Forward gradients have been recently introduced to bypass backpropagation in
autodifferentiation, while retaining unbiased estimators of true gradients. We
derive an optimality condition to obtain best approximating forward gradients,
which leads us to mathematical insights that suggest optimization in high
dimension is challenging with forward gradients. Our extensive experiments on
test functions support this claim.
</summary>
    <author>
      <name>Gabriel Belouze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures, associated implementation available at
  https://github.com/gbelouze/forward-gradient</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.06302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.06529v1</id>
    <updated>2022-09-14T10:07:44Z</updated>
    <published>2022-09-14T10:07:44Z</published>
    <title>Data Privacy and Trustworthy Machine Learning</title>
    <summary>  The privacy risks of machine learning models is a major concern when training
them on sensitive and personal data. We discuss the tradeoffs between data
privacy and the remaining goals of trustworthy machine learning (notably,
fairness, robustness, and explainability).
</summary>
    <author>
      <name>Martin Strobel</name>
    </author>
    <author>
      <name>Reza Shokri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSEC.2022.3178187</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSEC.2022.3178187" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Copyright \copyright 2022, IEEE</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: IEEE Security &amp; Privacy ( Volume: 20, Issue: 5,
  Sept.-Oct. 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.06529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.08422v1</id>
    <updated>2022-09-17T22:59:42Z</updated>
    <published>2022-09-17T22:59:42Z</published>
    <title>Computed Decision Weights and a New Learning Algorithm for Neural
  Classifiers</title>
    <summary>  In this paper we consider the possibility of computing rather than training
the decision layer weights of a neural classifier. Such a possibility arises in
two way, from making an appropriate choice of loss function and by solving a
problem of constrained optimization. The latter formulation leads to a
promising new learning process for pre-decision weights with both simplicity
and efficacy.
</summary>
    <author>
      <name>Eugene Wong</name>
    </author>
    <link href="http://arxiv.org/abs/2209.08422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.08422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.05650v1</id>
    <updated>2022-10-11T17:49:01Z</updated>
    <published>2022-10-11T17:49:01Z</published>
    <title>Regret Bounds for Risk-Sensitive Reinforcement Learning</title>
    <summary>  In safety-critical applications of reinforcement learning such as healthcare
and robotics, it is often desirable to optimize risk-sensitive objectives that
account for tail outcomes rather than expected reward. We prove the first
regret bounds for reinforcement learning under a general class of
risk-sensitive objectives including the popular CVaR objective. Our theory is
based on a novel characterization of the CVaR objective as well as a novel
optimistic MDP construction.
</summary>
    <author>
      <name>O. Bastani</name>
    </author>
    <author>
      <name>Y. J. Ma</name>
    </author>
    <author>
      <name>E. Shen</name>
    </author>
    <author>
      <name>W. Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2210.05650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.05650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10660v1</id>
    <updated>2022-10-19T15:31:42Z</updated>
    <published>2022-10-19T15:31:42Z</published>
    <title>Binary Orthogonal Non-negative Matrix Factorization</title>
    <summary>  We propose a method for computing binary orthogonal non-negative matrix
factorization (BONMF) for clustering and classification. The method is tested
on several representative real-world data sets. The numerical results confirm
that the method has improved accuracy compared to the related techniques. The
proposed method is fast for training and classification and space efficient.
</summary>
    <author>
      <name>S. Fathi Hafshejani</name>
    </author>
    <author>
      <name>D. Gaur</name>
    </author>
    <author>
      <name>S. Hossain</name>
    </author>
    <author>
      <name>R. Benkoczi</name>
    </author>
    <link href="http://arxiv.org/abs/2210.10660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.11997v2</id>
    <updated>2022-10-26T15:20:07Z</updated>
    <published>2022-10-21T14:39:01Z</published>
    <title>Extending F1 metric, probabilistic approach</title>
    <summary>  This article explores the extension of well-known F1 score used for assessing
the performance of binary classifiers. We propose the new metric using
probabilistic interpretation of precision, recall, specificity, and negative
predictive value. We describe its properties and compare it to common metrics.
Then we demonstrate its behavior in edge cases of the confusion matrix.
Finally, the properties of the metric are tested on binary classifier trained
on the real dataset.
</summary>
    <author>
      <name>Mikolaj Sitarz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.54364/AAIML.2023.1161</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.54364/AAIML.2023.1161" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.11997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12746v2</id>
    <updated>2022-10-26T17:23:29Z</updated>
    <published>2022-10-23T15:05:14Z</published>
    <title>Principal Component Classification</title>
    <summary>  We propose to directly compute classification estimates by learning features
encoded with their class scores using PCA. Our resulting model has a
encoder-decoder structure suitable for supervised learning, it is
computationally efficient and performs well for classification on several
datasets.
</summary>
    <author>
      <name>Rozenn Dahyot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; 5 figures; 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.04973v1</id>
    <updated>2022-11-09T15:38:50Z</updated>
    <published>2022-11-09T15:38:50Z</published>
    <title>Accelerating Adversarial Perturbation by 50% with Semi-backward
  Propagation</title>
    <summary>  Adversarial perturbation plays a significant role in the field of adversarial
robustness, which solves a maximization problem over the input data. We show
that the backward propagation of such optimization can accelerate $2\times$
(and thus the overall optimization including the forward propagation can
accelerate $1.5\times$), without any utility drop, if we only compute the
output gradient but not the parameter gradient during the backward propagation.
</summary>
    <author>
      <name>Zhiqi Bu</name>
    </author>
    <link href="http://arxiv.org/abs/2211.04973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16038v1</id>
    <updated>2022-11-29T09:22:42Z</updated>
    <published>2022-11-29T09:22:42Z</published>
    <title>Data-efficient Modeling of Optical Matrix Multipliers Using Transfer
  Learning</title>
    <summary>  We demonstrate transfer learning-assisted neural network models for optical
matrix multipliers with scarce measurement data. Our approach uses &lt;10\% of
experimental data needed for best performance and outperforms analytical models
for a Mach-Zehnder interferometer mesh.
</summary>
    <author>
      <name>Ali Cem</name>
    </author>
    <author>
      <name>Ognjen Jovanovic</name>
    </author>
    <author>
      <name>Siqi Yan</name>
    </author>
    <author>
      <name>Yunhong Ding</name>
    </author>
    <author>
      <name>Darko Zibar</name>
    </author>
    <author>
      <name>Francesco Da Ros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figues, submitted to CLEO</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.16038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.02364v1</id>
    <updated>2022-12-05T15:47:26Z</updated>
    <published>2022-12-05T15:47:26Z</published>
    <title>Indoor room Occupancy Counting based on LSTM and Environmental Sensor</title>
    <summary>  This paper realizes the estimation of classroom occupancy by using the CO2
sensor and deep learning technique named Long-Short-Term Memory. As a case of
connection with IoT and machine learning, I achieve the model to estimate the
people number in the classroom based on the environmental data exported from
the CO2 sensor, I also evaluate the performance of the model to show the
feasibility to apply our module to the real environment.
</summary>
    <author>
      <name>Zheyu Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2212.02364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.05663v2</id>
    <updated>2022-12-24T02:29:17Z</updated>
    <published>2022-12-12T02:28:54Z</published>
    <title>On an Interpretation of ResNets via Solution Constructions</title>
    <summary>  This paper first constructs a typical solution of ResNets for multi-category
classifications by the principle of gate-network controls and deep-layer
classifications, from which a general interpretation of the ResNet architecture
is given and the performance mechanism is explained. We then use more solutions
to further demonstrate the generality of that interpretation. The
universal-approximation capability of ResNets is proved.
</summary>
    <author>
      <name>Changcun Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2:writing improved</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.05663v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.05663v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.08046v1</id>
    <updated>2022-12-15T18:52:27Z</updated>
    <published>2022-12-15T18:52:27Z</published>
    <title>Silhouette: Toward Performance-Conscious and Transferable CPU Embeddings</title>
    <summary>  Learned embeddings are widely used to obtain concise data representation and
enable transfer learning between different data sets and tasks. In this paper,
we present Silhouette, our approach that leverages publicly-available
performance data sets to learn CPU embeddings. We show how these embeddings
enable transfer learning between data sets of different types and sizes. Each
of these scenarios leads to an improvement in accuracy for the target data set.
</summary>
    <author>
      <name>Tarikul Islam Papon</name>
    </author>
    <author>
      <name>Abdul Wasay</name>
    </author>
    <link href="http://arxiv.org/abs/2212.08046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.08046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.08769v1</id>
    <updated>2022-12-17T00:36:46Z</updated>
    <published>2022-12-17T00:36:46Z</published>
    <title>Improving Levenberg-Marquardt Algorithm for Neural Networks</title>
    <summary>  We explore the usage of the Levenberg-Marquardt (LM) algorithm for regression
(non-linear least squares) and classification (generalized Gauss-Newton
methods) tasks in neural networks. We compare the performance of the LM method
with other popular first-order algorithms such as SGD and Adam, as well as
other second-order algorithms such as L-BFGS , Hessian-Free and KFAC. We
further speed up the LM method by using adaptive momentum, learning rate line
search, and uphill step acceptance.
</summary>
    <author>
      <name>Omead Pooladzandi</name>
    </author>
    <author>
      <name>Yiming Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2212.08769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.08769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09175v1</id>
    <updated>2022-12-18T21:43:27Z</updated>
    <published>2022-12-18T21:43:27Z</published>
    <title>Predicting Citi Bike Demand Evolution Using Dynamic Graphs</title>
    <summary>  Bike sharing systems often suffer from poor capacity management as a result
of variable demand. These bike sharing systems would benefit from models to
predict demand in order to moderate the number of bikes stored at each station.
In this paper, we attempt to apply a graph neural network model to predict bike
demand in the New York City, Citi Bike dataset.
</summary>
    <author>
      <name>Alexander Saff</name>
    </author>
    <author>
      <name>Mayur Bhandary</name>
    </author>
    <author>
      <name>Siddharth Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/2212.09175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09507v1</id>
    <updated>2022-12-19T14:43:22Z</updated>
    <published>2022-12-19T14:43:22Z</published>
    <title>VC dimensions of group convolutional neural networks</title>
    <summary>  We study the generalization capacity of group convolutional neural networks.
We identify precise estimates for the VC dimensions of simple sets of group
convolutional neural networks. In particular, we find that for infinite groups
and appropriately chosen convolutional kernels, already two-parameter families
of convolutional neural networks have an infinite VC dimension, despite being
invariant to the action of an infinite group.
</summary>
    <author>
      <name>Philipp Christian Petersen</name>
    </author>
    <author>
      <name>Anna Sepliarskaia</name>
    </author>
    <link href="http://arxiv.org/abs/2212.09507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07, 68Q32, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11415v1</id>
    <updated>2022-12-21T23:52:42Z</updated>
    <published>2022-12-21T23:52:42Z</published>
    <title>Circumventing interpretability: How to defeat mind-readers</title>
    <summary>  The increasing capabilities of artificial intelligence (AI) systems make it
ever more important that we interpret their internals to ensure that their
intentions are aligned with human values. Yet there is reason to believe that
misaligned artificial intelligence will have a convergent instrumental
incentive to make its thoughts difficult for us to interpret. In this article,
I discuss many ways that a capable AI might circumvent scalable
interpretability methods and suggest a framework for thinking about these
potential future risks.
</summary>
    <author>
      <name>Lee Sharkey</name>
    </author>
    <link href="http://arxiv.org/abs/2212.11415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13082v1</id>
    <updated>2022-12-26T10:56:19Z</updated>
    <published>2022-12-26T10:56:19Z</published>
    <title>Quaternion Backpropagation</title>
    <summary>  Quaternion valued neural networks experienced rising popularity and interest
from researchers in the last years, whereby the derivatives with respect to
quaternions needed for optimization are calculated as the sum of the partial
derivatives with respect to the real and imaginary parts. However, we can show
that product- and chain-rule does not hold with this approach. We solve this by
employing the GHRCalculus and derive quaternion backpropagation based on this.
Furthermore, we experimentally prove the functionality of the derived
quaternion backpropagation.
</summary>
    <author>
      <name>Johannes Pöppelbaum</name>
    </author>
    <author>
      <name>Andreas Schwung</name>
    </author>
    <link href="http://arxiv.org/abs/2212.13082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00014v1</id>
    <updated>2022-12-30T14:41:53Z</updated>
    <published>2022-12-30T14:41:53Z</published>
    <title>Time series Forecasting to detect anomalous behaviours in Multiphase
  Flow Meters</title>
    <summary>  An Anomaly Detection (AD) System for Self-diagnosis has been developed for
Multiphase Flow Meter (MPFM). The system relies on machine learning algorithms
for time series forecasting, historical data have been used to train a model
and to predict the behavior of a sensor and, thus, to detect anomalies.
</summary>
    <author>
      <name>Tommaso Barbariol</name>
    </author>
    <author>
      <name>Davide Masiero</name>
    </author>
    <author>
      <name>Enrico Feltresi</name>
    </author>
    <author>
      <name>Gian Antonio Susto</name>
    </author>
    <link href="http://arxiv.org/abs/2301.00014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00636v1</id>
    <updated>2022-12-29T11:26:31Z</updated>
    <published>2022-12-29T11:26:31Z</published>
    <title>New Designed Loss Functions to Solve Ordinary Differential Equations
  with Artificial Neural Network</title>
    <summary>  This paper investigates the use of artificial neural networks (ANNs) to solve
differential equations (DEs) and the construction of the loss function which
meets both differential equation and its initial/boundary condition of a
certain DE. In section 2, the loss function is generalized to $n^\text{th}$
order ordinary differential equation(ODE). Other methods of construction are
examined in Section 3 and applied to three different models to assess their
effectiveness.
</summary>
    <author>
      <name>Xiao Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01231v1</id>
    <updated>2023-01-03T17:23:26Z</updated>
    <published>2023-01-03T17:23:26Z</published>
    <title>Machine Learning Approach to Polymerization Reaction Engineering:
  Determining Monomers Reactivity Ratios</title>
    <summary>  Here, we demonstrate how machine learning enables the prediction of
comonomers reactivity ratios based on the molecular structure of monomers. We
combined multi-task learning, multi-inputs, and Graph Attention Network to
build a model capable of predicting reactivity ratios based on the monomers
chemical structures.
</summary>
    <author>
      <name>Tung Nguyen</name>
    </author>
    <author>
      <name>Mona Bavarian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures in paper, 4 figures in supplementary</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.01231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03597v1</id>
    <updated>2023-01-09T17:09:53Z</updated>
    <published>2023-01-09T17:09:53Z</published>
    <title>On the Minimax Regret for Linear Bandits in a wide variety of Action
  Spaces</title>
    <summary>  As noted in the works of \cite{lattimore2020bandit}, it has been mentioned
that it is an open problem to characterize the minimax regret of linear bandits
in a wide variety of action spaces. In this article we present an optimal
regret lower bound for a wide class of convex action spaces.
</summary>
    <author>
      <name>Debangshu Banerjee</name>
    </author>
    <author>
      <name>Aditya Gopalan</name>
    </author>
    <link href="http://arxiv.org/abs/2301.03597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.04007v1</id>
    <updated>2023-01-10T14:53:28Z</updated>
    <published>2023-01-10T14:53:28Z</published>
    <title>Proceedings of the NeurIPS 2021 Workshop on Machine Learning for the
  Developing World: Global Challenges</title>
    <summary>  These are the proceedings of the 5th workshop on Machine Learning for the
Developing World (ML4D), held as part of the Thirty-fifth Conference on Neural
Information Processing Systems (NeurIPS) on December 14th, 2021.
</summary>
    <author>
      <name>Paula Rodriguez Diaz</name>
    </author>
    <author>
      <name>Tejumade Afonja</name>
    </author>
    <author>
      <name>Konstantin Klemmer</name>
    </author>
    <author>
      <name>Aya Salama</name>
    </author>
    <author>
      <name>Niveditha Kalavakonda</name>
    </author>
    <author>
      <name>Oluwafemi Azeez</name>
    </author>
    <author>
      <name>Simone Fobi</name>
    </author>
    <link href="http://arxiv.org/abs/2301.04007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09165v1</id>
    <updated>2023-01-22T17:21:00Z</updated>
    <published>2023-01-22T17:21:00Z</published>
    <title>Energy Prediction using Federated Learning</title>
    <summary>  In this work, we demonstrate the viability of using federated learning to
successfully predict energy consumption as well as solar production for all
households within a certain network using low-power and low-space consuming
embedded devices. We also demonstrate our prediction performance improving over
time without the need for sharing private consumer energy data. We simulate a
system with four nodes using data for one year to show this.
</summary>
    <author>
      <name>Meghana Bharadwaj</name>
    </author>
    <author>
      <name>Sanjana Sarda</name>
    </author>
    <link href="http://arxiv.org/abs/2301.09165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11442v3</id>
    <updated>2023-12-21T01:17:17Z</updated>
    <published>2023-01-26T22:06:24Z</published>
    <title>Communication-Efficient Collaborative Regret Minimization in Multi-Armed
  Bandits</title>
    <summary>  In this paper, we study the collaborative learning model, which concerns the
tradeoff between parallelism and communication overhead in multi-agent
multi-armed bandits. For regret minimization in multi-armed bandits, we present
the first set of tradeoffs between the number of rounds of communication among
the agents and the regret of the collaborative learning process.
</summary>
    <author>
      <name>Nikolai Karpov</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11442v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11442v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11658v1</id>
    <updated>2023-01-27T11:16:45Z</updated>
    <published>2023-01-27T11:16:45Z</published>
    <title>Semi-Supervised Machine Learning: a Homological Approach</title>
    <summary>  In this paper we describe the mathematical foundations of a new approach to
semi-supervised Machine Learning. Using techniques of Symbolic Computation and
Computer Algebra, we apply the concept of persistent homology to obtain a new
semi-supervised learning method.
</summary>
    <author>
      <name>Adrián Inés</name>
    </author>
    <author>
      <name>César Domínguez</name>
    </author>
    <author>
      <name>Jónathan Heras</name>
    </author>
    <author>
      <name>Gadea Mata</name>
    </author>
    <author>
      <name>Julio Rubio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of XVII Encuentro \'algebra computacional y
  aplicaciones (EACA 2022). arXiv admin note: text overlap with
  arXiv:2205.09617</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.00358v2</id>
    <updated>2023-06-24T05:31:55Z</updated>
    <published>2023-02-01T10:37:34Z</published>
    <title>Bandit Convex Optimisation Revisited: FTRL Achieves $\tilde{O}(t^{1/2})$
  Regret</title>
    <summary>  We show that a kernel estimator using multiple function evaluations can be
easily converted into a sampling-based bandit estimator with expectation equal
to the original kernel estimate. Plugging such a bandit estimator into the
standard FTRL algorithm yields a bandit convex optimisation algorithm that
achieves $\tilde{O}(t^{1/2})$ regret against adversarial time-varying convex
loss functions.
</summary>
    <author>
      <name>David Young</name>
    </author>
    <author>
      <name>Douglas Leith</name>
    </author>
    <author>
      <name>George Iosifidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Error in proof</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.00358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.00358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.00538v1</id>
    <updated>2023-02-01T16:03:41Z</updated>
    <published>2023-02-01T16:03:41Z</published>
    <title>Experimental observation on a low-rank tensor model for eigenvalue
  problems</title>
    <summary>  Here we utilize a low-rank tensor model (LTM) as a function approximator,
combined with the gradient descent method, to solve eigenvalue problems
including the Laplacian operator and the harmonic oscillator. Experimental
results show the superiority of the polynomial-based low-rank tensor model
(PLTM) compared to the tensor neural network (TNN). We also test such low-rank
architectures for the classification problem on the MNIST dataset.
</summary>
    <author>
      <name>Jun Hu</name>
    </author>
    <author>
      <name>Pengzhan Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2302.00538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.00538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.04972v2</id>
    <updated>2023-06-09T04:49:55Z</updated>
    <published>2023-02-09T23:22:48Z</published>
    <title>Differentially Private Optimization for Smooth Nonconvex ERM</title>
    <summary>  We develop simple differentially private optimization algorithms that move
along directions of (expected) descent to find an approximate second-order
solution for nonconvex ERM. We use line search, mini-batching, and a two-phase
strategy to improve the speed and practicality of the algorithm. Numerical
experiments demonstrate the effectiveness of these approaches.
</summary>
    <author>
      <name>Changyu Gao</name>
    </author>
    <author>
      <name>Stephen J. Wright</name>
    </author>
    <link href="http://arxiv.org/abs/2302.04972v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04972v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05371v1</id>
    <updated>2023-02-10T16:49:58Z</updated>
    <published>2023-02-10T16:49:58Z</published>
    <title>A Second-Order Method for Stochastic Bandit Convex Optimisation</title>
    <summary>  We introduce a simple and efficient algorithm for unconstrained zeroth-order
stochastic convex bandits and prove its regret is at most $(1 + r/d)[d^{1.5}
\sqrt{n} + d^3] polylog(n, d, r)$ where $n$ is the horizon, $d$ the dimension
and $r$ is the radius of a known ball containing the minimiser of the loss.
</summary>
    <author>
      <name>Tor Lattimore</name>
    </author>
    <author>
      <name>András György</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.05371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07263v1</id>
    <updated>2023-02-14T18:58:11Z</updated>
    <published>2023-02-14T18:58:11Z</published>
    <title>Interpolation Learning With Minimum Description Length</title>
    <summary>  We prove that the Minimum Description Length learning rule exhibits tempered
overfitting. We obtain tempered agnostic finite sample learning guarantees and
characterize the asymptotic behavior in the presence of random label noise.
</summary>
    <author>
      <name>Naren Sarayu Manoj</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/2302.07263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.10862v1</id>
    <updated>2023-02-21T18:20:50Z</updated>
    <published>2023-02-21T18:20:50Z</published>
    <title>A Note on Noisy Reservoir Computation</title>
    <summary>  In this note we extend the definition of the Information Processing Capacity
(IPC) by Dambre et al [1] to include the effects of stochastic reservoir
dynamics. We quantify the degradation of the IPC in the presence of this noise.
[1] Dambre et al. Scientific Reports 2, 514, (2012)
</summary>
    <author>
      <name>Anthony M. Polloreno</name>
    </author>
    <author>
      <name>Reuben R. W. Wang</name>
    </author>
    <author>
      <name>Nikolas A. Tezak</name>
    </author>
    <link href="http://arxiv.org/abs/2302.10862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.10862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13752v1</id>
    <updated>2023-02-27T13:31:21Z</updated>
    <published>2023-02-27T13:31:21Z</published>
    <title>A Brief Survey on the Approximation Theory for Sequence Modelling</title>
    <summary>  We survey current developments in the approximation theory of sequence
modelling in machine learning. Particular emphasis is placed on classifying
existing results for various model architectures through the lens of classical
approximation paradigms, and the insights one can gain from these results. We
also outline some future research directions towards building a theory of
sequence modelling.
</summary>
    <author>
      <name>Haotian Jiang</name>
    </author>
    <author>
      <name>Qianxiao Li</name>
    </author>
    <author>
      <name>Zhong Li</name>
    </author>
    <author>
      <name>Shida Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01936v1</id>
    <updated>2023-03-03T14:05:59Z</updated>
    <published>2023-03-03T14:05:59Z</published>
    <title>Multi-Agent Adversarial Training Using Diffusion Learning</title>
    <summary>  This work focuses on adversarial learning over graphs. We propose a general
adversarial training framework for multi-agent systems using diffusion
learning. We analyze the convergence properties of the proposed scheme for
convex optimization problems, and illustrate its enhanced robustness to
adversarial attacks.
</summary>
    <author>
      <name>Ying Cao</name>
    </author>
    <author>
      <name>Elsa Rizk</name>
    </author>
    <author>
      <name>Stefan Vlaski</name>
    </author>
    <author>
      <name>Ali H. Sayed</name>
    </author>
    <link href="http://arxiv.org/abs/2303.01936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.03195v1</id>
    <updated>2023-03-03T02:59:29Z</updated>
    <published>2023-03-03T02:59:29Z</published>
    <title>Query Learning Algorithm for Ordered Multi-Terminal Binary Decision
  Diagrams</title>
    <summary>  We propose a query learning algorithm for ordered multi-terminal binary
decision diagrams (OMTBDDs) using at most n equivalence and 2n(l\lcei\log_2
m\rceil+ 3n) membership queries by extending the algorithm for ordered binary
decision diagrams (OBDDs). Tightness of our upper bounds is checked in our
experiments using synthetically generated target OMTBDDs. Possibility of
applying our algorithm to classification problems is also indicated in our
other experiments using datasets of UCI Machine Learning Repository.
</summary>
    <author>
      <name>Atsuyoshi Nakamura</name>
    </author>
    <link href="http://arxiv.org/abs/2303.03195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.03195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08631v1</id>
    <updated>2023-03-15T13:58:07Z</updated>
    <published>2023-03-15T13:58:07Z</published>
    <title>Smoothed Q-learning</title>
    <summary>  In Reinforcement Learning the Q-learning algorithm provably converges to the
optimal solution. However, as others have demonstrated, Q-learning can also
overestimate the values and thereby spend too long exploring unhelpful states.
Double Q-learning is a provably convergent alternative that mitigates some of
the overestimation issues, though sometimes at the expense of slower
convergence. We introduce an alternative algorithm that replaces the max
operation with an average, resulting also in a provably convergent off-policy
algorithm which can mitigate overestimation yet retain similar convergence as
standard Q-learning.
</summary>
    <author>
      <name>David Barber</name>
    </author>
    <link href="http://arxiv.org/abs/2303.08631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.09705v2</id>
    <updated>2023-07-16T08:59:50Z</updated>
    <published>2023-03-17T00:27:55Z</published>
    <title>Batch Updating of a Posterior Tree Distribution over a Meta-Tree</title>
    <summary>  Previously, we proposed a probabilistic data generation model represented by
an unobservable tree and a sequential updating method to calculate a posterior
distribution over a set of trees. The set is called a meta-tree. In this paper,
we propose a more efficient batch updating method.
</summary>
    <author>
      <name>Yuta Nakahara</name>
    </author>
    <author>
      <name>Toshiyasu Matsushima</name>
    </author>
    <link href="http://arxiv.org/abs/2303.09705v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.09705v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17745v1</id>
    <updated>2023-03-30T23:43:22Z</updated>
    <published>2023-03-30T23:43:22Z</published>
    <title>A Note On Nonlinear Regression Under L2 Loss</title>
    <summary>  We investigate the nonlinear regression problem under L2 loss (square loss)
functions. Traditional nonlinear regression models often result in non-convex
optimization problems with respect to the parameter set. We show that a convex
nonlinear regression model exists for the traditional least squares problem,
which can be a promising towards designing more complex systems with easier to
train models.
</summary>
    <author>
      <name>Kaan Gokcesu</name>
    </author>
    <author>
      <name>Hakan Gokcesu</name>
    </author>
    <link href="http://arxiv.org/abs/2303.17745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.00285v1</id>
    <updated>2023-04-01T10:26:16Z</updated>
    <published>2023-04-01T10:26:16Z</published>
    <title>Branch Identification in Passive Optical Networks using Machine Learning</title>
    <summary>  A machine learning approach for improving monitoring in passive optical
networks with almost equidistant branches is proposed and experimentally
validated. It achieves a high diagnostic accuracy of 98.7% and an event
localization error of 0.5m
</summary>
    <author>
      <name>khouloud Abdelli</name>
    </author>
    <author>
      <name>Carsten Tropschug</name>
    </author>
    <author>
      <name>Helmut Griesser</name>
    </author>
    <author>
      <name>Sander Jansen</name>
    </author>
    <author>
      <name>Stephan Pachnicke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Optical Fiber Communication Conference (OFC) 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.00285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.00285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.00803v1</id>
    <updated>2023-04-03T08:50:58Z</updated>
    <published>2023-04-03T08:50:58Z</published>
    <title>A Tutorial Introduction to Reinforcement Learning</title>
    <summary>  In this paper, we present a brief survey of Reinforcement Learning (RL), with
particular emphasis on Stochastic Approximation (SA) as a unifying theme. The
scope of the paper includes Markov Reward Processes, Markov Decision Processes,
Stochastic Approximation algorithms, and widely used algorithms such as
Temporal Difference Learning and $Q$-learning.
</summary>
    <author>
      <name>Mathukumalli Vidyasagar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.00803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.00803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.05133v2</id>
    <updated>2023-04-18T15:57:29Z</updated>
    <published>2023-04-11T10:54:36Z</published>
    <title>Lecture Notes: Neural Network Architectures</title>
    <summary>  These lecture notes provide an overview of Neural Network architectures from
a mathematical point of view. Especially, Machine Learning with Neural Networks
is seen as an optimization problem. Covered are an introduction to Neural
Networks and the following architectures: Feedforward Neural Network,
Convolutional Neural Network, ResNet, and Recurrent Neural Network.
</summary>
    <author>
      <name>Evelyn Herberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added more references</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.05133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.05133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09499v1</id>
    <updated>2023-04-19T08:40:10Z</updated>
    <published>2023-04-19T08:40:10Z</published>
    <title>The Responsibility Problem in Neural Networks with Unordered Targets</title>
    <summary>  We discuss the discontinuities that arise when mapping unordered objects to
neural network outputs of fixed permutation, referred to as the responsibility
problem. Prior work has proved the existence of the issue by identifying a
single discontinuity. Here, we show that discontinuities under such models are
uncountably infinite, motivating further research into neural networks for
unordered data.
</summary>
    <author>
      <name>Ben Hayes</name>
    </author>
    <author>
      <name>Charalampos Saitis</name>
    </author>
    <author>
      <name>György Fazekas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for TinyPaper archival at ICLR 2023:
  https://openreview.net/forum?id=jd7Hy1jRiv4</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.09499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11654v1</id>
    <updated>2023-04-23T13:48:58Z</updated>
    <published>2023-04-23T13:48:58Z</published>
    <title>Stochastic Cell Transmission Models of Traffic Networks</title>
    <summary>  We introduce a rigorous framework for stochastic cell transmission models for
general traffic networks. The performance of traffic systems is evaluated based
on preference functionals and acceptable designs. The numerical implementation
combines simulation, Gaussian process regression, and a stochastic exploration
procedure. The approach is illustrated in two case studies.
</summary>
    <author>
      <name>Zachary Feinstein</name>
    </author>
    <author>
      <name>Marcel Kleiber</name>
    </author>
    <author>
      <name>Stefan Weber</name>
    </author>
    <link href="http://arxiv.org/abs/2304.11654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.01089v1</id>
    <updated>2023-05-01T21:03:53Z</updated>
    <published>2023-05-01T21:03:53Z</published>
    <title>Computing Expected Motif Counts for Exchangeable Graph Generative Models</title>
    <summary>  Estimating the expected value of a graph statistic is an important inference
task for using and learning graph models. This note presents a scalable
estimation procedure for expected motif counts, a widely used type of graph
statistic. The procedure applies for generative mixture models of the type used
in neural and Bayesian approaches to graph data.
</summary>
    <author>
      <name>Oliver Schulte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.01089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.01089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G09" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03863v1</id>
    <updated>2023-05-05T22:25:42Z</updated>
    <published>2023-05-05T22:25:42Z</published>
    <title>Software-based Automatic Differentiation is Flawed</title>
    <summary>  Various software efforts embrace the idea that object oriented programming
enables a convenient implementation of the chain rule, facilitating so-called
automatic differentiation via backpropagation. Such frameworks have no
mechanism for simplifying the expressions (obtained via the chain rule) before
evaluating them. As we illustrate below, the resulting errors tend to be
unbounded.
</summary>
    <author>
      <name>Daniel Johnson</name>
    </author>
    <author>
      <name>Trevor Maxfield</name>
    </author>
    <author>
      <name>Yongxu Jin</name>
    </author>
    <author>
      <name>Ronald Fedkiw</name>
    </author>
    <link href="http://arxiv.org/abs/2305.03863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09495v1</id>
    <updated>2023-05-16T14:47:55Z</updated>
    <published>2023-05-16T14:47:55Z</published>
    <title>Hardware Realization of Nonlinear Activation Functions for NN-based
  Optical Equalizers</title>
    <summary>  To reduce the complexity of the hardware implementation of neural
network-based optical channel equalizers, we demonstrate that the performance
of the biLSTM equalizer with approximated activation functions is close to that
of the original model.
</summary>
    <author>
      <name>Sasipim Srivallapanondh</name>
    </author>
    <author>
      <name>Pedro J. Freire</name>
    </author>
    <author>
      <name>Antonio Napoli</name>
    </author>
    <author>
      <name>Sergei K. Turitsyn</name>
    </author>
    <author>
      <name>Jaroslaw E. Prilepsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, 1 table, Conference on Lasers &amp; Electro-Optics
  2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.09495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.11831v2</id>
    <updated>2023-05-23T13:11:40Z</updated>
    <published>2023-05-19T17:13:42Z</published>
    <title>Regularization of Soft Actor-Critic Algorithms with Automatic
  Temperature Adjustment</title>
    <summary>  This work presents a comprehensive analysis to regularize the Soft
Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the
policy evaluation, the policy improvement and the temperature adjustment are
reformulated, addressing certain modification and enhancing the clarity of the
original theory in a more explicit manner.
</summary>
    <author>
      <name>Ben You</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work aims to clarify the ambiguity and revise certain errors in
  the original soft actor-cirtic articles</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.11831v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.11831v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.00586v1</id>
    <updated>2023-06-01T11:57:47Z</updated>
    <published>2023-06-01T11:57:47Z</published>
    <title>Evaluating the "Learning on Graphs" Conference Experience</title>
    <summary>  With machine learning conferences growing ever larger, and reviewing
processes becoming increasingly elaborate, more data-driven insights into their
workings are required. In this report, we present the results of a survey
accompanying the first "Learning on Graphs" (LoG) Conference. The survey was
directed to evaluate the submission and review process from different
perspectives, including authors, reviewers, and area chairs alike.
</summary>
    <author>
      <name>Bastian Rieck</name>
    </author>
    <author>
      <name>Corinna Coupette</name>
    </author>
    <link href="http://arxiv.org/abs/2306.00586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.00586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12640v2</id>
    <updated>2023-07-02T20:34:31Z</updated>
    <published>2023-06-22T02:50:16Z</published>
    <title>On Addressing the Limitations of Graph Neural Networks</title>
    <summary>  This report gives a summary of two problems about graph convolutional
networks (GCNs): over-smoothing and heterophily challenges, and outlines future
directions to explore.
</summary>
    <author>
      <name>Sitao Luan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proposal report and a concise version of Sitao Luan's thesis. The
  weight initialization part is quite interesting but will not be included in
  Sitao's formal thesis, thus Sitao put this preprint report online. arXiv
  admin note: substantial text overlap with arXiv:2109.05641, arXiv:2210.07606</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.12640v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12640v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13197v2</id>
    <updated>2023-07-19T16:19:24Z</updated>
    <published>2023-06-22T20:42:50Z</published>
    <title>Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What
  is Best?</title>
    <summary>  Gradient based attribution methods for neural networks working as classifiers
use gradients of network scores. Here we discuss the practical differences
between using gradients of pre-softmax scores versus post-softmax scores, and
their respective advantages and disadvantages.
</summary>
    <author>
      <name>Miguel Lerma</name>
    </author>
    <author>
      <name>Mirtha Lucas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPRS58416.2023.10179032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPRS58416.2023.10179032" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 2023 IEEE 13th International Conference on
  Pattern Recognition Systems (ICPRS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICPRS 13 (2023) 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.13197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13724v3</id>
    <updated>2023-11-22T22:21:37Z</updated>
    <published>2023-06-23T18:13:15Z</published>
    <title>Review of compressed embedding layers and their applications for
  recommender systems</title>
    <summary>  We review the literature on trainable, compressed embedding layers and
discuss their applicability for compressing gigantic neural recommender
systems. We also report the results we measured with our compressed embedding
layers.
</summary>
    <author>
      <name>Tamas Hajgato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.13724v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13724v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.02130v1</id>
    <updated>2023-07-05T09:11:23Z</updated>
    <published>2023-07-05T09:11:23Z</published>
    <title>Implicit Differentiation for Hyperparameter Tuning the Weighted
  Graphical Lasso</title>
    <summary>  We provide a framework and algorithm for tuning the hyperparameters of the
Graphical Lasso via a bilevel optimization problem solved with a first-order
method. In particular, we derive the Jacobian of the Graphical Lasso solution
with respect to its regularization hyperparameters.
</summary>
    <author>
      <name>Can Pouliquen</name>
    </author>
    <author>
      <name>Paulo Gonçalves</name>
    </author>
    <author>
      <name>Mathurin Massias</name>
    </author>
    <author>
      <name>Titouan Vayer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GRETSI 2023-XXIX{\`e}me Colloque Francophone de Traitement du
  Signal et des Images</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2307.02130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.02130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.06644v1</id>
    <updated>2023-07-13T09:20:57Z</updated>
    <published>2023-07-13T09:20:57Z</published>
    <title>An Improved Uniform Convergence Bound with Fat-Shattering Dimension</title>
    <summary>  The fat-shattering dimension characterizes the uniform convergence property
of real-valued functions. The state-of-the-art upper bounds feature a
multiplicative squared logarithmic factor on the sample complexity, leaving an
open gap with the existing lower bound. We provide an improved uniform
convergence bound that closes this gap.
</summary>
    <author>
      <name>Roberto Colomboni</name>
    </author>
    <author>
      <name>Emmanuel Esposito</name>
    </author>
    <author>
      <name>Andrea Paudice</name>
    </author>
    <link href="http://arxiv.org/abs/2307.06644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.06644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.07843v1</id>
    <updated>2023-07-15T16:19:37Z</updated>
    <published>2023-07-15T16:19:37Z</published>
    <title>Transformers are Universal Predictors</title>
    <summary>  We find limits to the Transformer architecture for language modeling and show
it has a universal prediction property in an information-theoretic sense. We
further analyze performance in non-asymptotic data regimes to understand the
role of various components of the Transformer architecture, especially in the
context of data-efficient training. We validate our theoretical analysis with
experiments on both synthetic and real datasets.
</summary>
    <author>
      <name>Sourya Basu</name>
    </author>
    <author>
      <name>Moulik Choraria</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Compression Workshop (ICML 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.07843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.09311v1</id>
    <updated>2023-07-18T14:56:12Z</updated>
    <published>2023-07-18T14:56:12Z</published>
    <title>Automatic Differentiation for Inverse Problems with Applications in
  Quantum Transport</title>
    <summary>  A neural solver and differentiable simulation of the quantum transmitting
boundary model is presented for the inverse quantum transport problem. The
neural solver is used to engineer continuous transmission properties and the
differentiable simulation is used to engineer current-voltage characteristics.
</summary>
    <author>
      <name>Ivan Williams</name>
    </author>
    <author>
      <name>Eric Polizzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.09311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.09311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.13333v1</id>
    <updated>2023-07-25T08:45:41Z</updated>
    <published>2023-07-25T08:45:41Z</published>
    <title>Feature Importance Measurement based on Decision Tree Sampling</title>
    <summary>  Random forest is effective for prediction tasks but the randomness of tree
generation hinders interpretability in feature importance analysis. To address
this, we proposed DT-Sampler, a SAT-based method for measuring feature
importance in tree-based model. Our method has fewer parameters than random
forest and provides higher interpretability and stability for the analysis in
real-world problems. An implementation of DT-Sampler is available at
https://github.com/tsudalab/DT-sampler.
</summary>
    <author>
      <name>Chao Huang</name>
    </author>
    <author>
      <name>Diptesh Das</name>
    </author>
    <author>
      <name>Koji Tsuda</name>
    </author>
    <link href="http://arxiv.org/abs/2307.13333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.13333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.00133v1</id>
    <updated>2023-07-31T19:58:12Z</updated>
    <published>2023-07-31T19:58:12Z</published>
    <title>A Suite of Fairness Datasets for Tabular Classification</title>
    <summary>  There have been many papers with algorithms for improving fairness of
machine-learning classifiers for tabular data. Unfortunately, most use only
very few datasets for their experimental evaluation. We introduce a suite of
functions for fetching 20 fairness datasets and providing associated fairness
metadata. Hopefully, these will lead to more rigorous experimental evaluations
in future fairness-aware machine learning research.
</summary>
    <author>
      <name>Martin Hirzel</name>
    </author>
    <author>
      <name>Michael Feffer</name>
    </author>
    <link href="http://arxiv.org/abs/2308.00133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.00133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.00720v1</id>
    <updated>2023-08-01T11:13:54Z</updated>
    <published>2023-08-01T11:13:54Z</published>
    <title>Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple
  example</title>
    <summary>  A very simple unidimensional function with Lipschitz continuous gradient is
constructed such that the ADAM algorithm with constant stepsize, started from
the origin, diverges when applied to minimize this function in the absence of
noise on the gradient. Divergence occurs irrespective of the choice of the
method parameters.
</summary>
    <author>
      <name>Ph. L. Toint</name>
    </author>
    <link href="http://arxiv.org/abs/2308.00720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.00720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65K10, 90C26, 90C30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.6.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.03102v1</id>
    <updated>2023-08-06T12:40:17Z</updated>
    <published>2023-08-06T12:40:17Z</published>
    <title>Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic
  Line Search</title>
    <summary>  This paper explores two recent methods for learning rate optimisation in
stochastic gradient descent: D-Adaptation (arXiv:2301.07733) and probabilistic
line search (arXiv:1502.02846). These approaches aim to alleviate the burden of
selecting an initial learning rate by incorporating distance metrics and
Gaussian process posterior estimates, respectively. In this report, I provide
an intuitive overview of both methods, discuss their shared design goals, and
devise scope for merging the two algorithms.
</summary>
    <author>
      <name>Max McGuinness</name>
    </author>
    <link href="http://arxiv.org/abs/2308.03102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.03102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.03888v2</id>
    <updated>2025-02-15T06:59:06Z</updated>
    <published>2023-08-04T10:55:56Z</published>
    <title>Deep neural networks from the perspective of ergodic theory</title>
    <summary>  The design of deep neural networks remains somewhat of an art rather than
precise science. By tentatively adopting ergodic theory considerations on top
of viewing the network as the time evolution of a dynamical system, with each
layer corresponding to a temporal instance, we show that some rules of thumb,
which might otherwise appear mysterious, can be attributed heuristics.
</summary>
    <author>
      <name>Fan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.03888v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.03888v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.07204v1</id>
    <updated>2023-08-14T15:16:39Z</updated>
    <published>2023-08-14T15:16:39Z</published>
    <title>Algorithms for the Training of Neural Support Vector Machines</title>
    <summary>  Neural support vector machines (NSVMs) allow for the incorporation of domain
knowledge in the design of the model architecture. In this article we introduce
a set of training algorithms for NSVMs that leverage the Pegasos algorithm and
provide a proof of concept by solving a set of standard machine learning tasks.
</summary>
    <author>
      <name>Lars Simon</name>
    </author>
    <author>
      <name>Manuel Radons</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.07204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.07204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.14093v1</id>
    <updated>2023-08-27T12:35:38Z</updated>
    <published>2023-08-27T12:35:38Z</published>
    <title>The inverse problem for neural networks</title>
    <summary>  We study the problem of computing the preimage of a set under a neural
network with piecewise-affine activation functions. We recall an old result
that the preimage of a polyhedral set is again a union of polyhedral sets and
can be effectively computed. We show several applications of computing the
preimage for analysis and interpretability of neural networks.
</summary>
    <author>
      <name>Marcelo Forets</name>
    </author>
    <author>
      <name>Christian Schilling</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-46002-9_14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-46002-9_14" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AISoLA 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2308.14093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.14093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00699v1</id>
    <updated>2023-09-01T18:42:53Z</updated>
    <published>2023-09-01T18:42:53Z</published>
    <title>Geometric Deep Learning: a Temperature Based Analysis of Graph Neural
  Networks</title>
    <summary>  We examine a Geometric Deep Learning model as a thermodynamic system treating
the weights as non-quantum and non-relativistic particles. We employ the notion
of temperature previously defined in [7] and study it in the various layers for
GCN and GAT models. Potential future applications of our findings are
discussed.
</summary>
    <author>
      <name>M. Lapenna</name>
    </author>
    <author>
      <name>F. Faglioni</name>
    </author>
    <author>
      <name>F. Zanchetta</name>
    </author>
    <author>
      <name>R. Fioresi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published on Proceedings of GSI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.06382v2</id>
    <updated>2023-10-09T15:24:11Z</updated>
    <published>2023-09-12T16:48:00Z</published>
    <title>Ensemble Mask Networks</title>
    <summary>  Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn
matrix-vector multiplication? This study introduces two mechanisms - flexible
masking to take matrix inputs, and a unique network pruning to respect the
mask's dependency structure. Networks can approximate fixed operations such as
matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the
mechanisms introduced with applications towards litmus-testing dependencies or
interaction order in graph-based models.
</summary>
    <author>
      <name>Jonny Luntzel</name>
    </author>
    <link href="http://arxiv.org/abs/2309.06382v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.06382v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.16854v1</id>
    <updated>2023-09-28T21:07:40Z</updated>
    <published>2023-09-28T21:07:40Z</published>
    <title>Applications of Federated Learning in IoT for Hyper Personalisation</title>
    <summary>  Billions of IoT devices are being deployed, taking advantage of faster
internet, and the opportunity to access more endpoints. Vast quantities of data
are being generated constantly by these devices but are not effectively being
utilised. Using FL training machine learning models over these multiple clients
without having to bring it to a central server. We explore how to use such a
model to implement ultra levels of personalization unlike before
</summary>
    <author>
      <name>Veer Dosi</name>
    </author>
    <link href="http://arxiv.org/abs/2309.16854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.16854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.06306v2</id>
    <updated>2023-10-11T01:00:45Z</updated>
    <published>2023-10-10T04:44:35Z</published>
    <title>Ensemble Active Learning by Contextual Bandits for AI Incubation in
  Manufacturing</title>
    <summary>  It is challenging but important to save annotation efforts in streaming data
acquisition to maintain data quality for supervised learning base learners. We
propose an ensemble active learning method to actively acquire samples for
annotation by contextual bandits, which is will enforce the
exploration-exploitation balance and leading to improved AI modeling
performance.
</summary>
    <author>
      <name>Yingyan Zeng</name>
    </author>
    <author>
      <name>Xiaoyu Chen</name>
    </author>
    <author>
      <name>Ran Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2310.06306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.06306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.10250v1</id>
    <updated>2023-10-16T10:19:45Z</updated>
    <published>2023-10-16T10:19:45Z</published>
    <title>Leveraging Topological Maps in Deep Reinforcement Learning for
  Multi-Object Navigation</title>
    <summary>  This work addresses the challenge of navigating expansive spaces with sparse
rewards through Reinforcement Learning (RL). Using topological maps, we elevate
elementary actions to object-oriented macro actions, enabling a simple Deep
Q-Network (DQN) agent to solve otherwise practically impossible environments.
</summary>
    <author>
      <name>Simon Hakenes</name>
    </author>
    <author>
      <name>Tobias Glasmachers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Abstract, Northern Lights Deep Learning Conference 2024, 3
  pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.10250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.10250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.11470v1</id>
    <updated>2023-05-24T13:38:38Z</updated>
    <published>2023-05-24T13:38:38Z</published>
    <title>Classic machine learning methods</title>
    <summary>  In this chapter, we present the main classic machine learning methods. A
large part of the chapter is devoted to supervised learning techniques for
classification and regression, including nearest-neighbor methods, linear and
logistic regressions, support vector machines and tree-based algorithms. We
also describe the problem of overfitting as well as strategies to overcome it.
We finally provide a brief overview of unsupervised learning methods, namely
for clustering and dimensionality reduction.
</summary>
    <author>
      <name>Johann Faouzi</name>
    </author>
    <author>
      <name>Olivier Colliot</name>
    </author>
    <link href="http://arxiv.org/abs/2310.11470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.11470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.02278v1</id>
    <updated>2023-11-04T00:00:13Z</updated>
    <published>2023-11-04T00:00:13Z</published>
    <title>Machine learning's own Industrial Revolution</title>
    <summary>  Machine learning is expected to enable the next Industrial Revolution.
However, lacking standardized and automated assembly networks, ML faces
significant challenges to meet ever-growing enterprise demands and empower
broad industries. In the Perspective, we argue that ML needs to first complete
its own Industrial Revolution, elaborate on how to best achieve its goals, and
discuss new opportunities to enable rapid translation from ML's innovation
frontier to mass production and utilization.
</summary>
    <author>
      <name>Yuan Luo</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Jingjing Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2311.02278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.02278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.18806v1</id>
    <updated>2023-11-30T18:51:50Z</updated>
    <published>2023-11-30T18:51:50Z</published>
    <title>Efficient Baseline for Quantitative Precipitation Forecasting in
  Weather4cast 2023</title>
    <summary>  Accurate precipitation forecasting is indispensable for informed
decision-making across various industries. However, the computational demands
of current models raise environmental concerns. We address the critical need
for accurate precipitation forecasting while considering the environmental
impact of computational resources and propose a minimalist U-Net architecture
to be used as a baseline for future weather forecasting initiatives.
</summary>
    <author>
      <name>Akshay Punjabi</name>
    </author>
    <author>
      <name>Pablo Izquierdo Ayala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, Weather4Cast 2023 challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.18806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.18806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.00373v1</id>
    <updated>2023-12-01T06:33:39Z</updated>
    <published>2023-12-01T06:33:39Z</published>
    <title>Streaming Bayesian Modeling for predicting Fat-Tailed Customer Lifetime
  Value</title>
    <summary>  We develop an online learning MCMC approach applicable for hierarchical
bayesian models and GLMS. We also develop a fat-tailed LTV model that
generalizes over several kinds of fat and thin tails. We demonstrate both
developments on commercial LTV data from a large mobile app.
</summary>
    <author>
      <name>Alexey V. Calabourdin</name>
    </author>
    <author>
      <name>Konstantin A. Aksenov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.00373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.00373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C10, 62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.00818v1</id>
    <updated>2023-11-29T21:52:34Z</updated>
    <published>2023-11-29T21:52:34Z</published>
    <title>The perpetual motion machine of AI-generated data and the distraction of
  ChatGPT-as-scientist</title>
    <summary>  Since ChatGPT works so well, are we on the cusp of solving science with AI?
Is not AlphaFold2 suggestive that the potential of LLMs in biology and the
sciences more broadly is limitless? Can we use AI itself to bridge the lack of
data in the sciences in order to then train an AI? Herein we present a
discussion of these topics.
</summary>
    <author>
      <name>Jennifer Listgarten</name>
    </author>
    <link href="http://arxiv.org/abs/2312.00818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.00818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.07698v1</id>
    <updated>2023-12-12T19:49:47Z</updated>
    <published>2023-12-12T19:49:47Z</published>
    <title>Machine Learning and Citizen Science Approaches for Monitoring the
  Changing Environment</title>
    <summary>  This dissertation will combine new tools and methodologies to answer pressing
questions regarding inundation area and hurricane events in complex,
heterogeneous changing environments. In addition to remote sensing approaches,
citizen science and machine learning are both emerging fields that harness
advancing technology to answer environmental management and disaster response
questions.
</summary>
    <author>
      <name>Sulong Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, Environment and Resources, U Wisconson Madison (2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.07698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.07698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.08418v1</id>
    <updated>2023-12-13T17:55:44Z</updated>
    <published>2023-12-13T17:55:44Z</published>
    <title>Automatic Bug Detection in Games using LSTM Networks</title>
    <summary>  We introduced a new framework to detect perceptual bugs using a Long
Short-Term Memory (LSTM) network, which detects bugs in video games as
anomalies. The detected buggy frames are then clustered to determine the
category of the occurred bug. The framework was evaluated on two First Person
Shooter (FPS) games. Results show the effectiveness of the framework.
</summary>
    <author>
      <name>Elham Azizi</name>
    </author>
    <author>
      <name>Loutfouz Zaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Games 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.08418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.08418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09352v1</id>
    <updated>2023-12-14T21:27:38Z</updated>
    <published>2023-12-14T21:27:38Z</published>
    <title>PBES: PCA Based Exemplar Sampling Algorithm for Continual Learning</title>
    <summary>  We propose a novel exemplar selection approach based on Principal Component
Analysis (PCA) and median sampling, and a neural network training regime in the
setting of class-incremental learning. This approach avoids the pitfalls due to
outliers in the data and is both simple to implement and use across various
incremental machine learning models. It also has independent usage as a
sampling algorithm. We achieve better performance compared to state-of-the-art
methods.
</summary>
    <author>
      <name>Sahil Nokhwal</name>
    </author>
    <author>
      <name>Nirman Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2312.09352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09766v1</id>
    <updated>2023-12-15T13:12:49Z</updated>
    <published>2023-12-15T13:12:49Z</published>
    <title>Celestial Machine Learning: From Data to Mars and Beyond with AI Feynman</title>
    <summary>  Can a machine or algorithm discover or learn Kepler's first law from
astronomical sightings alone? We emulate Johannes Kepler's discovery of the
equation of the orbit of Mars with the Rudolphine tables using AI Feynman, a
physics-inspired tool for symbolic regression.
</summary>
    <author>
      <name>Zi-Yu Khoo</name>
    </author>
    <author>
      <name>Abel Yang</name>
    </author>
    <author>
      <name>Jonathan Sze Choong Low</name>
    </author>
    <author>
      <name>Stéphane Bressan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-39821-6_41</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-39821-6_41" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v1: long version v2: accepted as a short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.09766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.10494v2</id>
    <updated>2024-01-06T13:38:01Z</updated>
    <published>2023-12-16T16:16:28Z</published>
    <title>Do Bayesian Neural Networks Improve Weapon System Predictive
  Maintenance?</title>
    <summary>  We implement a Bayesian inference process for Neural Networks to model the
time to failure of highly reliable weapon systems with interval-censored data
and time-varying covariates. We analyze and benchmark our approach, LaplaceNN,
on synthetic and real datasets with standard classification metrics such as
Receiver Operating Characteristic (ROC) Area Under Curve (AUC) Precision-Recall
(PR) AUC, and reliability curve visualizations.
</summary>
    <author>
      <name>Michael Potter</name>
    </author>
    <author>
      <name>Miru Jun</name>
    </author>
    <link href="http://arxiv.org/abs/2312.10494v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.10494v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01732v1</id>
    <updated>2024-01-03T13:11:59Z</updated>
    <published>2024-01-03T13:11:59Z</published>
    <title>Task and Explanation Network</title>
    <summary>  Explainability in deep networks has gained increased importance in recent
years. We argue herein that an AI must be tasked not just with a task but also
with an explanation of why said task was accomplished as such. We present a
basic framework -- Task and Explanation Network (TENet) -- which fully
integrates task completion and its explanation. We believe that the field of AI
as a whole should insist -- quite emphatically -- on explainability.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <link href="http://arxiv.org/abs/2401.01732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.10686v1</id>
    <updated>2024-01-19T13:33:23Z</updated>
    <published>2024-01-19T13:33:23Z</published>
    <title>Manipulating Sparse Double Descent</title>
    <summary>  This paper investigates the double descent phenomenon in two-layer neural
networks, focusing on the role of L1 regularization and representation
dimensions. It explores an alternative double descent phenomenon, named sparse
double descent. The study emphasizes the complex relationship between model
complexity, sparsity, and generalization, and suggests further research into
more diverse models and datasets. The findings contribute to a deeper
understanding of neural network training and optimization.
</summary>
    <author>
      <name>Ya Shi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2401.10686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.10686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.11993v1</id>
    <updated>2024-01-22T14:46:41Z</updated>
    <published>2024-01-22T14:46:41Z</published>
    <title>Expert-Driven Monitoring of Operational ML Models</title>
    <summary>  We propose Expert Monitoring, an approach that leverages domain expertise to
enhance the detection and mitigation of concept drift in machine learning (ML)
models. Our approach supports practitioners by consolidating domain expertise
related to concept drift-inducing events, making this expertise accessible to
on-call personnel, and enabling automatic adaptability with expert oversight.
</summary>
    <author>
      <name>Joran Leest</name>
    </author>
    <author>
      <name>Claudia Raibulet</name>
    </author>
    <author>
      <name>Ilias Gerostathopoulos</name>
    </author>
    <author>
      <name>Patricia Lago</name>
    </author>
    <link href="http://arxiv.org/abs/2401.11993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.11993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13843v1</id>
    <updated>2024-01-24T22:40:00Z</updated>
    <published>2024-01-24T22:40:00Z</published>
    <title>Enumerating the k-fold configurations in multi-class classification
  problems</title>
    <summary>  K-fold cross-validation is a widely used tool for assessing classifier
performance. The reproducibility crisis faced by artificial intelligence partly
results from the irreproducibility of reported k-fold cross-validation-based
performance scores. Recently, we introduced numerical techniques to test the
consistency of claimed performance scores and experimental setups. In a crucial
use case, the method relies on the combinatorial enumeration of all k-fold
configurations, for which we proposed an algorithm in the binary classification
case.
</summary>
    <author>
      <name>Attila Fazekas</name>
    </author>
    <author>
      <name>Gyorgy Kovacs</name>
    </author>
    <link href="http://arxiv.org/abs/2401.13843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.16383v1</id>
    <updated>2024-01-29T18:24:16Z</updated>
    <published>2024-01-29T18:24:16Z</published>
    <title>Learning logic programs by finding minimal unsatisfiable subprograms</title>
    <summary>  The goal of inductive logic programming (ILP) is to search for a logic
program that generalises training examples and background knowledge. We
introduce an ILP approach that identifies minimal unsatisfiable subprograms
(MUSPs). We show that finding MUSPs allows us to efficiently and soundly prune
the search space. Our experiments on multiple domains, including program
synthesis and game playing, show that our approach can reduce learning times by
99%.
</summary>
    <author>
      <name>Andrew Cropper</name>
    </author>
    <author>
      <name>Céline Hocquette</name>
    </author>
    <link href="http://arxiv.org/abs/2401.16383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.02354v1</id>
    <updated>2024-02-04T05:37:37Z</updated>
    <published>2024-02-04T05:37:37Z</published>
    <title>A Paradigm for Potential Model Performance Improvement in Classification
  and Regression Problems. A Proof of Concept</title>
    <summary>  A methodology that seeks to enhance model prediction performance is
presented. The method involves generating multiple auxiliary models that
capture relationships between attributes as a function of each other. Such
information serves to generate additional informative columns in the dataset
that can potentially enhance target prediction. A proof of case and related
code is provided.
</summary>
    <author>
      <name>Francisco Javier Lobo-Cabrera</name>
    </author>
    <link href="http://arxiv.org/abs/2402.02354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.02354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.02663v1</id>
    <updated>2024-02-05T01:20:59Z</updated>
    <published>2024-02-05T01:20:59Z</published>
    <title>Counterfactual Fairness Is Not Demographic Parity, and Other
  Observations</title>
    <summary>  Blanket statements of equivalence between causal concepts and purely
probabilistic concepts should be approached with care. In this short note, I
examine a recent claim that counterfactual fairness is equivalent to
demographic parity. The claim fails to hold up upon closer examination. I will
take the opportunity to address some broader misunderstandings about
counterfactual fairness.
</summary>
    <author>
      <name>Ricardo Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.02663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.02663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.03587v2</id>
    <updated>2024-05-22T10:39:38Z</updated>
    <published>2024-02-05T23:33:57Z</published>
    <title>Information-Theoretic Active Correlation Clustering</title>
    <summary>  We study correlation clustering where the pairwise similarities are not known
in advance. For this purpose, we employ active learning to query pairwise
similarities in a cost-efficient way. We propose a number of effective
information-theoretic acquisition functions based on entropy and information
gain. We extensively investigate the performance of our methods in different
settings and demonstrate their superior performance compared to the
alternatives.
</summary>
    <author>
      <name>Linus Aronsson</name>
    </author>
    <author>
      <name>Morteza Haghir Chehreghani</name>
    </author>
    <link href="http://arxiv.org/abs/2402.03587v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.03587v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.06614v1</id>
    <updated>2024-02-09T18:45:00Z</updated>
    <published>2024-02-09T18:45:00Z</published>
    <title>The Complexity of Sequential Prediction in Dynamical Systems</title>
    <summary>  We study the problem of learning to predict the next state of a dynamical
system when the underlying evolution function is unknown. Unlike previous work,
we place no parametric assumptions on the dynamical system, and study the
problem from a learning theory perspective. We define new combinatorial
measures and dimensions and show that they quantify the optimal mistake and
regret bounds in the realizable and agnostic setting respectively.
</summary>
    <author>
      <name>Vinod Raman</name>
    </author>
    <author>
      <name>Unique Subedi</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.06614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.08676v1</id>
    <updated>2024-02-13T18:56:55Z</updated>
    <published>2024-02-13T18:56:55Z</published>
    <title>A Convergence Analysis of Approximate Message Passing with Non-Separable
  Functions and Applications to Multi-Class Classification</title>
    <summary>  Motivated by the recent application of approximate message passing (AMP) to
the analysis of convex optimizations in multi-class classifications [Loureiro,
et. al., 2021], we present a convergence analysis of AMP dynamics with
non-separable multivariate nonlinearities. As an application, we present a
complete (and independent) analysis of the motivated convex optimization
problem.
</summary>
    <author>
      <name>Burak Çakmak</name>
    </author>
    <author>
      <name>Yue M. Lu</name>
    </author>
    <author>
      <name>Manfred Opper</name>
    </author>
    <link href="http://arxiv.org/abs/2402.08676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.08676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14759v1</id>
    <updated>2024-02-22T18:20:25Z</updated>
    <published>2024-02-22T18:20:25Z</published>
    <title>Generalising realisability in statistical learning theory under
  epistemic uncertainty</title>
    <summary>  The purpose of this paper is to look into how central notions in statistical
learning theory, such as realisability, generalise under the assumption that
train and test distribution are issued from the same credal set, i.e., a convex
set of probability distributions. This can be considered as a first step
towards a more general treatment of statistical learning under epistemic
uncertainty.
</summary>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2401.09435</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.14759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.19254v1</id>
    <updated>2024-02-29T15:26:03Z</updated>
    <published>2024-02-29T15:26:03Z</published>
    <title>Machine learning for modular multiplication</title>
    <summary>  Motivated by cryptographic applications, we investigate two machine learning
approaches to modular multiplication: namely circular regression and a
sequence-to-sequence transformer model. The limited success of both methods
demonstrated in our results gives evidence for the hardness of tasks involving
modular multiplication upon which cryptosystems are based.
</summary>
    <author>
      <name>Kristin Lauter</name>
    </author>
    <author>
      <name>Cathy Yuanchen Li</name>
    </author>
    <author>
      <name>Krystal Maughan</name>
    </author>
    <author>
      <name>Rachel Newton</name>
    </author>
    <author>
      <name>Megha Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures. Comments welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.19254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.19254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.08835v1</id>
    <updated>2024-03-13T08:10:18Z</updated>
    <published>2024-03-13T08:10:18Z</published>
    <title>Stacking-based deep neural network for player scouting in football 1</title>
    <summary>  Datascouting is one of the most known data applications in professional
sport, and specifically football. Its objective is to analyze huge database of
players in order to detect high potentials that can be then individually
considered by human scouts. In this paper, we propose a stacking-based deep
learning model to detect high potential football players. Applied on
open-source database, our model obtains significantly better results that
classical statistical methods.
</summary>
    <author>
      <name>Simon Lacan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT Nord Europe</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2403.08835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.08835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.10561v1</id>
    <updated>2024-03-14T08:46:07Z</updated>
    <published>2024-03-14T08:46:07Z</published>
    <title>A collection of the accepted papers for the Human-Centric Representation
  Learning workshop at AAAI 2024</title>
    <summary>  This non-archival index is not complete, as some accepted papers chose to
opt-out of inclusion. The list of all accepted papers is available on the
workshop website.
</summary>
    <author>
      <name>Dimitris Spathis</name>
    </author>
    <author>
      <name>Aaqib Saeed</name>
    </author>
    <author>
      <name>Ali Etemad</name>
    </author>
    <author>
      <name>Sana Tonekaboni</name>
    </author>
    <author>
      <name>Stefanos Laskaridis</name>
    </author>
    <author>
      <name>Shohreh Deldari</name>
    </author>
    <author>
      <name>Chi Ian Tang</name>
    </author>
    <author>
      <name>Patrick Schwab</name>
    </author>
    <author>
      <name>Shyam Tailor</name>
    </author>
    <link href="http://arxiv.org/abs/2403.10561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.10561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15416v1</id>
    <updated>2024-03-08T07:47:27Z</updated>
    <published>2024-03-08T07:47:27Z</published>
    <title>Fuzzy hyperparameters update in a second order optimization</title>
    <summary>  This research will present a hybrid approach to accelerate convergence in a
second order optimization. An online finite difference approximation of the
diagonal Hessian matrix will be introduced, along with fuzzy inferencing of
several hyperparameters. Competitive results have been achieved
</summary>
    <author>
      <name>Abdelaziz Bensadok</name>
    </author>
    <author>
      <name>Muhammad Zeeshan Babar</name>
    </author>
    <link href="http://arxiv.org/abs/2403.15416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00657v1</id>
    <updated>2024-03-31T12:01:34Z</updated>
    <published>2024-03-31T12:01:34Z</published>
    <title>Observations on Building RAG Systems for Technical Documents</title>
    <summary>  Retrieval augmented generation (RAG) for technical documents creates
challenges as embeddings do not often capture domain information. We review
prior art for important factors affecting RAG and perform experiments to
highlight best practices and potential challenges to build RAG systems for
technical documents.
</summary>
    <author>
      <name>Sumit Soman</name>
    </author>
    <author>
      <name>Sujoy Roychowdhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a Tiny Paper at ICLR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.00657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.02719v1</id>
    <updated>2024-04-03T13:21:58Z</updated>
    <published>2024-04-03T13:21:58Z</published>
    <title>Can We Understand Plasticity Through Neural Collapse?</title>
    <summary>  This paper explores the connection between two recently identified phenomena
in deep learning: plasticity loss and neural collapse. We analyze their
correlation in different scenarios, revealing a significant association during
the initial training phase on the first task. Additionally, we introduce a
regularization approach to mitigate neural collapse, demonstrating its
effectiveness in alleviating plasticity loss in this specific setting.
</summary>
    <author>
      <name>Guglielmo Bonifazi</name>
    </author>
    <author>
      <name>Iason Chalas</name>
    </author>
    <author>
      <name>Gian Hess</name>
    </author>
    <author>
      <name>Jakub Łucki</name>
    </author>
    <link href="http://arxiv.org/abs/2404.02719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.02719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.03058v1</id>
    <updated>2024-04-03T20:50:48Z</updated>
    <published>2024-04-03T20:50:48Z</published>
    <title>Automatic Extraction of Linguistic Description from Fuzzy Rule Base</title>
    <summary>  Neuro-fuzzy systems are a technique of explainable artificial intelligence
(XAI). They elaborate knowledge models as a set of fuzzy rules. Fuzzy sets are
crucial components of fuzzy rules. They are used to model linguistic terms. In
this paper, we present an automatic extraction of fuzzy rules in the natural
English language. Full implementation is available free from a public
repository.
</summary>
    <author>
      <name>Krzysztof Siminski</name>
    </author>
    <author>
      <name>Konrad Wnuk</name>
    </author>
    <link href="http://arxiv.org/abs/2404.03058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.03058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.03495v1</id>
    <updated>2024-04-04T14:50:50Z</updated>
    <published>2024-04-04T14:50:50Z</published>
    <title>About Test-time training for outlier detection</title>
    <summary>  In this paper, we introduce DOUST, our method applying test-time training for
outlier detection, significantly improving the detection performance. After
thoroughly evaluating our algorithm on common benchmark datasets, we discuss a
common problem and show that it disappears with a large enough test set. Thus,
we conclude that under reasonable conditions, our algorithm can reach almost
supervised performance even when no labeled outliers are given.
</summary>
    <author>
      <name>Simon Klüttermann</name>
    </author>
    <author>
      <name>Emmanuel Müller</name>
    </author>
    <link href="http://arxiv.org/abs/2404.03495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.03495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.05086v1</id>
    <updated>2024-04-07T22:00:50Z</updated>
    <published>2024-04-07T22:00:50Z</published>
    <title>A Note on LoRA</title>
    <summary>  LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently
adapting Large Language Models (LLMs) with remarkable simplicity and efficacy.
This note extends the original LoRA paper by offering new perspectives that
were not initially discussed and presents a series of insights for deploying
LoRA at scale. Without introducing new experiments, we aim to improve the
understanding and application of LoRA.
</summary>
    <author>
      <name>Vlad Fomenko</name>
    </author>
    <author>
      <name>Han Yu</name>
    </author>
    <author>
      <name>Jongho Lee</name>
    </author>
    <author>
      <name>Stanley Hsieh</name>
    </author>
    <author>
      <name>Weizhu Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2404.05086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.05086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.17651v1</id>
    <updated>2024-04-26T18:16:39Z</updated>
    <published>2024-04-26T18:16:39Z</published>
    <title>Hard ASH: Sparsity and the right optimizer make a continual learner</title>
    <summary>  In class incremental learning, neural networks typically suffer from
catastrophic forgetting. We show that an MLP featuring a sparse activation
function and an adaptive learning rate optimizer can compete with established
regularization techniques in the Split-MNIST task. We highlight the
effectiveness of the Adaptive SwisH (ASH) activation function in this context
and introduce a novel variant, Hard Adaptive SwisH (Hard ASH) to further
enhance the learning retention.
</summary>
    <author>
      <name>Santtu Keskinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2024 TinyPaper</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.17651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.17651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.09781v1</id>
    <updated>2024-05-16T03:00:41Z</updated>
    <published>2024-05-16T03:00:41Z</published>
    <title>An Independent Implementation of Quantum Machine Learning Algorithms in
  Qiskit for Genomic Data</title>
    <summary>  In this paper, we explore the power of Quantum Machine Learning as we extend,
implement and evaluate algorithms like Quantum Support Vector Classifier
(QSVC), Pegasos-QSVC, Variational Quantum Circuits (VQC), and Quantum Neural
Networks (QNN) in Qiskit with diverse feature mapping techniques for genomic
sequence classification.
</summary>
    <author>
      <name>Navneet Singh</name>
    </author>
    <author>
      <name>Shiva Raj Pokhrel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pager extended abstract</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGCOMM 2024, Sydney Australia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2405.09781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.09781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.14469v2</id>
    <updated>2024-08-29T15:41:52Z</updated>
    <published>2024-05-23T11:56:05Z</published>
    <title>Generalization of Hamiltonian algorithms</title>
    <summary>  The paper proves generalization results for a class of stochastic learning
algorithms. The method applies whenever the algorithm generates an absolutely
continuous distribution relative to some a-priori measure and the Radon Nikodym
derivative has subgaussian concentration. Applications are bounds for the Gibbs
algorithm and randomizations of stable deterministic algorithms as well as
PAC-Bayesian bounds with data-dependent priors.
</summary>
    <author>
      <name>Andreas Maurer</name>
    </author>
    <link href="http://arxiv.org/abs/2405.14469v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.14469v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.15430v1</id>
    <updated>2024-05-24T10:56:51Z</updated>
    <published>2024-05-24T10:56:51Z</published>
    <title>Counterexample-Guided Repair of Reinforcement Learning Systems Using
  Safety Critics</title>
    <summary>  Naively trained Deep Reinforcement Learning agents may fail to satisfy vital
safety constraints. To avoid costly retraining, we may desire to repair a
previously trained reinforcement learning agent to obviate unsafe behaviour. We
devise a counterexample-guided repair algorithm for repairing reinforcement
learning systems leveraging safety critics. The algorithm jointly repairs a
reinforcement learning agent and a safety critic using gradient-based
constrained optimisation.
</summary>
    <author>
      <name>David Boetius</name>
    </author>
    <author>
      <name>Stefan Leue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages + references</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.15430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.15430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.19559v1</id>
    <updated>2024-05-29T22:55:45Z</updated>
    <published>2024-05-29T22:55:45Z</published>
    <title>Clustering Mixtures of Discrete Distributions: A Note on Mitra's
  Algorithm</title>
    <summary>  In this note, we provide a refined analysis of Mitra's algorithm
\cite{mitra2008clustering} for classifying general discrete mixture
distribution models. Built upon spectral clustering
\cite{mcsherry2001spectral}, this algorithm offers compelling conditions for
probability distributions. We enhance this analysis by tailoring the model to
bipartite stochastic block models, resulting in more refined conditions.
Compared to those derived in \cite{mitra2008clustering}, our improved
separation conditions are obtained.
</summary>
    <author>
      <name>Mohamed Seif</name>
    </author>
    <author>
      <name>Yanxi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2405.19559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.19559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.08929v2</id>
    <updated>2024-06-23T23:18:07Z</updated>
    <published>2024-06-13T08:58:45Z</published>
    <title>Step-by-Step Diffusion: An Elementary Tutorial</title>
    <summary>  We present an accessible first course on diffusion models and flow matching
for machine learning, aimed at a technical audience with no diffusion
experience. We try to simplify the mathematical details as much as possible
(sometimes heuristically), while retaining enough precision to derive correct
algorithms.
</summary>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <author>
      <name>Arwen Bradley</name>
    </author>
    <author>
      <name>Hattie Zhou</name>
    </author>
    <author>
      <name>Madhu Advani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.08929v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.08929v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09592v1</id>
    <updated>2024-06-13T21:17:25Z</updated>
    <published>2024-06-13T21:17:25Z</published>
    <title>On Value Iteration Convergence in Connected MDPs</title>
    <summary>  This paper establishes that an MDP with a unique optimal policy and ergodic
associated transition matrix ensures the convergence of various versions of the
Value Iteration algorithm at a geometric rate that exceeds the discount factor
{\gamma} for both discounted and average-reward criteria.
</summary>
    <author>
      <name>Arsenii Mustafin</name>
    </author>
    <author>
      <name>Alex Olshevsky</name>
    </author>
    <author>
      <name>Ioannis Ch. Paschalidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.09592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.10002v2</id>
    <updated>2024-12-21T20:40:55Z</updated>
    <published>2024-06-14T13:16:48Z</published>
    <title>An elementary proof of a universal approximation theorem</title>
    <summary>  In this short note, we give an elementary proof of a universal approximation
theorem for neural networks with three hidden layers and increasing,
continuous, bounded activation function. The result is weaker than the best
known results, but the proof is elementary in the sense that no machinery
beyond undergraduate analysis is used.
</summary>
    <author>
      <name>Chris Monico</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added some additional clarification at several points in the
  arguments</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.10002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.16997v2</id>
    <updated>2024-10-14T13:56:47Z</updated>
    <published>2024-06-24T10:05:01Z</published>
    <title>Gate Recurrent Unit for Efficient Industrial Gas Identification</title>
    <summary>  In recent years, gas recognition technology has received considerable
attention. Nevertheless, the gas recognition area has faced obstacles in
implementing deep learning-based recognition solutions due to the absence of
standardized protocols. To tackle this problem, we suggest a new GRU. Compared
to other models, GRU obtains a higher identification accuracy.
</summary>
    <author>
      <name>Ding Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.16997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.16997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00452v1</id>
    <updated>2024-06-29T14:36:37Z</updated>
    <published>2024-06-29T14:36:37Z</published>
    <title>KHNNs: hypercomplex neural networks computations via Keras using
  TensorFlow and PyTorch</title>
    <summary>  Neural networks used in computations with more advanced algebras than real
numbers perform better in some applications. However, there is no general
framework for constructing hypercomplex neural networks. We propose a library
integrated with Keras that can do computations within TensorFlow and PyTorch.
It provides Dense and Convolutional 1D, 2D, and 3D layers architectures.
</summary>
    <author>
      <name>Agnieszka Niemczynowicz</name>
    </author>
    <author>
      <name>Radosław Antoni Kycia</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.01459v1</id>
    <updated>2024-07-01T16:54:07Z</updated>
    <published>2024-07-01T16:54:07Z</published>
    <title>On Implications of Scaling Laws on Feature Superposition</title>
    <summary>  Using results from scaling laws, this theoretical note argues that the
following two statements cannot be simultaneously true: 1. Superposition
hypothesis where sparse features are linearly represented across a layer is a
complete theory of feature representation. 2. Features are universal, meaning
two models trained on the same data and achieving equal performance will learn
identical features.
</summary>
    <author>
      <name>Pavan Katta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.01459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.01459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.12163v1</id>
    <updated>2024-07-16T20:40:08Z</updated>
    <published>2024-07-16T20:40:08Z</published>
    <title>Bellman Diffusion Models</title>
    <summary>  Diffusion models have seen tremendous success as generative architectures.
Recently, they have been shown to be effective at modelling policies for
offline reinforcement learning and imitation learning. We explore using
diffusion as a model class for the successor state measure (SSM) of a policy.
We find that enforcing the Bellman flow constraints leads to a simple Bellman
update on the diffusion step distribution.
</summary>
    <author>
      <name>Liam Schramm</name>
    </author>
    <author>
      <name>Abdeslam Boularias</name>
    </author>
    <link href="http://arxiv.org/abs/2407.12163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.12895v1</id>
    <updated>2024-07-17T13:21:53Z</updated>
    <published>2024-07-17T13:21:53Z</published>
    <title>A Survey on Universal Approximation Theorems</title>
    <summary>  This paper discusses various theorems on the approximation capabilities of
neural networks (NNs), which are known as universal approximation theorems
(UATs). The paper gives a systematic overview of UATs starting from the
preliminary results on function approximation, such as Taylor's theorem,
Fourier's theorem, Weierstrass approximation theorem, Kolmogorov - Arnold
representation theorem, etc. Theoretical and numerical aspects of UATs are
covered from both arbitrary width and depth.
</summary>
    <author>
      <name>Midhun T Augustine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.12895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.14166v1</id>
    <updated>2024-07-19T09:52:18Z</updated>
    <published>2024-07-19T09:52:18Z</published>
    <title>On Maximum Entropy Linear Feature Inversion</title>
    <summary>  We revisit the classical problem of inverting dimension-reducing linear
mappings using the maximum entropy (MaxEnt) criterion. In the literature,
solutions are problem-dependent, inconsistent, and use different entropy
measures. We propose a new unified approach that not only specializes to the
existing approaches, but offers solutions to new cases, such as when data
values are constrained to [0, 1], which has new applications in machine
learning.
</summary>
    <author>
      <name>Paul M Baggenstoss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted IEEE-Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.14166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.14166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.14968v1</id>
    <updated>2024-07-20T19:30:38Z</updated>
    <published>2024-07-20T19:30:38Z</published>
    <title>Technical report: Improving the properties of molecules generated by
  LIMO</title>
    <summary>  This technical report investigates variants of the Latent Inceptionism on
Molecules (LIMO) framework to improve the properties of generated molecules. We
conduct ablative studies of molecular representation, decoder model, and
surrogate model training scheme. The experiments suggest that an autogressive
Transformer decoder with GroupSELFIES achieves the best average properties for
the random generation task.
</summary>
    <author>
      <name>Vineet Thumuluri</name>
    </author>
    <author>
      <name>Peter Eckmann</name>
    </author>
    <author>
      <name>Michael K. Gilson</name>
    </author>
    <author>
      <name>Rose Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.14968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.14968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.17377v1</id>
    <updated>2024-07-24T15:57:55Z</updated>
    <published>2024-07-24T15:57:55Z</published>
    <title>Entropy Reweighted Conformal Classification</title>
    <summary>  Conformal Prediction (CP) is a powerful framework for constructing prediction
sets with guaranteed coverage. However, recent studies have shown that
integrating confidence calibration with CP can lead to a degradation in
efficiency. In this paper, We propose an adaptive approach that considers the
classifier's uncertainty and employs entropy-based reweighting to enhance the
efficiency of prediction sets for conformal classification. Our experimental
results demonstrate that this method significantly improves efficiency.
</summary>
    <author>
      <name>Rui Luo</name>
    </author>
    <author>
      <name>Nicolo Colombo</name>
    </author>
    <link href="http://arxiv.org/abs/2407.17377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.17377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07090v1</id>
    <updated>2024-08-09T14:16:31Z</updated>
    <published>2024-08-09T14:16:31Z</published>
    <title>Persistence kernels for classification: A comparative study</title>
    <summary>  The aim of the present work is a comparative study of different persistence
kernels applied to various classification problems. After some necessary
preliminaries on homology and persistence diagrams, we introduce five different
kernels that are then used to compare their performances of classification on
various datasets. We also provide the Python codes for the reproducibility of
results.
</summary>
    <author>
      <name>Cinzia Bandiziol</name>
    </author>
    <author>
      <name>Stefano De Marchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.07090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.10291v2</id>
    <updated>2024-08-22T20:24:07Z</updated>
    <published>2024-08-19T14:26:59Z</published>
    <title>Chatbots and Zero Sales Resistance</title>
    <summary>  It is argued that the pursuit of an ever increasing number of weights in
large-scale machine learning applications, besides being energetically
unsustainable, is also conducive to manipulative strategies whereby Science is
easily served as a strawman for economic and financial power. If machine
learning is meant to serve science ahead of vested business interests, a
paradigm shift is needed: from more weights and little insight to more insight
and less weights.
</summary>
    <author>
      <name>Sauro Succi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.10291v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.10291v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.14445v1</id>
    <updated>2024-08-26T17:36:51Z</updated>
    <published>2024-08-26T17:36:51Z</published>
    <title>Symmetry &amp; Critical Points</title>
    <summary>  Critical points of an invariant function may or may not be symmetric. We
prove, however, that if a symmetric critical point exists, those adjacent to it
are generically symmetry breaking. This mathematical mechanism is shown to
carry important implications for our ability to efficiently minimize invariant
nonconvex functions, in particular those associated with neural networks.
</summary>
    <author>
      <name>Yossi Arjevani</name>
    </author>
    <link href="http://arxiv.org/abs/2408.14445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.14445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.14483v1</id>
    <updated>2024-08-18T06:06:53Z</updated>
    <published>2024-08-18T06:06:53Z</published>
    <title>Gravix: Active Learning for Gravitational Waves Classification
  Algorithms</title>
    <summary>  arXiv admin note: This version has been removed by arXiv administrators due
to copyright infringement
</summary>
    <author>
      <name>Raja Vavekanand</name>
    </author>
    <author>
      <name>Kira Sam</name>
    </author>
    <author>
      <name>Vavek Bharwani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: This version has been removed by arXiv
  administrators due to copyright infringement and inappropriate text reuse
  from external sources</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.14483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.14483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.16806v1</id>
    <updated>2024-08-29T16:00:42Z</updated>
    <published>2024-08-29T16:00:42Z</published>
    <title>Physics-Informed Neural Networks and Extensions</title>
    <summary>  In this paper, we review the new method Physics-Informed Neural Networks
(PINNs) that has become the main pillar in scientific machine learning, we
present recent practical extensions, and provide a specific example in
data-driven discovery of governing differential equations.
</summary>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Paris Perdikaris</name>
    </author>
    <author>
      <name>Nazanin Ahmadi</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers of Science Awards 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.16806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.16806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03489v1</id>
    <updated>2024-09-05T12:56:39Z</updated>
    <published>2024-09-05T12:56:39Z</published>
    <title>Sparsifying Parametric Models with L0 Regularization</title>
    <summary>  This document contains an educational introduction to the problem of
sparsifying parametric models with L0 regularization. We utilize this approach
together with dictionary learning to learn sparse polynomial policies for deep
reinforcement learning to control parametric partial differential equations.
The code and a tutorial are provided here:
https://github.com/nicob15/Sparsifying-Parametric-Models-with-L0.
</summary>
    <author>
      <name>Nicolò Botteghi</name>
    </author>
    <author>
      <name>Urban Fasel</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03657v1</id>
    <updated>2024-09-05T16:11:36Z</updated>
    <published>2024-09-05T16:11:36Z</published>
    <title>Unsupervised Anomaly Detection and Localization with Generative
  Adversarial Networks</title>
    <summary>  We propose a novel unsupervised anomaly detection approach using generative
adversarial networks and SOP-derived spectrograms. Demonstrating remarkable
efficacy, our method achieves over 97% accuracy on SOP datasets from both
submarine and terrestrial fiber links, all achieved without the need for
labelled data.
</summary>
    <author>
      <name>Khouloud Abdelli</name>
    </author>
    <author>
      <name>Matteo Lonardi</name>
    </author>
    <author>
      <name>Jurgen Gripp</name>
    </author>
    <author>
      <name>Samuel Olsson</name>
    </author>
    <author>
      <name>Fabien Boitier</name>
    </author>
    <author>
      <name>Patricia Layec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECOC 2024</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECOC 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.03657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03667v1</id>
    <updated>2024-09-05T16:21:20Z</updated>
    <published>2024-09-05T16:21:20Z</published>
    <title>Threat Classification on Deployed Optical Networks Using MIMO Digital
  Fiber Sensing, Wavelets, and Machine Learning</title>
    <summary>  We demonstrate mechanical threats classification including jackhammers and
excavators, leveraging wavelet transform of MIMO-DFS output data across a 57-km
operational network link. Our machine learning framework incorporates transfer
learning and shows 93% classification accuracy from field data, with benefits
for optical network supervision.
</summary>
    <author>
      <name>Khouloud Abdelli</name>
    </author>
    <author>
      <name>Henrique Pavani</name>
    </author>
    <author>
      <name>Christian Dorize</name>
    </author>
    <author>
      <name>Sterenn Guerrier</name>
    </author>
    <author>
      <name>Haik Mardoyan</name>
    </author>
    <author>
      <name>Patricia Layec</name>
    </author>
    <author>
      <name>Jeremie Renaudier</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.06437v1</id>
    <updated>2024-09-10T11:42:22Z</updated>
    <published>2024-09-10T11:42:22Z</published>
    <title>A Short Information-Theoretic Analysis of Linear Auto-Regressive
  Learning</title>
    <summary>  In this note, we give a short information-theoretic proof of the consistency
of the Gaussian maximum likelihood estimator in linear auto-regressive models.
Our proof yields nearly optimal non-asymptotic rates for parameter recovery and
works without any invocation of stability in the case of finite hypothesis
classes.
</summary>
    <author>
      <name>Ingvar Ziemann</name>
    </author>
    <link href="http://arxiv.org/abs/2409.06437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.06437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.07401v1</id>
    <updated>2024-09-11T16:40:24Z</updated>
    <published>2024-09-11T16:40:24Z</published>
    <title>Convergence of continuous-time stochastic gradient descent with
  applications to linear deep neural networks</title>
    <summary>  We study a continuous-time approximation of the stochastic gradient descent
process for minimizing the expected loss in learning problems. The main results
establish general sufficient conditions for the convergence, extending the
results of Chatterjee (2022) established for (nonstochastic) gradient descent.
We show how the main result can be applied to the case of overparametrized
linear neural network training.
</summary>
    <author>
      <name>Gabor Lugosi</name>
    </author>
    <author>
      <name>Eulalia Nualart</name>
    </author>
    <link href="http://arxiv.org/abs/2409.07401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.12913v1</id>
    <updated>2024-09-19T17:10:14Z</updated>
    <published>2024-09-19T17:10:14Z</published>
    <title>Universal approximation theorem for neural networks with inputs from a
  topological vector space</title>
    <summary>  We study feedforward neural networks with inputs from a topological vector
space (TVS-FNNs). Unlike traditional feedforward neural networks, TVS-FNNs can
process a broader range of inputs, including sequences, matrices, functions and
more. We prove a universal approximation theorem for TVS-FNNs, which
demonstrates their capacity to approximate any continuous function defined on
this expanded input space.
</summary>
    <author>
      <name>Vugar Ismailov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.12913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.12913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A30, 41A65, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.13672v1</id>
    <updated>2024-09-20T17:26:47Z</updated>
    <published>2024-09-20T17:26:47Z</published>
    <title>Recent Advances in Non-convex Smoothness Conditions and Applicability to
  Deep Linear Neural Networks</title>
    <summary>  The presence of non-convexity in smooth optimization problems arising from
deep learning have sparked new smoothness conditions in the literature and
corresponding convergence analyses. We discuss these smoothness conditions,
order them, provide conditions for determining whether they hold, and evaluate
their applicability to training a deep linear neural network for binary
classification.
</summary>
    <author>
      <name>Vivak Patel</name>
    </author>
    <author>
      <name>Christian Varner</name>
    </author>
    <link href="http://arxiv.org/abs/2409.13672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.13672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65K10, 68T07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17502v1</id>
    <updated>2024-09-26T03:20:09Z</updated>
    <published>2024-09-26T03:20:09Z</published>
    <title>Broadcast Product: Shape-aligned Element-wise Multiplication and Beyond</title>
    <summary>  We propose a new operator defined between two tensors, the broadcast product.
The broadcast product calculates the Hadamard product after duplicating
elements to align the shapes of the two tensors. Complex tensor operations in
libraries like \texttt{numpy} can be succinctly represented as mathematical
expressions using the broadcast product. Finally, we propose a novel tensor
decomposition using the broadcast product, highlighting its potential
applications in dimensionality reduction.
</summary>
    <author>
      <name>Yusuke Matsui</name>
    </author>
    <author>
      <name>Tatsuya Yokota</name>
    </author>
    <link href="http://arxiv.org/abs/2409.17502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00038v1</id>
    <updated>2024-09-26T01:18:45Z</updated>
    <published>2024-09-26T01:18:45Z</published>
    <title>A Novel Spinor-Based Embedding Model for Transformers</title>
    <summary>  This paper proposes a novel approach to word embeddings in Transformer models
by utilizing spinors from geometric algebra. Spinors offer a rich mathematical
framework capable of capturing complex relationships and transformations in
high-dimensional spaces. By encoding words as spinors, we aim to enhance the
expressiveness and robustness of language representations. We present the
theoretical foundations of spinors, detail their integration into Transformer
architectures, and discuss potential advantages and challenges.
</summary>
    <author>
      <name>Rick White</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.00038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.08449v1</id>
    <updated>2024-10-11T01:38:27Z</updated>
    <published>2024-10-11T01:38:27Z</published>
    <title>Finite Sample and Large Deviations Analysis of Stochastic Gradient
  Algorithm with Correlated Noise</title>
    <summary>  We analyze the finite sample regret of a decreasing step size stochastic
gradient algorithm. We assume correlated noise and use a perturbed Lyapunov
function as a systematic approach for the analysis. Finally we analyze the
escape time of the iterates using large deviations theory.
</summary>
    <author>
      <name>George Yin</name>
    </author>
    <author>
      <name>Vikram Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/2410.08449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.08449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.11776v1</id>
    <updated>2024-10-15T16:56:34Z</updated>
    <published>2024-10-15T16:56:34Z</published>
    <title>Encoding architecture algebra</title>
    <summary>  Despite the wide variety of input types in machine learning, this diversity
is often not fully reflected in their representations or model architectures,
leading to inefficiencies throughout a model's lifecycle. This paper introduces
an algebraic approach to constructing input-encoding architectures that
properly account for the data's structure, providing a step toward achieving
more typeful machine learning.
</summary>
    <author>
      <name>Stephane Bersier</name>
    </author>
    <author>
      <name>Xinyi Chen-Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 6 figures. Keywords: typeful, algebraic data types,
  tensors, structured data</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.11776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.11776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14665v1</id>
    <updated>2024-10-18T17:55:15Z</updated>
    <published>2024-10-18T17:55:15Z</published>
    <title>Online Reinforcement Learning with Passive Memory</title>
    <summary>  This paper considers an online reinforcement learning algorithm that
leverages pre-collected data (passive memory) from the environment for online
interaction. We show that using passive memory improves performance and further
provide theoretical guarantees for regret that turns out to be near-minimax
optimal. Results show that the quality of passive memory determines
sub-optimality of the incurred regret. The proposed approach and results hold
in both continuous and discrete state-action spaces.
</summary>
    <author>
      <name>Anay Pattanaik</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <link href="http://arxiv.org/abs/2410.14665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18950v1</id>
    <updated>2024-10-24T17:50:08Z</updated>
    <published>2024-10-24T17:50:08Z</published>
    <title>Adjusted Overfitting Regression</title>
    <summary>  In this paper, I will introduce a new form of regression, that can adjust
overfitting and underfitting through, "distance-based regression." Overfitting
often results in finding false patterns causing inaccurate results, so by
having a new approach that minimizes overfitting, more accurate predictions can
be derived. Then I will proceed with a test of my regression form and show
additional ways to optimize the regression. Finally, I will apply my new
technique to a specific data set to demonstrate its practical value.
</summary>
    <author>
      <name>Dylan Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.19077v1</id>
    <updated>2024-10-24T18:33:32Z</updated>
    <published>2024-10-24T18:33:32Z</published>
    <title>Target Strangeness: A Novel Conformal Prediction Difficulty Estimator</title>
    <summary>  This paper introduces Target Strangeness, a novel difficulty estimator for
conformal prediction (CP) that offers an alternative approach for normalizing
prediction intervals (PIs). By assessing how atypical a prediction is within
the context of its nearest neighbours' target distribution, Target Strangeness
can surpass the current state-of-the-art performance. This novel difficulty
estimator is evaluated against others in the context of several conformal
regression experiments.
</summary>
    <author>
      <name>Alexis Bose</name>
    </author>
    <author>
      <name>Jonathan Ethier</name>
    </author>
    <author>
      <name>Paul Guinand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.19077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.19077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.23626v1</id>
    <updated>2024-10-31T04:25:24Z</updated>
    <published>2024-10-31T04:25:24Z</published>
    <title>An Application of the Holonomic Gradient Method to the Neural Tangent
  Kernel</title>
    <summary>  A holonomic system of linear partial differential equations is, roughly
speaking, a system whose solution space is finite dimensional. A distribution
that is a solution of a holonomic system is called a holonomic distribution. We
give methods to numerically evaluate dual activations of holonomic activator
distributions for neural tangent kernels. These methods are based on computer
algebra algorithms for rings of differential operators.
</summary>
    <author>
      <name>Akihiro Sakoda</name>
    </author>
    <author>
      <name>Nobuki Takayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.23626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.23626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.01302v1</id>
    <updated>2024-11-02T16:28:34Z</updated>
    <published>2024-11-02T16:28:34Z</published>
    <title>Regret of exploratory policy improvement and $q$-learning</title>
    <summary>  We study the convergence of $q$-learning and related algorithms introduced by
Jia and Zhou (J. Mach. Learn. Res., 24 (2023), 161) for controlled diffusion
processes. Under suitable conditions on the growth and regularity of the model
parameters, we provide a quantitative error and regret analysis of both the
exploratory policy improvement algorithm and the $q$-learning algorithm.
</summary>
    <author>
      <name>Wenpin Tang</name>
    </author>
    <author>
      <name>Xun Yu Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.01302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.01302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.01527v1</id>
    <updated>2024-11-03T11:09:53Z</updated>
    <published>2024-11-03T11:09:53Z</published>
    <title>Performance Evaluation of Deep Learning Models for Water Quality Index
  Prediction: A Comparative Study of LSTM, TCN, ANN, and MLP</title>
    <summary>  Environmental monitoring and predictive modeling of the Water Quality Index
(WQI) through the assessment of the water quality.
</summary>
    <author>
      <name>Muhammad Ismail</name>
    </author>
    <author>
      <name>Farkhanda Abbas</name>
    </author>
    <author>
      <name>Shahid Munir Shah</name>
    </author>
    <author>
      <name>Mahmoud Aljawarneh</name>
    </author>
    <author>
      <name>Lachhman Das Dhomeja</name>
    </author>
    <author>
      <name>Fazila Abbas</name>
    </author>
    <author>
      <name>Muhammad Shoaib</name>
    </author>
    <author>
      <name>Abdulwahed Fahad Alrefaei</name>
    </author>
    <author>
      <name>Mohammed Fahad Albeshr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.01527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.01527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.04396v1</id>
    <updated>2024-11-07T03:18:34Z</updated>
    <published>2024-11-07T03:18:34Z</published>
    <title>Remote Sensing-Based Assessment of Economic Development</title>
    <summary>  The goal of our project is to use satellite data (including nighttime light
data and remote sensing images) to give us some statistical estimation of the
economic development level of a selected area (Singapore). Findings from the
project could inform policymakers about areas needing intervention or support
for economic development initiatives. Insights gained might aid in targeted
policy formulation for infrastructure, agriculture, urban planning, or resource
management.
</summary>
    <author>
      <name>Yijian Pan</name>
    </author>
    <author>
      <name>Yongchang Ma</name>
    </author>
    <author>
      <name>Bolin Shen</name>
    </author>
    <author>
      <name>Linyang He</name>
    </author>
    <link href="http://arxiv.org/abs/2411.04396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.04396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.16615v2</id>
    <updated>2024-12-04T08:44:10Z</updated>
    <published>2024-11-25T17:54:29Z</published>
    <title>Graph Pooling by Local Cluster Selection</title>
    <summary>  Graph pooling is a family of operations which take graphs as input and
produce shrinked graphs as output. Modern graph pooling methods are trainable
and, in general inserted in Graph Neural Networks (GNNs) architectures as graph
shrinking operators along the (deep) processing pipeline. This work proposes a
novel procedure for pooling graphs, along with a node-centred graph pooling
operator.
</summary>
    <author>
      <name>Yizhu Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.16615v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.16615v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.19734v1</id>
    <updated>2024-11-29T14:27:28Z</updated>
    <published>2024-11-29T14:27:28Z</published>
    <title>A Note on Small Percolating Sets on Hypercubes via Generative AI</title>
    <summary>  We apply a generative AI pattern-recognition technique called PatternBoost to
study bootstrap percolation on hypercubes. With this, we slightly improve the
best existing upper bound for the size of percolating subsets of the hypercube.
</summary>
    <author>
      <name>Gergely Bérczi</name>
    </author>
    <author>
      <name>Adam Zsolt Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/2411.19734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R10, 68T07, 05C35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.06686v1</id>
    <updated>2024-12-09T17:28:29Z</updated>
    <published>2024-12-09T17:28:29Z</published>
    <title>Some Best Practices in Operator Learning</title>
    <summary>  Hyperparameters searches are computationally expensive. This paper studies
some general choices of hyperparameters and training methods specifically for
operator learning. It considers the architectures DeepONets, Fourier neural
operators and Koopman autoencoders for several differential equations to find
robust trends. Some options considered are activation functions, dropout and
stochastic weight averaging.
</summary>
    <author>
      <name>Dustin Enyeart</name>
    </author>
    <author>
      <name>Guang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2412.04578</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.06846v1</id>
    <updated>2024-12-08T02:11:33Z</updated>
    <published>2024-12-08T02:11:33Z</published>
    <title>Classifier-free guidance in LLMs Safety</title>
    <summary>  The paper describes LLM unlearning without a retaining dataset, using the
ORPO reinforcement learning method with inference enhanced by modified
classifier-free guidance. Significant improvement in unlearning, without
degradation of the model, is achieved through direct training on synthetic
replacement data in CFG-aware training regime, with classifier-free guidance
applied during the inference. This article is an extended version of the
NeurIPS 2024 LLM-PC submission, which was awarded second prize.
</summary>
    <author>
      <name>Roman Smirnov</name>
    </author>
    <link href="http://arxiv.org/abs/2412.06846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.06846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.07811v1</id>
    <updated>2024-12-10T03:54:55Z</updated>
    <published>2024-12-10T03:54:55Z</published>
    <title>Adversarial Autoencoders in Operator Learning</title>
    <summary>  DeepONets and Koopman autoencoders are two prevalent neural operator
architectures. These architectures are autoencoders. An adversarial addition to
an autoencoder have improved performance of autoencoders in various areas of
machine learning. In this paper, the use an adversarial addition for these two
neural operator architectures is studied.
</summary>
    <author>
      <name>Dustin Enyeart</name>
    </author>
    <author>
      <name>Guang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2412.06686,
  arXiv:2412.04578</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.07811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12936v1</id>
    <updated>2024-12-17T14:15:30Z</updated>
    <published>2024-12-17T14:15:30Z</published>
    <title>A simple DNN regression for the chemical composition in essential oil</title>
    <summary>  Although experimental design and methodological surveys for mono-molecular
activity/property has been extensively investigated, those for chemical
composition have received little attention, with the exception of a few prior
studies. In this study, we configured three simple DNN regressors to predict
essential oil property based on chemical composition. Despite showing
overfitting due to the small size of dataset, all models were trained
effectively in this study.
</summary>
    <author>
      <name>Yuki Harada</name>
    </author>
    <author>
      <name>Shuichi Maeda</name>
    </author>
    <author>
      <name>Masato Kiyama</name>
    </author>
    <author>
      <name>Shinichiro Nakamura</name>
    </author>
    <link href="http://arxiv.org/abs/2412.12936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.12936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.14543v1</id>
    <updated>2024-12-19T05:49:31Z</updated>
    <published>2024-12-19T05:49:31Z</published>
    <title>Transformer models are gauge invariant: A mathematical connection
  between AI and particle physics</title>
    <summary>  In particle physics, the fundamental forces are subject to symmetries called
gauge invariance. It is a redundancy in the mathematical description of any
physical system. In this article I will demonstrate that the transformer
architecture exhibits the same properties, and show that the default
representation of transformers has partially, but not fully removed the gauge
invariance.
</summary>
    <author>
      <name>Leo van Nierop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.14543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.14543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.20195v1</id>
    <updated>2024-12-28T16:09:25Z</updated>
    <published>2024-12-28T16:09:25Z</published>
    <title>Lower bounds on transformers with infinite precision</title>
    <summary>  In this note, we use the VC dimension technique to prove the first lower
bound against one-layer softmax transformers with infinite precision. We do so
for two tasks: function composition, considered by Peng, Narayanan, and
Papadimitriou, and the SUM$_2$ task, considered by Sanford, Hsu, and Telgarsky.
</summary>
    <author>
      <name>Alexander Kozachinskiy</name>
    </author>
    <link href="http://arxiv.org/abs/2412.20195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.01339v2</id>
    <updated>2025-01-03T16:17:24Z</updated>
    <published>2025-01-02T16:44:03Z</published>
    <title>Simultaneous Latent State Estimation and Latent Linear Dynamics
  Discovery from Image Observations</title>
    <summary>  The problem of state estimation has a long history with many successful
algorithms that allow analytical derivation or approximation of posterior
filtering distribution given the noisy observations. This report tries to
conclude previous works to resolve the problem of latent state estimation given
image-based observations and also suggests a new solution to this problem.
</summary>
    <author>
      <name>Nikita Kostin</name>
    </author>
    <link href="http://arxiv.org/abs/2501.01339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.01339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.02535v1</id>
    <updated>2025-01-05T13:32:13Z</updated>
    <published>2025-01-05T13:32:13Z</published>
    <title>A completely uniform transformer for parity</title>
    <summary>  We construct a 3-layer constant-dimension transformer, recognizing the parity
language, where neither parameter matrices nor the positional encoding depend
on the input length. This improves upon a construction of Chiang and Cholak who
use a positional encoding, depending on the input length (but their
construction has 2 layers).
</summary>
    <author>
      <name>Alexander Kozachinskiy</name>
    </author>
    <author>
      <name>Tomasz Steifer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.02535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.02535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.04403v1</id>
    <updated>2025-01-08T10:33:21Z</updated>
    <published>2025-01-08T10:33:21Z</published>
    <title>Rising Rested MAB with Linear Drift</title>
    <summary>  We consider non-stationary multi-arm bandit (MAB) where the expected reward
of each action follows a linear function of the number of times we executed the
action. Our main result is a tight regret bound of
$\tilde{\Theta}(T^{4/5}K^{3/5})$, by providing both upper and lower bounds. We
extend our results to derive instance dependent regret bounds, which depend on
the unknown parametrization of the linear drift of the rewards.
</summary>
    <author>
      <name>Omer Amichay</name>
    </author>
    <author>
      <name>Yishay Mansour</name>
    </author>
    <link href="http://arxiv.org/abs/2501.04403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.06505v1</id>
    <updated>2025-01-11T10:52:59Z</updated>
    <published>2025-01-11T10:52:59Z</published>
    <title>Online Algorithm for Aggregating Experts' Predictions with Unbounded
  Quadratic Loss</title>
    <summary>  We consider the problem of online aggregation of expert predictions with the
quadratic loss function. We propose an algorithm for aggregating expert
predictions which does not require a prior knowledge of the upper bound on the
losses. The algorithm is based on the exponential reweighing of expert losses.
</summary>
    <author>
      <name>Alexander Korotin</name>
    </author>
    <author>
      <name>Vladimir V'yugin</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1070/RM9961</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1070/RM9961" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Russian Math. Surveys, 75-5, 2020, 974-977</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.06505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08317v1</id>
    <updated>2025-01-14T18:52:27Z</updated>
    <published>2025-01-14T18:52:27Z</published>
    <title>A Similarity Measure Between Functions with Applications to Statistical
  Learning and Optimization</title>
    <summary>  In this note, we present a novel measure of similarity between two functions.
It quantifies how the sub-optimality gaps of two functions convert to each
other, and unifies several existing notions of functional similarity. We show
that it has convenient operation rules, and illustrate its use in empirical
risk minimization and non-stationary online optimization.
</summary>
    <author>
      <name>Chengpiao Huang</name>
    </author>
    <author>
      <name>Kaizheng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.08317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.16113v1</id>
    <updated>2025-01-27T15:04:35Z</updated>
    <published>2025-01-27T15:04:35Z</published>
    <title>Fixed-sized clusters $k$-Means</title>
    <summary>  We present a $k$-means-based clustering algorithm, which optimizes the mean
square error, for given cluster sizes. A straightforward application is
balanced clustering, where the sizes of each cluster are equal. In the
$k$-means assignment phase, the algorithm solves an assignment problem using
the Hungarian algorithm. This makes the assignment phase time complexity
$O(n^3)$. This enables clustering of datasets of size more than 5000 points.
</summary>
    <author>
      <name>Mikko I. Malinen</name>
    </author>
    <author>
      <name>Pasi Fränti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.16113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.16113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.18388v2</id>
    <updated>2025-02-04T07:57:53Z</updated>
    <published>2025-01-30T14:38:26Z</published>
    <title>Improved Replicable Boosting with Majority-of-Majorities</title>
    <summary>  We introduce a new replicable boosting algorithm which significantly improves
the sample complexity compared to previous algorithms. The algorithm works by
doing two layers of majority voting, using an improved version of the
replicable boosting algorithm introduced by Impagliazzo et al. [2022] in the
bottom layer.
</summary>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Markus Engelund Mathiasen</name>
    </author>
    <author>
      <name>Clement Svendsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.18388v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.18388v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.19149v1</id>
    <updated>2025-01-31T14:06:13Z</updated>
    <published>2025-01-31T14:06:13Z</published>
    <title>On the inductive bias of infinite-depth ResNets and the bottleneck rank</title>
    <summary>  We compute the minimum-norm weights of a deep linear ResNet, and find that
the inductive bias of this architecture lies between minimizing nuclear norm
and rank. This implies that, with appropriate hyperparameters, deep nonlinear
ResNets have an inductive bias towards minimizing bottleneck rank.
</summary>
    <author>
      <name>Enric Boix-Adsera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.19149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.19149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.06738v1</id>
    <updated>2025-02-10T18:07:09Z</updated>
    <published>2025-02-10T18:07:09Z</published>
    <title>Resurrecting saturated LLM benchmarks with adversarial encoding</title>
    <summary>  Recent work showed that small changes in benchmark questions can reduce LLMs'
reasoning and recall. We explore two such changes: pairing questions and adding
more answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We
find that for more capable models, these predictably reduce performance,
essentially heightening the performance ceiling of a benchmark and unsaturating
it again. We suggest this approach can resurrect old benchmarks.
</summary>
    <author>
      <name>Igor Ivanov</name>
    </author>
    <author>
      <name>Dmitrii Volkov</name>
    </author>
    <link href="http://arxiv.org/abs/2502.06738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.06738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07978v1</id>
    <updated>2025-02-11T21:52:19Z</updated>
    <published>2025-02-11T21:52:19Z</published>
    <title>A Survey of In-Context Reinforcement Learning</title>
    <summary>  Reinforcement learning (RL) agents typically optimize their policies by
performing expensive backward passes to update their network parameters.
However, some agents can solve new tasks without updating any parameters by
simply conditioning on additional context such as their action-observation
histories. This paper surveys work on such behavior, known as in-context
reinforcement learning.
</summary>
    <author>
      <name>Amir Moeini</name>
    </author>
    <author>
      <name>Jiuqi Wang</name>
    </author>
    <author>
      <name>Jacob Beck</name>
    </author>
    <author>
      <name>Ethan Blaser</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
    <author>
      <name>Rohan Chandra</name>
    </author>
    <author>
      <name>Shangtong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2502.07978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10647v1</id>
    <updated>2025-02-15T02:47:55Z</updated>
    <published>2025-02-15T02:47:55Z</published>
    <title>A Power Transform</title>
    <summary>  Power transforms, such as the Box-Cox transform and Tukey's ladder of powers,
are a fundamental tool in mathematics and statistics. These transforms are
primarily used for normalizing and standardizing datasets, effectively by
raising values to a power. In this work I present a novel power transform, and
I show that it serves as a unifying framework for wide family of loss
functions, kernel functions, probability distributions, bump functions, and
neural network activation functions.
</summary>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <link href="http://arxiv.org/abs/2502.10647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13692v1</id>
    <updated>2025-02-19T13:04:49Z</updated>
    <published>2025-02-19T13:04:49Z</published>
    <title>Tight Generalization Bounds for Large-Margin Halfspaces</title>
    <summary>  We prove the first generalization bound for large-margin halfspaces that is
asymptotically tight in the tradeoff between the margin, the fraction of
training points with the given margin, the failure probability and the number
of training points.
</summary>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Natascha Schalburg</name>
    </author>
    <link href="http://arxiv.org/abs/2502.13692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15834v1</id>
    <updated>2025-02-20T11:31:22Z</updated>
    <published>2025-02-20T11:31:22Z</published>
    <title>Challenges of Multi-Modal Coreset Selection for Depth Prediction</title>
    <summary>  Coreset selection methods are effective in accelerating training and reducing
memory requirements but remain largely unexplored in applied multimodal
settings. We adapt a state-of-the-art (SoTA) coreset selection technique for
multimodal data, focusing on the depth prediction task. Our experiments with
embedding aggregation and dimensionality reduction approaches reveal the
challenges of extending unimodal algorithms to multimodal scenarios,
highlighting the need for specialized methods to better capture inter-modal
relationships.
</summary>
    <author>
      <name>Viktor Moskvoretskii</name>
    </author>
    <author>
      <name>Narek Alvandian</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16931v1</id>
    <updated>2025-02-24T07:49:49Z</updated>
    <published>2025-02-24T07:49:49Z</published>
    <title>Machine learning and high dimensional vector search</title>
    <summary>  Machine learning and vector search are two research topics that developed in
parallel in nearby communities. However, unlike many other fields related to
big data, machine learning has not significantly impacted vector search. In
this opinion paper we attempt to explain this oddity. Along the way, we wander
over the numerous bridges between the two fields.
</summary>
    <author>
      <name>Matthijs Douze</name>
    </author>
    <link href="http://arxiv.org/abs/2502.16931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17872v1</id>
    <updated>2025-02-25T05:55:15Z</updated>
    <published>2025-02-25T05:55:15Z</published>
    <title>Contrastive Learning with Nasty Noise</title>
    <summary>  Contrastive learning has emerged as a powerful paradigm for self-supervised
representation learning. This work analyzes the theoretical limits of
contrastive learning under nasty noise, where an adversary modifies or replaces
training samples. Using PAC learning and VC-dimension analysis, lower and upper
bounds on sample complexity in adversarial settings are established.
Additionally, data-dependent sample complexity bounds based on the l2-distance
function are derived.
</summary>
    <author>
      <name>Ziruo Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2502.17872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00892v1</id>
    <updated>2025-03-02T13:32:28Z</updated>
    <published>2025-03-02T13:32:28Z</published>
    <title>Riemannian Integrated Gradients: A Geometric View of Explainable AI</title>
    <summary>  We introduce Riemannian Integrated Gradients (RIG); an extension of
Integrated Gradients (IG) to Riemannian manifolds. We demonstrate that RIG
restricts to IG when the Riemannian manifold is Euclidean space. We show that
feature attribution can be phrased as an eigenvalue problem where attributions
correspond to eigenvalues of a symmetric endomorphism.
</summary>
    <author>
      <name>Federico Costanza</name>
    </author>
    <author>
      <name>Lachlan Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/2503.00892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02803v1</id>
    <updated>2025-03-04T17:26:25Z</updated>
    <published>2025-03-04T17:26:25Z</published>
    <title>Inductive randomness predictors</title>
    <summary>  This paper introduces inductive randomness predictors, which form a superset
of inductive conformal predictors. Its focus is on a very simple special case,
binary inductive randomness predictors. It is interesting that binary inductive
randomness predictors have an advantage over inductive conformal predictors,
although they also have a serious disadvantage. This advantage will allow us to
reach the surprising conclusion that non-trivial inductive conformal predictors
are inadmissible in the sense of statistical decision theory.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.02803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32 (Primary) 62G15, 68T05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05289v1</id>
    <updated>2025-03-07T10:09:16Z</updated>
    <published>2025-03-07T10:09:16Z</published>
    <title>An Analytical Model for Overparameterized Learning Under Class Imbalance</title>
    <summary>  We study class-imbalanced linear classification in a high-dimensional
Gaussian mixture model. We develop a tight, closed form approximation for the
test error of several practical learning methods, including logit adjustment
and class dependent temperature. Our approximation allows us to analytically
tune and compare these methods, highlighting how and when they overcome the
pitfalls of standard cross-entropy minimization. We test our theoretical
findings on simulated data and imbalanced CIFAR10, MNIST and FashionMNIST
datasets.
</summary>
    <author>
      <name>Eliav Mor</name>
    </author>
    <author>
      <name>Yair Carmon</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13001v1</id>
    <updated>2025-03-17T09:56:39Z</updated>
    <published>2025-03-17T09:56:39Z</published>
    <title>Linear-Size Neural Network Representation of Piecewise Affine Functions
  in $\mathbb{R}^2$</title>
    <summary>  It is shown that any continuous piecewise affine (CPA) function
$\mathbb{R}^2\to\mathbb{R}$ with $p$ pieces can be represented by a ReLU neural
network with two hidden layers and $O(p)$ neurons. Unlike prior work, which
focused on convex pieces, this analysis considers CPA functions with connected
but potentially non-convex pieces.
</summary>
    <author>
      <name>Leo Zanotti</name>
    </author>
    <link href="http://arxiv.org/abs/2503.13001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14393v1</id>
    <updated>2025-03-18T16:28:14Z</updated>
    <published>2025-03-18T16:28:14Z</published>
    <title>On the clustering behavior of sliding windows</title>
    <summary>  Things can go spectacularly wrong when clustering timeseries data that has
been preprocessed with a sliding window. We highlight three surprising failures
that emerge depending on how the window size compares with the timeseries
length. In addition to computational examples, we present theoretical
explanations for each of these failure modes.
</summary>
    <author>
      <name>Boris Alexeev</name>
    </author>
    <author>
      <name>Wenyan Luo</name>
    </author>
    <author>
      <name>Dustin G. Mixon</name>
    </author>
    <author>
      <name>Yan X Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.5088v1</id>
    <updated>2013-01-22T07:10:34Z</updated>
    <published>2013-01-22T07:10:34Z</published>
    <title>Piecewise Linear Multilayer Perceptrons and Dropout</title>
    <summary>  We propose a new type of hidden layer for a multilayer perceptron, and
demonstrate that it obtains the best reported performance for an MLP on the
MNIST dataset.
</summary>
    <author>
      <name>Ian J. Goodfellow</name>
    </author>
    <link href="http://arxiv.org/abs/1301.5088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.5088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5738v1</id>
    <updated>2013-10-21T22:02:17Z</updated>
    <published>2013-10-21T22:02:17Z</published>
    <title>A Kernel for Hierarchical Parameter Spaces</title>
    <summary>  We define a family of kernels for mixed continuous/discrete hierarchical
parameter spaces and show that they are positive definite.
</summary>
    <author>
      <name>Frank Hutter</name>
    </author>
    <author>
      <name>Michael A. Osborne</name>
    </author>
    <link href="http://arxiv.org/abs/1310.5738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05549v1</id>
    <updated>2017-01-19T18:43:56Z</updated>
    <published>2017-01-19T18:43:56Z</published>
    <title>Deep Neural Networks - A Brief History</title>
    <summary>  Introduction to deep neural networks and their history.
</summary>
    <author>
      <name>Krzysztof J. Cios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07245v1</id>
    <updated>2019-09-16T14:44:19Z</updated>
    <published>2019-09-16T14:44:19Z</published>
    <title>BMVC 2019: Workshop on Interpretable and Explainable Machine Vision</title>
    <summary>  Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable
Machine Vision, Cardiff, UK, September 12, 2019.
</summary>
    <author>
      <name>Alun Preece</name>
    </author>
    <link href="http://arxiv.org/abs/1909.07245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03667v4</id>
    <updated>2024-05-10T19:54:45Z</updated>
    <published>2019-02-10T20:30:29Z</published>
    <title>Differential Similarity in Higher Dimensional Spaces: Theory and
  Applications</title>
    <summary>  This paper presents an extension and an elaboration of the theory of
differential similarity, which was originally proposed in arXiv:1401.2411
[cs.LG]. The goal is to develop an algorithm for clustering and coding that
combines a geometric model with a probabilistic model in a principled way. For
simplicity, the geometric model in the earlier paper was restricted to the
three-dimensional case. The present paper removes this restriction, and
considers the full $n$-dimensional case. Although the mathematical model is the
same, the strategies for computing solutions in the $n$-dimensional case are
different, and one of the main purposes of this paper is to develop and analyze
these strategies. Another main purpose is to devise techniques for estimating
the parameters of the model from sample data, again in $n$ dimensions. We
evaluate the solution strategies and the estimation techniques by applying them
to two familiar real-world examples: the classical MNIST dataset and the
CIFAR-10 dataset.
</summary>
    <author>
      <name>L. Thorne McCarty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">67 pages, 30 figures. Revised to match the published version of
  arXiv:1401.2411 [cs.LG]</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03667v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03667v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.05467v1</id>
    <updated>2019-12-11T17:05:18Z</updated>
    <published>2019-12-11T17:05:18Z</published>
    <title>MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low
  Resource Machine Translation</title>
    <summary>  Manipulating training data leads to robust neural models for MT.
</summary>
    <author>
      <name>Rumeng Li</name>
    </author>
    <author>
      <name>Xun Wang</name>
    </author>
    <author>
      <name>Hong Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/aaai.v34i05.6339</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/aaai.v34i05.6339" rel="related"/>
    <link href="http://arxiv.org/abs/1912.05467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.05467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07690v1</id>
    <updated>2020-12-14T16:41:23Z</updated>
    <published>2020-12-14T16:41:23Z</published>
    <title>A PAC-Bayesian Approach to Generalization Bounds for Graph Neural
  Networks</title>
    <summary>  In this paper, we derive generalization bounds for the two primary classes of
graph neural networks (GNNs), namely graph convolutional networks (GCNs) and
message passing GNNs (MPGNNs), via a PAC-Bayesian approach. Our result reveals
that the maximum node degree and spectral norm of the weights govern the
generalization bounds of both models. We also show that our bound for GCNs is a
natural generalization of the results developed in arXiv:1707.09564v2 [cs.LG]
for fully-connected and convolutional neural networks. For message passing
GNNs, our PAC-Bayes bound improves over the Rademacher complexity based bound
in arXiv:2002.06157v1 [cs.LG], showing a tighter dependency on the maximum node
degree and the maximum hidden dimension. The key ingredients of our proofs are
a perturbation analysis of GNNs and the generalization of PAC-Bayes analysis to
non-homogeneous GNNs. We perform an empirical study on several real-world graph
datasets and verify that our PAC-Bayes bound is tighter than others.
</summary>
    <author>
      <name>Renjie Liao</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Richard Zemel</name>
    </author>
    <link href="http://arxiv.org/abs/2012.07690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.05910v1</id>
    <updated>2022-05-12T07:04:14Z</updated>
    <published>2022-05-12T07:04:14Z</published>
    <title>Comments on: "Hybrid Semiparametric Bayesian Networks"</title>
    <summary>  Invited discussion on the paper "Hybrid Semiparametric Bayesian Networks" by
David Atienza, Pedro Larranaga and Concha Bielza (TEST, 2022).
</summary>
    <author>
      <name>Marco Scutari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">TEST (2022), 30, 328-330</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.05910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.00770v1</id>
    <updated>2022-10-03T08:59:12Z</updated>
    <published>2022-10-03T08:59:12Z</published>
    <title>Accelerate Reinforcement Learning with PID Controllers in the Pendulum
  Simulations</title>
    <summary>  We propose a Proportional Integral Derivative (PID) controller-based coaching
scheme to expedite reinforcement learning (RL).
</summary>
    <author>
      <name>Liping Bai</name>
    </author>
    <link href="http://arxiv.org/abs/2210.00770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.06938v1</id>
    <updated>2024-09-11T01:37:02Z</updated>
    <published>2024-09-11T01:37:02Z</published>
    <title>k-MLE, k-Bregman, k-VARs: Theory, Convergence, Computation</title>
    <summary>  We develop hard clustering based on likelihood rather than distance and prove
convergence. We also provide simulations and real data examples.
</summary>
    <author>
      <name>Zuogong Yue</name>
    </author>
    <author>
      <name>Victor Solo</name>
    </author>
    <link href="http://arxiv.org/abs/2409.06938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.06938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9905004v1</id>
    <updated>1999-05-10T20:52:23Z</updated>
    <published>1999-05-10T20:52:23Z</published>
    <title>Using Collective Intelligence to Route Internet Traffic</title>
    <summary>  A COllective INtelligence (COIN) is a set of interacting reinforcement
learning (RL) algorithms designed in an automated fashion so that their
collective behavior optimizes a global utility function. We summarize the
theory of COINs, then present experiments using that theory to design COINs to
control internet traffic routing. These experiments indicate that COINs
outperform all previously investigated RL-based, shortest path routing
algorithms.
</summary>
    <author>
      <name>David H. Wolpert</name>
    </author>
    <author>
      <name>Kagan Tumer</name>
    </author>
    <author>
      <name>Jeremy Frank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Information Processing Systems - 11, eds M. Kearns, S.
  Solla, D. Cohn, MIT Press, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9905004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9905004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9905015v1</id>
    <updated>1999-05-21T14:49:39Z</updated>
    <published>1999-05-21T14:49:39Z</published>
    <title>State Abstraction in MAXQ Hierarchical Reinforcement Learning</title>
    <summary>  Many researchers have explored methods for hierarchical reinforcement
learning (RL) with temporal abstractions, in which abstract actions are defined
that can perform many primitive actions before terminating. However, little is
known about learning with state abstractions, in which aspects of the state
space are ignored. In previous work, we developed the MAXQ method for
hierarchical RL. In this paper, we define five conditions under which state
abstraction can be combined with the MAXQ value function decomposition. We
prove that the MAXQ-Q learning algorithm converges under these conditions and
show experimentally that state abstraction is important for the successful
application of MAXQ-Q learning.
</summary>
    <author>
      <name>Thomas G. Dietterich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9905015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9905015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001004v1</id>
    <updated>2000-01-07T06:20:53Z</updated>
    <published>2000-01-07T06:20:53Z</published>
    <title>Multiplicative Algorithm for Orthgonal Groups and Independent Component
  Analysis</title>
    <summary>  The multiplicative Newton-like method developed by the author et al. is
extended to the situation where the dynamics is restricted to the orthogonal
group. A general framework is constructed without specifying the cost function.
Though the restriction to the orthogonal groups makes the problem somewhat
complicated, an explicit expression for the amount of individual jumps is
obtained. This algorithm is exactly second-order-convergent. The global
instability inherent in the Newton method is remedied by a
Levenberg-Marquardt-type variation. The method thus constructed can readily be
applied to the independent component analysis. Its remarkable performance is
illustrated by a numerical simulation.
</summary>
    <author>
      <name>Toshinao Akuzawa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RIKEN BSI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0001004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0002006v1</id>
    <updated>2000-02-09T06:44:28Z</updated>
    <published>2000-02-09T06:44:28Z</published>
    <title>Multiplicative Nonholonomic/Newton -like Algorithm</title>
    <summary>  We construct new algorithms from scratch, which use the fourth order cumulant
of stochastic variables for the cost function. The multiplicative updating rule
here constructed is natural from the homogeneous nature of the Lie group and
has numerous merits for the rigorous treatment of the dynamics. As one
consequence, the second order convergence is shown. For the cost function,
functions invariant under the componentwise scaling are choosen. By identifying
points which can be transformed to each other by the scaling, we assume that
the dynamics is in a coset space. In our method, a point can move toward any
direction in this coset. Thus, no prewhitening is required.
</summary>
    <author>
      <name>Toshinao Akuzawa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RIKEN BSI</arxiv:affiliation>
    </author>
    <author>
      <name>Noboru Murata</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RIKEN BSI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0960-0779(00)00077-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0960-0779(00)00077-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0002006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0002006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008022v1</id>
    <updated>2000-08-22T21:37:50Z</updated>
    <published>2000-08-22T21:37:50Z</published>
    <title>A Learning Approach to Shallow Parsing</title>
    <summary>  A SNoW based learning approach to shallow parsing tasks is presented and
studied experimentally. The approach learns to identify syntactic patterns by
combining simple predictors to produce a coherent inference. Two instantiations
of this approach are studied and experimental results for Noun-Phrases (NP) and
Subject-Verb (SV) phrases that compare favorably with the best published
results are presented. In doing that, we compare two ways of modeling the
problem of learning to recognize patterns and suggest that shallow parsing
patterns are better learned using open/close predictors than using
inside/outside predictors.
</summary>
    <author>
      <name>Marcia Muñoz</name>
    </author>
    <author>
      <name>Vasin Punyakanok</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <author>
      <name>Dav Zimak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTex 2e, 11 pages, 2 eps figures, 1 bbl file, uses colacl.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of EMNLP-VLC'99, pages 168-178</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011032v1</id>
    <updated>2000-11-21T21:51:01Z</updated>
    <published>2000-11-21T21:51:01Z</published>
    <title>Top-down induction of clustering trees</title>
    <summary>  An approach to clustering is presented that adapts the basic top-down
induction of decision trees method towards clustering. To this aim, it employs
the principles of instance based learning. The resulting methodology is
implemented in the TIC (Top down Induction of Clustering trees) system for
first order clustering. The TIC system employs the first order logical decision
tree representation of the inductive logic programming system Tilde. Various
experiments with TIC are presented, in both propositional and relational
domains.
</summary>
    <author>
      <name>Hendrik Blockeel</name>
    </author>
    <author>
      <name>Luc De Raedt</name>
    </author>
    <author>
      <name>Jan Ramon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning, Proceedings of the 15th International Conference
  (J. Shavlik, ed.), Morgan Kaufmann, 1998, pp. 55-63</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0011032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107032v1</id>
    <updated>2001-07-23T11:06:45Z</updated>
    <published>2001-07-23T11:06:45Z</published>
    <title>Coupled Clustering: a Method for Detecting Structural Correspondence</title>
    <summary>  This paper proposes a new paradigm and computational framework for
identification of correspondences between sub-structures of distinct composite
systems. For this, we define and investigate a variant of traditional data
clustering, termed coupled clustering, which simultaneously identifies
corresponding clusters within two data sets. The presented method is
demonstrated and evaluated for detecting topical correspondences in textual
corpora.
</summary>
    <author>
      <name>Zvika Marx</name>
    </author>
    <author>
      <name>Ido Dagan</name>
    </author>
    <author>
      <name>Joachim Buhmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">html with 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: C. E. Brodley and A. P. Danyluk (eds.), Proceedings of the
  18th International Conference on Machine Learning (ICML 2001), pp. 353-360</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0107032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.6; I.2.7; I.5.3; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107033v1</id>
    <updated>2001-07-25T15:50:43Z</updated>
    <published>2001-07-25T15:50:43Z</published>
    <title>Yet another zeta function and learning</title>
    <summary>  We study the convergence speed of the batch learning algorithm, and compare
its speed to that of the memoryless learning algorithm and of learning with
memory (as analyzed in joint work with N. Komarova). We obtain precise results
and show in particular that the batch learning algorithm is never worse than
the memoryless learning algorithm (at least asymptotically). Its performance
vis-a-vis learning with full memory is less clearcut, and depends on
certainprobabilistic assumptions. These results necessitate theintroduction of
the moment zeta function of a probability distribution and the study of some of
its properties.
</summary>
    <author>
      <name>Igor Rivin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0107033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111003v1</id>
    <updated>2001-11-01T03:02:19Z</updated>
    <published>2001-11-01T03:02:19Z</published>
    <title>The Use of Classifiers in Sequential Inference</title>
    <summary>  We study the problem of combining the outcomes of several different
classifiers in a way that provides a coherent inference that satisfies some
constraints. In particular, we develop two general approaches for an important
subproblem-identifying phrase structure. The first is a Markovian approach that
extends standard HMMs to allow the use of a rich observation structure and of
general classifiers to model state-observation dependencies. The second is an
extension of constraint satisfaction formalisms. We develop efficient
combination algorithms under both models and study them experimentally in the
context of shallow parsing.
</summary>
    <author>
      <name>Vasin Punyakanok</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems 13</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6, I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0201014v1</id>
    <updated>2002-01-17T13:42:23Z</updated>
    <published>2002-01-17T13:42:23Z</published>
    <title>The Dynamics of AdaBoost Weights Tells You What's Hard to Classify</title>
    <summary>  The dynamical evolution of weights in the Adaboost algorithm contains useful
information about the role that the associated data points play in the built of
the Adaboost model. In particular, the dynamics induces a bipartition of the
data set into two (easy/hard) classes. Easy points are ininfluential in the
making of the model, while the varying relevance of hard points can be gauged
in terms of an entropy value associated to their evolution. Smooth
approximations of entropy highlight regions where classification is most
uncertain. Promising results are obtained when methods proposed are applied in
the Optimal Sampling framework.
</summary>
    <author>
      <name>Bruno Caprile</name>
    </author>
    <author>
      <name>Cesare Furlanello</name>
    </author>
    <author>
      <name>Stefano Merler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, LaTeX, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0201014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0201014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204052v1</id>
    <updated>2002-04-26T14:33:29Z</updated>
    <published>2002-04-26T14:33:29Z</published>
    <title>Required sample size for learning sparse Bayesian networks with many
  variables</title>
    <summary>  Learning joint probability distributions on n random variables requires
exponential sample size in the generic case. Here we consider the case that a
temporal (or causal) order of the variables is known and that the (unknown)
graph of causal dependencies has bounded in-degree Delta. Then the joint
measure is uniquely determined by the probabilities of all (2 Delta+1)-tuples.
Upper bounds on the sample size required for estimating their probabilities can
be given in terms of the VC-dimension of the set of corresponding cylinder
sets. The sample size grows less than linearly with n.
</summary>
    <author>
      <name>Pawel Wocjan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitaet Karlsruhe</arxiv:affiliation>
    </author>
    <author>
      <name>Dominik Janzing</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitaet Karlsruhe</arxiv:affiliation>
    </author>
    <author>
      <name>Thomas Beth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitaet Karlsruhe</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211003v1</id>
    <updated>2002-11-01T18:09:56Z</updated>
    <published>2002-11-01T18:09:56Z</published>
    <title>Evaluation of the Performance of the Markov Blanket Bayesian Classifier
  Algorithm</title>
    <summary>  The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for
construction of probabilistic classifiers. This paper presents an empirical
comparison of the MBBC algorithm with three other Bayesian classifiers: Naive
Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these
are implemented using the K2 framework of Cooper and Herskovits. The
classifiers are compared in terms of their performance (using simple accuracy
measures and ROC curves) and speed, on a range of standard benchmark data sets.
It is concluded that MBBC is competitive in terms of speed and accuracy with
the other algorithms considered.
</summary>
    <author>
      <name>Michael G. Madden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages: Technical Report No. NUIG-IT-011002, Department of
  Information Technology, National University of Ireland, Galway (2002)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212029v1</id>
    <updated>2002-12-11T16:08:36Z</updated>
    <published>2002-12-11T16:08:36Z</published>
    <title>A Theory of Cross-Validation Error</title>
    <summary>  This paper presents a theory of error in cross-validation testing of
algorithms for predicting real-valued attributes. The theory justifies the
claim that predicting real-valued attributes requires balancing the conflicting
demands of simplicity and accuracy. Furthermore, the theory indicates precisely
how these conflicting demands must be balanced, in order to minimize
cross-validation error. A general theory is presented, then it is developed in
detail for linear regression and instance-based learning.
</summary>
    <author>
      <name>Peter D. Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Experimental and Theoretical Artificial Intelligence,
  (1994), 6, 361-391</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212037v1</id>
    <updated>2002-12-12T18:14:38Z</updated>
    <published>2002-12-12T18:14:38Z</published>
    <title>The Management of Context-Sensitive Features: A Review of Strategies</title>
    <summary>  In this paper, we review five heuristic strategies for handling
context-sensitive features in supervised machine learning from examples. We
discuss two methods for recovering lost (implicit) contextual information. We
mention some evidence that hybrid strategies can have a synergetic effect. We
then show how the work of several machine learning researchers fits into this
framework. While we do not claim that these strategies exhaust the
possibilities, it appears that the framework includes all of the techniques
that can be found in the published literature on contextsensitive learning.
</summary>
    <author>
      <name>Peter D. Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Machine Learning, Workshop on
  Learning in Context-Sensitive Domains, Bari, Italy, (1996), 60-66</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301007v1</id>
    <updated>2003-01-09T15:08:47Z</updated>
    <published>2003-01-09T15:08:47Z</published>
    <title>Kalman filter control in the reinforcement learning framework</title>
    <summary>  There is a growing interest in using Kalman-filter models in brain modelling.
In turn, it is of considerable importance to make Kalman-filters amenable for
reinforcement learning. In the usual formulation of optimal control it is
computed off-line by solving a backward recursion. In this technical note we
show that slight modification of the linear-quadratic-Gaussian Kalman-filter
model allows the on-line estimation of optimal control and makes the bridge to
reinforcement learning. Moreover, the learning rule for value estimation
assumes a Hebbian form weighted by the error of the value estimation.
</summary>
    <author>
      <name>Istvan Szita</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0301007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306120v2</id>
    <updated>2007-03-09T15:14:15Z</updated>
    <published>2003-06-22T08:00:09Z</published>
    <title>Reinforcement Learning with Linear Function Approximation and LQ control
  Converges</title>
    <summary>  Reinforcement learning is commonly used with function approximation. However,
very few positive results are known about the convergence of function
approximation based RL control algorithms. In this paper we show that TD(0) and
Sarsa(0) with linear function approximation is convergent for a simple class of
problems, where the system is linear and the costs are quadratic (the LQ
control problem). Furthermore, we show that for systems with Gaussian noise and
non-completely observable states (the LQG problem), the mentioned RL algorithms
are still convergent, if they are combined with Kalman filtering.
</summary>
    <author>
      <name>Istvan Szita</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306120v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306120v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408058v1</id>
    <updated>2004-08-25T20:25:43Z</updated>
    <published>2004-08-25T20:25:43Z</published>
    <title>Non-negative matrix factorization with sparseness constraints</title>
    <summary>  Non-negative matrix factorization (NMF) is a recently developed technique for
finding parts-based, linear representations of non-negative data. Although it
has successfully been applied in several applications, it does not always
result in parts-based representations. In this paper, we show how explicitly
incorporating the notion of `sparseness' improves the found decompositions.
Additionally, we provide complete MATLAB code both for standard NMF and for our
extension. Our hope is that this will further the application of these methods
to solving novel data-analysis problems.
</summary>
    <author>
      <name>Patrik O. Hoyer</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0408058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410015v1</id>
    <updated>2004-10-07T10:57:08Z</updated>
    <published>2004-10-07T10:57:08Z</published>
    <title>L1 regularization is better than L2 for learning and predicting chaotic
  systems</title>
    <summary>  Emergent behaviors are in the focus of recent research interest. It is then
of considerable importance to investigate what optimizations suit the learning
and prediction of chaotic systems, the putative candidates for emergence. We
have compared L1 and L2 regularizations on predicting chaotic time series using
linear recurrent neural networks. The internal representation and the weights
of the networks were optimized in a unifying framework. Computational tests on
different problems indicate considerable advantages for the L1 regularization:
It had considerably better learning time and better interpolating capabilities.
We shall argue that optimization viewed as a maximum likelihood estimation
justifies our results, because L1 regularization fits heavy-tailed
distributions -- an apparently general feature of emergent systems -- better.
</summary>
    <author>
      <name>Z. Szabo</name>
    </author>
    <author>
      <name>A. Lorincz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502004v1</id>
    <updated>2005-02-01T13:42:49Z</updated>
    <published>2005-02-01T13:42:49Z</published>
    <title>Asymptotic Log-loss of Prequential Maximum Likelihood Codes</title>
    <summary>  We analyze the Dawid-Rissanen prequential maximum likelihood codes relative
to one-parameter exponential family models M. If data are i.i.d. according to
an (essentially) arbitrary P, then the redundancy grows at rate c/2 ln n. We
show that c=v1/v2, where v1 is the variance of P, and v2 is the variance of the
distribution m* in M that is closest to P in KL divergence. This shows that
prequential codes behave quite differently from other important universal codes
such as the 2-part MDL, Shtarkov and Bayes codes, for which c=1. This behavior
is undesirable in an MDL model selection setting.
</summary>
    <author>
      <name>Peter Grunwald</name>
    </author>
    <author>
      <name>Steven de Rooij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, an abstract has been submitted to COLT 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505083v1</id>
    <updated>2005-05-30T21:12:00Z</updated>
    <published>2005-05-30T21:12:00Z</published>
    <title>Defensive forecasting</title>
    <summary>  We consider how to make probability forecasts of binary labels. Our main
mathematical result is that for any continuous gambling strategy used for
detecting disagreement between the forecasts and the actual labels, there
exists a forecasting strategy whose forecasts are ideal as far as this gambling
strategy is concerned. A forecasting strategy obtained in this way from a
gambling strategy demonstrating a strong law of large numbers is simplified and
studied empirically.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <author>
      <name>Akimichi Takemura</name>
    </author>
    <author>
      <name>Glenn Shafer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, to appear in the AIStats'2005 electronic
  proceedings</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Tenth International Workshop on Artificial
  Intelligence and Statistics, 2005, pages 365--372.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0505083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506004v4</id>
    <updated>2006-07-01T13:46:30Z</updated>
    <published>2005-06-01T14:03:20Z</published>
    <title>Non-asymptotic calibration and resolution</title>
    <summary>  We analyze a new algorithm for probability forecasting of binary observations
on the basis of the available data, without making any assumptions about the
way the observations are generated. The algorithm is shown to be well
calibrated and to have good resolution for long enough sequences of
observations and for a suitable choice of its parameter, a kernel on the
Cartesian product of the forecast space $[0,1]$ and the data space. Our main
results are non-asymptotic: we establish explicit inequalities, shown to be
tight, for the performance of the algorithm.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506004v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506004v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511088v1</id>
    <updated>2005-11-25T15:57:56Z</updated>
    <published>2005-11-25T15:57:56Z</published>
    <title>Bounds on Query Convergence</title>
    <summary>  The problem of finding an optimum using noisy evaluations of a smooth cost
function arises in many contexts, including economics, business, medicine,
experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t -
x*)^2 ] &gt;= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1,
&gt;...) generated by an unbiased feedback process observing noisy evaluations of
an unknown quadratic function maximised at x*. The bound is tight, as the proof
leads to a simple algorithm which meets it. We further establish a bound on the
total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] &gt;= O(sqrt(t)) These bounds may
impose practical limitations on an agent's performance, as O(eps^-4) queries
are made before the queries converge to x* with eps accuracy.
</summary>
    <author>
      <name>Barak A. Pearlmutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0511088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602093v1</id>
    <updated>2006-02-27T10:08:26Z</updated>
    <published>2006-02-27T10:08:26Z</published>
    <title>Rational stochastic languages</title>
    <summary>  The goal of the present paper is to provide a systematic and comprehensive
study of rational stochastic languages over a semiring K \in {Q, Q +, R, R+}. A
rational stochastic language is a probability distribution over a free monoid
\Sigma^* which is rational over K, that is which can be generated by a
multiplicity automata with parameters in K. We study the relations between the
classes of rational stochastic languages S rat K (\Sigma). We define the notion
of residual of a stochastic language and we use it to investigate properties of
several subclasses of rational stochastic languages. Lastly, we study the
representation of rational stochastic languages by means of multiplicity
automata.
</summary>
    <author>
      <name>François Denis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <author>
      <name>Yann Esposito</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605036v1</id>
    <updated>2006-05-08T23:38:13Z</updated>
    <published>2006-05-08T23:38:13Z</published>
    <title>Evaluating the Robustness of Learning from Implicit Feedback</title>
    <summary>  This paper evaluates the robustness of learning from implicit feedback in web
search. In particular, we create a model of user behavior by drawing upon user
studies in laboratory and real-world settings. The model is used to understand
the effect of user behavior on the performance of a learning algorithm for
ranked retrieval. We explore a wide range of possible user behaviors and find
that learning from implicit feedback can be surprisingly robust. This
complements previous results that demonstrated our algorithm's effectiveness in
a real-world search engine application.
</summary>
    <author>
      <name>Filip Radlinski</name>
    </author>
    <author>
      <name>Thorsten Joachims</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Presented at ICML Workshop on Learning In Web Search, 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605048v1</id>
    <updated>2006-05-11T03:27:12Z</updated>
    <published>2006-05-11T03:27:12Z</published>
    <title>On Learning Thresholds of Parities and Unions of Rectangles in Random
  Walk Models</title>
    <summary>  In a recent breakthrough, [Bshouty et al., 2005] obtained the first
passive-learning algorithm for DNFs under the uniform distribution. They showed
that DNFs are learnable in the Random Walk and Noise Sensitivity models. We
extend their results in several directions. We first show that thresholds of
parities, a natural class encompassing DNFs, cannot be learned efficiently in
the Noise Sensitivity model using only statistical queries. In contrast, we
show that a cyclic version of the Random Walk model allows to learn efficiently
polynomially weighted thresholds of parities. We also extend the algorithm of
Bshouty et al. to the case of Unions of Rectangles, a natural generalization of
DNFs to $\{0,...,b-1\}^n$.
</summary>
    <author>
      <name>S. Roch</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0605048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607067v1</id>
    <updated>2006-07-13T15:52:04Z</updated>
    <published>2006-07-13T15:52:04Z</published>
    <title>Competing with stationary prediction strategies</title>
    <summary>  In this paper we introduce the class of stationary prediction strategies and
construct a prediction algorithm that asymptotically performs as well as the
best continuous stationary strategy. We make mild compactness assumptions but
no stochastic assumptions about the environment. In particular, no assumption
of stationarity is made about the environment, and the stationarity of the
considered strategies only means that they do not depend explicitly on time; we
argue that it is natural to consider only stationary strategies even for highly
non-stationary environments.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607110v1</id>
    <updated>2006-07-25T15:57:56Z</updated>
    <published>2006-07-25T15:57:56Z</published>
    <title>A Theory of Probabilistic Boosting, Decision Trees and Matryoshki</title>
    <summary>  We present a theory of boosting probabilistic classifiers. We place ourselves
in the situation of a user who only provides a stopping parameter and a
probabilistic weak learner/classifier and compare three types of boosting
algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of
trees, which we call matryoshka. "Nested tree," "embedded tree" and "recursive
tree" are also appropriate names for this algorithm, which is one of our
contributions. Our other contribution is the theoretical analysis of the
algorithms, in which we give training error bounds. This analysis suggests that
the matryoshka leverages probabilistic weak classifiers more efficiently than
simple decision trees.
</summary>
    <author>
      <name>Etienne Grossmann</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0607110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.6; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608033v1</id>
    <updated>2006-08-06T16:10:05Z</updated>
    <published>2006-08-06T16:10:05Z</published>
    <title>A Study on Learnability for Rigid Lambek Grammars</title>
    <summary>  We present basic notions of Gold's "learnability in the limit" paradigm,
first presented in 1967, a formalization of the cognitive process by which a
native speaker gets to grasp the underlying grammar of his/her own native
language by being exposed to well formed sentences generated by that grammar.
Then we present Lambek grammars, a formalism issued from categorial grammars
which, although not as expressive as needed for a full formalization of natural
languages, is particularly suited to easily implement a natural interface
between syntax and semantics. In the last part of this work, we present a
learnability result for Rigid Lambek grammars from structured examples.
</summary>
    <author>
      <name>Roberto Bonato</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, LaBRI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0608033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609007v1</id>
    <updated>2006-09-03T21:30:03Z</updated>
    <published>2006-09-03T21:30:03Z</published>
    <title>A Massive Local Rules Search Approach to the Classification Problem</title>
    <summary>  An approach to the classification problem of machine learning, based on
building local classification rules, is developed. The local rules are
considered as projections of the global classification rules to the event we
want to classify. A massive global optimization algorithm is used for
optimization of quality criterion. The algorithm, which has polynomial
complexity in typical case, is used to find all high--quality local rules. The
other distinctive feature of the algorithm is the integration of attributes
levels selection (for ordered attributes) with rules searching and original
conflicting rules resolution strategy. The algorithm is practical; it was
tested on a number of data sets from UCI repository, and a comparison with the
other predicting techniques is presented.
</summary>
    <author>
      <name>Vladislav Malyshkin</name>
    </author>
    <author>
      <name>Ray Bakhramov</name>
    </author>
    <author>
      <name>Andrey Gorodetsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609045v1</id>
    <updated>2006-09-09T11:31:01Z</updated>
    <published>2006-09-09T11:31:01Z</published>
    <title>Metric entropy in competitive on-line prediction</title>
    <summary>  Competitive on-line prediction (also known as universal prediction of
individual sequences) is a strand of learning theory avoiding making any
stochastic assumptions about the way the observations are generated. The
predictor's goal is to compete with a benchmark class of prediction rules,
which is often a proper Banach function space. Metric entropy provides a
unifying framework for competitive on-line prediction: the numerous known upper
bounds on the metric entropy of various compact sets in function spaces readily
imply bounds on the performance of on-line prediction strategies. This paper
discusses strengths and limitations of the direct approach to competitive
on-line prediction via metric entropy, including comparisons to other
approaches.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609071v2</id>
    <updated>2007-02-14T06:51:03Z</updated>
    <published>2006-09-13T03:44:08Z</published>
    <title>A kernel method for canonical correlation analysis</title>
    <summary>  Canonical correlation analysis is a technique to extract common features from
a pair of multivariate data. In complex situations, however, it does not
extract useful features because of its linearity. On the other hand, kernel
method used in support vector machine is an efficient approach to improve such
a linear method. In this paper, we investigate the effectiveness of applying
kernel method to canonical correlation analysis.
</summary>
    <author>
      <name>Shotaro Akaho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper presented in IMPS2001 (International Meeting of
  Psychometric Society) 2007-Feb-14: typos in equations (23) and (24) in page 3
  of the first version have been corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609071v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609071v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701052v1</id>
    <updated>2007-01-08T17:03:31Z</updated>
    <published>2007-01-08T17:03:31Z</published>
    <title>Time Series Forecasting: Obtaining Long Term Trends with Self-Organizing
  Maps</title>
    <summary>  Kohonen self-organisation maps are a well know classification tool, commonly
used in a wide variety of problems, but with limited applications in time
series forecasting context. In this paper, we propose a forecasting method
specifically designed for multi-dimensional long-term trends prediction, with a
double application of the Kohonen algorithm. Practical applications of the
method are also presented.
</summary>
    <author>
      <name>Geoffroy Simon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE-MLG</arxiv:affiliation>
    </author>
    <author>
      <name>Amaury Lendasse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE-MLG</arxiv:affiliation>
    </author>
    <author>
      <name>Marie Cottrell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMOS, Matisse</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Claude Fort</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMOS, Matisse</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Verleysen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMOS, Matisse, Dice-MLG</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\`{a} la suite de la conf\'{e}rence ANNPR, Florence 2003</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letter 26 n0; 12 (05/2005) 1795-1808</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703138v1</id>
    <updated>2007-03-28T04:41:54Z</updated>
    <published>2007-03-28T04:41:54Z</published>
    <title>Reinforcement Learning for Adaptive Routing</title>
    <summary>  Reinforcement learning means learning a policy--a mapping of observations
into actions--based on feedback from the environment. The learning can be
viewed as browsing a set of policies while evaluating them by trial through
interaction with the environment. We present an application of gradient ascent
algorithm for reinforcement learning to a complex domain of packet routing in
network communication and compare the performance of this algorithm to other
routing methods on a benchmark problem.
</summary>
    <author>
      <name>Leonid Peshkin</name>
    </author>
    <author>
      <name>Virginia Savova</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Intnl Joint Conf on Neural Networks (IJCNN),
  2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; C.2.2; C.2.4; C.2.6; F.1.1; I.2.6; I.2.8; I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.2668v1</id>
    <updated>2007-04-20T08:26:29Z</updated>
    <published>2007-04-20T08:26:29Z</published>
    <title>Supervised Feature Selection via Dependence Estimation</title>
    <summary>  We introduce a framework for filtering features that employs the
Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence
between the features and the labels. The key idea is that good features should
maximise such dependence. Feature selection for various supervised learning
problems (including classification and regression) is unified under this
framework, and the solutions can be approximated using a backward-elimination
algorithm. We demonstrate the usefulness of our method on both artificial and
real world datasets.
</summary>
    <author>
      <name>Le Song</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <author>
      <name>Arthur Gretton</name>
    </author>
    <author>
      <name>Karsten Borgwardt</name>
    </author>
    <author>
      <name>Justin Bedo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.2668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.2765v1</id>
    <updated>2007-05-18T19:44:19Z</updated>
    <published>2007-05-18T19:44:19Z</published>
    <title>On the monotonization of the training set</title>
    <summary>  We consider the problem of minimal correction of the training set to make it
consistent with monotonic constraints. This problem arises during analysis of
data sets via techniques that require monotone data. We show that this problem
is NP-hard in general and is equivalent to finding a maximal independent set in
special orgraphs. Practically important cases of that problem considered in
detail. These are the cases when a partial order given on the replies set is a
total order or has a dimension 2. We show that the second case can be reduced
to maximization of a quadratic convex function on a convex set. For this case
we construct an approximate polynomial algorithm based on convex optimization.
</summary>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.2765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.2765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.0585v1</id>
    <updated>2007-06-05T05:55:07Z</updated>
    <published>2007-06-05T05:55:07Z</published>
    <title>A Novel Model of Working Set Selection for SMO Decomposition Methods</title>
    <summary>  In the process of training Support Vector Machines (SVMs) by decomposition
methods, working set selection is an important technique, and some exciting
schemes were employed into this field. To improve working set selection, we
propose a new model for working set selection in sequential minimal
optimization (SMO) decomposition methods. In this model, it selects B as
working set without reselection. Some properties are given by simple proof, and
experiments demonstrate that the proposed method is in general faster than
existing methods.
</summary>
    <author>
      <name>Zhendong Zhao</name>
    </author>
    <author>
      <name>Lei Yuan</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>Forrest Sheng Bao</name>
    </author>
    <author>
      <name>Shunyi Zhang Yanfei Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICTAI.2007.99</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICTAI.2007.99" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures, it was submitted to IEEE International
  conference of Tools on Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.0585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.0585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3679v1</id>
    <updated>2007-06-25T17:28:57Z</updated>
    <published>2007-06-25T17:28:57Z</published>
    <title>Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers
  Taking Values in R^Q</title>
    <summary>  Bounds on the risk play a crucial role in statistical learning theory. They
usually involve as capacity measure of the model studied the VC dimension or
one of its extensions. In classification, such "VC dimensions" exist for models
taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations
appropriate for the missing case, the one of models with values in R^Q. This
provides us with a new guaranteed risk for M-SVMs which appears superior to the
existing one.
</summary>
    <author>
      <name>Yann Guermeur</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ASMDA 2007 (2007) 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.3679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0509v1</id>
    <updated>2007-09-04T19:36:22Z</updated>
    <published>2007-09-04T19:36:22Z</published>
    <title>Filtering Additive Measurement Noise with Maximum Entropy in the Mean</title>
    <summary>  The purpose of this note is to show how the method of maximum entropy in the
mean (MEM) may be used to improve parametric estimation when the measurements
are corrupted by large level of noise. The method is developed in the context
on a concrete example: that of estimation of the parameter in an exponential
distribution. We compare the performance of our method with the bayesian and
maximum likelihood approaches.
</summary>
    <author>
      <name>Henryk Gzyl</name>
    </author>
    <author>
      <name>Enrique ter Horst</name>
    </author>
    <link href="http://arxiv.org/abs/0709.0509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0485v2</id>
    <updated>2008-06-27T18:45:01Z</updated>
    <published>2007-10-02T10:08:41Z</published>
    <title>Prediction with expert advice for the Brier game</title>
    <summary>  We show that the Brier game of prediction is mixable and find the optimal
learning rate and substitution function for it. The resulting prediction
algorithm is applied to predict results of football and tennis matches. The
theoretical performance guarantee turns out to be rather tight on these data
sets, especially in the case of the more extensive tennis data.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <author>
      <name>Fedor Zhdanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 22 figures, 2 tables. The conference version (8 pages) is
  published in the ICML 2008 Proceedings</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 10 (2009), 2413 - 2440</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3594v1</id>
    <updated>2007-11-22T15:05:35Z</updated>
    <published>2007-11-22T15:05:35Z</published>
    <title>Clustering with Transitive Distance and K-Means Duality</title>
    <summary>  Recent spectral clustering methods are a propular and powerful technique for
data clustering. These methods need to solve the eigenproblem whose
computational complexity is $O(n^3)$, where $n$ is the number of data samples.
In this paper, a non-eigenproblem based clustering method is proposed to deal
with the clustering problem. Its performance is comparable to the spectral
clustering algorithms but it is more efficient with computational complexity
$O(n^2)$. We show that with a transitive distance and an observed property,
called K-means duality, our algorithm can be used to handle data sets with
complex cluster shapes, multi-scale clusters, and noise. Moreover, no
parameters except the number of clusters need to be set in our algorithm.
</summary>
    <author>
      <name>Chunjing Xu</name>
    </author>
    <author>
      <name>Jianzhuang Liu</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.3594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.4452v1</id>
    <updated>2007-11-28T12:05:47Z</updated>
    <published>2007-11-28T12:05:47Z</published>
    <title>Covariance and PCA for Categorical Variables</title>
    <summary>  Covariances from categorical variables are defined using a regular simplex
expression for categories. The method follows the variance definition by Gini,
and it gives the covariance as a solution of simultaneous equations. The
calculated results give reasonable values for test data. A method of principal
component analysis (RS-PCA) is also proposed using regular simplex expressions,
which allows easy interpretation of the principal components. The proposed
methods apply to variable selection problem of categorical data USCensus1990
data. The proposed methods give appropriate criterion for the variable
selection problem of categorical
</summary>
    <author>
      <name>Hirotaka Niitsuma</name>
    </author>
    <author>
      <name>Takashi Okada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.4452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.4452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.3402v1</id>
    <updated>2007-12-20T13:06:50Z</updated>
    <published>2007-12-20T13:06:50Z</published>
    <title>Graph kernels between point clouds</title>
    <summary>  Point clouds are sets of points in two or three dimensions. Most kernel
methods for learning on sets of points have not yet dealt with the specific
geometrical invariances and practical constraints associated with point clouds
in computer vision and graphics. In this paper, we present extensions of graph
kernels for point clouds, which allow to use kernel methods for such ob jects
as shapes, line drawings, or any three-dimensional point clouds. In order to
design rich and numerically efficient kernels with as few free parameters as
possible, we use kernels between covariance matrices and their factorizations
on graphical models. We derive polynomial time dynamic programming recursions
and present applications to recognition of handwritten digits and Chinese
characters from few training examples.
</summary>
    <author>
      <name>Francis Bach</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">WILLOW Project - Inria/Ens</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0712.3402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.1002v1</id>
    <updated>2008-02-07T15:18:27Z</updated>
    <published>2008-02-07T15:18:27Z</published>
    <title>New Estimation Procedures for PLS Path Modelling</title>
    <summary>  Given R groups of numerical variables X1, ... XR, we assume that each group
is the result of one underlying latent variable, and that all latent variables
are bound together through a linear equation system. Moreover, we assume that
some explanatory latent variables may interact pairwise in one or more
equations. We basically consider PLS Path Modelling's algorithm to estimate
both latent variables and the model's coefficients. New "external" estimation
schemes are proposed that draw latent variables towards strong group structures
in a more flexible way. New "internal" estimation schemes are proposed to
enable PLSPM to make good use of variable group complementarity and to deal
with interactions. Application examples are given.
</summary>
    <author>
      <name>Xavier Bry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3M</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0802.1002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.1002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0188v2</id>
    <updated>2009-08-04T11:48:14Z</updated>
    <published>2008-04-01T14:55:33Z</published>
    <title>Support Vector Machine Classification with Indefinite Kernels</title>
    <summary>  We propose a method for support vector machine classification using
indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex
loss function, our algorithm simultaneously computes support vectors and a
proxy kernel matrix used in forming the loss. This can be interpreted as a
penalized kernel learning problem where indefinite kernel matrices are treated
as a noisy observations of a true Mercer kernel. Our formulation keeps the
problem convex and relatively large problems can be solved efficiently using
the projected gradient or analytic center cutting plane methods. We compare the
performance of our technique with other methods on several classic data sets.
</summary>
    <author>
      <name>Ronny Luss</name>
    </author>
    <author>
      <name>Alexandre d'Aspremont</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final journal version. A few typos fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.0188v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0188v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0924v2</id>
    <updated>2009-07-29T04:25:24Z</updated>
    <published>2008-04-06T18:14:34Z</published>
    <title>A Unified Semi-Supervised Dimensionality Reduction Framework for
  Manifold Learning</title>
    <summary>  We present a general framework of semi-supervised dimensionality reduction
for manifold learning which naturally generalizes existing supervised and
unsupervised learning frameworks which apply the spectral decomposition.
Algorithms derived under our framework are able to employ both labeled and
unlabeled examples and are able to handle complex problems where data form
separate clusters of manifolds. Our framework offers simple views, explains
relationships among existing frameworks and provides further extensions which
can improve existing algorithms. Furthermore, a new semi-supervised
kernelization framework called ``KPCA trick'' is proposed to handle non-linear
problems.
</summary>
    <author>
      <name>Ratthachat Chatpatanasiri</name>
    </author>
    <author>
      <name>Boonserm Kijsirikul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.0924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4451v2</id>
    <updated>2019-09-07T00:29:28Z</updated>
    <published>2008-04-28T17:14:53Z</published>
    <title>Dependence Structure Estimation via Copula</title>
    <summary>  Dependence strucuture estimation is one of the important problems in machine
learning domain and has many applications in different scientific areas. In
this paper, a theoretical framework for such estimation based on copula and
copula entropy -- the probabilistic theory of representation and measurement of
statistical dependence, is proposed. Graphical models are considered as a
special case of the copula framework. A method of the framework for estimating
maximum spanning copula is proposed. Due to copula, the method is irrelevant to
the properties of individual variables, insensitive to outlier and able to deal
with non-Gaussianity. Experiments on both simulated data and real dataset
demonstrated the effectiveness of the proposed method.
</summary>
    <author>
      <name>Jian Ma</name>
    </author>
    <author>
      <name>Zengqi Sun</name>
    </author>
    <link href="http://arxiv.org/abs/0804.4451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4682v1</id>
    <updated>2008-04-29T19:25:07Z</updated>
    <published>2008-04-29T19:25:07Z</published>
    <title>Introduction to Relational Networks for Classification</title>
    <summary>  The use of computational intelligence techniques for classification has been
used in numerous applications. This paper compares the use of a Multi Layer
Perceptron Neural Network and a new Relational Network on classifying the HIV
status of women at ante-natal clinics. The paper discusses the architecture of
the relational network and its merits compared to a neural network and most
other computational intelligence classifiers. Results gathered from the study
indicate comparable classification accuracies as well as revealed relationships
between data features in the classification data. Much higher classification
accuracies are recommended for future research in the area of HIV
classification as well as missing data estimation.
</summary>
    <author>
      <name>Vukosi Marivate</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0149v1</id>
    <updated>2008-05-01T20:25:27Z</updated>
    <published>2008-05-01T20:25:27Z</published>
    <title>On Recovery of Sparse Signals via $\ell_1$ Minimization</title>
    <summary>  This article considers constrained $\ell_1$ minimization methods for the
recovery of high dimensional sparse signals in three settings: noiseless,
bounded error and Gaussian noise. A unified and elementary treatment is given
in these noise settings for two $\ell_1$ minimization methods: the Dantzig
selector and $\ell_1$ minimization with an $\ell_2$ constraint. The results of
this paper improve the existing results in the literature by weakening the
conditions and tightening the error bounds. The improvement on the conditions
shows that signals with larger support can be recovered accurately. This paper
also establishes connections between restricted isometry property and the
mutual incoherence property. Some results of Candes, Romberg and Tao (2006) and
Donoho, Elad, and Temlyakov (2006) are extended.
</summary>
    <author>
      <name>T. Tony Cai</name>
    </author>
    <author>
      <name>Guangwu Xu</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/0805.0149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2752v1</id>
    <updated>2008-05-18T20:07:22Z</updated>
    <published>2008-05-18T20:07:22Z</published>
    <title>The Margitron: A Generalised Perceptron with Margin</title>
    <summary>  We identify the classical Perceptron algorithm with margin as a member of a
broader family of large margin classifiers which we collectively call the
Margitron. The Margitron, (despite its) sharing the same update rule with the
Perceptron, is shown in an incremental setting to converge in a finite number
of updates to solutions possessing any desirable fraction of the maximum
margin. Experiments comparing the Margitron with decomposition SVMs on tasks
involving linear kernels and 2-norm soft margin are also reported.
</summary>
    <author>
      <name>Constantinos Panagiotakopoulos</name>
    </author>
    <author>
      <name>Petroula Tsampouka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.2752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4210v1</id>
    <updated>2008-06-25T23:18:44Z</updated>
    <published>2008-06-25T23:18:44Z</published>
    <title>Agnostically Learning Juntas from Random Walks</title>
    <summary>  We prove that the class of functions g:{-1,+1}^n -&gt; {-1,+1} that only depend
on an unknown subset of k&lt;&lt;n variables (so-called k-juntas) is agnostically
learnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k},
and log(1/delta). In other words, there is an algorithm with the claimed
running time that, given epsilon, delta &gt; 0 and access to a random walk on
{-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -&gt; {-1,+1}, finds with
probability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f,
where opt(f) denotes the distance of a closest k-junta to f.
</summary>
    <author>
      <name>Jan Arpe</name>
    </author>
    <author>
      <name>Elchanan Mossel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.4210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4391v1</id>
    <updated>2008-06-26T20:21:06Z</updated>
    <published>2008-06-26T20:21:06Z</published>
    <title>Prediction with Expert Advice in Games with Unbounded One-Step Gains</title>
    <summary>  The games of prediction with expert advice are considered in this paper. We
present some modification of Kalai and Vempala algorithm of following the
perturbed leader for the case of unrestrictedly large one-step gains. We show
that in general case the cumulative gain of any probabilistic prediction
algorithm can be much worse than the gain of some expert of the pool.
Nevertheless, we give the lower bound for this cumulative gain in general case
and construct a universal algorithm which has the optimal performance; we also
prove that in case when one-step gains of experts of the pool have ``limited
deviations'' the performance of our algorithm is close to the performance of
the best expert.
</summary>
    <author>
      <name>Vladimir V. V'yugin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.4391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4484v2</id>
    <updated>2009-06-25T20:07:47Z</updated>
    <published>2008-06-27T10:49:33Z</published>
    <title>On empirical meaning of randomness with respect to a real parameter</title>
    <summary>  We study the empirical meaning of randomness with respect to a family of
probability distributions $P_\theta$, where $\theta$ is a real parameter, using
algorithmic randomness theory. In the case when for a computable probability
distribution $P_\theta$ an effectively strongly consistent estimate exists, we
show that the Levin's a priory semicomputable semimeasure of the set of all
$P_\theta$-random sequences is positive if and only if the parameter $\theta$
is a computable real number. The different methods for generating
``meaningful'' $P_\theta$-random sequences with noncomputable $\theta$ are
discussed.
</summary>
    <author>
      <name>Vladimir V'yugin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS 4649, pp. 387-396, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.4484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1270v1</id>
    <updated>2008-09-08T04:18:17Z</updated>
    <published>2008-09-08T04:18:17Z</published>
    <title>Predictive Hypothesis Identification</title>
    <summary>  While statistics focusses on hypothesis testing and on estimating (properties
of) the true sampling distribution, in machine learning the performance of
learning algorithms on future data is the primary issue. In this paper we
bridge the gap with a general principle (PHI) that identifies hypotheses with
best predictive performance. This includes predictive point and interval
estimation, simple and composite hypothesis testing, (mixture) model selection,
and others as special cases. For concrete instantiations we will recover
well-known methods, variations thereof, and new ones. PHI nicely justifies,
reconciles, and blends (a reparametrization invariant variation of) MAP, ML,
MDL, and moment estimation. One particular feature of PHI is that it can
genuinely deal with nested hypotheses.
</summary>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2792v3</id>
    <updated>2009-06-24T17:45:11Z</updated>
    <published>2008-09-16T20:05:00Z</published>
    <title>Predicting Abnormal Returns From News Using Text Classification</title>
    <summary>  We show how text from news articles can be used to predict intraday price
movements of financial assets using support vector machines. Multiple kernel
learning is used to combine equity returns with text as predictive features to
increase classification performance and we develop an analytic center cutting
plane method to solve the kernel learning problem efficiently. We observe that
while the direction of returns is not predictable using either text or returns,
their size is, with text features producing significantly better performance
than historical returns alone.
</summary>
    <author>
      <name>Ronny Luss</name>
    </author>
    <author>
      <name>Alexandre d'Aspremont</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Larger data sets, results on time of day effect, and use of delta
  hedged covered call options to trade on daily predictions</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2792v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2792v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4086v2</id>
    <updated>2011-01-08T03:16:39Z</updated>
    <published>2008-09-24T05:34:56Z</published>
    <title>Learning Hidden Markov Models using Non-Negative Matrix Factorization</title>
    <summary>  The Baum-Welsh algorithm together with its derivatives and variations has
been the main technique for learning Hidden Markov Models (HMM) from
observational data. We present an HMM learning algorithm based on the
non-negative matrix factorization (NMF) of higher order Markovian statistics
that is structurally different from the Baum-Welsh and its associated
approaches. The described algorithm supports estimation of the number of
recurrent states of an HMM and iterates the non-negative matrix factorization
(NMF) algorithm to improve the learned HMM parameters. Numerical examples are
provided as well.
</summary>
    <author>
      <name>George Cybenko</name>
    </author>
    <author>
      <name>Valentino Crespi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Information Theory in September
  2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.4086v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4086v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4632v1</id>
    <updated>2008-09-26T13:47:36Z</updated>
    <published>2008-09-26T13:47:36Z</published>
    <title>Surrogate Learning - An Approach for Semi-Supervised Classification</title>
    <summary>  We consider the task of learning a classifier from the feature space
$\mathcal{X}$ to the set of classes $\mathcal{Y} = \{0, 1\}$, when the features
can be partitioned into class-conditionally independent feature sets
$\mathcal{X}_1$ and $\mathcal{X}_2$. We show the surprising fact that the
class-conditional independence can be used to represent the original learning
task in terms of 1) learning a classifier from $\mathcal{X}_2$ to
$\mathcal{X}_1$ and 2) learning the class-conditional distribution of the
feature set $\mathcal{X}_1$. This fact can be exploited for semi-supervised
learning because the former task can be accomplished purely from unlabeled
samples. We present experimental evaluation of the idea in two real world
applications.
</summary>
    <author>
      <name>Sriharsha Veeramachaneni</name>
    </author>
    <author>
      <name>Ravikumar Kondadadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.4632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3525v1</id>
    <updated>2008-10-20T11:09:15Z</updated>
    <published>2008-10-20T11:09:15Z</published>
    <title>The use of entropy to measure structural diversity</title>
    <summary>  In this paper entropy based methods are compared and used to measure
structural diversity of an ensemble of 21 classifiers. This measure is mostly
applied in ecology, whereby species counts are used as a measure of diversity.
The measures used were Shannon entropy, Simpsons and the Berger Parker
diversity indexes. As the diversity indexes increased so did the accuracy of
the ensemble. An ensemble dominated by classifiers with the same structure
produced poor accuracy. Uncertainty rule from information theory was also used
to further define diversity. Genetic algorithms were used to find the optimal
ensemble by using the diversity indices as the cost function. The method of
voting was used to aggregate the decisions.
</summary>
    <author>
      <name>L. Masisi</name>
    </author>
    <author>
      <name>V. Nelwamondo</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1357v1</id>
    <updated>2008-12-07T15:22:27Z</updated>
    <published>2008-12-07T15:22:27Z</published>
    <title>A Novel Clustering Algorithm Based on Quantum Random Walk</title>
    <summary>  The enormous successes have been made by quantum algorithms during the last
decade. In this paper, we combine the quantum random walk (QRW) with the
problem of data clustering, and develop two clustering algorithms based on the
one dimensional QRW. Then, the probability distributions on the positions
induced by QRW in these algorithms are investigated, which also indicates the
possibility of obtaining better results. Consequently, the experimental results
have demonstrated that data points in datasets are clustered reasonably and
efficiently, and the clustering algorithms are of fast rates of convergence.
Moreover, the comparison with other algorithms also provides an indication of
the effectiveness of the proposed approach.
</summary>
    <author>
      <name>Qiang Li</name>
    </author>
    <author>
      <name>Yan He</name>
    </author>
    <author>
      <name>Jing-ping Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.1357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1869v1</id>
    <updated>2008-12-10T09:00:40Z</updated>
    <published>2008-12-10T09:00:40Z</published>
    <title>Convex Sparse Matrix Factorizations</title>
    <summary>  We present a convex formulation of dictionary learning for sparse signal
decomposition. Convexity is obtained by replacing the usual explicit upper
bound on the dictionary size by a convex rank-reducing term similar to the
trace norm. In particular, our formulation introduces an explicit trade-off
between size and sparsity of the decomposition of rectangular matrices. Using a
large set of synthetic examples, we compare the estimation abilities of the
convex and non-convex approaches, showing that while the convex formulation has
a single local minimum, this may lead in some cases to performance which is
inferior to the local minima of the non-convex formulation.
</summary>
    <author>
      <name>Francis Bach</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Mairal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Jean Ponce</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0812.1869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3147v1</id>
    <updated>2008-12-16T20:58:24Z</updated>
    <published>2008-12-16T20:58:24Z</published>
    <title>Comparison of Binary Classification Based on Signed Distance Functions
  with Support Vector Machines</title>
    <summary>  We investigate the performance of a simple signed distance function (SDF)
based method by direct comparison with standard SVM packages, as well as
K-nearest neighbor and RBFN methods. We present experimental results comparing
the SDF approach with other classifiers on both synthetic geometric problems
and five benchmark clinical microarray data sets. On both geometric problems
and microarray data sets, the non-optimized SDF based classifiers perform just
as well or slightly better than well-developed, standard SVM methods. These
results demonstrate the potential accuracy of SDF-based methods on some types
of problems.
</summary>
    <author>
      <name>Erik M. Boczko</name>
    </author>
    <author>
      <name>Todd Young</name>
    </author>
    <author>
      <name>Minhui Zie</name>
    </author>
    <author>
      <name>Di Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures. Presented at the Ohio Collaborative Conference on
  Bioinformatics (OCCBIO), June 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.3147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.4952v4</id>
    <updated>2009-05-20T17:40:23Z</updated>
    <published>2008-12-29T18:29:08Z</published>
    <title>Importance Weighted Active Learning</title>
    <summary>  We present a practical and statistically consistent scheme for actively
learning binary classifiers under general loss functions. Our algorithm uses
importance weighting to correct sampling bias, and by controlling the variance,
we are able to give rigorous label complexity bounds for the learning process.
Experiments on passively labeled data show that this approach reduces the label
complexity required to achieve good predictive performance on many learning
problems.
</summary>
    <author>
      <name>Alina Beygelzimer</name>
    </author>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <link href="http://arxiv.org/abs/0812.4952v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.4952v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.2376v1</id>
    <updated>2009-01-16T01:00:39Z</updated>
    <published>2009-01-16T01:00:39Z</published>
    <title>A Limit Theorem in Singular Regression Problem</title>
    <summary>  In statistical problems, a set of parameterized probability distributions is
used to estimate the true probability distribution. If Fisher information
matrix at the true distribution is singular, then it has been left unknown what
we can estimate about the true distribution from random samples. In this paper,
we study a singular regression problem and prove a limit theorem which shows
the relation between the singular regression problem and two birational
invariants, a real log canonical threshold and a singular fluctuation. The
obtained theorem has an important application to statistics, because it enables
us to estimate the generalization error from the training error without any
knowledge of the true probability distribution.
</summary>
    <author>
      <name>Sumio Watanabe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.2376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.2376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.3373v1</id>
    <updated>2009-02-19T13:47:53Z</updated>
    <published>2009-02-19T13:47:53Z</published>
    <title>Learning rules from multisource data for cardiac monitoring</title>
    <summary>  This paper formalises the concept of learning symbolic rules from multisource
data in a cardiac monitoring context. Our sources, electrocardiograms and
arterial blood pressure measures, describe cardiac behaviours from different
viewpoints. To learn interpretable rules, we use an Inductive Logic Programming
(ILP) method. We develop an original strategy to cope with the dimensionality
issues caused by using this ILP technique on a rich multisource language. The
results show that our method greatly improves the feasibility and the
efficiency of the process while staying accurate. They also confirm the
benefits of using multiple sources to improve the diagnosis of cardiac
arrhythmias.
</summary>
    <author>
      <name>Marie-Odile Cordier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Irisa</arxiv:affiliation>
    </author>
    <author>
      <name>Elisa Fromont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>René Quiniou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Irisa</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0902.3373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.3373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4228v1</id>
    <updated>2009-02-24T20:38:32Z</updated>
    <published>2009-02-24T20:38:32Z</published>
    <title>Multiplicative updates For Non-Negative Kernel SVM</title>
    <summary>  We present multiplicative updates for solving hard and soft margin support
vector machines (SVM) with non-negative kernels. They follow as a natural
extension of the updates for non-negative matrix factorization. No additional
param- eter setting, such as choosing learning, rate is required. Ex- periments
demonstrate rapid convergence to good classifiers. We analyze the rates of
asymptotic convergence of the up- dates and establish tight bounds. We test the
performance on several datasets using various non-negative kernels and report
equivalent generalization errors to that of a standard SVM.
</summary>
    <author>
      <name>Vamsi K. Potluru</name>
    </author>
    <author>
      <name>Sergey M. Plis</name>
    </author>
    <author>
      <name>Morten Morup</name>
    </author>
    <author>
      <name>Vince D. Calhoun</name>
    </author>
    <author>
      <name>Terran Lane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.4228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.0064v2</id>
    <updated>2009-04-19T04:18:30Z</updated>
    <published>2009-02-28T11:17:12Z</published>
    <title>Manipulation Robustness of Collaborative Filtering Systems</title>
    <summary>  A collaborative filtering system recommends to users products that similar
users like. Collaborative filtering systems influence purchase decisions, and
hence have become targets of manipulation by unscrupulous vendors. We provide
theoretical and empirical results demonstrating that while common nearest
neighbor algorithms, which are widely used in commercial systems, can be highly
susceptible to manipulation, two classes of collaborative filtering algorithms
which we refer to as linear and asymptotically linear are relatively robust.
These results provide guidance for the design of future collaborative filtering
systems.
</summary>
    <author>
      <name>Xiang Yan</name>
    </author>
    <author>
      <name>Benjamin Van Roy</name>
    </author>
    <link href="http://arxiv.org/abs/0903.0064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.0064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2870v2</id>
    <updated>2009-06-24T14:10:45Z</updated>
    <published>2009-03-16T22:52:06Z</published>
    <title>On $p$-adic Classification</title>
    <summary>  A $p$-adic modification of the split-LBG classification method is presented
in which first clusterings and then cluster centers are computed which locally
minimise an energy function. The outcome for a fixed dataset is independent of
the prime number $p$ with finitely many exceptions. The methods are applied to
the construction of $p$-adic classifiers in the context of learning.
</summary>
    <author>
      <name>Patrick Erik Bradley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1134/S2070046609040013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1134/S2070046609040013" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures, 1 table; added reference, corrected typos, minor
  content changes</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">p-Adic Numbers, Ultrametric Analysis, and Applications, Vol. 1,
  No. 4 (2009), 271-285</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.2870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2972v3</id>
    <updated>2009-05-20T18:44:07Z</updated>
    <published>2009-03-17T14:24:13Z</published>
    <title>Optimistic Simulated Exploration as an Incentive for Real Exploration</title>
    <summary>  Many reinforcement learning exploration techniques are overly optimistic and
try to explore every state. Such exploration is impossible in environments with
the unlimited number of states. I propose to use simulated exploration with an
optimistic model to discover promising paths for real exploration. This reduces
the needs for the real exploration.
</summary>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted, noted that the initial path was 217 steps long</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">POSTER 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.2972v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2972v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4217v2</id>
    <updated>2009-06-03T21:19:34Z</updated>
    <published>2009-03-25T00:28:44Z</published>
    <title>Conditional Probability Tree Estimation Analysis and Algorithms</title>
    <summary>  We consider the problem of estimating the conditional probability of a label
in time $O(\log n)$, where $n$ is the number of possible labels. We analyze a
natural reduction of this problem to a set of binary regression problems
organized in a tree structure, proving a regret bound that scales with the
depth of the tree. Motivated by this analysis, we propose the first online
algorithm which provably constructs a logarithmic depth tree on the set of
labels to solve this problem. We test the algorithm empirically, showing that
it works succesfully on a dataset with roughly $10^6$ labels.
</summary>
    <author>
      <name>Alina Beygelzimer</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Yuri Lifshits</name>
    </author>
    <author>
      <name>Gregory Sorkin</name>
    </author>
    <author>
      <name>Alex Strehl</name>
    </author>
    <link href="http://arxiv.org/abs/0903.4217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.0648v1</id>
    <updated>2009-04-03T20:30:24Z</updated>
    <published>2009-04-03T20:30:24Z</published>
    <title>Evolvability need not imply learnability</title>
    <summary>  We show that Boolean functions expressible as monotone disjunctive normal
forms are PAC-evolvable under a uniform distribution on the Boolean cube if the
hypothesis size is allowed to remain fixed. We further show that this result is
insufficient to prove the PAC-learnability of monotone Boolean functions,
thereby demonstrating a counter-example to a recent claim to the contrary. We
further discuss scenarios wherein evolvability and learnability will coincide
as well as scenarios under which they differ. The implications of the latter
case on the prospects of learning in complex hypothesis spaces is briefly
examined.
</summary>
    <author>
      <name>Nisheeth Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/0904.0648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.0648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.0814v1</id>
    <updated>2009-04-05T20:08:44Z</updated>
    <published>2009-04-05T20:08:44Z</published>
    <title>Stability Analysis and Learning Bounds for Transductive Regression
  Algorithms</title>
    <summary>  This paper uses the notion of algorithmic stability to derive novel
generalization bounds for several families of transductive regression
algorithms, both by using convexity and closed-form solutions. Our analysis
helps compare the stability of these algorithms. It also shows that a number of
widely used transductive regression algorithms are in fact unstable. Finally,
it reports the results of experiments with local transductive regression
demonstrating the benefit of our stability bounds for model selection, for one
of the algorithms, in particular for determining the radius of the local
neighborhood used by the algorithm.
</summary>
    <author>
      <name>Corinna Cortes</name>
    </author>
    <author>
      <name>Mehryar Mohri</name>
    </author>
    <author>
      <name>Dmitry Pechyony</name>
    </author>
    <author>
      <name>Ashish Rastogi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.0814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.0814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3667v1</id>
    <updated>2009-04-23T11:48:38Z</updated>
    <published>2009-04-23T11:48:38Z</published>
    <title>Considerations upon the Machine Learning Technologies</title>
    <summary>  Artificial intelligence offers superior techniques and methods by which
problems from diverse domains may find an optimal solution. The Machine
Learning technologies refer to the domain of artificial intelligence aiming to
develop the techniques allowing the computers to "learn". Some systems based on
Machine Learning technologies tend to eliminate the necessity of the human
intelligence while the others adopt a man-machine collaborative approach.
</summary>
    <author>
      <name>Alin Munteanu</name>
    </author>
    <author>
      <name>Cristina Ofelia Sofran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,exposed on 1st "European Conference on Computer Sciences &amp;
  Applications" - XA2006, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 133-138</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.3667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2347v1</id>
    <updated>2009-05-14T14:59:15Z</updated>
    <published>2009-05-14T14:59:15Z</published>
    <title>Combining Supervised and Unsupervised Learning for GIS Classification</title>
    <summary>  This paper presents a new hybrid learning algorithm for unsupervised
classification tasks. We combined Fuzzy c-means learning algorithm and a
supervised version of Minimerror to develop a hybrid incremental strategy
allowing unsupervised classifications. We applied this new approach to a
real-world database in order to know if the information contained in unlabeled
features of a Geographic Information System (GIS), allows to well classify it.
Finally, we compared our results to a classical supervised classification
obtained by a multilayer perceptron.
</summary>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Laurent Bougrain</name>
    </author>
    <author>
      <name>Frdéric Alexandre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.2347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2997v1</id>
    <updated>2009-05-18T23:21:35Z</updated>
    <published>2009-05-18T23:21:35Z</published>
    <title>Average-Case Active Learning with Costs</title>
    <summary>  We analyze the expected cost of a greedy active learning algorithm. Our
analysis extends previous work to a more general setting in which different
queries have different costs. Moreover, queries may have more than two possible
responses and the distribution over hypotheses may be non uniform. Specific
applications include active learning with label costs, active learning for
multiclass and partial label queries, and batch mode active learning. We also
discuss an approximate version of interest when there are very many queries.
</summary>
    <author>
      <name>Andrew Guillory</name>
    </author>
    <author>
      <name>Jeff Bilmes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.2997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0470v1</id>
    <updated>2009-06-02T11:52:36Z</updated>
    <published>2009-06-02T11:52:36Z</published>
    <title>An optimal linear separator for the Sonar Signals Classification task</title>
    <summary>  The problem of classifying sonar signals from rocks and mines first studied
by Gorman and Sejnowski has become a benchmark against which many learning
algorithms have been tested. We show that both the training set and the test
set of this benchmark are linearly separable, although with different
hyperplanes. Moreover, the complete set of learning and test patterns together,
is also linearly separable. We give the weights that separate these sets, which
may be used to compare results found by other algorithms.
</summary>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Mirta B. Gordon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0872v1</id>
    <updated>2009-06-04T10:25:08Z</updated>
    <published>2009-06-04T10:25:08Z</published>
    <title>Fast Weak Learner Based on Genetic Algorithm</title>
    <summary>  An approach to the acceleration of parametric weak classifier boosting is
proposed. Weak classifier is called parametric if it has fixed number of
parameters and, so, can be represented as a point into multidimensional space.
Genetic algorithm is used instead of exhaustive search to learn parameters of
such classifier. Proposed approach also takes cases when effective algorithm
for learning some of the classifier parameters exists into account. Experiments
confirm that such an approach can dramatically decrease classifier training
time while keeping both training and test errors small.
</summary>
    <author>
      <name>Boris Yangel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, acmsiggraph latex style packed with the latex source in the
  single archive</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.2027v2</id>
    <updated>2012-04-09T17:37:45Z</updated>
    <published>2009-06-11T00:22:58Z</published>
    <title>Matrix Completion from Noisy Entries</title>
    <summary>  Given a matrix M of low-rank, we consider the problem of reconstructing it
from noisy observations of a small, random subset of its entries. The problem
arises in a variety of applications, from collaborative filtering (the `Netflix
problem') to structure-from-motion and positioning. We study a low complexity
algorithm introduced by Keshavan et al.(2009), based on a combination of
spectral techniques and manifold optimization, that we call here OptSpace. We
prove performance guarantees that are order-optimal in a number of
circumstances.
</summary>
    <author>
      <name>Raghunandan H. Keshavan</name>
    </author>
    <author>
      <name>Andrea Montanari</name>
    </author>
    <author>
      <name>Sewoong Oh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.2027v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.2027v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5151v1</id>
    <updated>2009-06-28T17:47:22Z</updated>
    <published>2009-06-28T17:47:22Z</published>
    <title>Unsupervised Search-based Structured Prediction</title>
    <summary>  We describe an adaptation and application of a search-based structured
prediction algorithm "Searn" to unsupervised learning problems. We show that it
is possible to reduce unsupervised learning to supervised learning and
demonstrate a high-quality unsupervised shift-reduce parsing model. We
additionally show a close connection between unsupervised Searn and expectation
maximization. Finally, we demonstrate the efficacy of a semi-supervised
extension. The key idea that enables this is an application of the predict-self
idea for unsupervised learning.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Machine Learning,
  2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.5151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0809v1</id>
    <updated>2009-07-04T22:34:25Z</updated>
    <published>2009-07-04T22:34:25Z</published>
    <title>Learning as Search Optimization: Approximate Large Margin Methods for
  Structured Prediction</title>
    <summary>  Mappings to structured output spaces (strings, trees, partitions, etc.) are
typically learned using extensions of classification algorithms to simple
graphical structures (eg., linear chains) in which search and parameter
estimation can be performed exactly. Unfortunately, in many complex problems,
it is rare that exact search or parameter estimation is tractable. Instead of
learning exact models and searching via heuristic means, we embrace this
difficulty and treat the structured output problem in terms of approximate
search. We present a framework for learning as search optimization, and two
parameter updates with convergence theorems and bounds. Empirical evidence
shows that our integrated approach to learning and decoding can outperform
exact models at smaller computational cost.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <author>
      <name>Daniel Marcu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.0809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1812v1</id>
    <updated>2009-07-10T13:23:37Z</updated>
    <published>2009-07-10T13:23:37Z</published>
    <title>Fast search for Dirichlet process mixture models</title>
    <summary>  Dirichlet process (DP) mixture models provide a flexible Bayesian framework
for density estimation. Unfortunately, their flexibility comes at a cost:
inference in DP mixture models is computationally expensive, even when
conjugate distributions are used. In the common case when one seeks only a
maximum a posteriori assignment of data points to clusters, we show that search
algorithms provide a practical alternative to expensive MCMC and variational
techniques. When a true posterior sample is desired, the solution found by
search can serve as a good initializer for MCMC. Experimental results show that
using these techniques is it possible to apply DP mixture models to very large
data sets.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIStats 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.1812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1815v1</id>
    <updated>2009-07-10T13:25:48Z</updated>
    <published>2009-07-10T13:25:48Z</published>
    <title>Frustratingly Easy Domain Adaptation</title>
    <summary>  We describe an approach to domain adaptation that is appropriate exactly in
the case when one has enough ``target'' data to do slightly better than just
using only ``source'' data. Our approach is incredibly simple, easy to
implement as a preprocessing step (10 lines of Perl!) and outperforms
state-of-the-art approaches on a range of datasets. Moreover, it is trivially
extended to a multi-domain adaptation problem, where one has data from a
variety of different domains.
</summary>
    <author>
      <name>Hal Daumé III</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.1815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0939v1</id>
    <updated>2009-08-06T19:48:20Z</updated>
    <published>2009-08-06T19:48:20Z</published>
    <title>Clustering for Improved Learning in Maze Traversal Problem</title>
    <summary>  The maze traversal problem (finding the shortest distance to the goal from
any position in a maze) has been an interesting challenge in computational
intelligence. Recent work has shown that the cellular simultaneous recurrent
neural network (CSRN) can solve this problem for simple mazes. This thesis
focuses on exploiting relevant information about the maze to improve learning
and decrease the training time for the CSRN to solve mazes. Appropriate
variables are identified to create useful clusters using relevant information.
The CSRN was next modified to allow for an additional external input. With this
additional input, several methods were tested and results show that clustering
the mazes improves the overall learning of the traversal problem for the CSRN.
</summary>
    <author>
      <name>Eddie White</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 15 figures, Undergraduate Honors Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0638v1</id>
    <updated>2009-09-03T12:09:08Z</updated>
    <published>2009-09-03T12:09:08Z</published>
    <title>Median topographic maps for biomedical data sets</title>
    <summary>  Median clustering extends popular neural data analysis methods such as the
self-organizing map or neural gas to general data structures given by a
dissimilarity matrix only. This offers flexible and robust global data
inspection methods which are particularly suited for a variety of data as
occurs in biomedical domains. In this chapter, we give an overview about median
clustering and its properties and extensions, with a particular focus on
efficient implementations adapted to large scale data analysis.
</summary>
    <author>
      <name>Barbara Hammer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander Hasenfuß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-01805-3_6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-01805-3_6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Similarity-Based Clustering, Villmann, Th.; Biehl, M.; Hammer, B.;
  Verleysen, M. (Ed.) (2009) 92-117</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.0638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.2540v1</id>
    <updated>2009-10-14T07:43:03Z</updated>
    <published>2009-10-14T07:43:03Z</published>
    <title>Effectiveness and Limitations of Statistical Spam Filters</title>
    <summary>  In this paper we discuss the techniques involved in the design of the famous
statistical spam filters that include Naive Bayes, Term Frequency-Inverse
Document Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes
Additive Regression Tree. We compare these techniques with each other in terms
of accuracy, recall, precision, etc. Further, we discuss the effectiveness and
limitations of statistical filters in filtering out various types of spam from
legitimate e-mails.
</summary>
    <author>
      <name>M. Tariq Banday</name>
    </author>
    <author>
      <name>Tariq R. Jan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on New Trends in Statistics and
  Optimization, Organized by Department of Statistics, University of Kashmir,
  Srinagar, India, from 20th to 23rd October, 2008</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on New Trends in Statistics and
  Optimization, Organized by Department of Statistics, University of Kashmir,
  Srinagar, India, from 20th to 23rd October, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.2540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.2540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4627v1</id>
    <updated>2009-10-24T07:10:24Z</updated>
    <published>2009-10-24T07:10:24Z</published>
    <title>Self-concordant analysis for logistic regression</title>
    <summary>  Most of the non-asymptotic theoretical work in regression is carried out for
the square loss, where estimators can be obtained through closed-form
expressions. In this paper, we use and extend tools from the convex
optimization literature, namely self-concordant functions, to provide simple
extensions of theoretical results for the square loss to the logistic loss. We
apply the extension techniques to logistic regression with regularization by
the $\ell_2$-norm and regularization by the $\ell_1$-norm, showing that new
results for binary classification through logistic regression can be easily
derived from corresponding results for least-squares regression.
</summary>
    <author>
      <name>Francis Bach</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0910.4627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0054v2</id>
    <updated>2015-05-16T22:45:35Z</updated>
    <published>2009-10-31T02:56:18Z</published>
    <title>Learning Exponential Families in High-Dimensions: Strong Convexity and
  Sparsity</title>
    <summary>  The versatility of exponential families, along with their attendant convexity
properties, make them a popular and effective statistical model. A central
issue is learning these models in high-dimensions, such as when there is some
sparsity pattern of the optimal parameter. This work characterizes a certain
strong convexity property of general exponential families, which allow their
generalization ability to be quantified. In particular, we show how this
property can be used to analyze generic exponential families under L_1
regularization.
</summary>
    <author>
      <name>Sham M. Kakade</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <author>
      <name>Karthik Sridharan</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Errata added. Incorrect claim about cumulants of the Bernoulli
  distribution fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.0054v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0054v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0462v1</id>
    <updated>2009-11-03T00:27:36Z</updated>
    <published>2009-11-03T00:27:36Z</published>
    <title>Strange Bedfellows: Quantum Mechanics and Data Mining</title>
    <summary>  Last year, in 2008, I gave a talk titled {\it Quantum Calisthenics}. This
year I am going to tell you about how the work I described then has spun off
into a most unlikely direction. What I am going to talk about is how one maps
the problem of finding clusters in a given data set into a problem in quantum
mechanics. I will then use the tricks I described to let quantum evolution lets
the clusters come together on their own.
</summary>
    <author>
      <name>Marvin Weinstein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nuclphysbps.2010.02.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nuclphysbps.2010.02.009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, Invited Talk at Light Cone 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.0462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4863v2</id>
    <updated>2011-05-13T01:52:49Z</updated>
    <published>2009-11-25T14:26:54Z</published>
    <title>Statistical exponential families: A digest with flash cards</title>
    <summary>  This document describes concisely the ubiquitous class of exponential family
distributions met in statistics. The first part recalls definitions and
summarizes main properties and duality with Bregman divergences (all proofs are
skipped). The second part lists decompositions and related formula of common
exponential family distributions. We recall the Fisher-Rao-Riemannian
geometries and the dual affine connection information geometries of statistical
manifolds. It is intended to maintain and update this document and catalog by
adding new distribution items.
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <author>
      <name>Vincent Garcia</name>
    </author>
    <link href="http://arxiv.org/abs/0911.4863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.5029v1</id>
    <updated>2009-12-26T16:32:46Z</updated>
    <published>2009-12-26T16:32:46Z</published>
    <title>Complexity of stochastic branch and bound methods for belief tree search
  in Bayesian reinforcement learning</title>
    <summary>  There has been a lot of recent work on Bayesian methods for reinforcement
learning exhibiting near-optimal online performance. The main obstacle facing
such methods is that in most problems of interest, the optimal solution
involves planning in an infinitely large tree. However, it is possible to
obtain stochastic lower and upper bounds on the value of each tree node. This
enables us to use stochastic branch and bound algorithms to search the tree
efficiently. This paper proposes two such algorithms and examines their
complexity in this setting.
</summary>
    <author>
      <name>Christos Dimitrakakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure, ICAART 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.5029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.5029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0879v1</id>
    <updated>2010-01-06T12:40:13Z</updated>
    <published>2010-01-06T12:40:13Z</published>
    <title>Linear Probability Forecasting</title>
    <summary>  Multi-class classification is one of the most important tasks in machine
learning. In this paper we consider two online multi-class classification
problems: classification by a linear model and by a kernelized model. The
quality of predictions is measured by the Brier loss function. We suggest two
computationally efficient algorithms to work with these problems and prove
theoretical guarantees on their losses. We kernelize one of the algorithms and
prove theoretical guarantees on its loss. We perform experiments and compare
our algorithms with logistic regression.
</summary>
    <author>
      <name>Fedor Zhdanov</name>
    </author>
    <author>
      <name>Yuri Kalnishkan</name>
    </author>
    <link href="http://arxiv.org/abs/1001.0879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3478v1</id>
    <updated>2010-01-20T07:30:02Z</updated>
    <published>2010-01-20T07:30:02Z</published>
    <title>Role of Interestingness Measures in CAR Rule Ordering for Associative
  Classifier: An Empirical Approach</title>
    <summary>  Associative Classifier is a novel technique which is the integration of
Association Rule Mining and Classification. The difficult task in building
Associative Classifier model is the selection of relevant rules from a large
number of class association rules (CARs). A very popular method of ordering
rules for selection is based on confidence, support and antecedent size (CSA).
Other methods are based on hybrid orderings in which CSA method is combined
with other measures. In the present work, we study the effect of using
different interestingness measures of Association rules in CAR rule ordering
and selection for associative classifier.
</summary>
    <author>
      <name>S. Kannan</name>
    </author>
    <author>
      <name>R. Bhaskaran</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Vol. 2, Issue 1, January 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0709v1</id>
    <updated>2010-02-03T11:31:24Z</updated>
    <published>2010-02-03T11:31:24Z</published>
    <title>Aggregating Algorithm competing with Banach lattices</title>
    <summary>  The paper deals with on-line regression settings with signals belonging to a
Banach lattice. Our algorithms work in a semi-online setting where all the
inputs are known in advance and outcomes are unknown and given step by step. We
apply the Aggregating Algorithm to construct a prediction method whose
cumulative loss over all the input vectors is comparable with the cumulative
loss of any linear functional on the Banach lattice. As a by-product we get an
algorithm that takes signals from an arbitrary domain. Its cumulative loss is
comparable with the cumulative loss of any predictor function from Besov and
Triebel-Lizorkin spaces. We describe several applications of our setting.
</summary>
    <author>
      <name>Fedor Zhdanov</name>
    </author>
    <author>
      <name>Alexey Chernov</name>
    </author>
    <author>
      <name>Yuri Kalnishkan</name>
    </author>
    <link href="http://arxiv.org/abs/1002.0709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.3345v2</id>
    <updated>2010-05-20T23:39:23Z</updated>
    <published>2010-02-17T18:43:59Z</published>
    <title>Interactive Submodular Set Cover</title>
    <summary>  We introduce a natural generalization of submodular set cover and exact
active learning with a finite hypothesis class (query learning). We call this
new problem interactive submodular set cover. Applications include advertising
in social networks with hidden information. We give an approximation guarantee
for a novel greedy algorithm and give a hardness of approximation result which
matches up to constant factors. We also discuss negative results for simpler
approaches and present encouraging early experimental results.
</summary>
    <author>
      <name>Andrew Guillory</name>
    </author>
    <author>
      <name>Jeff Bilmes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.3345v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3345v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.4862v1</id>
    <updated>2010-02-25T20:31:05Z</updated>
    <published>2010-02-25T20:31:05Z</published>
    <title>Less Regret via Online Conditioning</title>
    <summary>  We analyze and evaluate an online gradient descent algorithm with adaptive
per-coordinate adjustment of learning rates. Our algorithm can be thought of as
an online version of batch gradient descent with a diagonal preconditioner.
This approach leads to regret bounds that are stronger than those of standard
online gradient descent for general online convex optimization problems.
Experimentally, we show that our algorithm is competitive with state-of-the-art
algorithms for large scale machine learning problems.
</summary>
    <author>
      <name>Matthew Streeter</name>
    </author>
    <author>
      <name>H. Brendan McMahan</name>
    </author>
    <link href="http://arxiv.org/abs/1002.4862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0024v1</id>
    <updated>2010-02-26T21:59:02Z</updated>
    <published>2010-02-26T21:59:02Z</published>
    <title>Asymptotic Analysis of Generative Semi-Supervised Learning</title>
    <summary>  Semisupervised learning has emerged as a popular framework for improving
modeling accuracy while controlling labeling cost. Based on an extension of
stochastic composite likelihood we quantify the asymptotic accuracy of
generative semi-supervised learning. In doing so, we complement
distribution-free analysis by providing an alternative framework to measure the
value associated with different labeling policies and resolve the fundamental
question of how much data to label and in what manner. We demonstrate our
approach with both simulation studies and real world experiments using naive
Bayes for text classification and MRFs and CRFs for structured prediction in
NLP.
</summary>
    <author>
      <name>Joshua V Dillon</name>
    </author>
    <author>
      <name>Krishnakumar Balasubramanian</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0470v2</id>
    <updated>2010-07-21T21:19:35Z</updated>
    <published>2010-03-01T22:32:18Z</published>
    <title>Unsupervised Supervised Learning II: Training Margin Based Classifiers
  without Labels</title>
    <summary>  Many popular linear classifiers, such as logistic regression, boosting, or
SVM, are trained by optimizing a margin-based risk function. Traditionally,
these risk functions are computed based on a labeled dataset. We develop a
novel technique for estimating such risks using only unlabeled data and the
marginal label distribution. We prove that the proposed risk estimator is
consistent on high-dimensional datasets and demonstrate it on synthetic and
real-world data. In particular, we show how the estimate is used for evaluating
classifiers in transfer learning, and for training classifiers with no labeled
data whatsoever.
</summary>
    <author>
      <name>Krishnakumar Balasubramanian</name>
    </author>
    <author>
      <name>Pinar Donmez</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 43 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1230v1</id>
    <updated>2010-04-08T03:06:24Z</updated>
    <published>2010-04-08T03:06:24Z</published>
    <title>Ontology-supported processing of clinical text using medical knowledge
  integration for multi-label classification of diagnosis coding</title>
    <summary>  This paper discusses the knowledge integration of clinical information
extracted from distributed medical ontology in order to ameliorate a machine
learning-based multi-label coding assignment system. The proposed approach is
implemented using a decision tree based cascade hierarchical technique on the
university hospital data for patients with Coronary Heart Disease (CHD). The
preliminary results obtained show a satisfactory finding.
</summary>
    <author>
      <name>Phanu Waraporn</name>
    </author>
    <author>
      <name>Phayung Meesad</name>
    </author>
    <author>
      <name>Gareth Clayton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS, Vol. 7 No. 3, March 2010,</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1982v1</id>
    <updated>2010-04-09T09:36:28Z</updated>
    <published>2010-04-09T09:36:28Z</published>
    <title>State-Space Dynamics Distance for Clustering Sequential Data</title>
    <summary>  This paper proposes a novel similarity measure for clustering sequential
data. We first construct a common state-space by training a single
probabilistic model with all the sequences in order to get a unified
representation for the dataset. Then, distances are obtained attending to the
transition matrices induced by each sequence in that state-space. This approach
solves some of the usual overfitting and scalability issues of the existing
semi-parametric techniques, that rely on training a model for each sequence.
Empirical studies on both synthetic and real-world datasets illustrate the
advantages of the proposed similarity measure for clustering sequences.
</summary>
    <author>
      <name>Darío García-García</name>
    </author>
    <author>
      <name>Emilio Parrado-Hernández</name>
    </author>
    <author>
      <name>Fernando Díaz-de-María</name>
    </author>
    <link href="http://arxiv.org/abs/1004.1982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3814v1</id>
    <updated>2010-04-21T23:09:06Z</updated>
    <published>2010-04-21T23:09:06Z</published>
    <title>Bregman Distance to L1 Regularized Logistic Regression</title>
    <summary>  In this work we investigate the relationship between Bregman distances and
regularized Logistic Regression model. We present a detailed study of Bregman
Distance minimization, a family of generalized entropy measures associated with
convex functions. We convert the L1-regularized logistic regression into this
more general framework and propose a primal-dual method based algorithm for
learning the parameters. We pose L1-regularized logistic regression into
Bregman distance minimization and then apply non-linear constrained
optimization techniques to estimate the parameters of the logistic model.
</summary>
    <author>
      <name>Mithun Das Gupta</name>
    </author>
    <author>
      <name>Thomas S. Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 images, shorter version published in ICPR 2008 by same
  authors.</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.3814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0047v1</id>
    <updated>2010-05-01T06:06:36Z</updated>
    <published>2010-05-01T06:06:36Z</published>
    <title>A Geometric View of Conjugate Priors</title>
    <summary>  In Bayesian machine learning, conjugate priors are popular, mostly due to
mathematical convenience. In this paper, we show that there are deeper reasons
for choosing a conjugate prior. Specifically, we formulate the conjugate prior
in the form of Bregman divergence and show that it is the inherent geometry of
conjugate priors that makes them appropriate and intuitive. This geometric
interpretation allows one to view the hyperparameters of conjugate priors as
the {\it effective} sample points, thus providing additional intuition. We use
this geometric understanding of conjugate priors to derive the hyperparameters
and expression of the prior used to couple the generative and discriminative
components of a hybrid model for semi-supervised learning.
</summary>
    <author>
      <name>Arvind Agarwal</name>
    </author>
    <author>
      <name>Hal Daume III</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.0047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0125v1</id>
    <updated>2010-05-02T06:40:21Z</updated>
    <published>2010-05-02T06:40:21Z</published>
    <title>Adaptive Bases for Reinforcement Learning</title>
    <summary>  We consider the problem of reinforcement learning using function
approximation, where the approximating basis can change dynamically while
interacting with the environment. A motivation for such an approach is
maximizing the value function fitness to the problem faced. Three errors are
considered: approximation square error, Bellman residual, and projected Bellman
residual. Algorithms under the actor-critic framework are presented, and shown
to converge. The advantage of such an adaptive basis is demonstrated in
simulations.
</summary>
    <author>
      <name>Dotan Di Castro</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1005.0125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5170v1</id>
    <updated>2010-05-25T16:07:25Z</updated>
    <published>2010-05-25T16:07:25Z</published>
    <title>Wirtinger's Calculus in general Hilbert Spaces</title>
    <summary>  The present report, has been inspired by the need of the author and its
colleagues to understand the underlying theory of Wirtinger's Calculus and to
further extend it to include the kernel case. The aim of the present manuscript
is twofold: a) it endeavors to provide a more rigorous presentation of the
related material, focusing on aspects that the author finds more insightful and
b) it extends the notions of Wirtinger's calculus on general Hilbert spaces
(such as Reproducing Hilbert Kernel Spaces).
</summary>
    <author>
      <name>P. Bouboulis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Report completed for the department of Informatics and
  Telecommunications of the University of Athens</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.5170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5462v2</id>
    <updated>2010-06-12T10:40:53Z</updated>
    <published>2010-05-29T15:27:16Z</published>
    <title>On the clustering aspect of nonnegative matrix factorization</title>
    <summary>  This paper provides a theoretical explanation on the clustering aspect of
nonnegative matrix factorization (NMF). We prove that even without imposing
orthogonality nor sparsity constraint on the basis and/or coefficient matrix,
NMF still can give clustering results, thus providing a theoretical support for
many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority
of the standard NMF as a clustering method.
</summary>
    <author>
      <name>Andri Mirzal</name>
    </author>
    <author>
      <name>Masashi Furukawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no figure, to appear in ICEIE 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.5462v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5462v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0475v1</id>
    <updated>2010-06-02T19:41:27Z</updated>
    <published>2010-06-02T19:41:27Z</published>
    <title>Prediction with Advice of Unknown Number of Experts</title>
    <summary>  In the framework of prediction with expert advice, we consider a recently
introduced kind of regret bounds: the bounds that depend on the effective
instead of nominal number of experts. In contrast to the NormalHedge bound,
which mainly depends on the effective number of experts and also weakly depends
on the nominal one, we obtain a bound that does not contain the nominal number
of experts at all. We use the defensive forecasting method and introduce an
application of defensive forecasting to multivalued supermartingales.
</summary>
    <author>
      <name>Alexey Chernov</name>
    </author>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages; draft version</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.0475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1138v3</id>
    <updated>2014-08-12T16:44:00Z</updated>
    <published>2010-06-06T21:05:27Z</published>
    <title>Online Learning via Sequential Complexities</title>
    <summary>  We consider the problem of sequential prediction and provide tools to study
the minimax value of the associated game. Classical statistical learning theory
provides several useful complexity measures to study learning with i.i.d. data.
Our proposed sequential complexities can be seen as extensions of these
measures to the sequential setting. The developed theory is shown to yield
precise learning guarantees for the problem of sequential prediction. In
particular, we show necessary and sufficient conditions for online learnability
in the setting of supervised learning. Several examples show the utility of our
framework: we can establish learnability without having to exhibit an explicit
online learning algorithm.
</summary>
    <author>
      <name>Alexander Rakhlin</name>
    </author>
    <author>
      <name>Karthik Sridharan</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <link href="http://arxiv.org/abs/1006.1138v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1138v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.1282v1</id>
    <updated>2010-07-08T03:58:25Z</updated>
    <published>2010-07-08T03:58:25Z</published>
    <title>A note on sample complexity of learning binary output neural networks
  under fixed input distributions</title>
    <summary>  We show that the learning sample complexity of a sigmoidal neural network
constructed by Sontag (1992) required to achieve a given misclassification
error under a fixed purely atomic distribution can grow arbitrarily fast: for
any prescribed rate of growth there is an input distribution having this rate
as the sample complexity, and the bound is asymptotically tight. The rate can
be superexponential, a non-recursive function, etc. We further observe that
Sontag's ANN is not Glivenko-Cantelli under any input distribution having a
non-atomic part.
</summary>
    <author>
      <name>Vladimir Pestov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SBRN.2010.10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SBRN.2010.10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, latex in IEEE conference proceedings format</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 2010 Eleventh Brazilian Symposium on Neural Networks (S\~ao
  Bernardo do Campo, SP, Brazil, 23-28 October 2010), IEEE Computer Society,
  2010, pp. 7-12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.1282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.2075v1</id>
    <updated>2010-07-13T10:54:14Z</updated>
    <published>2010-07-13T10:54:14Z</published>
    <title>Consistency of Feature Markov Processes</title>
    <summary>  We are studying long term sequence prediction (forecasting). We approach this
by investigating criteria for choosing a compact useful state representation.
The state is supposed to summarize useful information from the history. We want
a method that is asymptotically consistent in the sense it will provably
eventually only choose between alternatives that satisfy an optimality property
related to the used criterion. We extend our work to the case where there is
side information that one can take advantage of and, furthermore, we briefly
discuss the active setting where an agent takes actions to achieve desirable
outcomes.
</summary>
    <author>
      <name>Peter Sunehag</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 LaTeX pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 21st International Conf. on Algorithmic Learning Theory
  (ALT-2010) pages 360-374</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.2075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.2075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.2449v1</id>
    <updated>2010-07-14T22:41:30Z</updated>
    <published>2010-07-14T22:41:30Z</published>
    <title>A Brief Introduction to Temporality and Causality</title>
    <summary>  Causality is a non-obvious concept that is often considered to be related to
temporality. In this paper we present a number of past and present approaches
to the definition of temporality and causality from philosophical, physical,
and computational points of view. We note that time is an important ingredient
in many relationships and phenomena. The topic is then divided into the two
main areas of temporal discovery, which is concerned with finding relations
that are stretched over time, and causal discovery, where a claim is made as to
the causal influence of certain events on others. We present a number of
computational tools used for attempting to automatically discover temporal and
causal relations in data.
</summary>
    <author>
      <name>Kamran Karimi</name>
    </author>
    <link href="http://arxiv.org/abs/1007.2449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.2449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1398v1</id>
    <updated>2010-08-08T11:25:12Z</updated>
    <published>2010-08-08T11:25:12Z</published>
    <title>Semi-Supervised Kernel PCA</title>
    <summary>  We present three generalisations of Kernel Principal Components Analysis
(KPCA) which incorporate knowledge of the class labels of a subset of the data
points. The first, MV-KPCA, penalises within class variances similar to Fisher
discriminant analysis. The second, LSKPCA is a hybrid of least squares
regression and kernel PCA. The final LR-KPCA is an iteratively reweighted
version of the previous which achieves a sigmoid loss function on the labeled
points. We provide a theoretical risk bound as well as illustrative experiments
on real and toy data sets.
</summary>
    <author>
      <name>Christian Walder</name>
    </author>
    <author>
      <name>Ricardo Henao</name>
    </author>
    <author>
      <name>Morten Mørup</name>
    </author>
    <author>
      <name>Lars Kai Hansen</name>
    </author>
    <link href="http://arxiv.org/abs/1008.1398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.4232v1</id>
    <updated>2010-08-25T09:09:29Z</updated>
    <published>2010-08-25T09:09:29Z</published>
    <title>Online Learning in Case of Unbounded Losses Using the Follow Perturbed
  Leader Algorithm</title>
    <summary>  In this paper the sequential prediction problem with expert advice is
considered for the case where losses of experts suffered at each step cannot be
bounded in advance. We present some modification of Kalai and Vempala algorithm
of following the perturbed leader where weights depend on past losses of the
experts. New notions of a volume and a scaled fluctuation of a game are
introduced. We present a probabilistic algorithm protected from unrestrictedly
large one-step losses. This algorithm has the optimal performance in the case
when the scaled fluctuations of one-step losses of experts of the pool tend to
zero.
</summary>
    <author>
      <name>Vladimir V. V'yugin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.4232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.4232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.3702v1</id>
    <updated>2010-09-20T06:35:11Z</updated>
    <published>2010-09-20T06:35:11Z</published>
    <title>Totally Corrective Multiclass Boosting with Binary Weak Learners</title>
    <summary>  In this work, we propose a new optimization framework for multiclass boosting
learning. In the literature, AdaBoost.MO and AdaBoost.ECC are the two
successful multiclass boosting algorithms, which can use binary weak learners.
We explicitly derive these two algorithms' Lagrange dual problems based on
their regularized loss functions. We show that the Lagrange dual formulations
enable us to design totally-corrective multiclass algorithms by using the
primal-dual optimization technique. Experiments on benchmark data sets suggest
that our multiclass boosting can achieve a comparable generalization capability
with state-of-the-art, but the convergence speed is much faster than stage-wise
gradient descent boosting. In other words, the new totally corrective
algorithms can maximize the margin more aggressively.
</summary>
    <author>
      <name>Zhihui Hao</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <author>
      <name>Nick Barnes</name>
    </author>
    <author>
      <name>Bo Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.3702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.3702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.3896v2</id>
    <updated>2012-11-26T06:42:25Z</updated>
    <published>2010-09-20T17:35:35Z</published>
    <title>Optimistic Rates for Learning with a Smooth Loss</title>
    <summary>  We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) for
empirical risk minimization with an H-smooth loss function and a hypothesis
class with Rademacher complexity R_n, where L* is the best risk achievable by
the hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n},
this translates to a learning rate of O(RH/n) in the separable (L*=0) case and
O(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guarantees
for online and stochastic convex optimization with a smooth non-negative
objective.
</summary>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <author>
      <name>Karthik Sridharan</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <link href="http://arxiv.org/abs/1009.3896v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.3896v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.3958v1</id>
    <updated>2010-09-20T21:44:30Z</updated>
    <published>2010-09-20T21:44:30Z</published>
    <title>Approximate Inference and Stochastic Optimal Control</title>
    <summary>  We propose a novel reformulation of the stochastic optimal control problem as
an approximate inference problem, demonstrating, that such a interpretation
leads to new practical methods for the original problem. In particular we
characterise a novel class of iterative solutions to the stochastic optimal
control problem based on a natural relaxation of the exact dual formulation.
These theoretical insights are applied to the Reinforcement Learning problem
where they lead to new model free, off policy methods for discrete and
continuous problems.
</summary>
    <author>
      <name>Konrad Rawlik</name>
    </author>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <author>
      <name>Sethu Vijayakumar</name>
    </author>
    <link href="http://arxiv.org/abs/1009.3958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.3958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5972v1</id>
    <updated>2010-09-29T18:55:02Z</updated>
    <published>2010-09-29T18:55:02Z</published>
    <title>The Attentive Perceptron</title>
    <summary>  We propose a focus of attention mechanism to speed up the Perceptron
algorithm. Focus of attention speeds up the Perceptron algorithm by lowering
the number of features evaluated throughout training and prediction. Whereas
the traditional Perceptron evaluates all the features of each example, the
Attentive Perceptron evaluates less features for easy to classify examples,
thereby achieving significant speedups and small losses in prediction accuracy.
Focus of attention allows the Attentive Perceptron to stop the evaluation of
features at any interim point and filter the example. This creates an attentive
filter which concentrates computation at examples that are hard to classify,
and quickly filters examples that are easy to classify.
</summary>
    <author>
      <name>Raphael Pelossof</name>
    </author>
    <author>
      <name>Zhiliang Ying</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to New York Academy of Sciences Machine Learning symposium
  2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.5972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.4207v2</id>
    <updated>2010-11-14T17:19:42Z</updated>
    <published>2010-10-20T14:02:21Z</published>
    <title>Convex Analysis and Optimization with Submodular Functions: a Tutorial</title>
    <summary>  Set-functions appear in many areas of computer science and applied
mathematics, such as machine learning, computer vision, operations research or
electrical networks. Among these set-functions, submodular functions play an
important role, similar to convex functions on vector spaces. In this tutorial,
the theory of submodular functions is presented, in a self-contained way, with
all results shown from first principles. A good knowledge of convex analysis is
assumed.
</summary>
    <author>
      <name>Francis Bach</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, LIENS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1010.4207v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.4207v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.0350v1</id>
    <updated>2010-11-01T15:40:31Z</updated>
    <published>2010-11-01T15:40:31Z</published>
    <title>Developing courses with HoloRena, a framework for scenario- and game
  based e-learning environments</title>
    <summary>  However utilizing rich, interactive solutions can make learning more
effective and attractive, scenario- and game-based educational resources on the
web are not widely used. Creating these applications is a complex, expensive
and challenging process. Development frameworks and authoring tools hardly
support reusable components, teamwork and learning management
system-independent courseware architecture. In this article we initiate the
concept of a low-level, thick-client solution addressing these problems. With
some example applications we try to demonstrate, how a framework, based on this
concept can be useful for developing scenario- and game-based e-learning
environments.
</summary>
    <author>
      <name>Laszlo Juracz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Software Engineering &amp; Applications
  (IJSEA), October 2010, Volume 1, Number 4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1011.0350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.0350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.1936v1</id>
    <updated>2010-11-08T22:41:14Z</updated>
    <published>2010-11-08T22:41:14Z</published>
    <title>Blackwell Approachability and Low-Regret Learning are Equivalent</title>
    <summary>  We consider the celebrated Blackwell Approachability Theorem for two-player
games with vector payoffs. We show that Blackwell's result is equivalent, via
efficient reductions, to the existence of "no-regret" algorithms for Online
Linear Optimization. Indeed, we show that any algorithm for one such problem
can be efficiently converted into an algorithm for the other. We provide a
useful application of this reduction: the first efficient algorithm for
calibrated forecasting.
</summary>
    <author>
      <name>Jacob Abernethy</name>
    </author>
    <author>
      <name>Peter L. Bartlett</name>
    </author>
    <author>
      <name>Elad Hazan</name>
    </author>
    <link href="http://arxiv.org/abs/1011.1936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.1936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.0498v1</id>
    <updated>2010-12-02T17:04:19Z</updated>
    <published>2010-12-02T17:04:19Z</published>
    <title>Estimating Probabilities in Recommendation Systems</title>
    <summary>  Recommendation systems are emerging as an important business application with
significant economic impact. Currently popular systems include Amazon's book
recommendations, Netflix's movie recommendations, and Pandora's music
recommendations. In this paper we address the problem of estimating
probabilities associated with recommendation system data using non-parametric
kernel smoothing. In our estimation we interpret missing items as randomly
censored observations and obtain efficient computation schemes using
combinatorial properties of generating functions. We demonstrate our approach
with several case studies involving real world movie recommendation data. The
results are comparable with state-of-the-art techniques while also providing
probabilistic preference estimates outside the scope of traditional recommender
systems.
</summary>
    <author>
      <name>Mingxuan Sun</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <author>
      <name>Paul Kidwell</name>
    </author>
    <link href="http://arxiv.org/abs/1012.0498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.0498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.3923v2</id>
    <updated>2011-05-26T19:26:27Z</updated>
    <published>2011-02-18T21:26:16Z</published>
    <title>Concentration-Based Guarantees for Low-Rank Matrix Reconstruction</title>
    <summary>  We consider the problem of approximately reconstructing a partially-observed,
approximately low-rank matrix. This problem has received much attention lately,
mostly using the trace-norm as a surrogate to the rank. Here we study low-rank
matrix reconstruction using both the trace-norm, as well as the less-studied
max-norm, and present reconstruction guarantees based on existing analysis on
the Rademacher complexity of the unit balls of these norms. We show how these
are superior in several ways to recently published guarantees based on
specialized analysis.
</summary>
    <author>
      <name>Rina Foygel</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1102.3923v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.3923v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.4240v1</id>
    <updated>2011-02-21T14:48:20Z</updated>
    <published>2011-02-21T14:48:20Z</published>
    <title>Sparse neural networks with large learning diversity</title>
    <summary>  Coded recurrent neural networks with three levels of sparsity are introduced.
The first level is related to the size of messages, much smaller than the
number of available neurons. The second one is provided by a particular coding
rule, acting as a local constraint in the neural activity. The third one is a
characteristic of the low final connection density of the network after the
learning phase. Though the proposed network is very simple since it is based on
binary neurons and binary connections, it is able to learn a large number of
messages and recall them, even in presence of strong erasures. The performance
of the network is assessed as a classifier and as an associative memory.
</summary>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <link href="http://arxiv.org/abs/1102.4240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.4240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.5601v1</id>
    <updated>2011-04-29T11:39:40Z</updated>
    <published>2011-04-29T11:39:40Z</published>
    <title>Mean-Variance Optimization in Markov Decision Processes</title>
    <summary>  We consider finite horizon Markov decision processes under performance
measures that involve both the mean and the variance of the cumulative reward.
We show that either randomized or history-based policies can improve
performance. We prove that the complexity of computing a policy that maximizes
the mean reward under a variance constraint is NP-hard for some cases, and
strongly NP-hard for others. We finally offer pseudopolynomial exact and
approximation algorithms.
</summary>
    <author>
      <name>Shie Mannor</name>
    </author>
    <author>
      <name>John Tsitsiklis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A full version of an ICML 2011 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.5601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.5601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2274v1</id>
    <updated>2011-05-11T18:59:13Z</updated>
    <published>2011-05-11T18:59:13Z</published>
    <title>Data-Distributed Weighted Majority and Online Mirror Descent</title>
    <summary>  In this paper, we focus on the question of the extent to which online
learning can benefit from distributed computing. We focus on the setting in
which $N$ agents online-learn cooperatively, where each agent only has access
to its own data. We propose a generic data-distributed online learning
meta-algorithm. We then introduce the Distributed Weighted Majority and
Distributed Online Mirror Descent algorithms, as special cases. We show, using
both theoretical analysis and experiments, that compared to a single agent:
given the same computation time, these distributed algorithms achieve smaller
generalization errors; and given the same generalization errors, they can be
$N$ times faster.
</summary>
    <author>
      <name>Hua Ouyang</name>
    </author>
    <author>
      <name>Alexander Gray</name>
    </author>
    <link href="http://arxiv.org/abs/1105.2274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.4272v1</id>
    <updated>2011-05-21T17:28:12Z</updated>
    <published>2011-05-21T17:28:12Z</published>
    <title>Calibration with Changing Checking Rules and Its Application to
  Short-Term Trading</title>
    <summary>  We provide a natural learning process in which a financial trader without a
risk receives a gain in case when Stock Market is inefficient. In this process,
the trader rationally choose his gambles using a prediction made by a
randomized calibrated algorithm. Our strategy is based on Dawid's notion of
calibration with more general changing checking rules and on some modification
of Kakade and Foster's randomized algorithm for computing calibrated forecasts.
</summary>
    <author>
      <name>Vladimir Trunov</name>
    </author>
    <author>
      <name>Vladimir V'yugin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.4272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.4272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.4585v1</id>
    <updated>2011-05-23T19:10:03Z</updated>
    <published>2011-05-23T19:10:03Z</published>
    <title>PAC-Bayesian Analysis of the Exploration-Exploitation Trade-off</title>
    <summary>  We develop a coherent framework for integrative simultaneous analysis of the
exploration-exploitation and model order selection trade-offs. We improve over
our preceding results on the same subject (Seldin et al., 2011) by combining
PAC-Bayesian analysis with Bernstein-type inequality for martingales. Such a
combination is also of independent interest for studies of multiple
simultaneously evolving martingales.
</summary>
    <author>
      <name>Yevgeny Seldin</name>
    </author>
    <author>
      <name>Nicolò Cesa-Bianchi</name>
    </author>
    <author>
      <name>François Laviolette</name>
    </author>
    <author>
      <name>Peter Auer</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <author>
      <name>Jan Peters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">On-line Trading of Exploration and Exploitation 2 - ICML-2011
  workshop. http://explo.cs.ucl.ac.uk/workshop/</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.4585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.4585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.4701v3</id>
    <updated>2011-09-08T04:14:53Z</updated>
    <published>2011-05-24T07:58:30Z</published>
    <title>Online Learning, Stability, and Stochastic Gradient Descent</title>
    <summary>  In batch learning, stability together with existence and uniqueness of the
solution corresponds to well-posedness of Empirical Risk Minimization (ERM)
methods; recently, it was proved that CV_loo stability is necessary and
sufficient for generalization and consistency of ERM. In this note, we
introduce CV_on stability, which plays a similar note in online learning. We
show that stochastic gradient descent (SDG) with the usual hypotheses is CVon
stable and we then discuss the implications of CV_on stability for convergence
of SGD.
</summary>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <author>
      <name>Stephen Voinea</name>
    </author>
    <author>
      <name>Lorenzo Rosasco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.4701v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.4701v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5379v1</id>
    <updated>2011-05-26T19:19:30Z</updated>
    <published>2011-05-26T19:19:30Z</published>
    <title>Parallel Coordinate Descent for L1-Regularized Loss Minimization</title>
    <summary>  We propose Shotgun, a parallel coordinate descent algorithm for minimizing
L1-regularized losses. Though coordinate descent seems inherently sequential,
we prove convergence bounds for Shotgun which predict linear speedups, up to a
problem-dependent limit. We present a comprehensive empirical study of Shotgun
for Lasso and sparse logistic regression. Our theoretical predictions on the
potential for parallelism closely match behavior on real data. Shotgun
outperforms other published solvers on a range of large problems, proving to be
one of the most scalable algorithms for L1.
</summary>
    <author>
      <name>Joseph K. Bradley</name>
    </author>
    <author>
      <name>Aapo Kyrola</name>
    </author>
    <author>
      <name>Danny Bickson</name>
    </author>
    <author>
      <name>Carlos Guestrin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In the 28th International Conference on Machine Learning, July
  2011, Washington, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.5379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.6041v1</id>
    <updated>2011-05-30T17:02:09Z</updated>
    <published>2011-05-30T17:02:09Z</published>
    <title>The Perceptron with Dynamic Margin</title>
    <summary>  The classical perceptron rule provides a varying upper bound on the maximum
margin, namely the length of the current weight vector divided by the total
number of updates up to that time. Requiring that the perceptron updates its
internal state whenever the normalized margin of a pattern is found not to
exceed a certain fraction of this dynamic upper bound we construct a new
approximate maximum margin classifier called the perceptron with dynamic margin
(PDM). We demonstrate that PDM converges in a finite number of steps and derive
an upper bound on them. We also compare experimentally PDM with other
perceptron-like algorithms and support vector machines on hard margin tasks
involving linear kernels which are equivalent to 2-norm soft margin.
</summary>
    <author>
      <name>Constantinos Panagiotakopoulos</name>
    </author>
    <author>
      <name>Petroula Tsampouka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.6041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.6041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0221v1</id>
    <updated>2011-06-01T16:16:14Z</updated>
    <published>2011-06-01T16:16:14Z</published>
    <title>Evolutionary Algorithms for Reinforcement Learning</title>
    <summary>  There are two distinct approaches to solving reinforcement learning problems,
namely, searching in value function space and searching in policy space.
Temporal difference methods and evolutionary algorithms are well-known examples
of these approaches. Kaelbling, Littman and Moore recently provided an
informative survey of temporal difference methods. This article focuses on the
application of evolutionary algorithms to the reinforcement learning problem,
emphasizing alternative policy representations, credit assignment methods, and
problem-specific genetic operators. Strengths and weaknesses of the
evolutionary approach to reinforcement learning are presented, along with a
survey of representative applications.
</summary>
    <author>
      <name>J. J. Grefenstette</name>
    </author>
    <author>
      <name>D. E. Moriarty</name>
    </author>
    <author>
      <name>A. C. Schultz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.613</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.613" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal Of Artificial Intelligence Research, Volume 11, pages
  241-276, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.0221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0357v1</id>
    <updated>2011-06-02T02:31:04Z</updated>
    <published>2011-06-02T02:31:04Z</published>
    <title>Learning Hierarchical Sparse Representations using Iterative Dictionary
  Learning and Dimension Reduction</title>
    <summary>  This paper introduces an elemental building block which combines Dictionary
Learning and Dimension Reduction (DRDL). We show how this foundational element
can be used to iteratively construct a Hierarchical Sparse Representation (HSR)
of a sensory stream. We compare our approach to existing models showing the
generality of our simple prescription. We then perform preliminary experiments
using this framework, illustrating with the example of an object recognition
task using standard datasets. This work introduces the very first steps towards
an integrated framework for designing and analyzing various computational tasks
from learning to attention to action. The ultimate goal is building a
mathematically rigorous, integrated theory of intelligence.
</summary>
    <author>
      <name>Mohamad Tarifi</name>
    </author>
    <author>
      <name>Meera Sitharam</name>
    </author>
    <author>
      <name>Jeffery Ho</name>
    </author>
    <link href="http://arxiv.org/abs/1106.0357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0518v2</id>
    <updated>2011-06-13T14:32:55Z</updated>
    <published>2011-06-02T21:30:50Z</published>
    <title>Submodular Functions Are Noise Stable</title>
    <summary>  We show that all non-negative submodular functions have high {\em
noise-stability}. As a consequence, we obtain a polynomial-time learning
algorithm for this class with respect to any product distribution on
$\{-1,1\}^n$ (for any constant accuracy parameter $\epsilon$). Our algorithm
also succeeds in the agnostic setting. Previous work on learning submodular
functions required either query access or strong assumptions about the types of
submodular functions to be learned (and did not hold in the agnostic setting).
</summary>
    <author>
      <name>Mahdi Cheraghchi</name>
    </author>
    <author>
      <name>Adam Klivans</name>
    </author>
    <author>
      <name>Pravesh Kothari</name>
    </author>
    <author>
      <name>Homin K. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1106.0518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0676v1</id>
    <updated>2011-06-03T14:55:23Z</updated>
    <published>2011-06-03T14:55:23Z</published>
    <title>Optimizing Dialogue Management with Reinforcement Learning: Experiments
  with the NJFun System</title>
    <summary>  Designing the dialogue policy of a spoken dialogue system involves many
nontrivial choices. This paper presents a reinforcement learning approach for
automatically optimizing a dialogue policy, which addresses the technical
challenges in applying reinforcement learning to a working dialogue system with
human users. We report on the design, construction and empirical evaluation of
NJFun, an experimental spoken dialogue system that provides users with access
to information about fun things to do in New Jersey. Our results show that by
optimizing its performance via reinforcement learning, NJFun measurably
improves system performance.
</summary>
    <author>
      <name>M. Kearns</name>
    </author>
    <author>
      <name>D. Litman</name>
    </author>
    <author>
      <name>S. Singh</name>
    </author>
    <author>
      <name>M. Walker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.859</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.859" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal Of Artificial Intelligence Research, Volume 16, pages
  105-133, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.0676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.1216v2</id>
    <updated>2011-06-15T01:17:56Z</updated>
    <published>2011-06-06T23:55:00Z</published>
    <title>Using More Data to Speed-up Training Time</title>
    <summary>  In many recent applications, data is plentiful. By now, we have a rather
clear understanding of how more data can be used to improve the accuracy of
learning algorithms. Recently, there has been a growing interest in
understanding how more data can be leveraged to reduce the required training
runtime. In this paper, we study the runtime of learning as a function of the
number of available training examples, and underscore the main high-level
techniques. We provide some initial positive results showing that the runtime
can decrease exponentially while only requiring a polynomial growth of the
number of examples, and spell-out several interesting open problems.
</summary>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <author>
      <name>Eran Tromer</name>
    </author>
    <link href="http://arxiv.org/abs/1106.1216v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.1216v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.1622v1</id>
    <updated>2011-06-08T19:07:09Z</updated>
    <published>2011-06-08T19:07:09Z</published>
    <title>Large-Scale Convex Minimization with a Low-Rank Constraint</title>
    <summary>  We address the problem of minimizing a convex function over the space of
large matrices with low rank. While this optimization problem is hard in
general, we propose an efficient greedy algorithm and derive its formal
approximation guarantees. Each iteration of the algorithm involves
(approximately) finding the left and right singular vectors corresponding to
the largest singular value of a certain matrix, which can be calculated in
linear time. This leads to an algorithm which can scale to large matrices
arising in several applications such as matrix completion for collaborative
filtering and robust low rank matrix approximation.
</summary>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Alon Gonen</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.1622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.1622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2429v4</id>
    <updated>2013-09-11T10:55:26Z</updated>
    <published>2011-06-13T12:30:05Z</published>
    <title>Efficient Transductive Online Learning via Randomized Rounding</title>
    <summary>  Most traditional online learning algorithms are based on variants of mirror
descent or follow-the-leader. In this paper, we present an online algorithm
based on a completely different approach, tailored for transductive settings,
which combines "random playout" and randomized rounding of loss subgradients.
As an application of our approach, we present the first computationally
efficient online algorithm for collaborative filtering with trace-norm
constrained matrices. As a second application, we solve an open question
linking batch learning and transductive online learning
</summary>
    <author>
      <name>Nicolò Cesa-Bianchi</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in a Festschrift in honor of V.N. Vapnik. Preliminary
  version presented in NIPS 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2429v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2429v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3355v2</id>
    <updated>2012-03-05T21:36:51Z</updated>
    <published>2011-06-16T21:32:26Z</published>
    <title>On epsilon-optimality of the pursuit learning algorithm</title>
    <summary>  Estimator algorithms in learning automata are useful tools for adaptive,
real-time optimization in computer science and engineering applications. This
paper investigates theoretical convergence properties for a special case of
estimator algorithms: the pursuit learning algorithm. In this note, we identify
and fill a gap in existing proofs of probabilistic convergence for pursuit
learning. It is tradition to take the pursuit learning tuning parameter to be
fixed in practical applications, but our proof sheds light on the importance of
a vanishing sequence of tuning parameters in a theoretical convergence
analysis.
</summary>
    <author>
      <name>Ryan Martin</name>
    </author>
    <author>
      <name>Omkar Tilak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1239/jap/1346955334</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1239/jap/1346955334" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Applied Probability, 49(3), 795-805, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.3355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3395v1</id>
    <updated>2011-06-17T06:53:47Z</updated>
    <published>2011-06-17T06:53:47Z</published>
    <title>Decoding finger movements from ECoG signals using switching linear
  models</title>
    <summary>  One of the major challenges of ECoG-based Brain-Machine Interfaces is the
movement prediction of a human subject. Several methods exist to predict an arm
2-D trajectory. The fourth BCI Competition gives a dataset in which the aim is
to predict individual finger movements (5-D trajectory). The difficulty lies in
the fact that there is no simple relation between ECoG signals and finger
movement. We propose in this paper to decode finger flexions using switching
models. This method permits to simplify the system as it is now described as an
ensemble of linear models depending on an internal state. We show that an
interesting accuracy prediction can be obtained by such a model.
</summary>
    <author>
      <name>Rémi Flamary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Alain Rakotomamonjy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1106.3395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3396v1</id>
    <updated>2011-06-17T06:54:35Z</updated>
    <published>2011-06-17T06:54:35Z</published>
    <title>Large margin filtering for signal sequence labeling</title>
    <summary>  Signal Sequence Labeling consists in predicting a sequence of labels given an
observed sequence of samples. A naive way is to filter the signal in order to
reduce the noise and to apply a classification algorithm on the filtered
samples. We propose in this paper to jointly learn the filter with the
classifier leading to a large margin filtering for classification. This method
allows to learn the optimal cutoff frequency and phase of the filter that may
be different from zero. Two methods are proposed and tested on a toy dataset
and on a real life BCI dataset from BCI Competition III.
</summary>
    <author>
      <name>Rémi Flamary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Labbé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Alain Rakotomamonjy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2010.5495281</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2010.5495281" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics Speech and Signal
  Processing (ICASSP), 2010, Dallas : United States (2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.3396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.4064v2</id>
    <updated>2011-11-09T19:45:30Z</updated>
    <published>2011-06-21T00:37:23Z</published>
    <title>Algorithmic Programming Language Identification</title>
    <summary>  Motivated by the amount of code that goes unidentified on the web, we
introduce a practical method for algorithmically identifying the programming
language of source code. Our work is based on supervised learning and
intelligent statistical features. We also explored, but abandoned, a
grammatical approach. In testing, our implementation greatly outperforms that
of an existing tool that relies on a Bayesian classifier. Code is written in
Python and available under an MIT license.
</summary>
    <author>
      <name>David Klein</name>
    </author>
    <author>
      <name>Kyle Murray</name>
    </author>
    <author>
      <name>Simon Weber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. Code:
  https://github.com/simon-weber/Programming-Language-Identification</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.4064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.4251v1</id>
    <updated>2011-06-21T16:16:24Z</updated>
    <published>2011-06-21T16:16:24Z</published>
    <title>Learning with the Weighted Trace-norm under Arbitrary Sampling
  Distributions</title>
    <summary>  We provide rigorous guarantees on learning with the weighted trace-norm under
arbitrary sampling distributions. We show that the standard weighted trace-norm
might fail when the sampling distribution is not a product distribution (i.e.
when row and column indexes are not selected independently), present a
corrected variant for which we establish strong learning guarantees, and
demonstrate that it works better in practice. We provide guarantees when
weighting by either the true or empirical sampling distribution, and suggest
that even if the true distribution is known (or is uniform), weighting by the
empirical distribution may be beneficial.
</summary>
    <author>
      <name>Rina Foygel</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1106.4251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.4574v1</id>
    <updated>2011-06-22T20:59:20Z</updated>
    <published>2011-06-22T20:59:20Z</published>
    <title>Better Mini-Batch Algorithms via Accelerated Gradient Methods</title>
    <summary>  Mini-batch algorithms have been proposed as a way to speed-up stochastic
convex optimization problems. We study how such algorithms can be improved
using accelerated gradient methods. We provide a novel analysis, which shows
how standard gradient methods may sometimes be insufficient to obtain a
significant speed-up and propose a novel accelerated gradient algorithm, which
deals with this deficiency, enjoys a uniformly superior guarantee and works
well in practice.
</summary>
    <author>
      <name>Andrew Cotter</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <author>
      <name>Karthik Sridharan</name>
    </author>
    <link href="http://arxiv.org/abs/1106.4574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6258v2</id>
    <updated>2014-05-12T19:40:40Z</updated>
    <published>2011-06-30T15:03:58Z</published>
    <title>A Note on Improved Loss Bounds for Multiple Kernel Learning</title>
    <summary>  In this paper, we correct an upper bound, presented in~\cite{hs-11}, on the
generalisation error of classifiers learned through multiple kernel learning.
The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive}
dependence on the logarithm of the number of kernels and the margin achieved by
the classifier. However, there are some errors in parts of the proof which are
corrected in this paper. Unfortunately, the final result turns out to be a risk
bound which has a \emph{multiplicative} dependence on the logarithm of the
number of kernels and the margin achieved by the classifier.
</summary>
    <author>
      <name>Zakria Hussain</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <author>
      <name>Mario Marchand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended proof</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.6258v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6258v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1564v3</id>
    <updated>2014-03-12T07:08:13Z</updated>
    <published>2011-07-08T06:26:03Z</published>
    <title>Polyceptron: A Polyhedral Learning Algorithm</title>
    <summary>  In this paper we propose a new algorithm for learning polyhedral classifiers
which we call as Polyceptron. It is a Perception like algorithm which updates
the parameters only when the current classifier misclassifies any training
data. We give both batch and online version of Polyceptron algorithm. Finally
we give experimental results to show the effectiveness of our approach.
</summary>
    <author>
      <name>Naresh Manwani</name>
    </author>
    <author>
      <name>P. S. Sastry</name>
    </author>
    <link href="http://arxiv.org/abs/1107.1564v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1564v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3823v1</id>
    <updated>2011-07-19T19:43:10Z</updated>
    <published>2011-07-19T19:43:10Z</published>
    <title>Weakly Supervised Learning of Foreground-Background Segmentation using
  Masked RBMs</title>
    <summary>  We propose an extension of the Restricted Boltzmann Machine (RBM) that allows
the joint shape and appearance of foreground objects in cluttered images to be
modeled independently of the background. We present a learning scheme that
learns this representation directly from cluttered images with only very weak
supervision. The model generates plausible samples and performs
foreground-background segmentation. We demonstrate that representing foreground
objects independently of the background can be beneficial in recognition tasks.
</summary>
    <author>
      <name>Nicolas Heess</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Informatics</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Le Roux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Paris - Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>John Winn</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Artificial Neural Networks (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.3823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.0017v1</id>
    <updated>2011-07-29T21:07:51Z</updated>
    <published>2011-07-29T21:07:51Z</published>
    <title>Generating a Diverse Set of High-Quality Clusterings</title>
    <summary>  We provide a new framework for generating multiple good quality partitions
(clusterings) of a single data set. Our approach decomposes this problem into
two components, generating many high-quality partitions, and then grouping
these partitions to obtain k representatives. The decomposition makes the
approach extremely modular and allows us to optimize various criteria that
control the choice of representative partitions.
</summary>
    <author>
      <name>Jeff M. Phillips</name>
    </author>
    <author>
      <name>Parasaran Raman</name>
    </author>
    <author>
      <name>Suresh Venkatasubramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 5 Figures, 2nd MultiClust Workshop at ECML PKDD 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.0017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.3476v2</id>
    <updated>2011-09-02T08:47:01Z</updated>
    <published>2011-08-17T13:36:11Z</published>
    <title>Structured Sparsity and Generalization</title>
    <summary>  We present a data dependent generalization bound for a large class of
regularized algorithms which implement structured sparsity constraints. The
bound can be applied to standard squared-norm regularization, the Lasso, the
group Lasso, some versions of the group Lasso with overlapping groups, multiple
kernel learning and other regularization schemes. In all these cases
competitive results are obtained. A novel feature of our bound is that it can
be applied in an infinite dimensional setting such as the Lasso in a separable
Hilbert space or multiple kernel learning with a countable number of kernels.
</summary>
    <author>
      <name>Andreas Maurer</name>
    </author>
    <author>
      <name>Massimiliano Pontil</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research, 13:671-690, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.3476v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.3476v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.6088v1</id>
    <updated>2011-08-30T21:48:37Z</updated>
    <published>2011-08-30T21:48:37Z</published>
    <title>No Internal Regret via Neighborhood Watch</title>
    <summary>  We present an algorithm which attains O(\sqrt{T}) internal (and thus
external) regret for finite games with partial monitoring under the local
observability condition. Recently, this condition has been shown by (Bartok,
Pal, and Szepesvari, 2011) to imply the O(\sqrt{T}) rate for partial monitoring
games against an i.i.d. opponent, and the authors conjectured that the same
holds for non-stochastic adversaries. Our result is in the affirmative, and it
completes the characterization of possible rates for finite partial-monitoring
games, an open question stated by (Cesa-Bianchi, Lugosi, and Stoltz, 2006). Our
regret guarantees also hold for the more general model of partial monitoring
with random signals.
</summary>
    <author>
      <name>Dean Foster</name>
    </author>
    <author>
      <name>Alexander Rakhlin</name>
    </author>
    <link href="http://arxiv.org/abs/1108.6088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.6088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5311v1</id>
    <updated>2011-09-24T22:14:46Z</updated>
    <published>2011-09-24T22:14:46Z</published>
    <title>Bias Plus Variance Decomposition for Survival Analysis Problems</title>
    <summary>  Bias - variance decomposition of the expected error defined for regression
and classification problems is an important tool to study and compare different
algorithms, to find the best areas for their application. Here the
decomposition is introduced for the survival analysis problem. In our
experiments, we study bias -variance parts of the expected error for two
algorithms: original Cox proportional hazard regression and CoxPath, path
algorithm for L1-regularized Cox regression, on the series of increased
training sets. The experiments demonstrate that, contrary expectations, CoxPath
does not necessarily have an advantage over Cox regression.
</summary>
    <author>
      <name>Marina Sapir</name>
    </author>
    <link href="http://arxiv.org/abs/1109.5311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5664v4</id>
    <updated>2013-06-21T20:52:27Z</updated>
    <published>2011-09-26T18:44:00Z</published>
    <title>Deterministic Feature Selection for $k$-means Clustering</title>
    <summary>  We study feature selection for $k$-means clustering. Although the literature
contains many methods with good empirical performance, algorithms with provable
theoretical behavior have only recently been developed. Unfortunately, these
algorithms are randomized and fail with, say, a constant probability. We
address this issue by presenting a deterministic feature selection algorithm
for k-means with theoretical guarantees. At the heart of our algorithm lies a
deterministic method for decompositions of the identity.
</summary>
    <author>
      <name>Christos Boutsidis</name>
    </author>
    <author>
      <name>Malik Magdon-Ismail</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2013.2255021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2013.2255021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Transactions on Information Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5664v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5664v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1075v1</id>
    <updated>2011-10-05T19:03:35Z</updated>
    <published>2011-10-05T19:03:35Z</published>
    <title>The Augmented Complex Kernel LMS</title>
    <summary>  Recently, a unified framework for adaptive kernel based signal processing of
complex data was presented by the authors, which, besides offering techniques
to map the input data to complex Reproducing Kernel Hilbert Spaces, developed a
suitable Wirtinger-like Calculus for general Hilbert Spaces. In this short
paper, the extended Wirtinger's calculus is adopted to derive complex
kernel-based widely-linear estimation filters. Furthermore, we illuminate
several important characteristics of the widely linear filters. We show that,
although in many cases the gains from adopting widely linear estimation
filters, as alternatives to ordinary linear ones, are rudimentary, for the case
of kernel based widely linear filters significant performance improvements can
be obtained.
</summary>
    <author>
      <name>Pantelis Bouboulis</name>
    </author>
    <author>
      <name>Sergios Theodoridis</name>
    </author>
    <author>
      <name>Michael Mavroforakis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2012.2200479</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2012.2200479" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">manuscript submitted to IEE Transactions on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.1075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1781v1</id>
    <updated>2011-10-09T02:18:50Z</updated>
    <published>2011-10-09T02:18:50Z</published>
    <title>A Study of Unsupervised Adaptive Crowdsourcing</title>
    <summary>  We consider unsupervised crowdsourcing performance based on the model wherein
the responses of end-users are essentially rated according to how their
responses correlate with the majority of other responses to the same
subtasks/questions. In one setting, we consider an independent sequence of
identically distributed crowdsourcing assignments (meta-tasks), while in the
other we consider a single assignment with a large number of component
subtasks. Both problems yield intuitive results in which the overall
reliability of the crowd is a factor.
</summary>
    <author>
      <name>G. Kesidis</name>
    </author>
    <author>
      <name>A. Kurve</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.1781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2211v1</id>
    <updated>2011-10-10T21:58:58Z</updated>
    <published>2011-10-10T21:58:58Z</published>
    <title>Learning Symbolic Models of Stochastic Domains</title>
    <summary>  In this article, we work towards the goal of developing agents that can learn
to act in complex worlds. We develop a probabilistic, relational planning rule
representation that compactly models noisy, nondeterministic action effects,
and show how such rules can be effectively learned. Through experiments in
simple planning domains and a 3D simulated blocks world with realistic physics,
we demonstrate that this learning algorithm allows agents to effectively model
world dynamics.
</summary>
    <author>
      <name>L. P. Kaelbling</name>
    </author>
    <author>
      <name>H. M. Pasula</name>
    </author>
    <author>
      <name>L. S. Zettlemoyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.2113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.2113" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal Of Artificial Intelligence Research, Volume 29, pages
  309-352, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.2211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2626v1</id>
    <updated>2011-10-12T10:56:29Z</updated>
    <published>2011-10-12T10:56:29Z</published>
    <title>Analysis of Heart Diseases Dataset using Neural Network Approach</title>
    <summary>  One of the important techniques of Data mining is Classification. Many real
world problems in various fields such as business, science, industry and
medicine can be solved by using classification approach. Neural Networks have
emerged as an important tool for classification. The advantages of Neural
Networks helps for efficient classification of given data. In this study a
Heart diseases dataset is analyzed using Neural Network approach. To increase
the efficiency of the classification process parallel approach is also adopted
in the training phase.
</summary>
    <author>
      <name>K. Usha Rani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdkp.2011.1501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdkp.2011.1501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 1 table; International Journal of Data Mining &amp;
  Knowledge Management Process (IJDKP) Vol.1, No.5, September 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.2626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4416v1</id>
    <updated>2011-10-20T00:56:53Z</updated>
    <published>2011-10-20T00:56:53Z</published>
    <title>Data-dependent kernels in nearly-linear time</title>
    <summary>  We propose a method to efficiently construct data-dependent kernels which can
make use of large quantities of (unlabeled) data. Our construction makes an
approximation in the standard construction of semi-supervised kernels in
Sindhwani et al. 2005. In typical cases these kernels can be computed in
nearly-linear time (in the amount of data), improving on the cubic time of the
standard construction, enabling large scale semi-supervised learning in a
variety of contexts. The methods are validated on semi-supervised and
unsupervised problems on data sets containing upto 64,000 sample points.
</summary>
    <author>
      <name>Guy Lever</name>
    </author>
    <author>
      <name>Tom Diethe</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1110.4416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.5051v1</id>
    <updated>2011-10-23T14:41:21Z</updated>
    <published>2011-10-23T14:41:21Z</published>
    <title>Wikipedia Edit Number Prediction based on Temporal Dynamics Only</title>
    <summary>  In this paper, we describe our approach to the Wikipedia Participation
Challenge which aims to predict the number of edits a Wikipedia editor will
make in the next 5 months. The best submission from our team, "zeditor",
achieved 41.7% improvement over WMF's baseline predictive model and the final
rank of 3rd place among 96 teams. An interesting characteristic of our approach
is that only temporal dynamics features (i.e., how the number of edits changes
in recent periods, etc.) are used in a self-supervised learning framework,
which makes it easy to be generalised to other application domains.
</summary>
    <author>
      <name>Dell Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1110.5051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.6287v1</id>
    <updated>2011-10-28T10:20:25Z</updated>
    <published>2011-10-28T10:20:25Z</published>
    <title>Deciding of HMM parameters based on number of critical points for
  gesture recognition from motion capture data</title>
    <summary>  This paper presents a method of choosing number of states of a HMM based on
number of critical points of the motion capture data. The choice of Hidden
Markov Models(HMM) parameters is crucial for recognizer's performance as it is
the first step of the training and cannot be corrected automatically within
HMM. In this article we define predictor of number of states based on number of
critical points of the sequence and test its effectiveness against sample data.
</summary>
    <author>
      <name>Michał Cholewa</name>
    </author>
    <author>
      <name>Przemysław Głomb</name>
    </author>
    <link href="http://arxiv.org/abs/1110.6287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.6287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1124v1</id>
    <updated>2011-11-04T13:33:24Z</updated>
    <published>2011-11-04T13:33:24Z</published>
    <title>Tight Bounds on Proper Equivalence Query Learning of DNF</title>
    <summary>  We prove a new structural lemma for partial Boolean functions $f$, which we
call the seed lemma for DNF. Using the lemma, we give the first subexponential
algorithm for proper learning of DNF in Angluin's Equivalence Query (EQ) model.
The algorithm has time and query complexity $2^{(\tilde{O}{\sqrt{n}})}$, which
is optimal. We also give a new result on certificates for DNF-size, a simple
algorithm for properly PAC-learning DNF, and new results on EQ-learning $\log
n$-term DNF and decision trees.
</summary>
    <author>
      <name>Lisa Hellerstein</name>
    </author>
    <author>
      <name>Devorah Kletenik</name>
    </author>
    <author>
      <name>Linda Sellie</name>
    </author>
    <author>
      <name>Rocco Servedio</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1136v2</id>
    <updated>2011-11-14T21:16:59Z</updated>
    <published>2011-11-04T14:18:31Z</published>
    <title>Universal MMSE Filtering With Logarithmic Adaptive Regret</title>
    <summary>  We consider the problem of online estimation of a real-valued signal
corrupted by oblivious zero-mean noise using linear estimators. The estimator
is required to iteratively predict the underlying signal based on the current
and several last noisy observations, and its performance is measured by the
mean-square-error. We describe and analyze an algorithm for this task which: 1.
Achieves logarithmic adaptive regret against the best linear filter in
hindsight. This bound is assyptotically tight, and resolves the question of
Moon and Weissman [1]. 2. Runs in linear time in terms of the number of filter
coefficients. Previous constructions required at least quadratic time.
</summary>
    <author>
      <name>Dan Garber</name>
    </author>
    <author>
      <name>Elad Hazan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.1136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1422v1</id>
    <updated>2011-11-06T14:01:14Z</updated>
    <published>2011-11-06T14:01:14Z</published>
    <title>Robust Interactive Learning</title>
    <summary>  In this paper we propose and study a generalization of the standard
active-learning model where a more general type of query, class conditional
query, is allowed. Such queries have been quite useful in applications, but
have been lacking theoretical understanding. In this work, we characterize the
power of such queries under two well-known noise models. We give nearly tight
upper and lower bounds on the number of queries needed to learn both for the
general agnostic setting and for the bounded noise model. We further show that
our methods can be made adaptive to the (unknown) noise rate, with only
negligible loss in query complexity.
</summary>
    <author>
      <name>Maria-Florina Balcan</name>
    </author>
    <author>
      <name>Steve Hanneke</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.3735v1</id>
    <updated>2011-11-16T09:26:14Z</updated>
    <published>2011-11-16T09:26:14Z</published>
    <title>A Bayesian Model for Plan Recognition in RTS Games applied to StarCraft</title>
    <summary>  The task of keyhole (unobtrusive) plan recognition is central to adaptive
game AI. "Tech trees" or "build trees" are the core of real-time strategy (RTS)
game strategic (long term) planning. This paper presents a generic and simple
Bayesian model for RTS build tree prediction from noisy observations, which
parameters are learned from replays (game logs). This unsupervised machine
learning approach involves minimal work for the game developers as it leverage
players' data (com- mon in RTS). We applied it to StarCraft1 and showed that it
yields high quality and robust predictions, that can feed an adaptive AI.
</summary>
    <author>
      <name>Gabriel Synnaeve</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIG, LPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Bessière</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIG, LPPA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages; Artificial Intelligence and Interactive Digital
  Entertainment Conference (AIIDE 2011), Palo Alto : \'Etats-Unis (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.3735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.3846v1</id>
    <updated>2011-11-16T16:06:57Z</updated>
    <published>2011-11-16T16:06:57Z</published>
    <title>No Free Lunch versus Occam's Razor in Supervised Learning</title>
    <summary>  The No Free Lunch theorems are often used to argue that domain specific
knowledge is required to design successful algorithms. We use algorithmic
information theory to argue the case for a universal bias allowing an algorithm
to succeed in all interesting problem domains. Additionally, we give a new
algorithm for off-line classification, inspired by Solomonoff induction, with
good performance on all structured problems under reasonable assumptions. This
includes a proof of the efficacy of the well-known heuristic of randomly
selecting training data in the hope of reducing misclassification rates.
</summary>
    <author>
      <name>Tor Lattimore</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 LaTeX pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.3846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.4344v1</id>
    <updated>2011-12-19T14:21:00Z</updated>
    <published>2011-12-19T14:21:00Z</published>
    <title>A Scalable Multiclass Algorithm for Node Classification</title>
    <summary>  We introduce a scalable algorithm, MUCCA, for multiclass node classification
in weighted graphs. Unlike previously proposed methods for the same task, MUCCA
works in time linear in the number of nodes. Our approach is based on a
game-theoretic formulation of the problem in which the test labels are
expressed as a Nash Equilibrium of a certain game. However, in order to achieve
scalability, we find the equilibrium on a spanning tree of the original graph.
Experiments on real-world data reveal that MUCCA is much faster than its
competitors while achieving a similar predictive performance.
</summary>
    <author>
      <name>Giovanni Zappella</name>
    </author>
    <link href="http://arxiv.org/abs/1112.4344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.4344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2416v1</id>
    <updated>2012-01-11T21:03:55Z</updated>
    <published>2012-01-11T21:03:55Z</published>
    <title>Stochastic Low-Rank Kernel Learning for Regression</title>
    <summary>  We present a novel approach to learn a kernel-based regression function. It
is based on the useof conical combinations of data-based parameterized kernels
and on a new stochastic convex optimization procedure of which we establish
convergence guarantees. The overall learning procedure has the nice properties
that a) the learned conical combination is automatically designed to perform
the regression task at hand and b) the updates implicated by the optimization
procedure are quite inexpensive. In order to shed light on the appositeness of
our learning strategy, we present empirical results from experiments conducted
on various benchmark datasets.
</summary>
    <author>
      <name>Pierre Machart</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <author>
      <name>Thomas Peel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF, LATP</arxiv:affiliation>
    </author>
    <author>
      <name>Liva Ralaivola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <author>
      <name>Sandrine Anthoine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LATP</arxiv:affiliation>
    </author>
    <author>
      <name>Hervé Glotin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSIS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Machine Learning (ICML'11), Bellevue
  (Washington) : United States (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.2416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5217v1</id>
    <updated>2012-01-25T09:44:06Z</updated>
    <published>2012-01-25T09:44:06Z</published>
    <title>Unsupervised Classification Using Immune Algorithm</title>
    <summary>  Unsupervised classification algorithm based on clonal selection principle
named Unsupervised Clonal Selection Classification (UCSC) is proposed in this
paper. The new proposed algorithm is data driven and self-adaptive, it adjusts
its parameters to the data to make the classification operation as fast as
possible. The performance of UCSC is evaluated by comparing it with the well
known K-means algorithm using several artificial and real-life data sets. The
experiments show that the proposed UCSC algorithm is more reliable and has high
classification precision comparing to traditional classification methods such
as K-means.
</summary>
    <author>
      <name>M. T. Al-Muallim</name>
    </author>
    <author>
      <name>R. El-Kouatly</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/677-952</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/677-952" rel="related"/>
    <link href="http://arxiv.org/abs/1201.5217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3702v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Semi-supervised Learning with Density Based Distances</title>
    <summary>  We present a simple, yet effective, approach to Semi-Supervised Learning. Our
approach is based on estimating density-based distances (DBD) using a shortest
path calculation on a graph. These Graph-DBD estimates can then be used in any
distance-based supervised learning method, such as Nearest Neighbor methods and
SVMs with RBF kernels. In order to apply the method to very large data sets, we
also present a novel algorithm which integrates nearest neighbor computations
into the shortest path search and can find exact shortest paths even in
extremely large dense graphs. Significant runtime improvement over the commonly
used Laplacian regularization method is then shown on a large scale dataset.
</summary>
    <author>
      <name>Avleen S. Bijral</name>
    </author>
    <author>
      <name>Nathan Ratliff</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3716v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Boosting as a Product of Experts</title>
    <summary>  In this paper, we derive a novel probabilistic model of boosting as a Product
of Experts. We re-derive the boosting algorithm as a greedy incremental model
selection procedure which ensures that addition of new experts to the ensemble
does not decrease the likelihood of the data. These learning rules lead to a
generic boosting algorithm - POE- Boost which turns out to be similar to the
AdaBoost algorithm under certain assumptions on the expert probabilities. The
paper then extends the POEBoost algorithm to POEBoost.CS which handles
hypothesis that produce probabilistic predictions. This new algorithm is shown
to have better generalization performance compared to other state of the art
algorithms.
</summary>
    <author>
      <name>Narayanan U. Edakunni</name>
    </author>
    <author>
      <name>Gary Brown</name>
    </author>
    <author>
      <name>Tim Kovacs</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3727v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Bregman divergence as general framework to estimate unnormalized
  statistical models</title>
    <summary>  We show that the Bregman divergence provides a rich framework to estimate
unnormalized statistical models for continuous or discrete random variables,
that is, models which do not integrate or sum to one, respectively. We prove
that recent estimation methods such as noise-contrastive estimation, ratio
matching, and score matching belong to the proposed framework, and explain
their interconnection based on supervised learning. Further, we discuss the
role of boosting in unsupervised learning.
</summary>
    <author>
      <name>Michael Gutmann</name>
    </author>
    <author>
      <name>Jun-ichiro Hirayama</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3736v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Discovering causal structures in binary exclusive-or skew acyclic models</title>
    <summary>  Discovering causal relations among observed variables in a given data set is
a main topic in studies of statistics and artificial intelligence. Recently,
some techniques to discover an identifiable causal structure have been explored
based on non-Gaussianity of the observed data distribution. However, most of
these are limited to continuous data. In this paper, we present a novel causal
model for binary data and propose a new approach to derive an identifiable
causal structure governing the data based on skew Bernoulli distributions of
external noise. Experimental evaluation shows excellent performance for both
artificial and real world data sets.
</summary>
    <author>
      <name>Takanori Inazumi</name>
    </author>
    <author>
      <name>Takashi Washio</name>
    </author>
    <author>
      <name>Shohei Shimizu</name>
    </author>
    <author>
      <name>Joe Suzuki</name>
    </author>
    <author>
      <name>Akihiro Yamamoto</name>
    </author>
    <author>
      <name>Yoshinobu Kawahara</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3747v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Reconstructing Pompeian Households</title>
    <summary>  A database of objects discovered in houses in the Roman city of Pompeii
provides a unique view of ordinary life in an ancient city. Experts have used
this collection to study the structure of Roman households, exploring the
distribution and variability of tasks in architectural spaces, but such
approaches are necessarily affected by modern cultural assumptions. In this
study we present a data-driven approach to household archeology, treating it as
an unsupervised labeling problem. This approach scales to large data sets and
provides a more objective complement to human interpretation.
</summary>
    <author>
      <name>David Mimno</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3750v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Fractional Moments on Bandit Problems</title>
    <summary>  Reinforcement learning addresses the dilemma between exploration to find
profitable actions and exploitation to act according to the best observations
already made. Bandit problems are one such class of problems in stateless
environments that represent this explore/exploit situation. We propose a
learning algorithm for bandit problems based on fractional expectation of
rewards acquired. The algorithm is theoretically shown to converge on an
eta-optimal arm and achieve O(n) sample complexity. Experimental results show
the algorithm incurs substantially lower regrets than parameter-optimized
eta-greedy and SoftMax approaches and other low sample complexity
state-of-the-art techniques.
</summary>
    <author>
      <name>Ananda Narayanan B</name>
    </author>
    <author>
      <name>Balaraman Ravindran</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3753v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Partial Order MCMC for Structure Discovery in Bayesian Networks</title>
    <summary>  We present a new Markov chain Monte Carlo method for estimating posterior
probabilities of structural features in Bayesian networks. The method draws
samples from the posterior distribution of partial orders on the nodes; for
each sampled partial order, the conditional probabilities of interest are
computed exactly. We give both analytical and empirical results that suggest
the superiority of the new method compared to previous methods, which sample
either directed acyclic graphs or linear orders on the nodes.
</summary>
    <author>
      <name>Teppo Niinimaki</name>
    </author>
    <author>
      <name>Pekka Parviainen</name>
    </author>
    <author>
      <name>Mikko Koivisto</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3766v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Robust learning Bayesian networks for prior belief</title>
    <summary>  Recent reports have described that learning Bayesian networks are highly
sensitive to the chosen equivalent sample size (ESS) in the Bayesian Dirichlet
equivalence uniform (BDeu). This sensitivity often engenders some unstable or
undesirable results. This paper describes some asymptotic analyses of BDeu to
explain the reasons for the sensitivity and its effects. Furthermore, this
paper presents a proposal for a robust learning score for ESS by eliminating
the sensitive factors from the approximation of log-BDeu.
</summary>
    <author>
      <name>Maomi Ueno</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3771v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Tightening MRF Relaxations with Planar Subproblems</title>
    <summary>  We describe a new technique for computing lower-bounds on the minimum energy
configuration of a planar Markov Random Field (MRF). Our method successively
adds large numbers of constraints and enforces consistency over binary
projections of the original problem state space. These constraints are
represented in terms of subproblems in a dual-decomposition framework that is
optimized using subgradient techniques. The complete set of constraints we
consider enforces cycle consistency over the original graph. In practice we
find that the method converges quickly on most problems with the addition of a
few subproblems and outperforms existing methods for some interesting classes
of hard potentials.
</summary>
    <author>
      <name>Julian Yarkony</name>
    </author>
    <author>
      <name>Ragib Morshed</name>
    </author>
    <author>
      <name>Alexander T. Ihler</name>
    </author>
    <author>
      <name>Charless C. Fowlkes</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3775v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Kernel-based Conditional Independence Test and Application in Causal
  Discovery</title>
    <summary>  Conditional independence testing is an important problem, especially in
Bayesian network learning and causal discovery. Due to the curse of
dimensionality, testing for conditional independence of continuous variables is
particularly challenging. We propose a Kernel-based Conditional Independence
test (KCI-test), by constructing an appropriate test statistic and deriving its
asymptotic distribution under the null hypothesis of conditional independence.
The proposed method is computationally efficient and easy to implement.
Experimental results show that it outperforms other methods, especially when
the conditioning set is large or the sample size is not very large, in which
case other methods encounter difficulties.
</summary>
    <author>
      <name>Kun Zhang</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <author>
      <name>Dominik Janzing</name>
    </author>
    <author>
      <name>Bernhard Schoelkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3778v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Sparse Topical Coding</title>
    <summary>  We present sparse topical coding (STC), a non-probabilistic formulation of
topic models for discovering latent representations of large collections of
data. Unlike probabilistic topic models, STC relaxes the normalization
constraint of admixture proportions and the constraint of defining a normalized
likelihood function. Such relaxations make STC amenable to: 1) directly control
the sparsity of inferred representations by using sparsity-inducing
regularizers; 2) be seamlessly integrated with a convex error function (e.g.,
SVM hinge loss) for supervised learning; and 3) be efficiently learned with a
simply structured coordinate descent algorithm. Our results demonstrate the
advantages of STC and supervised MedSTC on identifying topical meanings of
words and improving classification accuracy and time efficiency.
</summary>
    <author>
      <name>Jun Zhu</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3782v1</id>
    <updated>2012-02-14T16:41:17Z</updated>
    <published>2012-02-14T16:41:17Z</published>
    <title>Graphical Models for Bandit Problems</title>
    <summary>  We introduce a rich class of graphical models for multi-armed bandit problems
that permit both the state or context space and the action space to be very
large, yet succinctly specify the payoffs for any context-action pair. Our main
result is an algorithm for such models whose regret is bounded by the number of
parameters and whose running time depends only on the treewidth of the graph
substructure induced by the action space.
</summary>
    <author>
      <name>Kareem Amin</name>
    </author>
    <author>
      <name>Michael Kearns</name>
    </author>
    <author>
      <name>Umar Syed</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3890v1</id>
    <updated>2012-02-17T11:59:55Z</updated>
    <published>2012-02-17T11:59:55Z</published>
    <title>PAC Bounds for Discounted MDPs</title>
    <summary>  We study upper and lower bounds on the sample-complexity of learning
near-optimal behaviour in finite-state discounted Markov Decision Processes
(MDPs). For the upper bound we make the assumption that each action leads to at
most two possible next-states and prove a new bound for a UCRL-style algorithm
on the number of time-steps when it is not Probably Approximately Correct
(PAC). The new lower bound strengthens previous work by being both more general
(it applies to all policies) and tighter. The upper and lower bounds match up
to logarithmic factors.
</summary>
    <author>
      <name>Tor Lattimore</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 LaTeX pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 23rd International Conf. on Algorithmic Learning Theory (ALT
  2012) pages 320-334</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.3890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0298v2</id>
    <updated>2012-03-06T09:01:19Z</updated>
    <published>2012-03-01T14:40:02Z</published>
    <title>Application of Gist SVM in Cancer Detection</title>
    <summary>  In this paper, we study the application of GIST SVM in disease prediction
(detection of cancer). Pattern classification problems can be effectively
solved by Support vector machines. Here we propose a classifier which can
differentiate patients having benign and malignant cancer cells. To improve the
accuracy of classification, we propose to determine the optimal size of the
training set and perform feature selection. To find the optimal size of the
training set, different sizes of training sets are experimented and the one
with highest classification rate is selected. The optimal features are selected
through their F-Scores.
</summary>
    <author>
      <name>S. Aruna</name>
    </author>
    <author>
      <name>S. P. Rajagopalan</name>
    </author>
    <author>
      <name>L. V. Nandakishore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 39-48</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0298v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0298v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3462v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Gaussian Process Topic Models</title>
    <summary>  We introduce Gaussian Process Topic Models (GPTMs), a new family of topic
models which can leverage a kernel among documents while extracting correlated
topics. GPTMs can be considered a systematic generalization of the Correlated
Topic Models (CTMs) using ideas from Gaussian Process (GP) based embedding.
Since GPTMs work with both a topic covariance matrix and a document kernel
matrix, learning GPTMs involves a novel component-solving a suitable Sylvester
equation capturing both topic and document dependencies. The efficacy of GPTMs
is demonstrated with experiments evaluating the quality of both topic modeling
and embedding.
</summary>
    <author>
      <name>Amrudin Agovic</name>
    </author>
    <author>
      <name>Arindam Banerjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3472v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Super-Samples from Kernel Herding</title>
    <summary>  We extend the herding algorithm to continuous spaces by using the kernel
trick. The resulting "kernel herding" algorithm is an infinite memory
deterministic process that learns to approximate a PDF with a collection of
samples. We show that kernel herding decreases the error of expectations of
functions in the Hilbert space at a rate O(1/T) which is much faster than the
usual O(1/pT) for iid random samples. We illustrate kernel herding by
approximating Bayesian predictive distributions.
</summary>
    <author>
      <name>Yutian Chen</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3483v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Regularized Maximum Likelihood for Intrinsic Dimension Estimation</title>
    <summary>  We propose a new method for estimating the intrinsic dimension of a dataset
by applying the principle of regularized maximum likelihood to the distances
between close neighbors. We propose a regularization scheme which is motivated
by divergence minimization principles. We derive the estimator by a Poisson
process approximation, argue about its convergence properties and apply it to a
number of simulated and real datasets. We also show it has the best overall
performance compared with two other intrinsic dimension estimators.
</summary>
    <author>
      <name>Mithun Das Gupta</name>
    </author>
    <author>
      <name>Thomas S. Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3492v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Approximating Higher-Order Distances Using Random Projections</title>
    <summary>  We provide a simple method and relevant theoretical analysis for efficiently
estimating higher-order lp distances. While the analysis mainly focuses on l4,
our methodology extends naturally to p = 6,8,10..., (i.e., when p is even).
Distance-based methods are popular in machine learning. In large-scale
applications, storing, computing, and retrieving the distances can be both
space and time prohibitive. Efficient algorithms exist for estimating lp
distances if 0 &lt; p &lt;= 2. The task for p &gt; 2 is known to be difficult. Our work
partially fills this gap.
</summary>
    <author>
      <name>Ping Li</name>
    </author>
    <author>
      <name>Michael W. Mahoney</name>
    </author>
    <author>
      <name>Yiyuan She</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3496v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Dirichlet Process Mixtures of Generalized Mallows Models</title>
    <summary>  We present a Dirichlet process mixture model over discrete incomplete
rankings and study two Gibbs sampling inference techniques for estimating
posterior clusterings. The first approach uses a slice sampling subcomponent
for estimating cluster parameters. The second approach marginalizes out several
cluster parameters by taking advantage of approximations to the conditional
posteriors. We empirically demonstrate (1) the effectiveness of this
approximation for improving convergence, (2) the benefits of the Dirichlet
process model over alternative clustering techniques for ranked data, and (3)
the applicability of the approach to exploring large realworld ranking
datasets.
</summary>
    <author>
      <name>Marina Meila</name>
    </author>
    <author>
      <name>Harr Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3516v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Modeling Events with Cascades of Poisson Processes</title>
    <summary>  We present a probabilistic model of events in continuous time in which each
event triggers a Poisson process of successor events. The ensemble of observed
events is thereby modeled as a superposition of Poisson processes. Efficient
inference is feasible under this model with an EM algorithm. Moreover, the EM
algorithm can be implemented as a distributed algorithm, permitting the model
to be applied to very large datasets. We apply these techniques to the modeling
of Twitter messages and the revision history of Wikipedia.
</summary>
    <author>
      <name>Aleksandr Simma</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3517v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>A Bayesian Matrix Factorization Model for Relational Data</title>
    <summary>  Relational learning can be used to augment one data source with other
correlated sources of information, to improve predictive accuracy. We frame a
large class of relational learning problems as matrix factorization problems,
and propose a hierarchical Bayesian model. Training our Bayesian model using
random-walk Metropolis-Hastings is impractically slow, and so we develop a
block Metropolis-Hastings sampler which uses the gradient and Hessian of the
likelihood to dynamically tune the proposal. We demonstrate that a predictive
model of brain response to stimuli can be improved by augmenting it with side
information about the stimuli.
</summary>
    <author>
      <name>Ajit P. Singh</name>
    </author>
    <author>
      <name>Geoffrey Gordon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3520v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Bayesian Model Averaging Using the k-best Bayesian Network Structures</title>
    <summary>  We study the problem of learning Bayesian network structures from data. We
develop an algorithm for finding the k-best Bayesian network structures. We
propose to compute the posterior probabilities of hypotheses of interest by
Bayesian model averaging over the k-best Bayesian networks. We present
empirical results on structural discovery over several real and synthetic data
sets and show that the method outperforms the model selection method and the
state of-the-art MCMC methods.
</summary>
    <author>
      <name>Jin Tian</name>
    </author>
    <author>
      <name>Ru He</name>
    </author>
    <author>
      <name>Lavanya Ram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3521v1</id>
    <updated>2012-03-15T11:17:56Z</updated>
    <published>2012-03-15T11:17:56Z</published>
    <title>Learning networks determined by the ratio of prior and data</title>
    <summary>  Recent reports have described that the equivalent sample size (ESS) in a
Dirichlet prior plays an important role in learning Bayesian networks. This
paper provides an asymptotic analysis of the marginal likelihood score for a
Bayesian network. Results show that the ratio of the ESS and sample size
determine the penalty of adding arcs in learning Bayesian networks. The number
of arcs increases monotonically as the ESS increases; the number of arcs
monotonically decreases as the ESS decreases. Furthermore, the marginal
likelihood score provides a unified expression of various score metrics by
changing prior knowledge.
</summary>
    <author>
      <name>Maomi Ueno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4521v1</id>
    <updated>2012-04-20T03:01:56Z</updated>
    <published>2012-04-20T03:01:56Z</published>
    <title>A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster
  Ensembles</title>
    <summary>  This paper introduces a privacy-aware Bayesian approach that combines
ensembles of classifiers and clusterers to perform semi-supervised and
transductive learning. We consider scenarios where instances and their
classification/clustering results are distributed across different data sites
and have sharing restrictions. As a special case, the privacy aware computation
of the model when instances of the target data are distributed across different
data sites, is also discussed. Experimental results show that the proposed
approach can provide good classification accuracies while adhering to the
data/model sharing constraints.
</summary>
    <author>
      <name>Ayan Acharya</name>
    </author>
    <author>
      <name>Eduardo R. Hruschka</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1204.4521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4990v1</id>
    <updated>2012-04-23T08:02:19Z</updated>
    <published>2012-04-23T08:02:19Z</published>
    <title>Objective Function Designing Led by User Preferences Acquisition</title>
    <summary>  Many real world problems can be defined as optimisation problems in which the
aim is to maximise an objective function. The quality of obtained solution is
directly linked to the pertinence of the used objective function. However,
designing such function, which has to translate the user needs, is usually
fastidious. In this paper, a method to help user objective functions designing
is proposed. Our approach, which is highly interactive, is based on man machine
dialogue and more particularly on the comparison of problem instance solutions
by the user. We propose an experiment in the domain of cartographic
generalisation that shows promising results.
</summary>
    <author>
      <name>Patrick Taillandier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UMMISCO</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Gaffuri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">COGIT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Information Technology and Applications,
  Hanoi : Viet Nam (2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2190v1</id>
    <updated>2012-06-11T13:00:51Z</updated>
    <published>2012-06-11T13:00:51Z</published>
    <title>Communication-Efficient Parallel Belief Propagation for Latent Dirichlet
  Allocation</title>
    <summary>  This paper presents a novel communication-efficient parallel belief
propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).
Based on the synchronous belief propagation (BP) algorithm, we first develop a
parallel belief propagation (PBP) algorithm on the parallel architecture.
Because the extensive communication delay often causes a low efficiency of
parallel topic modeling, we further use Zipf's law to reduce the total
communication cost in PBP. Extensive experiments on different data sets
demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces
more than 80% communication cost than the state-of-the-art parallel Gibbs
sampling (PGS) algorithm.
</summary>
    <author>
      <name>Jian-feng Yan</name>
    </author>
    <author>
      <name>Zhi-Qiang Liu</name>
    </author>
    <author>
      <name>Yang Gao</name>
    </author>
    <author>
      <name>Jia Zeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.2190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2691v1</id>
    <updated>2012-06-13T00:27:36Z</updated>
    <published>2012-06-13T00:27:36Z</published>
    <title>IDS: An Incremental Learning Algorithm for Finite Automata</title>
    <summary>  We present a new algorithm IDS for incremental learning of deterministic
finite automata (DFA). This algorithm is based on the concept of distinguishing
sequences introduced in (Angluin81). We give a rigorous proof that two versions
of this learning algorithm correctly learn in the limit. Finally we present an
empirical performance analysis that compares these two algorithms, focussing on
learning times and different types of learning queries. We conclude that IDS is
an efficient algorithm for software engineering applications of automata
learning, such as testing and model inference.
</summary>
    <author>
      <name>Muddassar A. Sindhu</name>
    </author>
    <author>
      <name>Karl Meinke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.2691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3072v1</id>
    <updated>2012-06-14T11:05:55Z</updated>
    <published>2012-06-14T11:05:55Z</published>
    <title>Statistical Consistency of Finite-dimensional Unregularized Linear
  Classification</title>
    <summary>  This manuscript studies statistical properties of linear classifiers obtained
through minimization of an unregularized convex risk over a finite sample.
Although the results are explicitly finite-dimensional, inputs may be passed
through feature maps; in this way, in addition to treating the consistency of
logistic regression, this analysis also handles boosting over a finite weak
learning class with, for instance, the exponential, logistic, and hinge losses.
In this finite-dimensional setting, it is still possible to fit arbitrary
decision boundaries: scaling the complexity of the weak learning class with the
sample size leads to the optimal classification risk almost surely.
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <link href="http://arxiv.org/abs/1206.3072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3241v1</id>
    <updated>2012-06-13T15:04:13Z</updated>
    <published>2012-06-13T15:04:13Z</published>
    <title>Approximating the Partition Function by Deleting and then Correcting for
  Model Edges</title>
    <summary>  We propose an approach for approximating the partition function which is
based on two steps: (1) computing the partition function of a simplified model
which is obtained by deleting model edges, and (2) rectifying the result by
applying an edge-by-edge correction. The approach leads to an intuitive
framework in which one can trade-off the quality of an approximation with the
complexity of computing it. It also includes the Bethe free energy
approximation as a degenerate case. We develop the approach theoretically in
this paper and provide a number of empirical results that reveal its practical
utility.
</summary>
    <author>
      <name>Arthur Choi</name>
    </author>
    <author>
      <name>Adnan Darwiche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3247v1</id>
    <updated>2012-06-13T15:09:01Z</updated>
    <published>2012-06-13T15:09:01Z</published>
    <title>Learning Convex Inference of Marginals</title>
    <summary>  Graphical models trained using maximum likelihood are a common tool for
probabilistic inference of marginal distributions. However, this approach
suffers difficulties when either the inference process or the model is
approximate. In this paper, the inference process is first defined to be the
minimization of a convex function, inspired by free energy approximations.
Learning is then done directly in terms of the performance of the inference
process at univariate marginal prediction. The main novelty is that this is a
direct minimization of emperical risk, where the risk measures the accuracy of
predicted marginals.
</summary>
    <author>
      <name>Justin Domke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3270v1</id>
    <updated>2012-06-13T15:38:07Z</updated>
    <published>2012-06-13T15:38:07Z</published>
    <title>Estimation and Clustering with Infinite Rankings</title>
    <summary>  This paper presents a natural extension of stagewise ranking to the the case
of infinitely many items. We introduce the infinite generalized Mallows model
(IGM), describe its properties and give procedures to estimate it from data.
For estimation of multimodal distributions we introduce the
Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The
experiments highlight the properties of the new model and demonstrate that
infinite models can be simple, elegant and practical.
</summary>
    <author>
      <name>Marina Meila</name>
    </author>
    <author>
      <name>Le Bao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3275v1</id>
    <updated>2012-06-13T15:40:51Z</updated>
    <published>2012-06-13T15:40:51Z</published>
    <title>Learning Hidden Markov Models for Regression using Path Aggregation</title>
    <summary>  We consider the task of learning mappings from sequential data to real-valued
responses. We present and evaluate an approach to learning a type of hidden
Markov model (HMM) for regression. The learning process involves inferring the
structure and parameters of a conventional HMM, while simultaneously learning a
regression model that maps features that characterize paths through the model
to continuous responses. Our results, in both synthetic and biological domains,
demonstrate the value of jointly learning the two components of our approach.
</summary>
    <author>
      <name>Keith Noto</name>
    </author>
    <author>
      <name>Mark Craven</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3721v1</id>
    <updated>2012-06-17T04:40:09Z</updated>
    <published>2012-06-17T04:40:09Z</published>
    <title>Constraint-free Graphical Model with Fast Learning Algorithm</title>
    <summary>  In this paper, we propose a simple, versatile model for learning the
structure and parameters of multivariate distributions from a data set.
Learning a Markov network from a given data set is not a simple problem,
because Markov networks rigorously represent Markov properties, and this rigor
imposes complex constraints on the design of the networks. Our proposed model
removes these constraints, acquiring important aspects from the information
geometry. The proposed parameter- and structure-learning algorithms are simple
to execute as they are based solely on local computation at each node.
Experiments demonstrate that our algorithms work appropriately.
</summary>
    <author>
      <name>Kazuya Takabatake</name>
    </author>
    <author>
      <name>Shotaro Akaho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures, submitted to UAI2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4110v1</id>
    <updated>2012-06-19T02:24:55Z</updated>
    <published>2012-06-19T02:24:55Z</published>
    <title>ConeRANK: Ranking as Learning Generalized Inequalities</title>
    <summary>  We propose a new data mining approach in ranking documents based on the
concept of cone-based generalized inequalities between vectors. A partial
ordering between two vectors is made with respect to a proper cone and thus
learning the preferences is formulated as learning proper cones. A pairwise
learning-to-rank algorithm (ConeRank) is proposed to learn a non-negative
subspace, formulated as a polyhedral cone, over document-pair differences. The
algorithm is regularized by controlling the `volume' of the cone. The
experimental studies on the latest and largest ranking dataset LETOR 4.0 shows
that ConeRank is competitive against other recent ranking approaches.
</summary>
    <author>
      <name>Truyen T. Tran</name>
    </author>
    <author>
      <name>Duc Son Pham</name>
    </author>
    <link href="http://arxiv.org/abs/1206.4110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4606v1</id>
    <updated>2012-06-18T14:43:42Z</updated>
    <published>2012-06-18T14:43:42Z</published>
    <title>TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing
  Multiple Ratings</title>
    <summary>  This paper revisits the problem of analyzing multiple ratings given by
different judges. Different from previous work that focuses on distilling the
true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic
insights into our in-house well-trained judges. We generalize the well-known
DawidSkene model (Dawid &amp; Skene, 1979) to a spectrum of probabilistic models
under the same "TrueLabel + Confusion" paradigm, and show that our proposed
hierarchical Bayesian model, called HybridConfusion, consistently outperforms
DawidSkene on both synthetic and real-world data sets.
</summary>
    <author>
      <name>Chao Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tencent Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Yi-Min Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft Research</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4607v1</id>
    <updated>2012-06-18T14:44:09Z</updated>
    <published>2012-06-18T14:44:09Z</published>
    <title>Distributed Tree Kernels</title>
    <summary>  In this paper, we propose the distributed tree kernels (DTK) as a novel
method to reduce time and space complexity of tree kernels. Using a linear
complexity algorithm to compute vectors for trees, we embed feature spaces of
tree fragments in low-dimensional spaces where the kernel computation is
directly done with dot product. We show that DTKs are faster, correlate with
tree kernels, and obtain a statistically similar performance in two natural
language processing tasks.
</summary>
    <author>
      <name>Fabio Massimo Zanzotto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rome-Tor Vergata</arxiv:affiliation>
    </author>
    <author>
      <name>Lorenzo Dell'Arciprete</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rome-Tor Vergata</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4608v1</id>
    <updated>2012-06-18T14:44:28Z</updated>
    <published>2012-06-18T14:44:28Z</published>
    <title>A Hybrid Algorithm for Convex Semidefinite Optimization</title>
    <summary>  We present a hybrid algorithm for optimizing a convex, smooth function over
the cone of positive semidefinite matrices. Our algorithm converges to the
global optimal solution and can be used to solve general large-scale
semidefinite programs and hence can be readily applied to a variety of machine
learning problems. We show experimental results on three machine learning
problems (matrix completion, metric learning, and sparse PCA) . Our approach
outperforms state-of-the-art algorithms.
</summary>
    <author>
      <name>Soeren Laue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Friedrich-Schiller-University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4617v1</id>
    <updated>2012-06-18T15:02:28Z</updated>
    <published>2012-06-18T15:02:28Z</published>
    <title>Continuous Inverse Optimal Control with Locally Optimal Examples</title>
    <summary>  Inverse optimal control, also known as inverse reinforcement learning, is the
problem of recovering an unknown reward function in a Markov decision process
from expert demonstrations of the optimal policy. We introduce a probabilistic
inverse optimal control algorithm that scales gracefully with task
dimensionality, and is suitable for large, continuous domains where even
computing a full policy is impractical. By using a local approximation of the
reward function, our method can also drop the assumption that the
demonstrations are globally optimal, requiring only local optimality. This
allows it to learn from examples that are unsuitable for prior methods.
</summary>
    <author>
      <name>Sergey Levine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Vladlen Koltun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4620v1</id>
    <updated>2012-06-18T15:04:54Z</updated>
    <published>2012-06-18T15:04:54Z</published>
    <title>Improved Information Gain Estimates for Decision Tree Induction</title>
    <summary>  Ensembles of classification and regression trees remain popular machine
learning methods because they define flexible non-parametric models that
predict well and are computationally efficient both during training and
testing. During induction of decision trees one aims to find predicates that
are maximally informative about the prediction target. To select good
predicates most approaches estimate an information-theoretic scoring function,
the information gain, both for classification and regression problems. We point
out that the common estimation procedures are biased and show that by replacing
them with improved estimators of the discrete and the differential entropy we
can obtain better decision trees. In effect our modifications yield improved
predictive performance and are simple to implement in any decision tree code.
</summary>
    <author>
      <name>Sebastian Nowozin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft Research Cambridge</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4623v1</id>
    <updated>2012-06-18T15:06:34Z</updated>
    <published>2012-06-18T15:06:34Z</published>
    <title>On the Size of the Online Kernel Sparsification Dictionary</title>
    <summary>  We analyze the size of the dictionary constructed from online kernel
sparsification, using a novel formula that expresses the expected determinant
of the kernel Gram matrix in terms of the eigenvalues of the covariance
operator. Using this formula, we are able to connect the cardinality of the
dictionary with the eigen-decay of the covariance operator. In particular, we
show that under certain technical conditions, the size of the dictionary will
always grow sub-linearly in the number of data points, and, as a consequence,
the kernel linear regressor constructed from the resulting dictionary is
consistent.
</summary>
    <author>
      <name>Yi Sun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDSIA</arxiv:affiliation>
    </author>
    <author>
      <name>Faustino Gomez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDSIA</arxiv:affiliation>
    </author>
    <author>
      <name>Juergen Schmidhuber</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDSIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4628v1</id>
    <updated>2012-06-18T15:07:55Z</updated>
    <published>2012-06-18T15:07:55Z</published>
    <title>Robust PCA in High-dimension: A Deterministic Approach</title>
    <summary>  We consider principal component analysis for contaminated data-set in the
high dimensional regime, where the dimensionality of each observation is
comparable or even more than the number of observations. We propose a
deterministic high-dimensional robust PCA algorithm which inherits all
theoretical properties of its randomized counterpart, i.e., it is tractable,
robust to contaminated points, easily kernelizable, asymptotic consistent and
achieves maximal robustness -- a breakdown point of 50%. More importantly, the
proposed method exhibits significantly better computational efficiency, which
makes it suitable for large-scale real applications.
</summary>
    <author>
      <name>Jiashi Feng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NUS</arxiv:affiliation>
    </author>
    <author>
      <name>Huan Xu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NUS</arxiv:affiliation>
    </author>
    <author>
      <name>Shuicheng Yan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NUS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4630v1</id>
    <updated>2012-06-18T15:08:38Z</updated>
    <published>2012-06-18T15:08:38Z</published>
    <title>Efficient Decomposed Learning for Structured Prediction</title>
    <summary>  Structured prediction is the cornerstone of several machine learning
applications. Unfortunately, in structured prediction settings with expressive
inter-variable interactions, exact inference-based learning algorithms, e.g.
Structural SVM, are often intractable. We present a new way, Decomposed
Learning (DecL), which performs efficient learning by restricting the inference
step to a limited part of the structured spaces. We provide characterizations
based on the structure, target parameters, and gold labels, under which DecL is
equivalent to exact learning. We then show that in real world settings, where
our theoretical assumptions may not completely hold, DecL-based algorithms are
significantly more efficient and as accurate as exact learning.
</summary>
    <author>
      <name>Rajhans Samdani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois, U-C</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Roth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois, U-C</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4643v1</id>
    <updated>2012-06-18T15:19:07Z</updated>
    <published>2012-06-18T15:19:07Z</published>
    <title>Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty</title>
    <summary>  We consider Markov decision processes under parameter uncertainty. Previous
studies all restrict to the case that uncertainties among different states are
uncoupled, which leads to conservative solutions. In contrast, we introduce an
intuitive concept, termed "Lightning Does not Strike Twice," to model coupled
uncertain parameters. Specifically, we require that the system can deviate from
its nominal parameters only a bounded number of times. We give probabilistic
guarantees indicating that this model represents real life situations and
devise tractable algorithms for computing optimal control policies using this
concept.
</summary>
    <author>
      <name>Shie Mannor</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion</arxiv:affiliation>
    </author>
    <author>
      <name>Ofir Mebel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion</arxiv:affiliation>
    </author>
    <author>
      <name>Huan Xu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National University of Singapore</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4645v1</id>
    <updated>2012-06-18T15:19:58Z</updated>
    <published>2012-06-18T15:19:58Z</published>
    <title>Ensemble Methods for Convex Regression with Applications to Geometric
  Programming Based Circuit Design</title>
    <summary>  Convex regression is a promising area for bridging statistical estimation and
deterministic convex optimization. New piecewise linear convex regression
methods are fast and scalable, but can have instability when used to
approximate constraints or objective functions for optimization. Ensemble
methods, like bagging, smearing and random partitioning, can alleviate this
problem and maintain the theoretical properties of the underlying estimator. We
empirically examine the performance of ensemble methods for prediction and
optimization, and then apply them to device modeling and constraint
approximation for geometric programming based circuit design.
</summary>
    <author>
      <name>Lauren Hannah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Duke University</arxiv:affiliation>
    </author>
    <author>
      <name>David Dunson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Duke University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4646v1</id>
    <updated>2012-06-18T15:20:14Z</updated>
    <published>2012-06-18T15:20:14Z</published>
    <title>Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings</title>
    <summary>  Stochastic neighbor embedding (SNE) and related nonlinear manifold learning
algorithms achieve high-quality low-dimensional representations of similarity
data, but are notoriously slow to train. We propose a generic formulation of
embedding algorithms that includes SNE and other existing algorithms, and study
their relation with spectral methods and graph Laplacians. This allows us to
define several partial-Hessian optimization strategies, characterize their
global and local convergence, and evaluate them empirically. We achieve up to
two orders of magnitude speedup over existing training methods with a strategy
(which we call the spectral direction) that adds nearly no overhead to the
gradient and yet is simple, scalable and applicable to several existing and
future embedding algorithms.
</summary>
    <author>
      <name>Max Vladymyrov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Merced</arxiv:affiliation>
    </author>
    <author>
      <name>Miguel Carreira-Perpinan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Merced</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4647v1</id>
    <updated>2012-06-18T15:22:24Z</updated>
    <published>2012-06-18T15:22:24Z</published>
    <title>Active Learning for Matching Problems</title>
    <summary>  Effective learning of user preferences is critical to easing user burden in
various types of matching problems. Equally important is active query selection
to further reduce the amount of preference information users must provide. We
address the problem of active learning of user preferences for matching
problems, introducing a novel method for determining probabilistic matchings,
and developing several new active learning strategies that are sensitive to the
specific matching objective. Experiments with real-world data sets spanning
diverse domains demonstrate that matching-sensitive active learning
</summary>
    <author>
      <name>Laurent Charlin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Rich Zemel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Craig Boutilier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4650v1</id>
    <updated>2012-06-18T15:23:37Z</updated>
    <published>2012-06-18T15:23:37Z</published>
    <title>Analysis of Kernel Mean Matching under Covariate Shift</title>
    <summary>  In real supervised learning scenarios, it is not uncommon that the training
and test sample follow different probability distributions, thus rendering the
necessity to correct the sampling bias. Focusing on a particular covariate
shift problem, we derive high probability confidence bounds for the kernel mean
matching (KMM) estimator, whose convergence rate turns out to depend on some
regularity measure of the regression function and also on some capacity measure
of the kernel. By comparing KMM with the natural plug-in estimator, we
establish the superiority of the former hence provide concrete
evidence/understanding to the effectiveness of KMM under covariate shift.
</summary>
    <author>
      <name>Yaoliang Yu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Alberta</arxiv:affiliation>
    </author>
    <author>
      <name>Csaba Szepesvari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Alberta</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4651v1</id>
    <updated>2012-06-18T15:24:01Z</updated>
    <published>2012-06-18T15:24:01Z</published>
    <title>Is margin preserved after random projection?</title>
    <summary>  Random projections have been applied in many machine learning algorithms.
However, whether margin is preserved after random projection is non-trivial and
not well studied. In this paper we analyse margin distortion after random
projection, and give the conditions of margin preservation for binary
classification problems. We also extend our analysis to margin for multiclass
problems, and provide theoretical bounds on multiclass margin on the projected
data.
</summary>
    <author>
      <name>Qinfeng Shi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Adelaide</arxiv:affiliation>
    </author>
    <author>
      <name>Chunhua Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Adelaide</arxiv:affiliation>
    </author>
    <author>
      <name>Rhys Hill</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Adelaide</arxiv:affiliation>
    </author>
    <author>
      <name>Anton van den Hengel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">the University of Adelaide</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4656v1</id>
    <updated>2012-06-18T15:26:13Z</updated>
    <published>2012-06-18T15:26:13Z</published>
    <title>Machine Learning that Matters</title>
    <summary>  Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.
</summary>
    <author>
      <name>Kiri Wagstaff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jet Propulsion Laboratory</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Twenty-Ninth International Conference on
  Machine Learning (ICML), p. 529-536</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.4656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4657v1</id>
    <updated>2012-06-18T15:26:34Z</updated>
    <published>2012-06-18T15:26:34Z</published>
    <title>Projection-free Online Learning</title>
    <summary>  The computational bottleneck in applying online learning to massive data sets
is usually the projection step. We present efficient online learning algorithms
that eschew projections in favor of much more efficient linear optimization
steps using the Frank-Wolfe technique. We obtain a range of regret bounds for
online convex optimization, with better bounds for specific cases such as
stochastic online smooth convex optimization.
  Besides the computational advantage, other desirable features of our
algorithms are that they are parameter-free in the stochastic case and produce
sparse decisions. We apply our algorithms to computationally intensive
applications of collaborative filtering, and show the theoretical improvements
to be clearly visible on standard datasets.
</summary>
    <author>
      <name>Elad Hazan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion</arxiv:affiliation>
    </author>
    <author>
      <name>Satyen Kale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM T.J. Watson Research Center</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4659v1</id>
    <updated>2012-06-18T15:27:56Z</updated>
    <published>2012-06-18T15:27:56Z</published>
    <title>Max-Margin Nonparametric Latent Feature Models for Link Prediction</title>
    <summary>  We present a max-margin nonparametric latent feature model, which unites the
ideas of max-margin learning and Bayesian nonparametrics to discover
discriminative latent features for link prediction and automatically infer the
unknown latent social dimension. By minimizing a hinge-loss using the linear
expectation operator, we can perform posterior inference efficiently without
dealing with a highly nonlinear link likelihood function; by using a
fully-Bayesian formulation, we can avoid tuning regularization constants.
Experimental results on real datasets appear to demonstrate the benefits
inherited from max-margin learning and fully-Bayesian nonparametric inference.
</summary>
    <author>
      <name>Jun Zhu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4661v1</id>
    <updated>2012-06-18T15:30:13Z</updated>
    <published>2012-06-18T15:30:13Z</published>
    <title>Predicting accurate probabilities with a ranking loss</title>
    <summary>  In many real-world applications of machine learning classifiers, it is
essential to predict the probability of an example belonging to a particular
class. This paper proposes a simple technique for predicting probabilities
based on optimizing a ranking loss, followed by isotonic regression. This
semi-parametric technique offers both good ranking and regression performance,
and models a richer set of probability distributions than statistical
workhorses such as logistic regression. We provide experimental results that
show the effectiveness of this technique on real-world applications of
probability prediction.
</summary>
    <author>
      <name>Aditya Menon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC San Diego</arxiv:affiliation>
    </author>
    <author>
      <name>Xiaoqian Jiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC San Diego</arxiv:affiliation>
    </author>
    <author>
      <name>Shankar Vembu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Charles Elkan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC San Diego</arxiv:affiliation>
    </author>
    <author>
      <name>Lucila Ohno-Machado</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC San Diego</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4663v1</id>
    <updated>2012-06-18T15:30:52Z</updated>
    <published>2012-06-18T15:30:52Z</published>
    <title>The Convexity and Design of Composite Multiclass Losses</title>
    <summary>  We consider composite loss functions for multiclass prediction comprising a
proper (i.e., Fisher-consistent) loss over probability distributions and an
inverse link function. We establish conditions for their (strong) convexity and
explore the implications. We also show how the separation of concerns afforded
by using this composite representation allows for the design of families of
losses with the same Bayes risk.
</summary>
    <author>
      <name>Mark Reid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Australian National University and NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Williamson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Australian National University and NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>Peng Sun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4664v1</id>
    <updated>2012-06-18T15:31:13Z</updated>
    <published>2012-06-18T15:31:13Z</published>
    <title>Tighter Variational Representations of f-Divergences via Restriction to
  Probability Measures</title>
    <summary>  We show that the variational representations for f-divergences currently used
in the literature can be tightened. This has implications to a number of
methods recently proposed based on this representation. As an example
application we use our tighter representation to derive a general f-divergence
estimator based on two i.i.d. samples and derive the dual program for this
estimator that performs well empirically. We also point out a connection
between our estimator and MMD.
</summary>
    <author>
      <name>Avraham Ruderman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Australian National University and NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>Mark Reid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Australian National University and NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>Dario Garcia-Garcia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Australian National University and NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>James Petterson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4671v1</id>
    <updated>2012-06-18T15:35:02Z</updated>
    <published>2012-06-18T15:35:02Z</published>
    <title>Dependent Hierarchical Normalized Random Measures for Dynamic Topic
  Modeling</title>
    <summary>  We develop dependent hierarchical normalized random measures and apply them
to dynamic topic modeling. The dependency arises via superposition, subsampling
and point transition on the underlying Poisson processes of these measures. The
measures used include normalised generalised Gamma processes that demonstrate
power law properties, unlike Dirichlet processes used previously in dynamic
topic modeling. Inference for the model includes adapting a recently developed
slice sampler to directly manipulate the underlying Poisson process.
Experiments performed on news, blogs, academic and Twitter collections
demonstrate the technique gives superior perplexity over a number of previous
models.
</summary>
    <author>
      <name>Changyou Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANU &amp; NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>Nan Ding</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Purdue University</arxiv:affiliation>
    </author>
    <author>
      <name>Wray Buntine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4674v1</id>
    <updated>2012-06-18T15:36:16Z</updated>
    <published>2012-06-18T15:36:16Z</published>
    <title>Comparison-Based Learning with Rank Nets</title>
    <summary>  We consider the problem of search through comparisons, where a user is
presented with two candidate objects and reveals which is closer to her
intended target. We study adaptive strategies for finding the target, that
require knowledge of rank relationships but not actual distances between
objects. We propose a new strategy based on rank nets, and show that for target
distributions with a bounded doubling constant, it finds the target in a number
of comparisons close to the entropy of the target distribution and, hence, of
the optimum. We extend these results to the case of noisy oracles, and compare
this strategy to prior art over multiple datasets.
</summary>
    <author>
      <name>Amin Karbasi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EPFL</arxiv:affiliation>
    </author>
    <author>
      <name>Stratis Ioannidis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technicolor</arxiv:affiliation>
    </author>
    <author>
      <name>laurent Massoulie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technicolor</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4677v1</id>
    <updated>2012-06-18T15:37:07Z</updated>
    <published>2012-06-18T15:37:07Z</published>
    <title>Semi-Supervised Learning of Class Balance under Class-Prior Change by
  Distribution Matching</title>
    <summary>  In real-world classification problems, the class balance in the training
dataset does not necessarily reflect that of the test dataset, which can cause
significant estimation bias. If the class ratio of the test dataset is known,
instance re-weighting or resampling allows systematical bias correction.
However, learning the class ratio of the test dataset is challenging when no
labeled data is available from the test domain. In this paper, we propose to
estimate the class ratio in the test dataset by matching probability
distributions of training and test input data. We demonstrate the utility of
the proposed approach through experiments.
</summary>
    <author>
      <name>Marthinus Du Plessis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tokyo Institute of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Masashi Sugiyama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tokyo Institute of Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4678v1</id>
    <updated>2012-06-18T15:37:23Z</updated>
    <published>2012-06-18T15:37:23Z</published>
    <title>Linear Regression with Limited Observation</title>
    <summary>  We consider the most common variants of linear regression, including Ridge,
Lasso and Support-vector regression, in a setting where the learner is allowed
to observe only a fixed number of attributes of each example at training time.
We present simple and efficient algorithms for these problems: for Lasso and
Ridge regression they need the same total number of attributes (up to
constants) as do full-information algorithms, for reaching a certain accuracy.
For Support-vector regression, we require exponentially less attributes
compared to the state of the art. By that, we resolve an open problem recently
posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to
be justified by superior performance compared to the state of the art.
</summary>
    <author>
      <name>Elad Hazan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion</arxiv:affiliation>
    </author>
    <author>
      <name>Tomer Koren</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4681v1</id>
    <updated>2012-06-18T15:40:11Z</updated>
    <published>2012-06-18T15:40:11Z</published>
    <title>LPQP for MAP: Putting LP Solvers to Better Use</title>
    <summary>  MAP inference for general energy functions remains a challenging problem.
While most efforts are channeled towards improving the linear programming (LP)
based relaxation, this work is motivated by the quadratic programming (QP)
relaxation. We propose a novel MAP relaxation that penalizes the
Kullback-Leibler divergence between the LP pairwise auxiliary variables, and QP
equivalent terms given by the product of the unaries. We develop two efficient
algorithms based on variants of this relaxation. The algorithms minimize the
non-convex objective using belief propagation and dual decomposition as
building blocks. Experiments on synthetic and real-world data show that the
solutions returned by our algorithms substantially improve over the LP
relaxation.
</summary>
    <author>
      <name>Patrick Pletscher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETH Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>Sharon Wulff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETH Zurich</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5162v2</id>
    <updated>2012-12-04T19:35:34Z</updated>
    <published>2012-06-22T14:36:15Z</published>
    <title>Fast Variational Inference in the Conjugate Exponential Family</title>
    <summary>  We present a general method for deriving collapsed variational inference
algo- rithms for probabilistic models in the conjugate exponential family. Our
method unifies many existing approaches to collapsed variational inference. Our
collapsed variational inference leads to a new lower bound on the marginal
likelihood. We exploit the information geometry of the bound to derive much
faster optimization methods based on conjugate gradients for these models. Our
approach is very general and is easily applied to any model where the mean
field update equations have been derived. Empirically we show significant
speed-ups for probabilistic models optimized using our bound.
</summary>
    <author>
      <name>James Hensman</name>
    </author>
    <author>
      <name>Magnus Rattray</name>
    </author>
    <author>
      <name>Neil D. Lawrence</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NIPS 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.5162v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5162v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5248v1</id>
    <updated>2012-06-20T14:55:04Z</updated>
    <published>2012-06-20T14:55:04Z</published>
    <title>Statistical Translation, Heat Kernels and Expected Distances</title>
    <summary>  High dimensional structured data such as text and images is often poorly
understood and misrepresented in statistical modeling. The standard histogram
representation suffers from high variance and performs poorly in general. We
explore novel connections between statistical translation, heat kernels on
manifolds and graphs, and expected distances. These connections provide a new
framework for unsupervised metric learning for text documents. Experiments
indicate that the resulting distances are generally superior to their more
standard counterparts.
</summary>
    <author>
      <name>Joshua Dillon</name>
    </author>
    <author>
      <name>Yi Mao</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.5248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5274v1</id>
    <updated>2012-06-20T15:06:08Z</updated>
    <published>2012-06-20T15:06:08Z</published>
    <title>On Discarding, Caching, and Recalling Samples in Active Learning</title>
    <summary>  We address challenges of active learning under scarce informational resources
in non-stationary environments. In real-world settings, data labeled and
integrated into a predictive model may become invalid over time. However, the
data can become informative again with switches in context and such changes may
indicate unmodeled cyclic or other temporal dynamics. We explore principles for
discarding, caching, and recalling labeled data points in active learning based
on computations of value of information. We review key concepts and study the
value of the methods via investigations of predictive performance and costs of
acquiring data for simulated and real-world data sets.
</summary>
    <author>
      <name>Ashish Kapoor</name>
    </author>
    <author>
      <name>Eric J. Horvitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.5274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5533v2</id>
    <updated>2012-09-16T17:49:12Z</updated>
    <published>2012-06-24T19:17:35Z</published>
    <title>Practical recommendations for gradient-based training of deep
  architectures</title>
    <summary>  Learning algorithms related to artificial neural networks and in particular
for Deep Learning may seem to involve many bells and whistles, called
hyper-parameters. This chapter is meant as a practical guide with
recommendations for some of the most commonly used hyper-parameters, in
particular in the context of learning algorithms based on back-propagated
gradient and gradient-based optimization. It also discusses how to deal with
the fact that more interesting results can be obtained when allowing one to
adjust many hyper-parameters. Overall, it describes elements of the practice
used to successfully and efficiently train and debug large-scale and often deep
multi-layer neural networks. It closes with open questions about the training
difficulties observed with deeper architectures.
</summary>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5533v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5533v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5766v4</id>
    <updated>2012-10-28T07:03:15Z</updated>
    <published>2012-06-25T18:49:44Z</published>
    <title>Learning mixtures of spherical Gaussians: moment methods and spectral
  decompositions</title>
    <summary>  This work provides a computationally efficient and statistically consistent
moment-based estimator for mixtures of spherical Gaussians. Under the condition
that component means are in general position, a simple spectral decomposition
technique yields consistent parameter estimates from low-order observable
moments, without additional minimum separation assumptions needed by previous
computationally efficient estimation procedures. Thus computational and
information-theoretic barriers to efficient estimation in mixture models are
precluded when the mixture components have means in general position and
spherical covariances. Some connections are made to estimation problems related
to independent component analysis.
</summary>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <author>
      <name>Sham M. Kakade</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5766v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5766v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5882v1</id>
    <updated>2012-06-26T05:10:36Z</updated>
    <published>2012-06-26T05:10:36Z</published>
    <title>Exact Recovery of Sparsely-Used Dictionaries</title>
    <summary>  We consider the problem of learning sparsely used dictionaries with an
arbitrary square dictionary and a random, sparse coefficient matrix. We prove
that $O (n \log n)$ samples are sufficient to uniquely determine the
coefficient matrix. Based on this proof, we design a polynomial-time algorithm,
called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that
it probably recovers the dictionary and coefficient matrix when the coefficient
matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the
true dictionary as well as the coefficients with probability higher than many
state-of-the-art algorithms.
</summary>
    <author>
      <name>Daniel A. Spielman</name>
    </author>
    <author>
      <name>Huan Wang</name>
    </author>
    <author>
      <name>John Wright</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6015v1</id>
    <updated>2012-06-26T14:56:33Z</updated>
    <published>2012-06-26T14:56:33Z</published>
    <title>Transductive Classification Methods for Mixed Graphs</title>
    <summary>  In this paper we provide a principled approach to solve a transductive
classification problem involving a similar graph (edges tend to connect nodes
with same labels) and a dissimilar graph (edges tend to connect nodes with
opposing labels). Most of the existing methods, e.g., Information
Regularization (IR), Weighted vote Relational Neighbor classifier (WvRN) etc,
assume that the given graph is only a similar graph. We extend the IR and WvRN
methods to deal with mixed graphs. We evaluate the proposed extensions on
several benchmark datasets as well as two real world datasets and demonstrate
the usefulness of our ideas.
</summary>
    <author>
      <name>Sundararajan Sellamanickam</name>
    </author>
    <author>
      <name>Sathiya Keerthi Selvaraj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 2 Tables, 2 Figures, KDD Workshop - MLG'11 San Diego, CA,
  USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6141v1</id>
    <updated>2012-06-26T23:39:00Z</updated>
    <published>2012-06-26T23:39:00Z</published>
    <title>Directed Time Series Regression for Control</title>
    <summary>  We propose directed time series regression, a new approach to estimating
parameters of time-series models for use in certainty equivalent model
predictive control. The approach combines merits of least squares regression
and empirical optimization. Through a computational study involving a
stochastic version of a well known inverted pendulum balancing problem, we
demonstrate that directed time series regression can generate significant
improvements in controller performance over either of the aforementioned
alternatives.
</summary>
    <author>
      <name>Yi-Hao Kao</name>
    </author>
    <author>
      <name>Benjamin Van Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1206.6141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6383v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Feature Selection via Probabilistic Outputs</title>
    <summary>  This paper investigates two feature-scoring criteria that make use of
estimated class probabilities: one method proposed by \citet{shen} and a
complementary approach proposed below. We develop a theoretical framework to
analyze each criterion and show that both estimate the spread (across all
values of a given feature) of the probability that an example belongs to the
positive class. Based on our analysis, we predict when each scoring technique
will be advantageous over the other and give empirical results validating our
predictions.
</summary>
    <author>
      <name>Andrea Danyluk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Williams College</arxiv:affiliation>
    </author>
    <author>
      <name>Nicholas Arnosti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6392v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Modeling Temporal Dependencies in High-Dimensional Sequences:
  Application to Polyphonic Music Generation and Transcription</title>
    <summary>  We investigate the problem of modeling symbolic sequences of polyphonic music
in a completely general piano-roll representation. We introduce a probabilistic
model based on distribution estimators conditioned on a recurrent neural
network that is able to discover temporal dependencies in high-dimensional
sequences. Our approach outperforms many traditional models of polyphonic music
on a variety of realistic datasets. We show how our musical language model can
serve as a symbolic prior to improve the accuracy of polyphonic transcription.
</summary>
    <author>
      <name>Nicolas Boulanger-Lewandowski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite de Montreal</arxiv:affiliation>
    </author>
    <author>
      <name>Yoshua Bengio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite de Montreal</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Vincent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite de Montreal</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6405v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Bounded Planning in Passive POMDPs</title>
    <summary>  In Passive POMDPs actions do not affect the world state, but still incur
costs. When the agent is bounded by information-processing constraints, it can
only keep an approximation of the belief. We present a variational principle
for the problem of maintaining the information which is most useful for
minimizing the cost, and introduce an efficient and simple algorithm for
finding an optimum.
</summary>
    <author>
      <name>Roy Fox</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hebrew University</arxiv:affiliation>
    </author>
    <author>
      <name>Naftali Tishby</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hebrew University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6409v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Scaling Up Coordinate Descent Algorithms for Large $\ell_1$
  Regularization Problems</title>
    <summary>  We present a generic framework for parallel coordinate descent (CD)
algorithms that includes, as special cases, the original sequential algorithms
Cyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm.
We introduce two novel parallel algorithms that are also special
cases---Thread-Greedy CD and Coloring-Based CD---and give performance
measurements for an OpenMP implementation of these.
</summary>
    <author>
      <name>Chad Scherrer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Pacific Northwest National Lab</arxiv:affiliation>
    </author>
    <author>
      <name>Mahantesh Halappanavar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Pacific Northwest National Lab</arxiv:affiliation>
    </author>
    <author>
      <name>Ambuj Tewari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Texas</arxiv:affiliation>
    </author>
    <author>
      <name>David Haglin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Pacific Northwest National Lab</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6410v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>On the Partition Function and Random Maximum A-Posteriori Perturbations</title>
    <summary>  In this paper we relate the partition function to the max-statistics of
random variables. In particular, we provide a novel framework for approximating
and bounding the partition function using MAP inference on randomly perturbed
models. As a result, we can use efficient MAP solvers such as graph-cuts to
evaluate the corresponding partition function. We show that our method excels
in the typical "high signal - high coupling" regime that results in ragged
energy landscapes difficult for alternative approaches.
</summary>
    <author>
      <name>Tamir Hazan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TTIC</arxiv:affiliation>
    </author>
    <author>
      <name>Tommi Jaakkola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6420v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Distributed Parameter Estimation via Pseudo-likelihood</title>
    <summary>  Estimating statistical models within sensor networks requires distributed
algorithms, in which both data and computation are distributed across the nodes
of the network. We propose a general approach for distributed learning based on
combining local estimators defined by pseudo-likelihood components,
encompassing a number of combination methods, and provide both theoretical and
experimental analysis. We show that simple linear combination or max-voting
methods, when combined with second-order information, are statistically
competitive with more advanced and costly joint optimization. Our algorithms
have many attractive properties including low communication and computational
cost and "any-time" behavior.
</summary>
    <author>
      <name>Qiang Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Irvine</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander Ihler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Irvine</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6425v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Sparse Stochastic Inference for Latent Dirichlet allocation</title>
    <summary>  We present a hybrid algorithm for Bayesian topic models that combines the
efficiency of sparse Gibbs sampling with the scalability of online stochastic
inference. We used our algorithm to analyze a corpus of 1.2 million books (33
billion words) with thousands of topics. Our approach reduces the bias of
variational inference and generalizes to many Bayesian hidden-variable models.
</summary>
    <author>
      <name>David Mimno</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Princeton University</arxiv:affiliation>
    </author>
    <author>
      <name>Matt Hoffman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>David Blei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Princeton University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6435v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Rethinking Collapsed Variational Bayes Inference for LDA</title>
    <summary>  We propose a novel interpretation of the collapsed variational Bayes
inference with a zero-order Taylor expansion approximation, called CVB0
inference, for latent Dirichlet allocation (LDA). We clarify the properties of
the CVB0 inference by using the alpha-divergence. We show that the CVB0
inference is composed of two different divergence projections: alpha=1 and -1.
This interpretation will help shed light on CVB0 works.
</summary>
    <author>
      <name>Issei Sato</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Tokyo</arxiv:affiliation>
    </author>
    <author>
      <name>Hiroshi Nakagawa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Tokyo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6436v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Efficient Structured Prediction with Latent Variables for General
  Graphical Models</title>
    <summary>  In this paper we propose a unified framework for structured prediction with
latent variables which includes hidden conditional random fields and latent
structured support vector machines as special cases. We describe a local
entropy approximation for this general formulation using duality, and derive an
efficient message passing algorithm that is guaranteed to converge. We
demonstrate its effectiveness in the tasks of image segmentation as well as 3D
indoor scene understanding from single images, showing that our approach is
superior to latent structured support vector machines and hidden conditional
random fields.
</summary>
    <author>
      <name>Alexander Schwing</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETH Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>Tamir Hazan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TTIC</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Pollefeys</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETH Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>Raquel Urtasun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TTIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6442v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Minimizing The Misclassification Error Rate Using a Surrogate Convex
  Loss</title>
    <summary>  We carefully study how well minimizing convex surrogate loss functions,
corresponds to minimizing the misclassification error rate for the problem of
binary classification with linear predictors. In particular, we show that
amongst all convex surrogate losses, the hinge loss gives essentially the best
possible bound, of all convex loss functions, for the misclassification error
rate of the resulting linear predictor in terms of the best possible margin
error rate. We also provide lower bounds for specific convex surrogates that
show how different commonly used losses qualitatively differ from each other.
</summary>
    <author>
      <name>Shai Ben-David</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Waterloo</arxiv:affiliation>
    </author>
    <author>
      <name>David Loker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Waterloo</arxiv:affiliation>
    </author>
    <author>
      <name>Nathan Srebro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TTIC</arxiv:affiliation>
    </author>
    <author>
      <name>Karthik Sridharan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pennsylvania</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6453v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Adaptive Canonical Correlation Analysis Based On Matrix Manifolds</title>
    <summary>  In this paper, we formulate the Canonical Correlation Analysis (CCA) problem
on matrix manifolds. This framework provides a natural way for dealing with
matrix constraints and tools for building efficient algorithms even in an
adaptive setting. Finally, an adaptive CCA algorithm is proposed and applied to
a change detection problem in EEG signals.
</summary>
    <author>
      <name>Florian Yger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Maxime Berar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Gasso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSA de Rouen</arxiv:affiliation>
    </author>
    <author>
      <name>Alain Rakotomamonjy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSA de Rouen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6470v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>A Combinatorial Algebraic Approach for the Identifiability of Low-Rank
  Matrix Completion</title>
    <summary>  In this paper, we review the problem of matrix completion and expose its
intimate relations with algebraic geometry, combinatorics and graph theory. We
present the first necessary and sufficient combinatorial conditions for
matrices of arbitrary rank to be identifiable from a set of matrix entries,
yielding theoretical constraints and new algorithms for the problem of matrix
completion. We conclude by algorithmically evaluating the tightness of the
given conditions and algorithms for practically relevant matrix sizes, showing
that the algebraic-combinatoric approach can lead to improvements over
state-of-the-art matrix completion methods.
</summary>
    <author>
      <name>Franz Kiraly</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Berlin</arxiv:affiliation>
    </author>
    <author>
      <name>Ryota Tomioka</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Tokyo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6813v1</id>
    <updated>2012-06-27T15:36:47Z</updated>
    <published>2012-06-27T15:36:47Z</published>
    <title>A concentration theorem for projections</title>
    <summary>  X in R^D has mean zero and finite second moments. We show that there is a
precise sense in which almost all linear projections of X into R^d (for d &lt; D)
look like a scale-mixture of spherical Gaussians -- specifically, a mixture of
distributions N(0, sigma^2 I_d) where the weight of the particular sigma
component is P (| X |^2 = sigma^2 D). The extent of this effect depends upon
the ratio of d to D, and upon a particular coefficient of eccentricity of X's
distribution. We explore this result in a variety of experiments.
</summary>
    <author>
      <name>Sanjoy Dasgupta</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <author>
      <name>Nakul Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6846v1</id>
    <updated>2012-06-27T16:23:17Z</updated>
    <published>2012-06-27T16:23:17Z</published>
    <title>Approximate Separability for Weak Interaction in Dynamic Systems</title>
    <summary>  One approach to monitoring a dynamic system relies on decomposition of the
system into weakly interacting subsystems. An earlier paper introduced a notion
of weak interaction called separability, and showed that it leads to exact
propagation of marginals for prediction. This paper addresses two questions
left open by the earlier paper: can we define a notion of approximate
separability that occurs naturally in practice, and do separability and
approximate separability lead to accurate monitoring? The answer to both
questions is afirmative. The paper also analyzes the structure of approximately
separable decompositions, and provides some explanation as to why these models
perform well.
</summary>
    <author>
      <name>Avi Pfeffer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6847v1</id>
    <updated>2012-06-27T16:23:41Z</updated>
    <published>2012-06-27T16:23:41Z</published>
    <title>Identifying the Relevant Nodes Without Learning the Model</title>
    <summary>  We propose a method to identify all the nodes that are relevant to compute
all the conditional probability distributions for a given set of nodes. Our
method is simple, effcient, consistent, and does not require learning a
Bayesian network first. Therefore, our method can be applied to
high-dimensional databases, e.g. gene expression databases.
</summary>
    <author>
      <name>Jose M. Pena</name>
    </author>
    <author>
      <name>Roland Nilsson</name>
    </author>
    <author>
      <name>Johan Björkegren</name>
    </author>
    <author>
      <name>Jesper Tegnér</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6860v1</id>
    <updated>2012-06-27T16:27:25Z</updated>
    <published>2012-06-27T16:27:25Z</published>
    <title>Predicting Conditional Quantiles via Reduction to Classification</title>
    <summary>  We show how to reduce the process of predicting general order statistics (and
the median in particular) to solving classification. The accompanying
theoretical statement shows that the regret of the classifier bounds the regret
of the quantile regression under a quantile loss. We also test this reduction
empirically against existing quantile regression methods on large real-world
datasets and discover that it provides state-of-the-art performance.
</summary>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Roberto Oliveira</name>
    </author>
    <author>
      <name>Bianca Zadrozny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6863v1</id>
    <updated>2012-06-27T16:28:18Z</updated>
    <published>2012-06-27T16:28:18Z</published>
    <title>Bayesian Multicategory Support Vector Machines</title>
    <summary>  We show that the multi-class support vector machine (MSVM) proposed by Lee
et. al. (2004), can be viewed as a MAP estimation procedure under an
appropriate probabilistic interpretation of the classifier. We also show that
this interpretation can be extended to a hierarchical Bayesian architecture and
to a fully-Bayesian inference procedure for multi-class classification based on
data augmentation. We present empirical results that show that the advantages
of the Bayesian formalism are obtained without a loss in classification
accuracy.
</summary>
    <author>
      <name>Zhihua Zhang</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.0166v3</id>
    <updated>2013-01-16T19:19:34Z</updated>
    <published>2012-06-30T23:07:03Z</published>
    <title>On Multilabel Classification and Ranking with Partial Feedback</title>
    <summary>  We present a novel multilabel/ranking algorithm working in partial
information settings. The algorithm is based on 2nd-order descent methods, and
relies on upper-confidence bounds to trade-off exploration and exploitation. We
analyze this algorithm in a partial adversarial setting, where covariates can
be adversarial, but multilabel probabilities are ruled by (generalized) linear
models. We show O(T^{1/2} log T) regret bounds, which improve in several ways
on the existing results. We test the effectiveness of our upper-confidence
scheme by contrasting against full-information baselines on real-world
multilabel datasets, often obtaining comparable performance.
</summary>
    <author>
      <name>Claudio Gentile</name>
    </author>
    <author>
      <name>Francesco Orabona</name>
    </author>
    <link href="http://arxiv.org/abs/1207.0166v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.0166v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1358v1</id>
    <updated>2012-07-04T12:14:50Z</updated>
    <published>2012-07-04T12:14:50Z</published>
    <title>Unsupervised spectral learning</title>
    <summary>  In spectral clustering and spectral image segmentation, the data is partioned
starting from a given matrix of pairwise similarities S. the matrix S is
constructed by hand, or learned on a separate training set. In this paper we
show how to achieve spectral clustering in unsupervised mode. Our algorithm
starts with a set of observed pairwise features, which are possible components
of an unknown, parametric similarity function. This function is learned
iteratively, at the same time as the clustering of the data. The algorithm
shows promosing results on synthetic and real data.
</summary>
    <author>
      <name>Susan Shortreed</name>
    </author>
    <author>
      <name>Marina Meila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.1358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3790v1</id>
    <updated>2012-07-16T08:49:34Z</updated>
    <published>2012-07-16T08:49:34Z</published>
    <title>Accuracy Measures for the Comparison of Classifiers</title>
    <summary>  The selection of the best classification algorithm for a given dataset is a
very widespread problem. It is also a complex one, in the sense it requires to
make several important methodological choices. Among them, in this work we
focus on the measure used to assess the classification performance and rank the
algorithms. We present the most popular measures and discuss their properties.
Despite the numerous measures proposed over the years, many of them turn out to
be equivalent in this specific case, to have interpretation problems, or to be
unsuitable for our purpose. Consequently, classic overall success rate or
marginal rates should be preferred for this specific task.
</summary>
    <author>
      <name>Vincent Labatut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">BIT Lab</arxiv:affiliation>
    </author>
    <author>
      <name>Hocine Cherifi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Le2i</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Conference on Information Technology, amman :
  Jordanie (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4112v1</id>
    <updated>2012-07-11T14:42:26Z</updated>
    <published>2012-07-11T14:42:26Z</published>
    <title>Algebraic Statistics in Model Selection</title>
    <summary>  We develop the necessary theory in computational algebraic geometry to place
Bayesian networks into the realm of algebraic statistics. We present an
algebra{statistics dictionary focused on statistical modeling. In particular,
we link the notion of effiective dimension of a Bayesian network with the
notion of algebraic dimension of a variety. We also obtain the independence and
non{independence constraints on the distributions over the observable variables
implied by a Bayesian network with hidden variables, via a generating set of an
ideal of polynomials associated to the network. These results extend previous
work on the subject. Finally, the relevance of these results for model
selection is discussed.
</summary>
    <author>
      <name>Luis David Garcia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4113v1</id>
    <updated>2012-07-11T14:42:45Z</updated>
    <published>2012-07-11T14:42:45Z</published>
    <title>On-line Prediction with Kernels and the Complexity Approximation
  Principle</title>
    <summary>  The paper describes an application of Aggregating Algorithm to the problem of
regression. It generalizes earlier results concerned with plain linear
regression to kernel techniques and presents an on-line algorithm which
performs nearly as well as any oblivious kernel predictor. The paper contains
the derivation of an estimate on the performance of this algorithm. The
estimate is then used to derive an application of the Complexity Approximation
Principle to kernel methods.
</summary>
    <author>
      <name>Alex Gammerman</name>
    </author>
    <author>
      <name>Yuri Kalnishkan</name>
    </author>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4131v1</id>
    <updated>2012-07-11T14:48:54Z</updated>
    <published>2012-07-11T14:48:54Z</published>
    <title>Exponential Families for Conditional Random Fields</title>
    <summary>  In this paper we de ne conditional random elds in reproducing kernel Hilbert
spaces and show connections to Gaussian Process classi cation. More speci
cally, we prove decomposition results for undirected graphical models and we
give constructions for kernels. Finally we present e cient means of solving the
optimization problem using reduced rank decompositions and we show how
stationarity can be exploited e ciently in the optimization process.
</summary>
    <author>
      <name>Yasemin Altun</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <author>
      <name>Thomas Hofmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4139v1</id>
    <updated>2012-07-11T14:53:33Z</updated>
    <published>2012-07-11T14:53:33Z</published>
    <title>An Extended Cencov-Campbell Characterization of Conditional Information
  Geometry</title>
    <summary>  We formulate and prove an axiomatic characterization of conditional
information geometry, for both the normalized and the nonnormalized cases. This
characterization extends the axiomatic derivation of the Fisher geometry by
Cencov and Campbell to the cone of positive conditional models, and as a
special case to the manifold of conditional distributions. Due to the close
connection between the conditional I-divergence and the product Fisher
information metric the characterization provides a new axiomatic interpretation
of the primal problems underlying logistic regression and AdaBoost.
</summary>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4404v1</id>
    <updated>2012-07-18T16:07:36Z</updated>
    <published>2012-07-18T16:07:36Z</published>
    <title>Better Mixing via Deep Representations</title>
    <summary>  It has previously been hypothesized, and supported with some experimental
evidence, that deeper representations, when well trained, tend to do a better
job at disentangling the underlying factors of variation. We study the
following related conjecture: better representations, in the sense of better
disentangling, can be exploited to produce faster-mixing Markov chains.
Consequently, mixing would be more efficient at higher levels of
representation. To better understand why and how this is happening, we propose
a secondary conjecture: the higher-level samples fill more uniformly the space
they occupy and the high-density manifolds tend to unfold when represented at
higher levels. The paper discusses these hypotheses and tests them
experimentally through visualization and measurements of mixing and
interpolating between samples.
</summary>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Grégoire Mesnil</name>
    </author>
    <author>
      <name>Yann Dauphin</name>
    </author>
    <author>
      <name>Salah Rifai</name>
    </author>
    <link href="http://arxiv.org/abs/1207.4404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2523v1</id>
    <updated>2012-08-13T08:30:14Z</updated>
    <published>2012-08-13T08:30:14Z</published>
    <title>Path Integral Control by Reproducing Kernel Hilbert Space Embedding</title>
    <summary>  We present an embedding of stochastic optimal control problems, of the so
called path integral form, into reproducing kernel Hilbert spaces. Using
consistent, sample based estimates of the embedding leads to a model free,
non-parametric approach for calculation of an approximate solution to the
control problem. This formulation admits a decomposition of the problem into an
invariant and task dependent component. Consequently, we make much more
efficient use of the sample data compared to previous sample based approaches
in this domain, e.g., by allowing sample re-use across tasks. Numerical
examples on test problems, which illustrate the sample efficiency, are
provided.
</summary>
    <author>
      <name>Konrad Rawlik</name>
    </author>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <author>
      <name>Sethu Vijayakumar</name>
    </author>
    <link href="http://arxiv.org/abs/1208.2523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3689v1</id>
    <updated>2012-08-17T20:58:20Z</updated>
    <published>2012-08-17T20:58:20Z</published>
    <title>An improvement direction for filter selection techniques using
  information theory measures and quadratic optimization</title>
    <summary>  Filter selection techniques are known for their simplicity and efficiency.
However this kind of methods doesn't take into consideration the features
inter-redundancy. Consequently the un-removed redundant features remain in the
final classification model, giving lower generalization performance. In this
paper we propose to use a mathematical optimization method that reduces
inter-features redundancy and maximize relevance between each feature and the
target variable.
</summary>
    <author>
      <name>Waad Bouaguel</name>
    </author>
    <author>
      <name>Ghazi Bel Mufti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 tables, (IJARAI) International Journal of Advanced
  Research in Artificial Intelligence</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Research in Artificial
  Intelligence, 1:7-11, 8 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.3689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2501v1</id>
    <updated>2012-09-12T05:28:32Z</updated>
    <published>2012-09-12T05:28:32Z</published>
    <title>Performance Evaluation of Predictive Classifiers For Knowledge Discovery
  From Engineering Materials Data Sets</title>
    <summary>  In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are
successively applied on materials informatics to classify the engineering
materials into different classes for the selection of materials that suit the
input design specifications. Here, the classifiers are analyzed individually
and their performance evaluation is analyzed with confusion matrix predictive
parameters and standard measures, the classification results are analyzed on
different class of materials. Comparison of classifiers has found that naive
Bayesian classifier is more accurate and better than the C4.5 DTC. The
knowledge discovered by the naive bayesian classifier can be employed for
decision making in materials selection in manufacturing industries.
</summary>
    <author>
      <name>Hemanth K. S Doreswamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Volume 3,No 3,March 2011 7 pages, 6 tables, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.2501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2673v2</id>
    <updated>2012-09-24T18:28:44Z</updated>
    <published>2012-09-12T17:39:37Z</published>
    <title>Conditional validity of inductive conformal predictors</title>
    <summary>  Conformal predictors are set predictors that are automatically valid in the
sense of having coverage probability equal to or exceeding a given confidence
level. Inductive conformal predictors are a computationally efficient version
of conformal predictors satisfying the same property of validity. However,
inductive conformal predictors have been only known to control unconditional
coverage probability. This paper explores various versions of conditional
validity and various ways to achieve them using inductive conformal predictors
and their modifications.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 9 figures, 2 tables; to appear in the ACML 2012 Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.2673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 62G15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2693v1</id>
    <updated>2012-09-12T19:14:21Z</updated>
    <published>2012-09-12T19:14:21Z</published>
    <title>Regret Bounds for Restless Markov Bandits</title>
    <summary>  We consider the restless Markov bandit problem, in which the state of each
arm evolves according to a Markov process independently of the learner's
actions. We suggest an algorithm that after $T$ steps achieves
$\tilde{O}(\sqrt{T})$ regret with respect to the best policy that knows the
distributions of all arms. No assumptions on the Markov chains are made except
that they are irreducible. In addition, we show that index-based policies are
necessarily suboptimal for the considered problem.
</summary>
    <author>
      <name>Ronald Ortner</name>
    </author>
    <author>
      <name>Daniil Ryabko</name>
    </author>
    <author>
      <name>Peter Auer</name>
    </author>
    <author>
      <name>Rémi Munos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of The 23rd International Conference on Algorithmic
  Learning Theory (ALT 2012)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ALT, Lyon, France, LNCS 7568, pp.214-228, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5833v2</id>
    <updated>2012-10-11T06:21:09Z</updated>
    <published>2012-09-26T05:26:58Z</published>
    <title>Locality-Sensitive Hashing with Margin Based Feature Selection</title>
    <summary>  We propose a learning method with feature selection for Locality-Sensitive
Hashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.
These bit arrays can be used to perform similarity searches and personal
authentication. The proposed method uses bit arrays longer than those used in
the end for similarity and other searches and by learning selects the bits that
will be used. We demonstrated this method can effectively perform optimization
for cases such as fingerprint images with a large number of labels and
extremely few data that share the same labels, as well as verifying that it is
also effective for natural images, handwritten digits, and speech features.
</summary>
    <author>
      <name>Makiko Konoshima</name>
    </author>
    <author>
      <name>Yui Noma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.5833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5991v1</id>
    <updated>2012-09-26T16:31:32Z</updated>
    <published>2012-09-26T16:31:32Z</published>
    <title>Subset Selection for Gaussian Markov Random Fields</title>
    <summary>  Given a Gaussian Markov random field, we consider the problem of selecting a
subset of variables to observe which minimizes the total expected squared
prediction error of the unobserved variables. We first show that finding an
exact solution is NP-hard even for a restricted class of Gaussian Markov random
fields, called Gaussian free fields, which arise in semi-supervised learning
and computer vision. We then give a simple greedy approximation algorithm for
Gaussian free fields on arbitrary graphs. Finally, we give a message passing
algorithm for general Gaussian Markov random fields on bounded tree-width
graphs.
</summary>
    <author>
      <name>Satyaki Mahalanabis</name>
    </author>
    <author>
      <name>Daniel Stefankovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.5991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.6329v1</id>
    <updated>2012-09-27T18:57:26Z</updated>
    <published>2012-09-27T18:57:26Z</published>
    <title>More Is Better: Large Scale Partially-supervised Sentiment
  Classification - Appendix</title>
    <summary>  We describe a bootstrapping algorithm to learn from partially labeled data,
and the results of an empirical study for using it to improve performance of
sentiment classification using up to 15 million unlabeled Amazon product
reviews. Our experiments cover semi-supervised learning, domain adaptation and
weakly supervised learning. In some cases our methods were able to reduce test
error by more than half using such large amount of data.
  NOTICE: This is only the supplementary material.
</summary>
    <author>
      <name>Yoav Haimovitch</name>
    </author>
    <author>
      <name>Koby Crammer</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the appendix to the paper "More Is Better: Large Scale
  Partially-supervised Sentiment Classification" accepted to ACML 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.6329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.6419v1</id>
    <updated>2012-09-28T04:12:14Z</updated>
    <published>2012-09-28T04:12:14Z</published>
    <title>Partial Gaussian Graphical Model Estimation</title>
    <summary>  This paper studies the partial estimation of Gaussian graphical models from
high-dimensional empirical observations. We derive a convex formulation for
this problem using $\ell_1$-regularized maximum-likelihood estimation, which
can be solved via a block coordinate descent algorithm. Statistical estimation
performance can be established for our method. The proposed approach has
competitive empirical performance compared to existing methods, as demonstrated
by various experiments on synthetic and real datasets.
</summary>
    <author>
      <name>Xiao-Tong Yuan</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 5 figures, 4tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.6419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.2162v1</id>
    <updated>2012-10-08T07:15:57Z</updated>
    <published>2012-10-08T07:15:57Z</published>
    <title>Semisupervised Classifier Evaluation and Recalibration</title>
    <summary>  How many labeled examples are needed to estimate a classifier's performance
on a new dataset? We study the case where data is plentiful, but labels are
expensive. We show that by making a few reasonable assumptions on the structure
of the data, it is possible to estimate performance curves, with confidence
bounds, using a small number of ground truth labels. Our approach, which we
call Semisupervised Performance Evaluation (SPE), is based on a generative
model for the classifier's confidence scores. In addition to estimating the
performance of classifiers on new datasets, SPE can be used to recalibrate a
classifier by re-estimating the class-conditional confidence distributions.
</summary>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <author>
      <name>Pietro Perona</name>
    </author>
    <link href="http://arxiv.org/abs/1210.2162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.2162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.4862v1</id>
    <updated>2012-10-16T17:38:45Z</updated>
    <published>2012-10-16T17:38:45Z</published>
    <title>Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits</title>
    <summary>  We present and prove properties of a new offline policy evaluator for an
exploration learning setting which is superior to previous evaluators. In
particular, it simultaneously and correctly incorporates techniques from
importance weighting, doubly robust evaluation, and nonstationary policy
evaluation approaches. In addition, our approach allows generating longer
histories by careful control of a bias-variance tradeoff, and further decreases
variance by incorporating information about randomness of the target policy.
Empirical evidence from synthetic and realworld exploration learning problems
shows the new evaluator successfully unifies previous approaches and uses
information an order of magnitude more efficiently.
</summary>
    <author>
      <name>Miroslav Dudik</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Lihong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.4862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.4862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6001v3</id>
    <updated>2013-06-07T09:45:45Z</updated>
    <published>2012-10-22T19:02:21Z</published>
    <title>Reducing statistical time-series problems to binary classification</title>
    <summary>  We show how binary classification methods developed to work on i.i.d. data
can be used for solving statistical problems that are seemingly unrelated to
classification and concern highly-dependent time series. Specifically, the
problems of time-series clustering, homogeneity testing and the three-sample
problem are addressed. The algorithms that we construct for solving these
problems are based on a new metric between time-series distributions, which can
be evaluated using binary classification methods. Universal consistency of the
proposed algorithms is proven under most general assumptions. The theoretical
results are illustrated with experiments on synthetic and real-world data.
</summary>
    <author>
      <name>Daniil Ryabko</name>
    </author>
    <author>
      <name>Jérémie Mary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of NIPS 2012, pp. 2069-2077</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6001v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6001v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6292v2</id>
    <updated>2014-02-06T11:29:49Z</updated>
    <published>2012-10-23T17:12:01Z</published>
    <title>A density-sensitive hierarchical clustering method</title>
    <summary>  We define a hierarchical clustering method: $\alpha$-unchaining single
linkage or $SL(\alpha)$. The input of this algorithm is a finite metric space
and a certain parameter $\alpha$. This method is sensitive to the density of
the distribution and offers some solution to the so called chaining effect. We
also define a modified version, $SL^*(\alpha)$, to treat the chaining through
points or small blocks. We study the theoretical properties of these methods
and offer some theoretical background for the treatment of chaining effects.
</summary>
    <author>
      <name>Álvaro Martínez-Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6292v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6292v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.7657v1</id>
    <updated>2012-10-29T13:30:27Z</updated>
    <published>2012-10-29T13:30:27Z</published>
    <title>Text Classification with Compression Algorithms</title>
    <summary>  This work concerns a comparison of SVM kernel methods in text categorization
tasks. In particular I define a kernel function that estimates the similarity
between two objects computing by their compressed lengths. In fact, compression
algorithms can detect arbitrarily long dependencies within the text strings.
Data text vectorization looses information in feature extractions and is highly
sensitive by textual language. Furthermore, these methods are language
independent and require no text preprocessing. Moreover, the accuracy computed
on the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater than
Gaussian, linear and polynomial kernels. The method limits are represented by
computational time complexity of the Gram matrix and by very poor performance
on non-textual datasets.
</summary>
    <author>
      <name>Antonio Giuliano Zippo</name>
    </author>
    <link href="http://arxiv.org/abs/1210.7657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.7657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0025v2</id>
    <updated>2014-06-21T13:47:44Z</updated>
    <published>2012-10-31T20:38:04Z</published>
    <title>Venn-Abers predictors</title>
    <summary>  This paper continues study, both theoretical and empirical, of the method of
Venn prediction, concentrating on binary prediction problems. Venn predictors
produce probability-type predictions for the labels of test objects which are
guaranteed to be well calibrated under the standard assumption that the
observations are generated independently from the same distribution. We give a
simple formalization and proof of this property. We also introduce Venn-Abers
predictors, a new class of Venn predictors based on the idea of isotonic
regression, and report promising empirical results both for Venn-Abers
predictors and for their more computationally efficient simplified version.
</summary>
    <author>
      <name>Vladimir Vovk</name>
    </author>
    <author>
      <name>Ivan Petej</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages; to appear in the UAI 2014 Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1043v1</id>
    <updated>2012-11-05T21:40:38Z</updated>
    <published>2012-11-05T21:40:38Z</published>
    <title>Soft (Gaussian CDE) regression models and loss functions</title>
    <summary>  Regression, unlike classification, has lacked a comprehensive and effective
approach to deal with cost-sensitive problems by the reuse (and not a
re-training) of general regression models. In this paper, a wide variety of
cost-sensitive problems in regression (such as bids, asymmetric losses and
rejection rules) can be solved effectively by a lightweight but powerful
approach, consisting of: (1) the conversion of any traditional one-parameter
crisp regression model into a two-parameter soft regression model, seen as a
normal conditional density estimator, by the use of newly-introduced enrichment
methods; and (2) the reframing of an enriched soft regression model to new
contexts by an instance-dependent optimisation of the expected loss derived
from the conditional normal distribution.
</summary>
    <author>
      <name>Jose Hernandez-Orallo</name>
    </author>
    <link href="http://arxiv.org/abs/1211.1043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1513v2</id>
    <updated>2013-03-27T09:00:24Z</updated>
    <published>2012-11-07T10:57:38Z</published>
    <title>K-Plane Regression</title>
    <summary>  In this paper, we present a novel algorithm for piecewise linear regression
which can learn continuous as well as discontinuous piecewise linear functions.
The main idea is to repeatedly partition the data and learn a liner model in in
each partition. While a simple algorithm incorporating this idea does not work
well, an interesting modification results in a good algorithm. The proposed
algorithm is similar in spirit to $k$-means clustering algorithm. We show that
our algorithm can also be viewed as an EM algorithm for maximum likelihood
estimation of parameters under a reasonable probability model. We empirically
demonstrate the effectiveness of our approach by comparing its performance with
the state of art regression learning algorithms on some real world datasets.
</summary>
    <author>
      <name>Naresh Manwani</name>
    </author>
    <author>
      <name>P. S. Sastry</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2014.08.058</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2014.08.058" rel="related"/>
    <link href="http://arxiv.org/abs/1211.1513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1799v1</id>
    <updated>2012-11-08T09:22:11Z</updated>
    <published>2012-11-08T09:22:11Z</published>
    <title>Algorithm for Missing Values Imputation in Categorical Data with Use of
  Association Rules</title>
    <summary>  This paper presents algorithm for missing values imputation in categorical
data. The algorithm is based on using association rules and is presented in
three variants. Experimental shows better accuracy of missing values imputation
using the algorithm then using most common attribute value.
</summary>
    <author>
      <name>Jiří Kaiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 tables, 2011 Third International Joint Journal Conference
  in Computer, Electronics and Electrical, ACEEE International Journal on
  Recent Trends in Engineering &amp; Technology, Vol. 06, Is. 01, Nov 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACEEE International Journal on Recent Trends in Engineering &amp;
  Technology 6 (2011) 111-114</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.1799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2260v1</id>
    <updated>2012-11-09T22:13:10Z</updated>
    <published>2012-11-09T22:13:10Z</published>
    <title>No-Regret Algorithms for Unconstrained Online Convex Optimization</title>
    <summary>  Some of the most compelling applications of online convex optimization,
including online prediction and classification, are unconstrained: the natural
feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in
this setting unless constraints on the comparator point x^* are known in
advance. We present algorithms that, without such prior knowledge, offer
near-optimal regret bounds with respect to any choice of x^*. In particular,
regret with respect to x^* = 0 is constant. We then prove lower bounds showing
that our guarantees are near-optimal in this setting.
</summary>
    <author>
      <name>Matthew Streeter</name>
    </author>
    <author>
      <name>H. Brendan McMahan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.2260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5063v2</id>
    <updated>2013-02-16T00:35:48Z</updated>
    <published>2012-11-21T15:40:11Z</published>
    <title>On the difficulty of training Recurrent Neural Networks</title>
    <summary>  There are two widely known issues with properly training Recurrent Neural
Networks, the vanishing and the exploding gradient problems detailed in Bengio
et al. (1994). In this paper we attempt to improve the understanding of the
underlying issues by exploring these problems from an analytical, a geometric
and a dynamical systems perspective. Our analysis is used to justify a simple
yet effective solution. We propose a gradient norm clipping strategy to deal
with exploding gradients and a soft constraint for the vanishing gradients
problem. We validate empirically our hypothesis and proposed solutions in the
experimental section.
</summary>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved description of the exploding gradient problem and
  description and analysis of the vanishing gradient problem</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5687v1</id>
    <updated>2012-11-24T17:51:57Z</updated>
    <published>2012-11-24T17:51:57Z</published>
    <title>Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep
  Extensions</title>
    <summary>  We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture
modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or
surpasses the state-of-the-art on texture synthesis and inpainting by
parametric models. We also develop a novel RBM model with a spike-and-slab
visible layer and binary variables in the hidden layer. This model is designed
to be stacked on top of the TssRBM. We show the resulting deep belief network
(DBN) is a powerful generative model that improves on single-layer models and
is capable of modeling not only single high-resolution and challenging textures
but also multiple textures.
</summary>
    <author>
      <name>Heng Luo</name>
    </author>
    <author>
      <name>Pierre Luc Carrier</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1211.5687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6013v2</id>
    <updated>2013-07-14T00:09:14Z</updated>
    <published>2012-11-26T16:27:18Z</published>
    <title>Online Stochastic Optimization with Multiple Objectives</title>
    <summary>  In this paper we propose a general framework to characterize and solve the
stochastic optimization problems with multiple objectives underlying many real
world learning applications. We first propose a projection based algorithm
which attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on the
theory of Lagrangian in constrained optimization, we devise a novel primal-dual
stochastic approximation algorithm which attains the optimal convergence rate
of $O(T^{-1/2})$ for general Lipschitz continuous objectives.
</summary>
    <author>
      <name>Mehrdad Mahdavi</name>
    </author>
    <author>
      <name>Tianbao Yang</name>
    </author>
    <author>
      <name>Rong Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS Workshop on Optimization for Machine Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.6013v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6013v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6302v3</id>
    <updated>2013-10-18T17:02:13Z</updated>
    <published>2012-11-27T13:46:59Z</published>
    <title>Duality between subgradient and conditional gradient methods</title>
    <summary>  Given a convex optimization problem and its dual, there are many possible
first-order algorithms. In this paper, we show the equivalence between mirror
descent algorithms and algorithms generalizing the conditional gradient method.
This is done through convex duality, and implies notably that for certain
problems, such as for supervised machine learning problems with non-smooth
losses or problems regularized by non-smooth regularizers, the primal
subgradient method and the dual conditional gradient method are formally
equivalent. The dual interpretation leads to a form of line search for mirror
descent, as well as guarantees of convergence for primal-dual certificates.
</summary>
    <author>
      <name>Francis Bach</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Paris - Rocquencourt, LIENS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1211.6302v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6302v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6851v1</id>
    <updated>2012-11-29T09:22:19Z</updated>
    <published>2012-11-29T09:22:19Z</published>
    <title>Classification Recouvrante Basée sur les Méthodes à Noyau</title>
    <summary>  Overlapping clustering problem is an important learning issue in which
clusters are not mutually exclusive and each object may belongs simultaneously
to several clusters. This paper presents a kernel based method that produces
overlapping clusters on a high feature space using mercer kernel techniques to
improve separability of input patterns. The proposed method, called
OKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping
$k$-means) method to produce overlapping schemes. Experiments are performed on
overlapping dataset and empirical results obtained with OKM-K outperform
results obtained with OKM.
</summary>
    <author>
      <name>Chiheb-Eddine Ben N'Cir</name>
    </author>
    <author>
      <name>Nadia Essoussi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Les 43\`emes Journ\'ees de Statistique</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Les 43\`emes Journ\'ees de Statistique 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.6851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1131v1</id>
    <updated>2012-12-05T19:03:39Z</updated>
    <published>2012-12-05T19:03:39Z</published>
    <title>Using Wikipedia to Boost SVD Recommender Systems</title>
    <summary>  Singular Value Decomposition (SVD) has been used successfully in recent years
in the area of recommender systems. In this paper we present how this model can
be extended to consider both user ratings and information from Wikipedia. By
mapping items to Wikipedia pages and quantifying their similarity, we are able
to use this information in order to improve recommendation accuracy, especially
when the sparsity is high. Another advantage of the proposed approach is the
fact that it can be easily integrated into any other SVD implementation,
regardless of additional parameters that may have been added to it. Preliminary
experimental results on the MovieLens dataset are encouraging.
</summary>
    <author>
      <name>Gilad Katz</name>
    </author>
    <author>
      <name>Guy Shani</name>
    </author>
    <author>
      <name>Bracha Shapira</name>
    </author>
    <author>
      <name>Lior Rokach</name>
    </author>
    <link href="http://arxiv.org/abs/1212.1131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1936v1</id>
    <updated>2012-12-09T23:28:02Z</updated>
    <published>2012-12-09T23:28:02Z</published>
    <title>High-dimensional sequence transduction</title>
    <summary>  We investigate the problem of transforming an input sequence into a
high-dimensional output sequence in order to transcribe polyphonic audio music
into symbolic notation. We introduce a probabilistic model based on a recurrent
neural network that is able to learn realistic output distributions given the
input and we devise an efficient algorithm to search for the global mode of
that distribution. The resulting method produces musically plausible
transcriptions even under high levels of noise and drastically outperforms
previous state-of-the-art approaches on five datasets of synthesized sounds and
real recordings, approximately halving the test error rate.
</summary>
    <author>
      <name>Nicolas Boulanger-Lewandowski</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Pascal Vincent</name>
    </author>
    <link href="http://arxiv.org/abs/1212.1936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2002v2</id>
    <updated>2012-12-20T20:55:23Z</updated>
    <published>2012-12-10T09:22:06Z</published>
    <title>A simpler approach to obtaining an O(1/t) convergence rate for the
  projected stochastic subgradient method</title>
    <summary>  In this note, we present a new averaging technique for the projected
stochastic subgradient method. By using a weighted average with a weight of t+1
for each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)
with both an easy proof and an easy implementation. The new scheme is compared
empirically to existing techniques, with similar performance behavior.
</summary>
    <author>
      <name>Simon Lacoste-Julien</name>
    </author>
    <author>
      <name>Mark Schmidt</name>
    </author>
    <author>
      <name>Francis Bach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures. Changes with previous version: Added reference to
  concurrently submitted work arXiv:1212.1824v1; clarifications added; typos
  corrected; title changed to 'subgradient method' as 'subgradient descent' is
  misnomer</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C15, 68T05, 65K10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2136v2</id>
    <updated>2013-06-18T12:03:42Z</updated>
    <published>2012-12-10T17:12:51Z</published>
    <title>A class of random fields on complete graphs with tractable partition
  function</title>
    <summary>  The aim of this short note is to draw attention to a method by which the
partition function and marginal probabilities for a certain class of random
fields on complete graphs can be computed in polynomial time. This class
includes Ising models with homogeneous pairwise potentials but arbitrary
(inhomogeneous) unary potentials. Similarly, the partition function and
marginal probabilities can be computed in polynomial time for random fields on
complete bipartite graphs, provided they have homogeneous pairwise potentials.
We expect that these tractable classes of large scale random fields can be very
useful for the evaluation of approximation algorithms by providing exact error
estimates.
</summary>
    <author>
      <name>Boris Flach</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2013.99</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2013.99" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in IEEE TPAMI (short paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2415v1</id>
    <updated>2012-12-11T13:19:54Z</updated>
    <published>2012-12-11T13:19:54Z</published>
    <title>Robust Face Recognition using Local Illumination Normalization and
  Discriminant Feature Point Selection</title>
    <summary>  Face recognition systems must be robust to the variation of various factors
such as facial expression, illumination, head pose and aging. Especially, the
robustness against illumination variation is one of the most important problems
to be solved for the practical use of face recognition systems. Gabor wavelet
is widely used in face detection and recognition because it gives the
possibility to simulate the function of human visual system. In this paper, we
propose a method for extracting Gabor wavelet features which is stable under
the variation of local illumination and show experiment results demonstrating
its effectiveness.
</summary>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Jinsong Kim</name>
    </author>
    <author>
      <name>Cholhun Kim</name>
    </author>
    <author>
      <name>Jongchol Jo</name>
    </author>
    <author>
      <name>Sunam Han</name>
    </author>
    <link href="http://arxiv.org/abs/1212.2415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2510v1</id>
    <updated>2012-10-19T15:08:42Z</updated>
    <published>2012-10-19T15:08:42Z</published>
    <title>Markov Random Walk Representations with Continuous Distributions</title>
    <summary>  Representations based on random walks can exploit discrete data distributions
for clustering and classification. We extend such representations from discrete
to continuous distributions. Transition probabilities are now calculated using
a diffusion equation with a diffusion coefficient that inversely depends on the
data density. We relate this diffusion equation to a path integral and derive
the corresponding path probability measure. The framework is useful for
incorporating continuous data densities and prior knowledge.
</summary>
    <author>
      <name>Chen-Hsiang Yeang</name>
    </author>
    <author>
      <name>Martin Szummer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2513v1</id>
    <updated>2012-10-19T15:08:28Z</updated>
    <published>2012-10-19T15:08:28Z</published>
    <title>Efficient Parametric Projection Pursuit Density Estimation</title>
    <summary>  Product models of low dimensional experts are a powerful way to avoid the
curse of dimensionality. We present the ``under-complete product of experts'
(UPoE), where each expert models a one dimensional projection of the data. The
UPoE is fully tractable and may be interpreted as a parametric probabilistic
model for projection pursuit. Its ML learning rules are identical to the
approximate learning rules proposed before for under-complete ICA. We also
derive an efficient sequential learning algorithm and discuss its relationship
to projection pursuit density estimation and feature induction algorithms for
additive random field models.
</summary>
    <author>
      <name>Max Welling</name>
    </author>
    <author>
      <name>Richard S. Zemel</name>
    </author>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2514v1</id>
    <updated>2012-10-19T15:08:24Z</updated>
    <published>2012-10-19T15:08:24Z</published>
    <title>Boltzmann Machine Learning with the Latent Maximum Entropy Principle</title>
    <summary>  We present a new statistical learning paradigm for Boltzmann machines based
on a new inference principle we have proposed: the latent maximum entropy
principle (LME). LME is different both from Jaynes maximum entropy principle
and from standard maximum likelihood estimation.We demonstrate the LME
principle BY deriving new algorithms for Boltzmann machine parameter
estimation, and show how robust and fast new variant of the EM algorithm can be
developed.Our experiments show that estimation based on LME generally yields
better results than maximum likelihood estimation, particularly when inferring
hidden units from small amounts of data.
</summary>
    <author>
      <name>Shaojun Wang</name>
    </author>
    <author>
      <name>Dale Schuurmans</name>
    </author>
    <author>
      <name>Fuchun Peng</name>
    </author>
    <author>
      <name>Yunxin Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5701v1</id>
    <updated>2012-12-22T15:46:49Z</updated>
    <published>2012-12-22T15:46:49Z</published>
    <title>ADADELTA: An Adaptive Learning Rate Method</title>
    <summary>  We present a novel per-dimension learning rate method for gradient descent
called ADADELTA. The method dynamically adapts over time using only first order
information and has minimal computational overhead beyond vanilla stochastic
gradient descent. The method requires no manual tuning of a learning rate and
appears robust to noisy gradient information, different model architecture
choices, various data modalities and selection of hyperparameters. We show
promising results compared to other methods on the MNIST digit classification
task using a single machine and on a large scale voice dataset in a distributed
cluster environment.
</summary>
    <author>
      <name>Matthew D. Zeiler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0015v1</id>
    <updated>2012-12-31T21:07:21Z</updated>
    <published>2012-12-31T21:07:21Z</published>
    <title>Bethe Bounds and Approximating the Global Optimum</title>
    <summary>  Inference in general Markov random fields (MRFs) is NP-hard, though
identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with
submodular cost functions is efficiently solvable using graph cuts. Marginal
inference, however, even for this restricted class, is in #P. We prove new
formulations of derivatives of the Bethe free energy, provide bounds on the
derivatives and bracket the locations of stationary points, introducing a new
technique called Bethe bound propagation. Several results apply to pairwise
models whether associative or not. Applying these to discretized
pseudo-marginals in the associative case we present a polynomial time
approximation scheme for global optimization provided the maximum degree is
$O(\log n)$, and discuss several extensions.
</summary>
    <author>
      <name>Adrian Weller</name>
    </author>
    <author>
      <name>Tony Jebara</name>
    </author>
    <link href="http://arxiv.org/abs/1301.0015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0104v1</id>
    <updated>2013-01-01T16:25:17Z</updated>
    <published>2013-01-01T16:25:17Z</published>
    <title>Policy Evaluation with Variance Related Risk Criteria in Markov Decision
  Processes</title>
    <summary>  In this paper we extend temporal difference policy evaluation algorithms to
performance criteria that include the variance of the cumulative reward. Such
criteria are useful for risk management, and are important in domains such as
finance and process control. We propose both TD(0) and LSTD(lambda) variants
with linear function approximation, prove their convergence, and demonstrate
their utility in a 4-dimensional continuous state space problem.
</summary>
    <author>
      <name>Aviv Tamar</name>
    </author>
    <author>
      <name>Dotan Di Castro</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMLR Workshop and Conference Proceedings 28 (3): 495-503, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.0104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0586v1</id>
    <updated>2012-12-12T15:57:27Z</updated>
    <published>2012-12-12T15:57:27Z</published>
    <title>Staged Mixture Modelling and Boosting</title>
    <summary>  In this paper, we introduce and evaluate a data-driven staged mixture
modeling technique for building density, regression, and classification models.
Our basic approach is to sequentially add components to a finite mixture model
using the structural expectation maximization (SEM) algorithm. We show that our
technique is qualitatively similar to boosting. This correspondence is a
natural byproduct of the fact that we use the SEM algorithm to sequentially fit
the mixture model. Finally, in our experimental evaluation, we demonstrate the
effectiveness of our approach on a variety of prediction and density estimation
tasks using real-world data.
</summary>
    <author>
      <name>Christopher Meek</name>
    </author>
    <author>
      <name>Bo Thiesson</name>
    </author>
    <author>
      <name>David Heckerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0602v1</id>
    <updated>2012-12-12T15:58:30Z</updated>
    <published>2012-12-12T15:58:30Z</published>
    <title>Unsupervised Active Learning in Large Domains</title>
    <summary>  Active learning is a powerful approach to analyzing data effectively. We show
that the feasibility of active learning depends crucially on the choice of
measure with respect to which the query is being optimized. The standard
information gain, for example, does not permit an accurate evaluation with a
small committee, a representative subset of the model space. We propose a
surrogate measure requiring only a small committee and discuss the properties
of this new measure. We devise, in addition, a bootstrap approach for committee
selection. The advantages of this approach are illustrated in the context of
recovering (regulatory) network models.
</summary>
    <author>
      <name>Harald Steck</name>
    </author>
    <author>
      <name>Tommi S. Jaakkola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2278v1</id>
    <updated>2013-01-10T16:24:10Z</updated>
    <published>2013-01-10T16:24:10Z</published>
    <title>Discovering Multiple Constraints that are Frequently Approximately
  Satisfied</title>
    <summary>  Some high-dimensional data.sets can be modelled by assuming that there are
many different linear constraints, each of which is Frequently Approximately
Satisfied (FAS) by the data. The probability of a data vector under the model
is then proportional to the product of the probabilities of its constraint
violations. We describe three methods of learning products of constraints using
a heavy-tailed probability distribution for the violations.
</summary>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <author>
      <name>Yee Whye Teh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2286v1</id>
    <updated>2013-01-10T16:24:45Z</updated>
    <published>2013-01-10T16:24:45Z</published>
    <title>Iterative Markov Chain Monte Carlo Computation of Reference Priors and
  Minimax Risk</title>
    <summary>  We present an iterative Markov chainMonte Carlo algorithm for
computingreference priors and minimax risk forgeneral parametric families.
Ourapproach uses MCMC techniques based onthe Blahut-Arimoto algorithm
forcomputing channel capacity ininformation theory. We give astatistical
analysis of the algorithm,bounding the number of samples requiredfor the
stochastic algorithm to closelyapproximate the deterministic algorithmin each
iteration. Simulations arepresented for several examples fromexponential
families. Although we focuson applications to reference priors andminimax risk,
the methods and analysiswe develop are applicable to a muchbroader class of
optimization problemsand iterative algorithms.
</summary>
    <author>
      <name>John Lafferty</name>
    </author>
    <author>
      <name>Larry A. Wasserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3192v1</id>
    <updated>2013-01-15T00:54:38Z</updated>
    <published>2013-01-15T00:54:38Z</published>
    <title>Matrix Approximation under Local Low-Rank Assumption</title>
    <summary>  Matrix approximation is a common tool in machine learning for building
accurate prediction models for recommendation systems, text mining, and
computer vision. A prevalent assumption in constructing matrix approximations
is that the partially observed matrix is of low-rank. We propose a new matrix
approximation model where we assume instead that the matrix is only locally of
low-rank, leading to a representation of the observed matrix as a weighted sum
of low-rank matrices. We analyze the accuracy of the proposed local low-rank
modeling. Our experiments show improvements in prediction accuracy in
recommendation tasks.
</summary>
    <author>
      <name>Joonseok Lee</name>
    </author>
    <author>
      <name>Seungyeon Kim</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <author>
      <name>Yoram Singer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, Workshop submission to the First International
  Conference on Learning Representations (ICLR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3193v1</id>
    <updated>2013-01-15T01:07:14Z</updated>
    <published>2013-01-15T01:07:14Z</published>
    <title>Learning Graphical Model Parameters with Approximate Marginal Inference</title>
    <summary>  Likelihood based-learning of graphical models faces challenges of
computational-complexity and robustness to model mis-specification. This paper
studies methods that fit parameters directly to maximize a measure of the
accuracy of predicted marginals, taking into account both model and inference
approximations at training time. Experiments on imaging problems suggest
marginalization-based learning performs better than likelihood-based
approximations on difficult problems where the model being fit is approximate
in nature.
</summary>
    <author>
      <name>Justin Domke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2013.31</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2013.31" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear, IEEE Transactions on Pattern Analysis and Machine
  Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3485v2</id>
    <updated>2013-03-21T17:02:48Z</updated>
    <published>2013-01-15T20:52:50Z</published>
    <title>A Semantic Matching Energy Function for Learning with Multi-relational
  Data</title>
    <summary>  Large-scale relational learning becomes crucial for handling the huge amounts
of structured data generated daily in many application domains ranging from
computational biology or information retrieval, to natural language processing.
In this paper, we present a new neural network architecture designed to embed
multi-relational graphs into a flexible continuous vector space in which the
original data is kept and enhanced. The network is trained to encode the
semantics of these graphs in order to assign high probabilities to plausible
components. We empirically show that it reaches competitive performance in link
prediction on standard datasets from the literature.
</summary>
    <author>
      <name>Xavier Glorot</name>
    </author>
    <author>
      <name>Antoine Bordes</name>
    </author>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3539v1</id>
    <updated>2013-01-16T01:07:38Z</updated>
    <published>2013-01-16T01:07:38Z</published>
    <title>Learning Features with Structure-Adapting Multi-view Exponential Family
  Harmoniums</title>
    <summary>  We proposea graphical model for multi-view feature extraction that
automatically adapts its structure to achieve better representation of data
distribution. The proposed model, structure-adapting multi-view harmonium
(SA-MVH) has switch parameters that control the connection between hidden nodes
and input views, and learn the switch parameter while training. Numerical
experiments on synthetic and a real-world dataset demonstrate the useful
behavior of the SA-MVH, compared to existing multi-view feature extraction
methods.
</summary>
    <author>
      <name>Yoonseop Kang</name>
    </author>
    <author>
      <name>Seungjin Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, ICLR2013 workshop track submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3557v1</id>
    <updated>2013-01-16T02:12:07Z</updated>
    <published>2013-01-16T02:12:07Z</published>
    <title>Stochastic Pooling for Regularization of Deep Convolutional Neural
  Networks</title>
    <summary>  We introduce a simple and effective method for regularizing large
convolutional neural networks. We replace the conventional deterministic
pooling operations with a stochastic procedure, randomly picking the activation
within each pooling region according to a multinomial distribution, given by
the activities within the pooling region. The approach is hyper-parameter free
and can be combined with other regularization approaches, such as dropout and
data augmentation. We achieve state-of-the-art performance on four image
datasets, relative to other approaches that do not utilize data augmentation.
</summary>
    <author>
      <name>Matthew D. Zeiler</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3575v1</id>
    <updated>2013-01-16T03:52:09Z</updated>
    <published>2013-01-16T03:52:09Z</published>
    <title>Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative
  Clustering</title>
    <summary>  Large scale agglomerative clustering is hindered by computational burdens. We
propose a novel scheme where exact inter-instance distance calculation is
replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing
(KLSH) hashed values. This results in a method that drastically decreases
computation time. Additionally, we take advantage of certain labeled data
points via distance metric learning to achieve a competitive precision and
recall comparing to K-Means but in much less computation time.
</summary>
    <author>
      <name>Boyi Xie</name>
    </author>
    <author>
      <name>Shuheng Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3577v3</id>
    <updated>2013-03-20T15:37:33Z</updated>
    <published>2013-01-16T04:07:46Z</published>
    <title>Saturating Auto-Encoders</title>
    <summary>  We introduce a simple new regularizer for auto-encoders whose hidden-unit
activation functions contain at least one zero-gradient (saturated) region.
This regularizer explicitly encourages activations in the saturated region(s)
of the corresponding activation function. We call these Saturating
Auto-Encoders (SATAE). We show that the saturation regularizer explicitly
limits the SATAE's ability to reconstruct inputs which are not near the data
manifold. Furthermore, we show that a wide variety of features can be learned
when different activation functions are used. Finally, connections are
established with the Contractive and Sparse Auto-Encoders.
</summary>
    <author>
      <name>Rostislav Goroshin</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3577v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3577v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3753v2</id>
    <updated>2013-01-19T19:38:36Z</updated>
    <published>2013-01-16T17:04:10Z</published>
    <title>Switched linear encoding with rectified linear autoencoders</title>
    <summary>  Several recent results in machine learning have established formal
connections between autoencoders---artificial neural network models that
attempt to reproduce their inputs---and other coding models like sparse coding
and K-means. This paper explores in depth an autoencoder model that is
constructed using rectified linear activations on its hidden units. Our
analysis builds on recent results to further unify the world of sparse linear
coding models. We provide an intuitive interpretation of the behavior of these
coding models and demonstrate this intuition using small, artificial datasets
with known distributions.
</summary>
    <author>
      <name>Leif Johnson</name>
    </author>
    <author>
      <name>Craig Corcoran</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3816v1</id>
    <updated>2013-01-16T20:16:02Z</updated>
    <published>2013-01-16T20:16:02Z</published>
    <title>Learning Output Kernels for Multi-Task Problems</title>
    <summary>  Simultaneously solving multiple related learning tasks is beneficial under a
variety of circumstances, but the prior knowledge necessary to correctly model
task relationships is rarely available in practice. In this paper, we develop a
novel kernel-based multi-task learning technique that automatically reveals
structural inter-task relationships. Building over the framework of output
kernel learning (OKL), we introduce a method that jointly learns multiple
functions and a low-rank multi-task kernel by solving a non-convex
regularization problem. Optimization is carried out via a block coordinate
descent strategy, where each subproblem is solved using suitable conjugate
gradient (CG) type iterative methods for linear operator equations. The
effectiveness of the proposed approach is demonstrated on pharmacological and
collaborative filtering data.
</summary>
    <author>
      <name>Francesco Dinuzzo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2013.02.024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2013.02.024" rel="related"/>
    <link href="http://arxiv.org/abs/1301.3816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3890v1</id>
    <updated>2013-01-16T15:52:30Z</updated>
    <published>2013-01-16T15:52:30Z</published>
    <title>Monte Carlo Inference via Greedy Importance Sampling</title>
    <summary>  We present a new method for conducting Monte Carlo inference in graphical
models which combines explicit search with generalized importance sampling. The
idea is to reduce the variance of importance sampling by searching for
significant points in the target distribution. We prove that it is possible to
introduce search and still maintain unbiasedness. We then demonstrate our
procedure on a few simple inference tasks and show that it can improve the
inference quality of standard MCMC methods, including Gibbs sampling,
Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous work
which showed how greedy importance sampling could be correctly realized in the
one-dimensional case.
</summary>
    <author>
      <name>Dale Schuurmans</name>
    </author>
    <author>
      <name>Finnegan Southey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3896v1</id>
    <updated>2013-01-16T15:52:53Z</updated>
    <published>2013-01-16T15:52:53Z</published>
    <title>An Uncertainty Framework for Classification</title>
    <summary>  We define a generalized likelihood function based on uncertainty measures and
show that maximizing such a likelihood function for different measures induces
different types of classifiers. In the probabilistic framework, we obtain
classifiers that optimize the cross-entropy function. In the possibilistic
framework, we obtain classifiers that maximize the interclass margin.
Furthermore, we show that the support vector machine is a sub-class of these
maximum-margin classifiers.
</summary>
    <author>
      <name>Loo-Nin Teow</name>
    </author>
    <author>
      <name>Kia-Fock Loe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4157v1</id>
    <updated>2013-01-17T16:48:46Z</updated>
    <published>2013-01-17T16:48:46Z</published>
    <title>On the Product Rule for Classification Problems</title>
    <summary>  We discuss theoretical aspects of the product rule for classification
problems in supervised machine learning for the case of combining classifiers.
We show that (1) the product rule arises from the MAP classifier supposing
equivalent priors and conditional independence given a class; (2) under some
conditions, the product rule is equivalent to minimizing the sum of the squared
distances to the respective centers of the classes related with different
features, such distances being weighted by the spread of the classes; (3)
observing some hypothesis, the product rule is equivalent to concatenating the
vectors of features.
</summary>
    <author>
      <name>Marcelo Cicconet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.4157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4769v2</id>
    <updated>2013-02-28T17:44:24Z</updated>
    <published>2013-01-21T07:28:44Z</published>
    <title>A Correlation Clustering Approach to Link Classification in Signed
  Networks -- Full Version --</title>
    <summary>  Motivated by social balance theory, we develop a theory of link
classification in signed networks using the correlation clustering index as
measure of label regularity. We derive learning bounds in terms of correlation
clustering within three fundamental transductive learning settings: online,
batch and active. Our main algorithmic contribution is in the active setting,
where we introduce a new family of efficient link classifiers based on covering
the input graph with small circuits. These are the first active algorithms for
link classification with mistake bounds that hold for arbitrary signed
networks.
</summary>
    <author>
      <name>Nicolo Cesa-Bianchi</name>
    </author>
    <author>
      <name>Claudio Gentile</name>
    </author>
    <author>
      <name>Fabio Vitale</name>
    </author>
    <author>
      <name>Giovanni Zappella</name>
    </author>
    <link href="http://arxiv.org/abs/1301.4769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6316v3</id>
    <updated>2013-03-18T18:37:37Z</updated>
    <published>2013-01-27T04:51:21Z</published>
    <title>Hierarchical Data Representation Model - Multi-layer NMF</title>
    <summary>  In this paper, we propose a data representation model that demonstrates
hierarchical feature learning using nsNMF. We extend unit algorithm into
several layers. Experiments with document and image data successfully
discovered feature hierarchies. We also prove that proposed method results in
much better classification and reconstruction performance, especially for small
number of features. feature hierarchies.
</summary>
    <author>
      <name>Hyun Ah Song</name>
    </author>
    <author>
      <name>Soo-Young Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1301.6316v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6316v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.7375v1</id>
    <updated>2013-01-30T15:03:47Z</updated>
    <published>2013-01-30T15:03:47Z</published>
    <title>Learning by Transduction</title>
    <summary>  We describe a method for predicting a classification of an object given
classifications of the objects in the training set, assuming that the pairs
object/classification are generated by an i.i.d. process from a continuous
probability distribution. Our method is a modification of Vapnik's
support-vector machine; its main novelty is that it gives not only the
prediction itself but also a practicable measure of the evidence found in
support of that prediction. We also describe a procedure for assigning degrees
of confidence to predictions made by the support vector machine. Some
experimental results are presented, and possible extensions of the algorithms
are discussed.
</summary>
    <author>
      <name>Alex Gammerman</name>
    </author>
    <author>
      <name>Volodya Vovk</name>
    </author>
    <author>
      <name>Vladimir Vapnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.7375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.7375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.7411v1</id>
    <updated>2013-01-30T15:06:43Z</updated>
    <published>2013-01-30T15:06:43Z</published>
    <title>On the Geometry of Bayesian Graphical Models with Hidden Variables</title>
    <summary>  In this paper we investigate the geometry of the likelihood of the unknown
parameters in a simple class of Bayesian directed graphs with hidden variables.
This enables us, before any numerical algorithms are employed, to obtain
certain insights in the nature of the unidentifiability inherent in such
models, the way posterior densities will be sensitive to prior densities and
the typical geometrical form these posterior densities might take. Many of
these insights carry over into more complicated Bayesian networks with
systematic missing data.
</summary>
    <author>
      <name>Raffaella Settimi</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.7411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.7411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0283v2</id>
    <updated>2013-03-19T21:17:56Z</updated>
    <published>2013-03-01T03:45:42Z</published>
    <title>Inverse Signal Classification for Financial Instruments</title>
    <summary>  The paper presents new machine learning methods: signal composition, which
classifies time-series regardless of length, type, and quantity; and
self-labeling, a supervised-learning enhancement. The paper describes further
the implementation of the methods on a financial search engine system using a
collection of 7,881 financial instruments traded during 2011 to identify
inverse behavior among the time-series.
</summary>
    <author>
      <name>Uri Kartoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1303.0073</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.0283v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0283v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2104v1</id>
    <updated>2013-03-08T20:46:27Z</updated>
    <published>2013-03-08T20:46:27Z</published>
    <title>Transfer Learning for Voice Activity Detection: A Denoising Deep Neural
  Network Perspective</title>
    <summary>  Mismatching problem between the source and target noisy corpora severely
hinder the practical use of the machine-learning-based voice activity detection
(VAD). In this paper, we try to address this problem in the transfer learning
prospective. Transfer learning tries to find a common learning machine or a
common feature subspace that is shared by both the source corpus and the target
corpus. The denoising deep neural network is used as the learning machine.
Three transfer techniques, which aim to learn common feature representations,
are used for analysis. Experimental results demonstrate the effectiveness of
the transfer learning schemes on the mismatch problem.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <author>
      <name>Ji Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been submitted to the conference "INTERSPEECH2013" in
  March 4, 2013 for review</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.2104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2314v1</id>
    <updated>2013-03-10T12:00:59Z</updated>
    <published>2013-03-10T12:00:59Z</published>
    <title>Mini-Batch Primal and Dual Methods for SVMs</title>
    <summary>  We address the issue of using mini-batches in stochastic optimization of
SVMs. We show that the same quantity, the spectral norm of the data, controls
the parallelization speedup obtained for both primal stochastic subgradient
descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it
to derive novel variants of mini-batched SDCA. Our guarantees for both methods
are expressed in terms of the original nonsmooth primal problem based on the
hinge-loss.
</summary>
    <author>
      <name>Martin Takáč</name>
    </author>
    <author>
      <name>Avleen Bijral</name>
    </author>
    <author>
      <name>Peter Richtárik</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1303.2314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2506v1</id>
    <updated>2013-03-11T13:06:49Z</updated>
    <published>2013-03-11T13:06:49Z</published>
    <title>Monte-Carlo utility estimates for Bayesian reinforcement learning</title>
    <summary>  This paper introduces a set of algorithms for Monte-Carlo Bayesian
reinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the
Bayes-optimal value function is employed to construct an optimistic policy.
Secondly, gradient-based algorithms for approximate upper and lower bounds are
introduced. Finally, we introduce a new class of gradient algorithms for
Bayesian Bellman error minimisation. We theoretically show that the gradient
methods are sound. Experimentally, we demonstrate the superiority of the upper
bound method in terms of reward obtained. However, we also show that the
Bayesian Bellman error method is a close second, despite its significant
computational simplicity.
</summary>
    <author>
      <name>Christos Dimitrakakis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CDC.2013.6761048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CDC.2013.6761048" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 1 table, submitted to IEEE conference on decision
  and control</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.2506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.3055v1</id>
    <updated>2013-03-12T23:25:37Z</updated>
    <published>2013-03-12T23:25:37Z</published>
    <title>Online Learning in Markov Decision Processes with Adversarially Chosen
  Transition Probability Distributions</title>
    <summary>  We study the problem of learning Markov decision processes with finite state
and action spaces when the transition probability distributions and loss
functions are chosen adversarially and are allowed to change with time. We
introduce an algorithm whose regret with respect to any policy in a comparison
class grows as the square root of the number of rounds of the game, provided
the transition probabilities satisfy a uniform mixing condition. Our approach
is efficient as long as the comparison class is polynomial and we can compute
expectations over sample paths for each policy. Designing an efficient
algorithm with small regret for the general case remains an open problem.
</summary>
    <author>
      <name>Yasin Abbasi-Yadkori</name>
    </author>
    <author>
      <name>Peter L. Bartlett</name>
    </author>
    <author>
      <name>Csaba Szepesvari</name>
    </author>
    <link href="http://arxiv.org/abs/1303.3055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4172v1</id>
    <updated>2013-03-18T07:33:29Z</updated>
    <published>2013-03-18T07:33:29Z</published>
    <title>Margins, Shrinkage, and Boosting</title>
    <summary>  This manuscript shows that AdaBoost and its immediate variants can produce
approximate maximum margin classifiers simply by scaling step size choices with
a fixed small constant. In this way, when the unscaled step size is an optimal
choice, these results provide guarantees for Friedman's empirically successful
"shrinkage" procedure for gradient boosting (Friedman, 2000). Guarantees are
also provided for a variety of other step sizes, affirming the intuition that
increasingly regularized line searches provide improved margin guarantees. The
results hold for the exponential loss and similar losses, most notably the
logistic loss.
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, ICML 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4664v1</id>
    <updated>2013-03-19T17:00:22Z</updated>
    <published>2013-03-19T17:00:22Z</published>
    <title>Large-Scale Learning with Less RAM via Randomization</title>
    <summary>  We reduce the memory footprint of popular large-scale online learning methods
by projecting our weight vector onto a coarse discrete set using randomized
rounding. Compared to standard 32-bit float encodings, this reduces RAM usage
by more than 50% during training and by up to 95% when making predictions from
a fixed model, with almost no loss in accuracy. We also show that randomized
counting can be used to implement per-coordinate learning rates, improving
model quality with little additional RAM. We prove these memory-saving methods
achieve regret guarantees similar to their exact variants. Empirical evaluation
confirms excellent performance, dominating standard approaches across memory
versus accuracy tradeoffs.
</summary>
    <author>
      <name>Daniel Golovin</name>
    </author>
    <author>
      <name>D. Sculley</name>
    </author>
    <author>
      <name>H. Brendan McMahan</name>
    </author>
    <author>
      <name>Michael Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of ICML 2013 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5403v1</id>
    <updated>2013-03-13T12:52:37Z</updated>
    <published>2013-03-13T12:52:37Z</published>
    <title>An Entropy-based Learning Algorithm of Bayesian Conditional Trees</title>
    <summary>  This article offers a modification of Chow and Liu's learning algorithm in
the context of handwritten digit recognition. The modified algorithm directs
the user to group digits into several classes consisting of digits that are
hard to distinguish and then constructing an optimal conditional tree
representation for each class of digits instead of for each single digit as
done by Chow and Liu (1968). Advantages and extensions of the new method are
discussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which
offer a different entropy-based learning algorithm are shown to rest on
inappropriate assumptions.
</summary>
    <author>
      <name>Dan Geiger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6001v1</id>
    <updated>2013-03-24T22:33:15Z</updated>
    <published>2013-03-24T22:33:15Z</published>
    <title>Generalizing k-means for an arbitrary distance matrix</title>
    <summary>  The original k-means clustering method works only if the exact vectors
representing the data points are known. Therefore calculating the distances
from the centroids needs vector operations, since the average of abstract data
points is undefined. Existing algorithms can be extended for those cases when
the sole input is the distance matrix, and the exact representing vectors are
unknown. This extension may be named relational k-means after a notation for a
similar algorithm invented for fuzzy clustering. A method is then proposed for
generalizing k-means for scenarios when the data points have absolutely no
connection with a Euclidean space.
</summary>
    <author>
      <name>Balázs Szalkai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.6001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3708v1</id>
    <updated>2013-04-12T19:09:56Z</updated>
    <published>2013-04-12T19:09:56Z</published>
    <title>Advice-Efficient Prediction with Expert Advice</title>
    <summary>  Advice-efficient prediction with expert advice (in analogy to label-efficient
prediction) is a variant of prediction with expert advice game, where on each
round of the game we are allowed to ask for advice of a limited number $M$ out
of $N$ experts. This setting is especially interesting when asking for advice
of every expert on every round is expensive. We present an algorithm for
advice-efficient prediction with expert advice that achieves
$O(\sqrt{\frac{N}{M}T\ln N})$ regret on $T$ rounds of the game.
</summary>
    <author>
      <name>Yevgeny Seldin</name>
    </author>
    <author>
      <name>Peter Bartlett</name>
    </author>
    <author>
      <name>Koby Crammer</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7158v1</id>
    <updated>2013-04-26T13:28:47Z</updated>
    <published>2013-04-26T13:28:47Z</published>
    <title>Irreflexive and Hierarchical Relations as Translations</title>
    <summary>  We consider the problem of embedding entities and relations of knowledge
bases in low-dimensional vector spaces. Unlike most existing approaches, which
are primarily efficient for modeling equivalence relations, our approach is
designed to explicitly model irreflexive relations, such as hierarchies, by
interpreting them as translations operating on the low-dimensional embeddings
of the entities. Preliminary experiments show that, despite its simplicity and
a smaller number of parameters than previous approaches, our approach achieves
state-of-the-art performance according to standard evaluation protocols on data
from WordNet and Freebase.
</summary>
    <author>
      <name>Antoine Bordes</name>
    </author>
    <author>
      <name>Nicolas Usunier</name>
    </author>
    <author>
      <name>Alberto Garcia-Duran</name>
    </author>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Oksana Yakhnenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted at the ICML 2013 workshop "Structured Learning: Inferring
  Graphs from Structured and Unstructured Inputs"</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0423v1</id>
    <updated>2013-05-02T13:03:53Z</updated>
    <published>2013-05-02T13:03:53Z</published>
    <title>Testing Hypotheses by Regularized Maximum Mean Discrepancy</title>
    <summary>  Do two data samples come from different distributions? Recent studies of this
fundamental problem focused on embedding probability distributions into
sufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to
compare distributions by the distance between their embeddings. We show that
Regularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based
hypothesis testing, yields substantial improvements even when sample sizes are
small, and excels at hypothesis tests involving multiple comparisons with power
control. We derive asymptotic distributions under the null and alternative
hypotheses, and assess power control. Outstanding results are obtained on:
challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar
dataset.
</summary>
    <author>
      <name>Somayeh Danafar</name>
    </author>
    <author>
      <name>Paola M. V. Rancoita</name>
    </author>
    <author>
      <name>Tobias Glasmachers</name>
    </author>
    <author>
      <name>Kevin Whittingstall</name>
    </author>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <link href="http://arxiv.org/abs/1305.0423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0445v2</id>
    <updated>2013-06-07T02:35:21Z</updated>
    <published>2013-05-02T14:33:28Z</published>
    <title>Deep Learning of Representations: Looking Forward</title>
    <summary>  Deep learning research aims at discovering learning algorithms that discover
multiple levels of distributed representations, with higher levels representing
more abstract concepts. Although the study of deep learning has already led to
impressive theoretical results, learning algorithms and breakthrough
experiments, several challenges lie ahead. This paper proposes to examine some
of these challenges, centering on the questions of scaling deep learning
algorithms to much larger models and datasets, reducing optimization
difficulties due to ill-conditioning or local minima, designing more efficient
and powerful inference and sampling procedures, and learning to disentangle the
factors of variation underlying the observed data. It also proposes a few
forward-looking research directions aimed at overcoming these challenges.
</summary>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1305.0445v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0445v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0626v1</id>
    <updated>2013-05-03T06:25:41Z</updated>
    <published>2013-05-03T06:25:41Z</published>
    <title>An Improved EM algorithm</title>
    <summary>  In this paper, we firstly give a brief introduction of expectation
maximization (EM) algorithm, and then discuss the initial value sensitivity of
expectation maximization algorithm. Subsequently, we give a short proof of EM's
convergence. Then, we implement experiments with the expectation maximization
algorithm (We implement all the experiments on Gaussion mixture model (GMM)).
Our experiment with expectation maximization is performed in the following
three cases: initialize randomly; initialize with result of K-means; initialize
with result of K-medoids. The experiment result shows that expectation
maximization algorithm depend on its initial state or parameters. And we found
that EM initialized with K-medoids performed better than both the one
initialized with K-means and the one initialized randomly.
</summary>
    <author>
      <name>Fuqiang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1305.0626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2648v1</id>
    <updated>2013-05-13T00:15:14Z</updated>
    <published>2013-05-13T00:15:14Z</published>
    <title>Boosting with the Logistic Loss is Consistent</title>
    <summary>  This manuscript provides optimization guarantees, generalization bounds, and
statistical consistency results for AdaBoost variants which replace the
exponential loss with the logistic and similar losses (specifically, twice
differentiable convex losses which are Lipschitz and tend to zero on one side).
  The heart of the analysis is to show that, in lieu of explicit regularization
and constraints, the structure of the problem is fairly rigidly controlled by
the source distribution itself. The first control of this type is in the
separable case, where a distribution-dependent relaxed weak learning rate
induces speedy convergence with high probability over any sample. Otherwise, in
the nonseparable case, the convex surrogate risk itself exhibits
distribution-dependent levels of curvature, and consequently the algorithm's
output has small norm with high probability.
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, COLT 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.2648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.4081v1</id>
    <updated>2013-05-17T13:53:17Z</updated>
    <published>2013-05-17T13:53:17Z</published>
    <title>Conditions for Convergence in Regularized Machine Learning Objectives</title>
    <summary>  Analysis of the convergence rates of modern convex optimization algorithms
can be achived through binary means: analysis of emperical convergence, or
analysis of theoretical convergence. These two pathways of capturing
information diverge in efficacy when moving to the world of distributed
computing, due to the introduction of non-intuitive, non-linear slowdowns
associated with broadcasting, and in some cases, gathering operations. Despite
these nuances in the rates of convergence, we can still show the existence of
convergence, and lower bounds for the rates. This paper will serve as a helpful
cheat-sheet for machine learning practitioners encountering this problem class
in the field.
</summary>
    <author>
      <name>Patrick Hop</name>
    </author>
    <author>
      <name>Xinghao Pan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.4081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.4081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5078v1</id>
    <updated>2013-05-22T10:43:25Z</updated>
    <published>2013-05-22T10:43:25Z</published>
    <title>A Comparison of Random Forests and Ferns on Recognition of Instruments
  in Jazz Recordings</title>
    <summary>  In this paper, we first apply random ferns for classification of real music
recordings of a jazz band. No initial segmentation of audio data is assumed,
i.e., no onset, offset, nor pitch data are needed. The notion of random ferns
is described in the paper, to familiarize the reader with this classification
algorithm, which was introduced quite recently and applied so far in image
recognition tasks. The performance of random ferns is compared with random
forests for the same data. The results of experiments are presented in the
paper, and conclusions are drawn.
</summary>
    <author>
      <name>Alicja A. Wieczorkowska</name>
    </author>
    <author>
      <name>Miron B. Kursa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Foundations of Intelligent Systems, Lecture Notes in Computer
  Science Volume 7661, 2012, pp 208-217</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.5078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6568v1</id>
    <updated>2013-05-28T17:47:08Z</updated>
    <published>2013-05-28T17:47:08Z</published>
    <title>Reinforcement Learning for the Soccer Dribbling Task</title>
    <summary>  We propose a reinforcement learning solution to the \emph{soccer dribbling
task}, a scenario in which a soccer agent has to go from the beginning to the
end of a region keeping possession of the ball, as an adversary attempts to
gain possession. While the adversary uses a stationary policy, the dribbler
learns the best action to take at each decision point. After defining
meaningful variables to represent the state space, and high-level macro-actions
to incorporate domain knowledge, we describe our application of the
reinforcement learning algorithm \emph{Sarsa} with CMAC for function
approximation. Our experiments show that, after the training period, the
dribbler is able to accomplish its task against a strong adversary around 58%
of the time.
</summary>
    <author>
      <name>Arthur Carvalho</name>
    </author>
    <author>
      <name>Renato Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CIG.2011.6031994</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CIG.2011.6031994" rel="related"/>
    <link href="http://arxiv.org/abs/1305.6568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0261v1</id>
    <updated>2013-07-01T02:49:08Z</updated>
    <published>2013-07-01T02:49:08Z</published>
    <title>WebSets: Extracting Sets of Entities from the Web Using Unsupervised
  Information Extraction</title>
    <summary>  We describe a open-domain information extraction method for extracting
concept-instance pairs from an HTML corpus. Most earlier approaches to this
problem rely on combining clusters of distributionally similar terms and
concept-instance pairs obtained with Hearst patterns. In contrast, our method
relies on a novel approach for clustering terms found in HTML tables, and then
assigning concept names to these clusters using Hearst patterns. The method can
be efficiently applied to a large corpus, and experimental results on several
datasets show that our method can accurately extract large numbers of
concept-instance pairs.
</summary>
    <author>
      <name>Bhavana Dalvi</name>
    </author>
    <author>
      <name>William W. Cohen</name>
    </author>
    <author>
      <name>Jamie Callan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; International Conference on Web Search and Data Mining 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.0261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4564v1</id>
    <updated>2013-07-17T10:24:00Z</updated>
    <published>2013-07-17T10:24:00Z</published>
    <title>From Bandits to Experts: A Tale of Domination and Independence</title>
    <summary>  We consider the partial observability model for multi-armed bandits,
introduced by Mannor and Shamir. Our main result is a characterization of
regret in the directed observability model in terms of the dominating and
independence numbers of the observability graph. We also show that in the
undirected case, the learner can achieve optimal regret without even accessing
the observability graph before selecting an action. Both results are shown
using variants of the Exp3 algorithm operating on the observability graph in a
time-efficient manner.
</summary>
    <author>
      <name>Noga Alon</name>
    </author>
    <author>
      <name>Nicolò Cesa-Bianchi</name>
    </author>
    <author>
      <name>Claudio Gentile</name>
    </author>
    <author>
      <name>Yishay Mansour</name>
    </author>
    <link href="http://arxiv.org/abs/1307.4564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4653v1</id>
    <updated>2013-07-17T14:38:47Z</updated>
    <published>2013-07-17T14:38:47Z</published>
    <title>A New Convex Relaxation for Tensor Completion</title>
    <summary>  We study the problem of learning a tensor from a set of linear measurements.
A prominent methodology for this problem is based on a generalization of trace
norm regularization, which has been used extensively for learning low rank
matrices, to the tensor setting. In this paper, we highlight some limitations
of this approach and propose an alternative convex relaxation on the Euclidean
ball. We then describe a technique to solve the associated regularization
problem, which builds upon the alternating direction method of multipliers.
Experiments on one synthetic dataset and two real datasets indicate that the
proposed method improves significantly over tensor trace norm regularization in
terms of estimation error, while remaining computationally tractable.
</summary>
    <author>
      <name>Bernardino Romera-Paredes</name>
    </author>
    <author>
      <name>Massimiliano Pontil</name>
    </author>
    <link href="http://arxiv.org/abs/1307.4653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5599v1</id>
    <updated>2013-07-22T06:50:21Z</updated>
    <published>2013-07-22T06:50:21Z</published>
    <title>Performance comparison of State-of-the-art Missing Value Imputation
  Algorithms on Some Bench mark Datasets</title>
    <summary>  Decision making from data involves identifying a set of attributes that
contribute to effective decision making through computational intelligence. The
presence of missing values greatly influences the selection of right set of
attributes and this renders degradation in classification accuracies of the
classifiers. As missing values are quite common in data collection phase during
field experiments or clinical trails appropriate handling would improve the
classifier performance. In this paper we present a review of recently developed
missing value imputation algorithms and compare their performance on some bench
mark datasets.
</summary>
    <author>
      <name>M. Naresh Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.5599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6814v1</id>
    <updated>2013-07-25T17:07:39Z</updated>
    <published>2013-07-25T17:07:39Z</published>
    <title>A Propound Method for the Improvement of Cluster Quality</title>
    <summary>  In this paper Knockout Refinement Algorithm (KRA) is proposed to refine
original clusters obtained by applying SOM and K-Means clustering algorithms.
KRA Algorithm is based on Contingency Table concepts. Metrics are computed for
the Original and Refined Clusters. Quality of Original and Refined Clusters are
compared in terms of metrics. The proposed algorithm (KRA) is tested in the
educational domain and results show that it generates better quality clusters
in terms of improved metric values.
</summary>
    <author>
      <name>Shveta Kundra Bhatia</name>
    </author>
    <author>
      <name>V. S. Dixit</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012 ISSN (Online): 1694-0814</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7028v1</id>
    <updated>2013-07-26T13:24:31Z</updated>
    <published>2013-07-26T13:24:31Z</published>
    <title>Infinite Mixtures of Multivariate Gaussian Processes</title>
    <summary>  This paper presents a new model called infinite mixtures of multivariate
Gaussian processes, which can be used to learn vector-valued functions and
applied to multitask learning. As an extension of the single multivariate
Gaussian process, the mixture model has the advantages of modeling multimodal
data and alleviating the computationally cubic complexity of the multivariate
Gaussian process. A Dirichlet process prior is adopted to allow the (possibly
infinite) number of mixture components to be automatically inferred from
training data, and Markov chain Monte Carlo sampling techniques are used for
parameter and latent variable inference. Preliminary experimental results on
multivariate regression show the feasibility of the proposed model.
</summary>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Machine Learning and
  Cybernetics, 2013, pages 1011-1016</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7286v1</id>
    <updated>2013-07-27T18:00:43Z</updated>
    <published>2013-07-27T18:00:43Z</published>
    <title>A Review of Machine Learning based Anomaly Detection Techniques</title>
    <summary>  Intrusion detection is so much popular since the last two decades where
intrusion is attempted to break into or misuse the system. It is mainly of two
types based on the intrusions, first is Misuse or signature based detection and
the other is Anomaly detection. In this paper Machine learning based methods
which are one of the types of Anomaly detection techniques is discussed.
</summary>
    <author>
      <name>Harjinder Kaur</name>
    </author>
    <author>
      <name>Gurpreet Singh</name>
    </author>
    <author>
      <name>Jaspreet Minhas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages. arXiv admin note: text overlap with arXiv:1204.6416 by other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7303v1</id>
    <updated>2013-07-27T20:33:34Z</updated>
    <published>2013-07-27T20:33:34Z</published>
    <title>Learning to Understand by Evolving Theories</title>
    <summary>  In this paper, we describe an approach that enables an autonomous system to
infer the semantics of a command (i.e. a symbol sequence representing an
action) in terms of the relations between changes in the observations and the
action instances. We present a method of how to induce a theory (i.e. a
semantic description) of the meaning of a command in terms of a minimal set of
background knowledge. The only thing we have is a sequence of observations from
which we extract what kinds of effects were caused by performing the command.
This way, we yield a description of the semantics of the action and, hence, a
definition.
</summary>
    <author>
      <name>Martin E. Mueller</name>
    </author>
    <author>
      <name>Madhura D. Thosar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KRR Workshop at ICLP 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; I.2.4; I.2.6; I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8430v1</id>
    <updated>2013-07-31T19:18:11Z</updated>
    <published>2013-07-31T19:18:11Z</published>
    <title>Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ)</title>
    <summary>  We present an efficient algorithm for simultaneously training sparse
generalized linear models across many related problems, which may arise from
bootstrapping, cross-validation and nonparametric permutation testing. Our
approach leverages the redundancies across problems to obtain significant
computational improvements relative to solving the problems sequentially by a
conventional algorithm. We demonstrate our fast simultaneous training of
generalized linear models (FaSTGLZ) algorithm on a number of real-world
datasets, and we run otherwise computationally intensive bootstrapping and
permutation test analyses that are typically necessary for obtaining
statistically rigorous classification results and meaningful interpretation.
Code is freely available at http://liinc.bme.columbia.edu/fastglz.
</summary>
    <author>
      <name>Bryan R. Conroy</name>
    </author>
    <author>
      <name>Jennifer M. Walz</name>
    </author>
    <author>
      <name>Brian Cheung</name>
    </author>
    <author>
      <name>Paul Sajda</name>
    </author>
    <link href="http://arxiv.org/abs/1307.8430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3513v1</id>
    <updated>2013-08-15T21:21:05Z</updated>
    <published>2013-08-15T21:21:05Z</published>
    <title>Hidden Parameter Markov Decision Processes: A Semiparametric Regression
  Approach for Discovering Latent Task Parametrizations</title>
    <summary>  Control applications often feature tasks with similar, but not identical,
dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP),
a framework that parametrizes a family of related dynamical systems with a
low-dimensional set of latent factors, and introduce a semiparametric
regression approach for learning its structure from data. In the control
setting, we show that a learned HiP-MDP rapidly identifies the dynamics of a
new task instance, allowing an agent to flexibly adapt to task variations.
</summary>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <author>
      <name>George Konidaris</name>
    </author>
    <link href="http://arxiv.org/abs/1308.3513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3558v1</id>
    <updated>2013-08-16T05:48:29Z</updated>
    <published>2013-08-16T05:48:29Z</published>
    <title>Fast Stochastic Alternating Direction Method of Multipliers</title>
    <summary>  In this paper, we propose a new stochastic alternating direction method of
multipliers (ADMM) algorithm, which incrementally approximates the full
gradient in the linearized ADMM formulation. Besides having a low per-iteration
complexity as existing stochastic ADMM algorithms, the proposed algorithm
improves the convergence rate on convex problems from $O(\frac 1 {\sqrt{T}})$
to $O(\frac 1 T)$, where $T$ is the number of iterations. This matches the
convergence rate of the batch ADMM algorithm, but without the need to visit all
the samples in each iteration. Experiments on the graph-guided fused lasso
demonstrate that the new algorithm is significantly faster than
state-of-the-art stochastic and batch ADMM algorithms.
</summary>
    <author>
      <name>Leon Wenliang Zhong</name>
    </author>
    <author>
      <name>James T. Kwok</name>
    </author>
    <link href="http://arxiv.org/abs/1308.3558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3750v1</id>
    <updated>2013-08-17T03:56:03Z</updated>
    <published>2013-08-17T03:56:03Z</published>
    <title>Comment on "robustness and regularization of support vector machines" by
  H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510,
  2009, arXiv:0803.3490)</title>
    <summary>  This paper comments on the published work dealing with robustness and
regularization of support vector machines (Journal of Machine Learning
Research, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They
proposed a theorem to show that it is possible to relate robustness in the
feature space and robustness in the sample space directly. In this paper, we
propose a counter example that rejects their theorem.
</summary>
    <author>
      <name>Yahya Forghani</name>
    </author>
    <author>
      <name>Hadi Sadoghi Yazdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages. This paper has been accepted with minor revision in journal
  of machine learning research (JMLR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4828v1</id>
    <updated>2013-08-22T11:39:06Z</updated>
    <published>2013-08-22T11:39:06Z</published>
    <title>The Sample-Complexity of General Reinforcement Learning</title>
    <summary>  We present a new algorithm for general reinforcement learning where the true
environment is known to belong to a finite class of N arbitrary models. The
algorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with
high probability. Infinite classes are also considered where we show that
compactness is a key criterion for determining the existence of uniform
sample-complexity bounds. A matching lower bound is given for the finite case.
</summary>
    <author>
      <name>Tor Lattimore</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <author>
      <name>Peter Sunehag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.4828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1543v1</id>
    <updated>2013-09-06T06:06:15Z</updated>
    <published>2013-09-06T06:06:15Z</published>
    <title>A Comparism of the Performance of Supervised and Unsupervised Machine
  Learning Techniques in evolving Awale/Mancala/Ayo Game Player</title>
    <summary>  Awale games have become widely recognized across the world, for their
innovative strategies and techniques which were used in evolving the agents
(player) and have produced interesting results under various conditions. This
paper will compare the results of the two major machine learning techniques by
reviewing their performance when using minimax, endgame database, a combination
of both techniques or other techniques, and will determine which are the best
techniques.
</summary>
    <author>
      <name>O. A. Randle</name>
    </author>
    <author>
      <name>O. O. Ogunduyile</name>
    </author>
    <author>
      <name>T. Zuva</name>
    </author>
    <author>
      <name>N. A. Fashola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Game Theory and Technology (IJGTT),
  Vol.1, No.1, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.1543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6818v1</id>
    <updated>2013-09-26T12:35:03Z</updated>
    <published>2013-09-26T12:35:03Z</published>
    <title>Boosting in the presence of label noise</title>
    <summary>  Boosting is known to be sensitive to label noise. We studied two approaches
to improve AdaBoost's robustness against labelling errors. One is to employ a
label-noise robust classifier as a base learner, while the other is to modify
the AdaBoost algorithm to be more robust. Empirical evaluation shows that a
committee of robust classifiers, although converges faster than non label-noise
aware AdaBoost, is still susceptible to label noise. However, pairing it with
the new robust Boosting algorithm we propose here results in a more resilient
algorithm under mislabelling.
</summary>
    <author>
      <name>Jakramate Bootkrajang</name>
    </author>
    <author>
      <name>Ata Kaban</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6835v1</id>
    <updated>2013-09-26T12:41:06Z</updated>
    <published>2013-09-26T12:41:06Z</published>
    <title>Gaussian Processes for Big Data</title>
    <summary>  We introduce stochastic variational inference for Gaussian process models.
This enables the application of Gaussian process (GP) models to data sets
containing millions of data points. We show how GPs can be vari- ationally
decomposed to depend on a set of globally relevant inducing variables which
factorize the model in the necessary manner to perform variational inference.
Our ap- proach is readily extended to models with non-Gaussian likelihoods and
latent variable models based around Gaussian processes. We demonstrate the
approach on a simple toy problem and two real world data sets.
</summary>
    <author>
      <name>James Hensman</name>
    </author>
    <author>
      <name>Nicolo Fusi</name>
    </author>
    <author>
      <name>Neil D. Lawrence</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6850v1</id>
    <updated>2013-09-26T12:45:59Z</updated>
    <published>2013-09-26T12:45:59Z</published>
    <title>Structured Convex Optimization under Submodular Constraints</title>
    <summary>  A number of discrete and continuous optimization problems in machine learning
are related to convex minimization problems under submodular constraints. In
this paper, we deal with a submodular function with a directed graph structure,
and we show that a wide range of convex optimization problems under submodular
constraints can be solved much more efficiently than general submodular
optimization methods by a reduction to a maximum flow problem. Furthermore, we
give some applications, including sparse optimization methods, in which the
proposed methods are effective. Additionally, we evaluate the performance of
the proposed method through computational experiments.
</summary>
    <author>
      <name>Kiyohito Nagano</name>
    </author>
    <author>
      <name>Yoshinobu Kawahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6860v1</id>
    <updated>2013-09-26T12:49:46Z</updated>
    <published>2013-09-26T12:49:46Z</published>
    <title>Identifying Finite Mixtures of Nonparametric Product Distributions and
  Causal Inference of Confounders</title>
    <summary>  We propose a kernel method to identify finite mixtures of nonparametric
product distributions. It is based on a Hilbert space embedding of the joint
distribution. The rank of the constructed tensor is equal to the number of
mixture components. We present an algorithm to recover the components by
partitioning the data points into clusters such that the variables are jointly
conditionally independent given the cluster. This method can be used to
identify finite confounders.
</summary>
    <author>
      <name>Eleni Sgouritsa</name>
    </author>
    <author>
      <name>Dominik Janzing</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <author>
      <name>Bernhard Schoelkopf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6867v1</id>
    <updated>2013-09-26T12:51:22Z</updated>
    <published>2013-09-26T12:51:22Z</published>
    <title>Speedy Model Selection (SMS) for Copula Models</title>
    <summary>  We tackle the challenge of efficiently learning the structure of expressive
multivariate real-valued densities of copula graphical models. We start by
theoretically substantiating the conjecture that for many copula families the
magnitude of Spearman's rank correlation coefficient is monotone in the
expected contribution of an edge in network, namely the negative copula
entropy. We then build on this theory and suggest a novel Bayesian approach
that makes use of a prior over values of Spearman's rho for learning
copula-based models that involve a mix of copula families. We demonstrate the
generalization effectiveness of our highly efficient approach on sizable and
varied real-life datasets.
</summary>
    <author>
      <name>Yaniv Tenzer</name>
    </author>
    <author>
      <name>Gal Elidan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6868v1</id>
    <updated>2013-09-26T12:51:47Z</updated>
    <published>2013-09-26T12:51:47Z</published>
    <title>Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs</title>
    <summary>  We seek to learn an effective policy for a Markov Decision Process (MDP) with
continuous states via Q-Learning. Given a set of basis functions over state
action pairs we search for a corresponding set of linear weights that minimizes
the mean Bellman residual. Our algorithm uses a Kalman filter model to estimate
those weights and we have developed a simpler approximate Kalman filter model
that outperforms the current state of the art projected TD-Learning methods on
several standard benchmark problems.
</summary>
    <author>
      <name>Charles Tripp</name>
    </author>
    <author>
      <name>Ross D. Shachter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty
  in Artificial Intelligence (UAI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7598v1</id>
    <updated>2013-09-29T13:48:52Z</updated>
    <published>2013-09-29T13:48:52Z</published>
    <title>On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori
  Perturbations</title>
    <summary>  In this paper we describe how MAP inference can be used to sample efficiently
from Gibbs distributions. Specifically, we provide means for drawing either
approximate or unbiased samples from Gibbs' distributions by introducing low
dimensional perturbations and solving the corresponding MAP assignments. Our
approach also leads to new ways to derive lower bounds on partition functions.
We demonstrate empirically that our method excels in the typical "high signal -
high coupling" regime. The setting results in ragged energy landscapes that are
challenging for alternative approaches to sampling and/or lower bounds.
</summary>
    <author>
      <name>Tamir Hazan</name>
    </author>
    <author>
      <name>Subhransu Maji</name>
    </author>
    <author>
      <name>Tommi Jaakkola</name>
    </author>
    <link href="http://arxiv.org/abs/1309.7598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7676v1</id>
    <updated>2013-09-29T23:45:59Z</updated>
    <published>2013-09-29T23:45:59Z</published>
    <title>An upper bound on prototype set size for condensed nearest neighbor</title>
    <summary>  The condensed nearest neighbor (CNN) algorithm is a heuristic for reducing
the number of prototypical points stored by a nearest neighbor classifier,
while keeping the classification rule given by the reduced prototypical set
consistent with the full set. I present an upper bound on the number of
prototypical points accumulated by CNN. The bound originates in a bound on the
number of times the decision rule is updated during training in the multiclass
perceptron algorithm, and thus is independent of training set size.
</summary>
    <author>
      <name>Eric Christiansen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This was submitted to the journal Artificial Intelligence in 2009,
  and while it was considered technically sound, it was also believed to be of
  minor importance. My research has since moved on, so I'm unlikely to attempt
  a resubmission</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7959v1</id>
    <updated>2013-09-19T07:10:53Z</updated>
    <published>2013-09-19T07:10:53Z</published>
    <title>Exploration and Exploitation in Visuomotor Prediction of Autonomous
  Agents</title>
    <summary>  This paper discusses various techniques to let an agent learn how to predict
the effects of its own actions on its sensor data autonomously, and their
usefulness to apply them to visual sensors. An Extreme Learning Machine is used
for visuomotor prediction, while various autonomous control techniques that can
aid the prediction process by balancing exploration and exploitation are
discussed and tested in a simple system: a camera moving over a 2D greyscale
image.
</summary>
    <author>
      <name>Laurens Bliek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Award-winning paper of the internal conference 'Almende research
  workshop 2013'</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1840v1</id>
    <updated>2013-10-07T16:04:28Z</updated>
    <published>2013-10-07T16:04:28Z</published>
    <title>Parallel coordinate descent for the Adaboost problem</title>
    <summary>  We design a randomised parallel version of Adaboost based on previous studies
on parallel coordinate descent. The algorithm uses the fact that the logarithm
of the exponential loss is a function with coordinate-wise Lipschitz continuous
gradient, in order to define the step lengths. We provide the proof of
convergence for this randomised Adaboost algorithm and a theoretical
parallelisation speedup factor. We finally provide numerical examples on
learning problems of various sizes that show that the algorithm is competitive
with concurrent approaches, especially for large scale problems.
</summary>
    <author>
      <name>Olivier Fercoq</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMLA.2013.72</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMLA.2013.72" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, extended version of the paper presented to
  ICMLA'13</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
