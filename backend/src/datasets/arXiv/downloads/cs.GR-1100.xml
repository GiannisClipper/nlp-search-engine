<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.GR%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.GR&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/qKOhy8spERqpkxDwB+eTn7yK85k</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">7578</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/9301112v1</id>
    <updated>1990-04-01T00:00:00Z</updated>
    <published>1990-04-01T00:00:00Z</published>
    <title>A note on digitized angles</title>
    <summary>  We study the configurations of pixels that occur when two digitized straight
lines meet each other.
</summary>
    <author>
      <name>Donald E. Knuth</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Publishing 3 (1990), no. 2, 99--104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9301112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9301112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0302013v1</id>
    <updated>2003-02-12T05:16:12Z</updated>
    <published>2003-02-12T05:16:12Z</published>
    <title>Cg in Two Pages</title>
    <summary>  Cg is a language for programming GPUs. This paper describes Cg briefly.
</summary>
    <author>
      <name>Mark J. Kilgard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0302013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0302013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2824v1</id>
    <updated>2013-03-12T10:20:34Z</updated>
    <published>2013-03-12T10:20:34Z</published>
    <title>Fourth-order flows in surface modelling</title>
    <summary>  This short article is a brief account of the usage of fourth-order curvature
flow in surface modelling.
</summary>
    <author>
      <name>Ty Kang</name>
    </author>
    <link href="http://arxiv.org/abs/1303.2824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6786v2</id>
    <updated>2014-06-27T07:51:43Z</updated>
    <published>2014-06-26T07:02:33Z</published>
    <title>3D Texture Coordinates on Polygon Mesh Sequences</title>
    <summary>  A method for creating 3D texture coordinates for a sequence of polygon meshes
with changing topology and vertex motion vectors.
</summary>
    <author>
      <name>Eric Mootz</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6786v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6786v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03898v1</id>
    <updated>2016-07-22T12:52:09Z</updated>
    <published>2016-07-22T12:52:09Z</published>
    <title>Curvature transformation</title>
    <summary>  A transformation based on mean curvature is introduced which morphs
triangulated surfaces into round spheres.
</summary>
    <author>
      <name>Dimitris Vartziotis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="53C44" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05773v1</id>
    <updated>2016-08-20T03:25:50Z</updated>
    <published>2016-08-20T03:25:50Z</published>
    <title>Extending Scatterplots to Scalar Fields</title>
    <summary>  Embedding high-dimensional data into a 2D canvas is a popular strategy for
their visualization.
</summary>
    <author>
      <name>Shenghui Cheng</name>
    </author>
    <author>
      <name>Pengcheng Cui</name>
    </author>
    <author>
      <name>Klaus Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.06080v1</id>
    <updated>2020-06-10T21:50:02Z</updated>
    <published>2020-06-10T21:50:02Z</published>
    <title>Least-Squares Affine Reflection Using Eigen Decomposition</title>
    <summary>  This note summarizes the steps to computing the best-fitting affine
reflection that aligns two sets of corresponding points.
</summary>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 page</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.06080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.06080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.15477v1</id>
    <updated>2023-09-27T08:18:04Z</updated>
    <published>2023-09-27T08:18:04Z</published>
    <title>A Tutorial on Uniform B-Spline</title>
    <summary>  This document facilitates understanding of core concepts about uniform
B-spline and its matrix representation.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2309.15477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.15477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0600v1</id>
    <updated>2013-04-02T11:55:33Z</updated>
    <published>2013-04-02T11:55:33Z</published>
    <title>Software for creating pictures in the LaTeX environment</title>
    <summary>  To create a text with graphic instructions for output pictures into LATEX
document, we offer software that allows us to build a picture in WIZIWIG mode
and for setting the text with these graphical instructions.
</summary>
    <author>
      <name>Bezhentcev Roman Vadimovich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, 2 formulas, sourcecode</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.0600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07953v1</id>
    <updated>2016-01-29T00:08:24Z</updated>
    <published>2016-01-29T00:08:24Z</published>
    <title>Boolean Operations using Generalized Winding Numbers</title>
    <summary>  The generalized winding number function measures insideness for arbitrary
oriented triangle meshes. Exploiting this, I similarly generalize binary
boolean operations to act on such meshes. The resulting operations for union,
intersection, difference, etc. avoid volumetric discretization or
pre-processing.
</summary>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 page, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05064v1</id>
    <updated>2018-12-16T13:06:49Z</updated>
    <published>2018-12-16T13:06:49Z</published>
    <title>A novel 3D display based on micro-volumetric scanning and real time
  reconstruction of holograms principle</title>
    <summary>  The present study proposes a novel 3D display contains a micro-volumetric
scanning system (MVS) and a real time reconstruction hologram system (RTRH).
</summary>
    <author>
      <name>Guangjun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1706.03231</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.05064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11133v1</id>
    <updated>2020-03-24T22:21:28Z</updated>
    <published>2020-03-24T22:21:28Z</published>
    <title>Global Illumination of non-Euclidean spaces</title>
    <summary>  This paper presents a path tracer algorithm to compute the global
illumination of non-Euclidean manifolds. We use the 3D torus as an example.
</summary>
    <author>
      <name>Tiago Novello</name>
    </author>
    <author>
      <name>Vinicius da Silva</name>
    </author>
    <author>
      <name>Luiz Velho</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12661v1</id>
    <updated>2020-06-22T23:02:10Z</updated>
    <published>2020-06-22T23:02:10Z</published>
    <title>SN-Engine, a Scale-free Geometric Modelling Environment</title>
    <summary>  We present a new scale-free geometric modelling environment designed by the
author of the paper. It allows one to consistently treat geometric objects of
arbitrary size and offers extensive analytic and computational support for
visualization of both real and artificial sceneries.
</summary>
    <author>
      <name>T. A. Zhukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.12661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01618v1</id>
    <updated>2021-03-02T10:19:17Z</updated>
    <published>2021-03-02T10:19:17Z</published>
    <title>An analytic BRDF for materials with spherical Lambertian scatterers</title>
    <summary>  We present a new analytic BRDF for porous materials comprised of spherical
Lambertian scatterers. The BRDF has a single parameter: the albedo of the
Lambertian particles. The resulting appearance exhibits strong back scattering
and saturation effects that height-field-based models such as Oren-Nayar cannot
reproduce.
</summary>
    <author>
      <name>Eugene d'Eon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09357v1</id>
    <updated>2020-09-20T05:45:36Z</updated>
    <published>2020-09-20T05:45:36Z</published>
    <title>3D Modeling and WebVR Implementation using Azure Kinect, Open3D, and
  Three.js</title>
    <summary>  This paper proposes a method of extracting an RGB-D image usingAzure Kinect,
a depth camera, creating afragment,i.e., 6D images (RGBXYZ), usingOpen3D,
creatingit as a point cloud object, and implementing webVR usingthree.js.
Furthermore, it presents limitations and potentials for development.
</summary>
    <author>
      <name>Won Joon Yun</name>
    </author>
    <author>
      <name>Joongheon Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.00855v3</id>
    <updated>2022-02-08T12:47:32Z</updated>
    <published>2022-02-02T02:29:12Z</published>
    <title>Extension: Adaptive Sampling with Implicit Radiance Field</title>
    <summary>  This manuscript discusses the extension of adaptive light field sampling with
implicit radiance fields.
</summary>
    <author>
      <name>Yuchi Huo</name>
    </author>
    <link href="http://arxiv.org/abs/2202.00855v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.00855v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13216v1</id>
    <updated>2022-06-27T12:07:29Z</updated>
    <published>2022-06-27T12:07:29Z</published>
    <title>Clipping and Intersection Algorithms: Short Survey and References</title>
    <summary>  This contribution presents a brief survey of clipping and intersection
algorithms in E2 and E3 with a nearly complete list of relevant references.
Some algorithms use the projective extension of the Euclidean space and
vector-vector operations, which supports GPU and SSE use.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.13216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68xx, 68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.08818v1</id>
    <updated>2024-11-13T17:57:13Z</updated>
    <published>2024-11-13T17:57:13Z</published>
    <title>On integer sequences for rendering limit sets of Kleinian groups</title>
    <summary>  We present a technique for rendering limit sets for kleinian groups, based
upon the base transformation of integers and which aims at saving memory
resources and being faster than the traditional dictionary based approach.
</summary>
    <author>
      <name>Alessandro Rosa</name>
    </author>
    <link href="http://arxiv.org/abs/2411.08818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.08818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08383v1</id>
    <updated>2025-04-11T09:36:38Z</updated>
    <published>2025-04-11T09:36:38Z</published>
    <title>Generation of Zoomable maps with Rivers and Fjords</title>
    <summary>  This paper presents a method for generating maps with rivers and fjords. The
method is based on recursive subdivision of triangles and allows unlimited zoom
on details without requiring generation of a full map at high resolution.
</summary>
    <author>
      <name>Torben Æ. Mogensen</name>
    </author>
    <author>
      <name>Emil N. Isenbecker</name>
    </author>
    <link href="http://arxiv.org/abs/2504.08383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304011v1</id>
    <updated>2003-04-08T14:17:53Z</updated>
    <published>2003-04-08T14:17:53Z</published>
    <title>Embedded Reflection Mapping</title>
    <summary>  Environment maps are used to simulate reflections off curved objects. We
present a technique to reflect a user, or a group of users, in a real
environment, onto a virtual object, in a virtual reality application, using the
live video feeds from a set of cameras, in real-time. Our setup can be used in
a variety of environments ranging from outdoor or indoor scenes.
</summary>
    <author>
      <name>Paul Anderson</name>
    </author>
    <author>
      <name>Goncalo Carvalho</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0304011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503054v1</id>
    <updated>2005-03-22T16:59:56Z</updated>
    <published>2005-03-22T16:59:56Z</published>
    <title>Analytic Definition of Curves and Surfaces by Parabolic Blending</title>
    <summary>  A procedure for interpolating between specified points of a curve or surface
is described. The method guarantees slope continuity at all junctions. A
surface panel divided into p x q contiguous patches is completely specified by
the coordinates of (p+1) x (q+1) points. Each individual patch, however,
depends parametrically on the coordinates of 16 points, allowing shape
flexibility and global conformity.
</summary>
    <author>
      <name>A. W. Overhauser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0510087v1</id>
    <updated>2005-10-31T09:40:00Z</updated>
    <published>2005-10-31T09:40:00Z</published>
    <title>MathPSfrag: Creating Publication-Quality Labels in Mathematica Plots</title>
    <summary>  This article introduces a Mathematica package providing a graphics export
function that automatically replaces Mathematica expressions in a graphic by
the corresponding LaTeX constructs and positions them correctly. It thus
facilitates the creation of publication-quality Enscapulated PostScript (EPS)
graphics.
</summary>
    <author>
      <name>J. Grosse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures, for associated Mathematica package, see
  http://wwwth.mppmu.mpg.de/members/jgrosse/mathpsfrag/MathPSfrag-1.0.tar.gz</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0510087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0510087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512098v1</id>
    <updated>2005-12-26T14:45:32Z</updated>
    <published>2005-12-26T14:45:32Z</published>
    <title>Mathematical models of the complex surfaces in simulation and
  visualization systems</title>
    <summary>  Modeling, simulation and visualization of three-dimension complex bodies
widely use mathematical model of curves and surfaces. The most important curves
and surfaces for these purposes are curves and surfaces in Hermite and Bezier
forms, splines and NURBS. Article is devoted to survey this way to use
geometrical data in various computer graphics systems and adjacent fields.
</summary>
    <author>
      <name>Dmitry P. Paukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.0121v1</id>
    <updated>2007-12-02T07:25:59Z</updated>
    <published>2007-12-02T07:25:59Z</published>
    <title>Efficient Binary and Run Length Morphology and its Application to
  Document Image Processing</title>
    <summary>  This paper describes the implementation and evaluation of an open source
library for mathematical morphology based on packed binary and run-length
compressed images for document imaging applications. Abstractions and patterns
useful in the implementation of the interval operations are described. A number
of benchmarks and comparisons to bit-blit based implementations on standard
document images are provided.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/0712.0121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.0121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4; I.4.10; I.7.4; I.7.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.2175v1</id>
    <updated>2008-01-15T18:34:44Z</updated>
    <published>2008-01-15T18:34:44Z</published>
    <title>MathPSfrag 2: Convenient LaTeX Labels in Mathematica</title>
    <summary>  This article introduces the next version of MathPSfrag. MathPSfrag is a
Mathematica package that during export automatically replaces all expressions
in a plot by corresponding LaTeX commands. The new version can also produce
LaTeX independent images; e.g., PDF files for inclusion in pdfLaTeX. Moreover
from these files a preview is generated and shown within Mathematica.
</summary>
    <author>
      <name>Johannes Große</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, package can be found at
  http://wwwth.mppmu.mpg.de/members/jgrosse/mathpsfrag/</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.2175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.2175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4854v1</id>
    <updated>2009-10-26T10:55:16Z</updated>
    <published>2009-10-26T10:55:16Z</published>
    <title>Yet Another Pacman 3D Adventures</title>
    <summary>  This game is meant to be extension of the overly-beaten pacman-style game
(code-named "Yet Another Pacman 3D Adventures", or YAP3DAD) from the proposed
ideas and other projects with advance visual and computer graphics features,
including a-game-in-a-game approach. The project is an open-source project
published on SourceForge.net for possible future development and extension.
</summary>
    <author>
      <name>Serguei A. Mokhov</name>
    </author>
    <author>
      <name>Yingying She</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 8 figures. A 2006 report, corresponding to the open source
  project here: http://sourceforge.net/projects/yap3dad/</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.4854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3481v2</id>
    <updated>2010-02-03T16:35:49Z</updated>
    <published>2010-01-20T07:35:21Z</published>
    <title>Resolution scalability improvement for JPEG2000 standard color image</title>
    <summary>  Removed by arXiv administration. This article was plagiarised from
http://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other
locations.
</summary>
    <author>
      <name>U. Vijayasankar</name>
    </author>
    <author>
      <name>S. Prasadh.</name>
    </author>
    <author>
      <name>A. Arul Lawrence Selvakumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removed by arXiv administration. This article was plagiarised from
  http://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other
  locations</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.3481v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3481v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0243v1</id>
    <updated>2010-12-31T12:30:34Z</updated>
    <published>2010-12-31T12:30:34Z</published>
    <title>Across Browsers SVG Implementation</title>
    <summary>  In this work SVG will be translated into VML or HTML by using Javascript
based on Backbase Client Framework. The target of this project is to implement
SVG to be viewed in Internet Explorer without any plug-in and work together
with other Backbase Client Framework languages. The result of this project will
be added as an extension to the current Backbase Client Framework.
</summary>
    <author>
      <name>Liang Wang</name>
    </author>
    <author>
      <name>Nies Huijsmans</name>
    </author>
    <author>
      <name>Michael S. Lew</name>
    </author>
    <author>
      <name>Dan Tsymbala</name>
    </author>
    <link href="http://arxiv.org/abs/1101.0243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0301v1</id>
    <updated>2010-12-31T21:23:11Z</updated>
    <published>2010-12-31T21:23:11Z</published>
    <title>Specular holography</title>
    <summary>  By tooling an spot-illuminated surface to control the flow of specular glints
under motion, one can produce holographic view-dependent imagery. This paper
presents the differential equation that governs the shape of the specular
surfaces, and illustrates how solutions can be constructed for different kinds
of motion, lighting, host surface geometries, and fabrication constraints,
leading to some novel forms of holography.
</summary>
    <author>
      <name>Matthew Brand</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/AO.50.005042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/AO.50.005042" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3013v1</id>
    <updated>2011-07-15T08:58:09Z</updated>
    <published>2011-07-15T08:58:09Z</published>
    <title>Linear-Time Poisson-Disk Patterns</title>
    <summary>  We present an algorithm for generating Poisson-disc patterns taking O(N) time
to generate $N$ points. The method is based on a grid of regions which can
contain no more than one point in the final pattern, and uses an explicit model
of point arrival times under a uniform Poisson process.
</summary>
    <author>
      <name>Thouis R. Jones</name>
    </author>
    <author>
      <name>David R. Karger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/2151237X.2011.617173</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/2151237X.2011.617173" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.3013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1914v2</id>
    <updated>2011-10-24T05:56:49Z</updated>
    <published>2011-09-09T06:49:52Z</published>
    <title>Jacobians and Hessians of Mean Value Coordinates for Closed Triangular
  Meshes</title>
    <summary>  In this technical note, we present the formulae of the derivatives of the
Mean Value Coordinates based transformations, using an enclosing triangle mesh,
acting as a cage for the deformation of an interior object.
</summary>
    <author>
      <name>Jean-Marc Thiery</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Tierny</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Tamy Boubekeur</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1109.1914v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1914v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6007v1</id>
    <updated>2013-01-25T11:03:18Z</updated>
    <published>2013-01-25T11:03:18Z</published>
    <title>Immersive VR Visualizations by VFIVE. Part 1: Development</title>
    <summary>  We have been developing a visualization application for CAVE-type virtual
reality (VR) systems for more than a decade. This application, VFIVE, is
currently used in several CAVE systems in Japan for routine visualizations. It
is also used as a base system of further developments of advanced
visualizations. The development of VFIVE is summarized.
</summary>
    <author>
      <name>Akira Kageyama</name>
    </author>
    <author>
      <name>Nobuaki Ohno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.6007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1240v1</id>
    <updated>2013-10-04T12:27:46Z</updated>
    <published>2013-10-04T12:27:46Z</published>
    <title>Compression of animated 3D models using HO-SVD</title>
    <summary>  This work presents an analysis of Higher Order Singular Value Decomposition
(HO-SVD) applied to lossy compression of 3D mesh animations. We describe
strategies for choosing a number of preserved spatial and temporal components
after tensor decomposition. Compression error is measured using three metrics
(MSE, Hausdorff, MSDM). Results are compared with a method based on Principal
Component Analysis (PCA) and presented on a set of animations with typical mesh
deformations.
</summary>
    <author>
      <name>Michał Romaszewski</name>
    </author>
    <author>
      <name>Piotr Gawron</name>
    </author>
    <author>
      <name>Sebastian Opozda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6935v1</id>
    <updated>2013-12-25T06:56:35Z</updated>
    <published>2013-12-25T06:56:35Z</published>
    <title>Application of polynomial texture mapping in process of digitalization
  of cultural heritage</title>
    <summary>  In this paper we present modern texture mapping techniques and several
applications of polynomial texture mapping in cultural heritage programs. We
also consider some well-known and some new methods for mathematical procedure
that is involved in generation of polynomial texture maps.
</summary>
    <author>
      <name>Branko Malesevic</name>
    </author>
    <author>
      <name>Ratko Obradovic</name>
    </author>
    <author>
      <name>Bojan Banjac</name>
    </author>
    <author>
      <name>Ivana Jovovic</name>
    </author>
    <author>
      <name>Milica Makragic</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1902v1</id>
    <updated>2014-05-08T12:15:59Z</updated>
    <published>2014-05-08T12:15:59Z</published>
    <title>Proofs of two Theorems concerning Sparse Spacetime Constraints</title>
    <summary>  In the SIGGRAPH 2014 paper [SvTSH14] an approach for animating deformable
objects using sparse spacetime constraints is introduced. This report contains
the proofs of two theorems presented in the paper.
</summary>
    <author>
      <name>Christian Schulz</name>
    </author>
    <author>
      <name>Christoph von Tycowicz</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Klaus Hildebrandt</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07515v1</id>
    <updated>2015-06-24T19:56:56Z</updated>
    <published>2015-06-24T19:56:56Z</published>
    <title>The Vector Space of Convex Curves: How to Mix Shapes</title>
    <summary>  We present a novel, log-radius profile representation for convex curves and
define a new operation for combining the shape features of curves. Unlike the
standard, angle profile-based methods, this operation accurately combines the
shape features in a visually intuitive manner. This method have implications in
shape analysis as well as in investigating how the brain perceives and
generates curved shapes and motions.
</summary>
    <author>
      <name>Dongsung Huh</name>
    </author>
    <link href="http://arxiv.org/abs/1506.07515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06821v1</id>
    <updated>2016-03-22T15:14:21Z</updated>
    <published>2016-03-22T15:14:21Z</published>
    <title>A Report on Shape Deformation with a Stretching and Bending Energy</title>
    <summary>  In this report we describe a mesh editing system that we implemented that
uses a natural stretching and bending energy defined over smooth surfaces. As
such, this energy behaves uniformly under various mesh resolutions. All of the
elements of our approach already exist in the literature. We hope that our
discussions of these energies helps to shed light on the behaviors of these
methods and provides a unified discussion of these methods.
</summary>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Steven J. Gortler</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01396v1</id>
    <updated>2016-05-04T19:45:32Z</updated>
    <published>2016-05-04T19:45:32Z</published>
    <title>Squares that Look Round: Transforming Spherical Images</title>
    <summary>  We propose M\"obius transformations as the natural rotation and scaling tools
for editing spherical images. As an application we produce spherical Droste
images. We obtain other self-similar visual effects using rational functions,
elliptic functions, and Schwarz-Christoffel maps.
</summary>
    <author>
      <name>Saul Schleimer</name>
    </author>
    <author>
      <name>Henry Segerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures with many subfigures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A66, 33C75, 30F10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09737v1</id>
    <updated>2016-05-31T17:39:49Z</updated>
    <published>2016-05-31T17:39:49Z</published>
    <title>3D Printed Stencils for Texturing Flat Surfaces</title>
    <summary>  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
</summary>
    <author>
      <name>Vaibhav Vavilala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01933v1</id>
    <updated>2016-08-05T16:39:27Z</updated>
    <published>2016-08-05T16:39:27Z</published>
    <title>Geoplotlib: a Python Toolbox for Visualizing Geographical Data</title>
    <summary>  We introduce geoplotlib, an open-source python toolbox for visualizing
geographical data. geoplotlib supports the development of hardware-accelerated
interactive visualizations in pure python, and provides implementations of dot
maps, kernel density estimation, spatial graphs, Voronoi tesselation,
shapefiles and many more common spatial visualizations. We describe geoplotlib
design, functionalities and use cases.
</summary>
    <author>
      <name>Andrea Cuttone</name>
    </author>
    <author>
      <name>Sune Lehmann</name>
    </author>
    <author>
      <name>Jakob Eg Larsen</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02897v1</id>
    <updated>2017-04-10T15:13:40Z</updated>
    <published>2017-04-10T15:13:40Z</published>
    <title>Projection Mapping Technologies for AR</title>
    <summary>  This invited talk will present recent projection mapping technologies for
augmented reality. First, fundamental technologies are briefly explained, which
have been proposed to overcome the technical limitations of ordinary
projectors. Second, augmented reality (AR) applications using projection
mapping technologies are introduced.
</summary>
    <author>
      <name>Daisuke Iwai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, 23rd International Display Workshops in
  conjunction with Asia Display 2016. arXiv admin note: substantial text
  overlap with arXiv:1510.02710</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Display Workshops (IDW), pp.
  1076-1078, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.02897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07559v1</id>
    <updated>2017-08-24T21:12:51Z</updated>
    <published>2017-08-24T21:12:51Z</published>
    <title>Efficient barycentric point sampling on meshes</title>
    <summary>  We present an easy-to-implement and efficient analytical inversion algorithm
for the unbiased random sampling of a set of points on a triangle mesh whose
surface density is specified by barycentric interpolation of non-negative
per-vertex weights. The correctness of the inversion algorithm is verified via
statistical tests, and we show that it is faster on average than rejection
sampling.
</summary>
    <author>
      <name>Jamie Portsmouth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02721v2</id>
    <updated>2017-09-21T03:48:18Z</updated>
    <published>2017-08-16T07:02:54Z</published>
    <title>On the correlation between a level of structure order and properties of
  composites. In Memory of Yu.L. Klimontovich</title>
    <summary>  Proposed the computerized method for calculating the relative level of order
composites. Correlation between a level of structure order and properties of
solids is shown. Discussed the possibility of clarifying the terminology used
in describing the structure.
</summary>
    <author>
      <name>Alexander Herega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02721v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02721v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00442v1</id>
    <updated>2018-01-01T13:25:16Z</updated>
    <published>2018-01-01T13:25:16Z</published>
    <title>O(lgN) Line Clipping Algorithm in E2</title>
    <summary>  A new O(lg N) line clipping algorithm in E2 against a convex window is
presented. The main advantage of the presented algorithm is the principal
acceleration of the line clipping problem solution. A comparison of the
proposed algorithm with others shows a significant improvement in run-time.
Experimental results for selected known algorithms are also shown.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0097-8493(94)90064-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0097-8493(94)90064-7" rel="related"/>
    <link href="http://arxiv.org/abs/1801.00442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09358v1</id>
    <updated>2018-01-29T04:39:56Z</updated>
    <published>2018-01-29T04:39:56Z</published>
    <title>Smooth, Efficient, and Interruptible Zooming and Panning</title>
    <summary>  This paper introduces a novel technique for smooth and efficient zooming and
panning based on dynamical systems in hyperbolic space. Unlike the technique of
van Wijk and Nuij, the animations produced by our technique are smooth at the
endpoints and when interrupted by a change of target. To analyze the results of
our technique, we introduce world/screen diagrams, a novel technique for
visualizing zooming and panning animations.
</summary>
    <author>
      <name>Andrew Reach</name>
    </author>
    <author>
      <name>Chris North</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07555v2</id>
    <updated>2018-05-17T21:04:30Z</updated>
    <published>2018-02-21T13:16:56Z</published>
    <title>$C^1$ analysis of 2D subdivision schemes refining point-normal pairs
  with the circle average</title>
    <summary>  This article continues the investigation started in [9] on subdivision
schemes refining 2D point-normal pairs, obtained by modifying linear
subdivision schemes using the circle average. While in [9] the convergence of
the Modified Lane-Riesenfeld algorithm and the Modified 4-Point schemes is
proved, here we show that the curves generated by these two schemes are $C^1$.
</summary>
    <author>
      <name>Evgeny Lipovetsky</name>
    </author>
    <author>
      <name>Nira Dyn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Incomplete proof</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07555v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07555v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00073v1</id>
    <updated>2018-02-28T20:39:59Z</updated>
    <published>2018-02-28T20:39:59Z</published>
    <title>Resolution Improvement of the Common Method for Presentating Arbitrary
  Space Curves Voxel</title>
    <summary>  The task of voxel resolution for a space curve in video memory of 3D display
is set. Furthermore, an approach solution of voxel resolution of arbitrary
space curve, given in parametric form, is studied. Numerous numbers of
intensive experiments are conducted and interesting results with significant
recommendations are presented.
</summary>
    <author>
      <name>Anas M. Al-Oraiqat</name>
    </author>
    <author>
      <name>E. A. Bashkov</name>
    </author>
    <author>
      <name>S. A. Zori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electronics Communication and Computer
  Engineering, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.00073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02524v1</id>
    <updated>2019-04-03T12:48:40Z</updated>
    <published>2019-04-03T12:48:40Z</published>
    <title>x3ogre: Connecting X3D to a state of the art rendering engine</title>
    <summary>  We connect X3D to the state of the art OGRE renderer using our prototypical
x3ogre implementation. At this we perform a comparison of both on a conceptual
level, highlighting similarities and differences. Our implementation allows
swapping X3D concepts for OGRE concepts and vice versa. We take advantage of
this to analyse current shortcomings in X3D and propose X3D extensions to
overcome those.
</summary>
    <author>
      <name>Pavel Rojtberg</name>
    </author>
    <author>
      <name>Benjamin Audenrith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3055624.3075949</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3055624.3075949" rel="related"/>
    <link href="http://arxiv.org/abs/1904.02524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02681v1</id>
    <updated>2019-08-07T15:30:20Z</updated>
    <published>2019-08-07T15:30:20Z</published>
    <title>Rendering Point Clouds with Compute Shaders</title>
    <summary>  We propose a compute shader based point cloud rasterizer with up to 10 times
higher performance than classic point-based rendering with the GL_POINT
primitive. In addition to that, our rasterizer offers 5 byte depth-buffer
precision with uniform or customizable distribution, and we show that it is
possible to implement a high-quality splatting method that blends together
overlapping fragments while still maintaining higher frame-rates than the
traditional approach.
</summary>
    <author>
      <name>Markus Schütz</name>
    </author>
    <author>
      <name>Michael Wimmer</name>
    </author>
    <link href="http://arxiv.org/abs/1908.02681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06402v2</id>
    <updated>2020-06-11T18:03:49Z</updated>
    <published>2019-10-14T20:10:45Z</published>
    <title>Addressing Troubles with Double Bubbles: Convergence and Stability at
  Multi-Bubble Junctions</title>
    <summary>  In this report we discuss and propose a correction to a convergence and
stability issue occurring in the work of Da et al.[2015], in which they
proposed a numerical model to simulate soap bubbles.
</summary>
    <author>
      <name>Yun Fei</name>
    </author>
    <author>
      <name>Christopher Batty</name>
    </author>
    <author>
      <name>Eitan Grinspun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, technical report of Columbia University</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09533v2</id>
    <updated>2020-08-12T15:26:48Z</updated>
    <published>2020-02-21T19:51:54Z</published>
    <title>Real-Time Visualization in Non-Isotropic Geometries</title>
    <summary>  Non-isotropic geometries are of interest to low-dimensional topologists,
physicists and cosmologists. However, they are challenging to comprehend and
visualize. We present novel methods of computing real-time native geodesic
rendering of non-isotropic geometries. Our methods can be applied not only to
visualization, but also are essential for potential applications in machine
learning and video games.
</summary>
    <author>
      <name>Eryk Kopczyński</name>
    </author>
    <author>
      <name>Dorota Celińska-Kopczyńska</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09533v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09533v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="53A35 Non-Euclidean differential geometry" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11347v1</id>
    <updated>2020-02-26T08:14:02Z</updated>
    <published>2020-02-26T08:14:02Z</published>
    <title>A multi-sided generalization of the $C^0$ Coons patch</title>
    <summary>  Most multi-sided transfinite surfaces require cross-derivatives at the
boundaries. Here we show a general $n$-sided patch that interpolates all
boundaries based on only positional information. The surface is a weighted sum
of $n$ Coons patches, using a parameterization based on Wachspress coordinates.
</summary>
    <author>
      <name>Péter Salvi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on the Advances of Information
  Technology, pp. 110-111, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04686v1</id>
    <updated>2020-06-08T15:36:07Z</updated>
    <published>2020-06-08T15:36:07Z</published>
    <title>RBF Solver for Quaternions Interpolation</title>
    <summary>  In this paper we adapt the RBF Solver to work with quaternions by taking
advantage of their Lie Algebra and exponential map. This will allow to work
with quaternions as if they were normal vectors in R^3 and blend them in a very
efficient way.
</summary>
    <author>
      <name>Rinaldi Fabio</name>
    </author>
    <author>
      <name>Dolci Daniele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 21 figures, 2 txt files</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.04686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N01, 68N15, 68N17, 68N19" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08057v2</id>
    <updated>2021-06-04T00:00:58Z</updated>
    <published>2021-04-16T12:06:53Z</published>
    <title>Signed Distance Function Computation from an Implicit Surface</title>
    <summary>  We describe in this short note a technique to convert an implicit surface
into a Signed Distance Function (SDF) while exactly preserving the zero
level-set of the implicit. The proposed approach relies on embedding the input
implicit in the final layer of a neural network, which is trained to minimize a
loss function characterizing the SDF.
</summary>
    <author>
      <name>Pierre-Alain Fayolle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fix typos</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13704v2</id>
    <updated>2021-10-01T09:01:04Z</updated>
    <published>2021-09-28T13:16:29Z</published>
    <title>Common Artifacts in Volume Rendering</title>
    <summary>  Direct Volume Rendering is a popular and powerful visualization method for
voxel data and other volumetric scalar data sets. Particularly, in medical
applications volume rendering is very commonly used, and has become one of the
state of the art methods for 3D visualization of medical data. In this article,
some of the most common artifacts encountered will be discussed, and their
possible remedies.
</summary>
    <author>
      <name>Daniel Ruijters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.13704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00731v1</id>
    <updated>2021-12-01T08:43:03Z</updated>
    <published>2021-12-01T08:43:03Z</published>
    <title>Is Projection Mapping Natural? Towards Physical World Augmentation
  Consistent with Light Field Context</title>
    <summary>  Projection mapping seamlessly merges real and virtual worlds. Although much
effort was made to improve its image qualities so far, projection mapping is
still unnatural. We introduce the first steps towards natural projection
mapping by making the projection results consistent with the light field
context of our daily scene.
</summary>
    <author>
      <name>Daisuke Iwai</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the International Display Workshops (IDW), pp.
  964-967, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.00731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02141v1</id>
    <updated>2022-03-04T05:53:24Z</updated>
    <published>2022-03-04T05:53:24Z</published>
    <title>Weighted Simultaneous Algebra Reconstruction Technique (wSART) for
  Additive Light Field Synthesis</title>
    <summary>  We apply an iterative weighting scheme for additive light field synthesis.
Unlike previous work optimizing additive light field evenly over viewpoints, we
constrain the optimization to deliver a reconstructed light field of high image
quality for viewpoints of large weight.
</summary>
    <author>
      <name>Chen Gao</name>
    </author>
    <author>
      <name>Linqi Dong</name>
    </author>
    <author>
      <name>Liang Xu</name>
    </author>
    <author>
      <name>Xu Liu</name>
    </author>
    <author>
      <name>Haifeng Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Display Technology 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00233v1</id>
    <updated>2018-12-01T17:54:36Z</updated>
    <published>2018-12-01T17:54:36Z</published>
    <title>AIR: Anywhere Immersive Reality with User-Perspective Projection</title>
    <summary>  Projection-based augmented reality (AR) has much potential, but is limited in
that it requires burdensome installations and prone to geometric distortions on
display surface. To overcome these limitations, we propose AIR. It can be
carried and placed anywhere to project AR using pan/tilting motors, while
providing the user with distortion-free projection of a correct 3D view.
</summary>
    <author>
      <name>JungHyun Byun</name>
    </author>
    <author>
      <name>SeungHo Chae</name>
    </author>
    <author>
      <name>YoonSik Yang</name>
    </author>
    <author>
      <name>TackDon Han</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2312/egsh.20171001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2312/egsh.20171001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at EUROGRAPHICS 2017 as Short Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09792v1</id>
    <updated>2020-01-27T13:58:19Z</updated>
    <published>2020-01-27T13:58:19Z</published>
    <title>Running on Raygun</title>
    <summary>  With the introduction of Nvidia RTX hardware, ray tracing is now viable as a
general real time rendering technique for complex 3D scenes. Leveraging this
new technology, we present Raygun, an open source rendering, simulation, and
game engine focusing on simplicity, expandability, and the topic of ray tracing
realized through Nvidia's Vulkan ray tracing extension.
</summary>
    <author>
      <name>Alexander Hirsch</name>
    </author>
    <author>
      <name>Peter Thoman</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09714v1</id>
    <updated>2020-10-17T03:25:55Z</updated>
    <published>2020-10-17T03:25:55Z</published>
    <title>A Convenient Generalization of Schlick's Bias and Gain Functions</title>
    <summary>  We present a generalization of Schlick's bias and gain functions -- simple
parametric curve-shaped functions for inputs in [0, 1]. Our single function
includes both bias and gain as special cases, and is able to describe other
smooth and monotonic curves with variable degrees of asymmetry.
</summary>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01747v1</id>
    <updated>2021-02-02T20:44:59Z</updated>
    <published>2021-02-02T20:44:59Z</published>
    <title>Real-time rendering of complex fractals</title>
    <summary>  This chapter describes how to use intersection and closest-hit shaders to
implement real-time visualizations of complex fractals using distance
functions. The Mandelbulb and Julia Sets are used as examples.
</summary>
    <author>
      <name>Vinícius da Silva</name>
    </author>
    <author>
      <name>Tiago Novello</name>
    </author>
    <author>
      <name>Hélio Lopes</name>
    </author>
    <author>
      <name>Luiz Velho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.01747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05 (Primary) 37-04, 37F10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00914v1</id>
    <updated>2022-05-02T13:52:07Z</updated>
    <published>2022-05-02T13:52:07Z</published>
    <title>Realistic soft-body tearing under 10ms in VR</title>
    <summary>  We present a novel integration of a real-time continuous tearing algorithm
for 3D meshes in VR, suitable for devices of low CPU/GPU specifications, along
with a suitable particle decomposition that allows soft-body deformations on
both the original and the torn model.
</summary>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Antonis Protopsaltis</name>
    </author>
    <author>
      <name>Michail Tamiolakis</name>
    </author>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, submitted for the Siggraph '22 Poster Session
  (Vancouver, 8-11 Aug 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00923v1</id>
    <updated>2022-05-02T14:07:15Z</updated>
    <published>2022-05-02T14:07:15Z</published>
    <title>Recording and replaying psychomotor user actions in VR</title>
    <summary>  We introduce a novel method that describes the functionality and
characteristics of an efficient VR recorder with replay capabilities,
implemented in a modern game engine, publicly available for free.
</summary>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Ilias Chrysovergis</name>
    </author>
    <author>
      <name>Mike Kentros</name>
    </author>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, Submitted to the Siggraph '22 Poster Session
  (Vancouver, 8-11 Aug 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00480v1</id>
    <updated>2022-06-01T13:24:17Z</updated>
    <published>2022-06-01T13:24:17Z</published>
    <title>Sex and Gender in the Computer Graphics Research Literature</title>
    <summary>  We survey the treatment of sex and gender in the Computer Graphics research
literature from an algorithmic fairness perspective. The established practices
on the use of gender and sex in our community are scientifically incorrect and
constitute a form of algorithmic bias with potential harmful effects. We
propose ways of addressing these as technical limitations.
</summary>
    <author>
      <name>Ana Dodik</name>
    </author>
    <author>
      <name>Silvia Sellán</name>
    </author>
    <author>
      <name>Theodore Kim</name>
    </author>
    <author>
      <name>Amanda Phillips</name>
    </author>
    <link href="http://arxiv.org/abs/2206.00480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04729v1</id>
    <updated>2022-08-07T09:41:04Z</updated>
    <published>2022-08-07T09:41:04Z</published>
    <title>New Geometric Continuity Solution of Parametric Surfaces</title>
    <summary>  This paper presents a new approach to computation of geometric continuity for
parametric bi-cubic patches, based on a simple mathematical reformulation which
leads to simple additional conditions to be applied in the patching
computation. The paper presents an Hermite formulation of a bicubic parametric
patch, but reformulations can be made also for B\'ezier and B-Spline patches as
well. The presented approach is convenient for the cases when valencies of
corners are different from the value 4, in general.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4826048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4826048" rel="related"/>
    <link href="http://arxiv.org/abs/2208.04729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68xx, 68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09549v1</id>
    <updated>2022-08-19T21:19:16Z</updated>
    <published>2022-08-19T21:19:16Z</published>
    <title>Generalized Projection Matrices</title>
    <summary>  Projection matrices are necessary for a large portion of rendering computer
graphics. There are primarily two different types of projection matrices --
perspective and orthographic -- which are used frequently, and are
traditionally treated as mutually incompatible with each other in how they are
defined. Here, we bridge the gap between the two different forms of projection
matrices to present a single generalized projection matrix that can represent
both.
</summary>
    <author>
      <name>S. J. D. MacIntosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.09549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15232v2</id>
    <updated>2023-02-17T07:51:10Z</updated>
    <published>2022-10-27T07:40:54Z</published>
    <title>Visualizing Squircular Implicit Surfaces</title>
    <summary>  The squircle is an intermediate shape between the square and the circle. In
this paper, we examine and discuss equations for different types of squircles.
We then build upon these 2D shapes to come-up with various 3D surfaces based on
squircles.
</summary>
    <author>
      <name>Chamberlain Fong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">proof of periodic squircle; conjecture on Schwarz P surface</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.15232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06589v2</id>
    <updated>2023-01-26T18:12:22Z</updated>
    <published>2022-12-13T14:06:40Z</published>
    <title>Patches of developable surfaces bounded by NURBS curves</title>
    <summary>  In this talk we review the problem of constructing a developable surface
patch bounded by two rational or NURBS (Non-Uniform Rational B-spline) curves.
</summary>
    <author>
      <name>L. Fernandez-Jambrina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures; Modelling for Engineering &amp; Human Behaviour 2022,
  1-6 (2022) I.S.B.N.: 978-84-09-47037-2</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06589v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06589v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06468v1</id>
    <updated>2023-02-13T15:45:28Z</updated>
    <published>2023-02-13T15:45:28Z</published>
    <title>Fast Real-Time Shading for Polygonal Hair</title>
    <summary>  Though a lot of improvement has been made to hair rendering techniques in the
recent years, realistic rendering of hair remains a challenge, especially in
real time. In this paper, we propose a fast technique to approximate the
shading of hair lighted by an environment map, direct lighting or a global
illumination system, without having to render deep opacity maps or requiring
additional artistic work.
</summary>
    <author>
      <name>Martin Gerard</name>
    </author>
    <link href="http://arxiv.org/abs/2302.06468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06984v1</id>
    <updated>2023-03-13T10:34:05Z</updated>
    <published>2023-03-13T10:34:05Z</published>
    <title>Experiencing avatar direction in low cost theatrical mixed reality setup</title>
    <summary>  We introduce the setup and programming framework of AvatarStaging theatrical
mixed reality experiment. We focus on a configuration addressing movement
issues between physical and 3D digital spaces from performers and directors'
points of view. We propose 3 practical exercises.
</summary>
    <author>
      <name>Georges Gagneré</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INREV, UP8, UPL</arxiv:affiliation>
    </author>
    <author>
      <name>Cédric Plessiet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INREV, UP8, UPL</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3212721.3212892</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3212721.3212892" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MOCO '18: 5th International Conference on Movement and Computing,
  Jun 2018, Genoa, Italy. pp.1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.06984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08138v2</id>
    <updated>2023-06-16T22:17:07Z</updated>
    <published>2023-06-13T21:08:05Z</published>
    <title>Ergonomic-Centric Holography: Optimizing Realism,Immersion, and Comfort
  for Holographic Display</title>
    <summary>  We introduce ergonomic-centric holography, an algorithmic framework that
simultaneously optimizes for realistic incoherent defocus, unrestricted pupil
movements in the eye box, and high-order diffractions for filtering-free
holography. The proposed method outperforms prior algorithms on holographic
display prototypes operating in unfiltered and pupil-mimicking modes, offering
the potential to enhance next-generation virtual and augmented reality
experiences.
</summary>
    <author>
      <name>Liang Shi</name>
    </author>
    <author>
      <name>DongHun Ryu</name>
    </author>
    <author>
      <name>Wojciech Matusik</name>
    </author>
    <link href="http://arxiv.org/abs/2306.08138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00275v1</id>
    <updated>2023-09-01T06:10:57Z</updated>
    <published>2023-09-01T06:10:57Z</published>
    <title>Technical Companion to Example-Based Procedural Modeling Using Graph
  Grammars</title>
    <summary>  This is a companion piece to my paper on "Example-Based Procedural Modeling
Using Graph Grammars." This paper examines some of the theoretical issues in
more detail. This paper discusses some more complex parts of the
implementation, why certain algorithmic decisions were made, proves the
algorithm can solve certain classes of problems, and examines other interesting
theoretical questions.
</summary>
    <author>
      <name>Paul Merrell</name>
    </author>
    <link href="http://arxiv.org/abs/2309.00275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.11993v1</id>
    <updated>2023-09-21T12:04:15Z</updated>
    <published>2023-09-21T12:04:15Z</published>
    <title>Neural Stochastic Screened Poisson Reconstruction</title>
    <summary>  Reconstructing a surface from a point cloud is an underdetermined problem. We
use a neural network to study and quantify this reconstruction uncertainty
under a Poisson smoothness prior. Our algorithm addresses the main limitations
of existing work and can be fully integrated into the 3D scanning pipeline,
from obtaining an initial reconstruction to deciding on the next best sensor
position and updating the reconstruction upon capturing more data.
</summary>
    <author>
      <name>Silvia Sellán</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/2309.11993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.11993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.01772v1</id>
    <updated>2023-10-03T03:39:30Z</updated>
    <published>2023-10-03T03:39:30Z</published>
    <title>SnB Collaborative Visualization</title>
    <summary>  We describe a system for visualization and editing of data in a computational
chemistry environment. The system is a collaborative tool allowing researchers
using virtual reality and/or desktop computer displays to work together on
results of the Shake-and-Bake structure determination application.
</summary>
    <author>
      <name>Dave Pape</name>
    </author>
    <author>
      <name>Amin Ghadersohi</name>
    </author>
    <author>
      <name>Josephine Anstey</name>
    </author>
    <author>
      <name>Amit Makwana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, Proceedings of 2nd INTUITION International
  Workshop, Paris, France, 24-25 Nov 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.01772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.01772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.12270v1</id>
    <updated>2023-12-19T15:52:28Z</updated>
    <published>2023-12-19T15:52:28Z</published>
    <title>Sketch Vision: Artificial Intelligence with Sight for Imagination</title>
    <summary>  Visual design relies on seeing things in different ways, acting on them, and
seeing results to act again. Parametric design tools are often not robust to
design changes that result from sketching over the visualization of their
output. We propose a sketch to 3d workflow as an experiment medium for
evaluating neural networks and their latent spaces as a representation that is
robust to overlay sketching.
</summary>
    <author>
      <name>Demircan Tas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Written for 4.453 Creative Machine Learning at MIT, Spring 2023. 9
  pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.12270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.12270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01629v2</id>
    <updated>2024-03-21T15:16:34Z</updated>
    <published>2024-03-03T22:29:37Z</published>
    <title>VR Research at Fraunhofer IGD, Darmstadt, Germany</title>
    <summary>  We present a historical outline of the research and developments of Virtual
Reality at the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt,
Germany, from 1990 through 2000.
</summary>
    <author>
      <name>Wolfgang Felger</name>
    </author>
    <author>
      <name>Martin Göbel</name>
    </author>
    <author>
      <name>Dirk Reiners</name>
    </author>
    <author>
      <name>Gabriel Zachmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VR 2024 Workshop "Archiving VR"</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.01629v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01629v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.2; I.3.7; I.3.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.10647v1</id>
    <updated>2024-03-15T19:30:20Z</updated>
    <published>2024-03-15T19:30:20Z</published>
    <title>Building An Efficient Grid On GPU</title>
    <summary>  Grid space partitioning is a technique to speed up queries to graphics
databases. We present a parallel grid construction algorithm which can
efficiently construct a structured grid on GPU hardware. Our approach is
substantially faster than existing uniform grid construction algorithms,
especially on non-homogeneous scenes. Indeed, it can populate a grid in
real-time (at rates over 25 Hz), for architectural scenes with 10 million
triangles.
</summary>
    <author>
      <name>Vasco Costa</name>
    </author>
    <author>
      <name>João M. Pereira</name>
    </author>
    <author>
      <name>Joaquim Jorge</name>
    </author>
    <link href="http://arxiv.org/abs/2403.10647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.10647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00938v1</id>
    <updated>2024-05-02T01:40:01Z</updated>
    <published>2024-05-02T01:40:01Z</published>
    <title>Virtual Psychedelia</title>
    <summary>  We present an approach to designing 3D Iterated Function Systems (IFS) within
the Unity Editor and rendered to VR in real-time. Objects are modeled as a
hierarchical tree of primitive shapes and operators, editable using a graphical
user interface allowing artists to develop psychedelic scenes with little to no
coding knowledge, and is easily extensible for more advanced users to add their
own primitive shapes and operators.
</summary>
    <author>
      <name>Jacob Yenney</name>
    </author>
    <author>
      <name>Weichen Liu</name>
    </author>
    <author>
      <name>Ying C. Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures. Submitted to IEEE VIS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.00938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.11098v2</id>
    <updated>2024-05-21T18:34:11Z</updated>
    <published>2024-05-17T21:26:41Z</published>
    <title>Generative AI for 2D Character Animation</title>
    <summary>  In this pilot project, we teamed up with artists to develop new workflows for
2D animation while producing a short educational cartoon. We identified several
workflows to streamline the animation process, bringing the artists' vision to
the screen more effectively.
</summary>
    <author>
      <name>Jaime Guajardo</name>
    </author>
    <author>
      <name>Ozgun Bursalioglu</name>
    </author>
    <author>
      <name>Dan B Goldman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://genai-2d-character-animation.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11098v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11098v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05880v1</id>
    <updated>2024-12-08T10:06:42Z</updated>
    <published>2024-12-08T10:06:42Z</published>
    <title>Leveraging virtual technologies to enhance museums and art collections:
  insights from project CHANGES</title>
    <summary>  We investigated the use of virtual technologies to digitise and enhance
cultural heritage (CH), aligning with Open Science and FAIR principles. Through
case studies in museums, we developed reproducible workflows, 3D models, and
tools fostering accessibility, inclusivity, and sustainability of CH.
Applications include interdisciplinary research, educational innovation, and CH
preservation.
</summary>
    <author>
      <name>Gianluca Genovese</name>
    </author>
    <author>
      <name>Ivan Heibi</name>
    </author>
    <author>
      <name>Silvio Peroni</name>
    </author>
    <author>
      <name>Sofia Pescarin</name>
    </author>
    <link href="http://arxiv.org/abs/2412.05880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.05880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.06541v1</id>
    <updated>2025-02-10T15:07:41Z</updated>
    <published>2025-02-10T15:07:41Z</published>
    <title>Physically-Based Mesh Generation for Confined 3D Point Clouds Using
  Flexible Foil Models</title>
    <summary>  We propose a method for constructing high-quality, closed-surface meshes from
confined 3D point clouds via a physically-based simulation of flexible foils
under spatial constraints. The approach integrates dynamic elasticity,
pressure-driven deformation, and adaptive snapping to fixed vertices, providing
a robust framework for realistic and physically accurate mesh creation.
Applications in computer graphics and computational geometry are discussed.
</summary>
    <author>
      <name>Netzer Moriya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.06541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.06541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49Mxx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.12229v1</id>
    <updated>2025-03-15T18:46:41Z</updated>
    <published>2025-03-15T18:46:41Z</published>
    <title>Shadow Art Kanji: Inverse Rendering Application</title>
    <summary>  Finding a balance between artistic beauty and machine-generated imagery is
always a difficult task. This project seeks to create 3D models that, when
illuminated, cast shadows resembling Kanji characters. It aims to combine
artistic expression with computational techniques, providing an accurate and
efficient approach to visualizing these Japanese characters through shadows.
</summary>
    <author>
      <name>William Louis Rothman</name>
    </author>
    <author>
      <name>Yasuyuki Matsushita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 10 figures, 8 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.12229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.12229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810020v1</id>
    <updated>1998-10-22T20:44:35Z</updated>
    <published>1998-10-22T20:44:35Z</published>
    <title>Computational Geometry Column 33</title>
    <summary>  Several recent SIGGRAPH papers on surface simplification are described.
</summary>
    <author>
      <name>Joseph O'Rourke</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Internat. J. Comput. Geom. Appl., 8(3) 381-384, 1998. Also in
  SIGACT News, 29(2) (Issue 107) 14-16, 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9810020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2;I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810021v1</id>
    <updated>1998-10-22T21:01:36Z</updated>
    <published>1998-10-22T21:01:36Z</published>
    <title>Computational Geometry Column 32</title>
    <summary>  The proof of Dey's new k-set bound is illustrated.
</summary>
    <author>
      <name>Joseph O'Rourke</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Internat. J. Comput. Geom. Appl.,7(5) 509-513, 1997. Also in
  SIGACT News, 29(2) (Issue 107) 14-16, 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9810021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03436v1</id>
    <updated>2021-09-08T05:11:01Z</updated>
    <published>2021-09-08T05:11:01Z</published>
    <title>Convergence Analysis of the Algorithm in "Efficient and Robust Discrete
  Conformal Equivalence with Boundary"</title>
    <summary>  In this note we prove that the version of Newton algorithm with line search
we used in [2] converges quadratically.
</summary>
    <author>
      <name>Denis Zorin</name>
    </author>
    <link href="http://arxiv.org/abs/2109.03436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206029v1</id>
    <updated>2002-06-20T06:21:15Z</updated>
    <published>2002-06-20T06:21:15Z</published>
    <title>Computer-Generated Photorealistic Hair</title>
    <summary>  This paper presents an efficient method for generating and rendering
photorealistic hair in two dimensional pictures. The method consists of three
major steps. Simulating an artist drawing is used to design the rough hair
shape. A convolution based filter is then used to generate photorealistic hair
patches. A refine procedure is finally used to blend the boundaries of the
patches with surrounding areas. This method can be used to create all types of
photorealistic human hair (head hair, facial hair and body hair). It is also
suitable for fur and grass generation. Applications of this method include:
hairstyle designing/editing, damaged hair image restoration, human hair
animation, virtual makeover of a human, and landscape creation.
</summary>
    <author>
      <name>Alice J. Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0206029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212043v1</id>
    <updated>2002-12-13T05:33:10Z</updated>
    <published>2002-12-13T05:33:10Z</published>
    <title>Computing Conformal Structure of Surfaces</title>
    <summary>  This paper solves the problem of computing conformal structures of general
2-manifolds represented as triangle meshes. We compute conformal structures in
the following way: first compute homology bases from simplicial complex
structures, then construct dual cohomology bases and diffuse them to harmonic
1-forms. Next, we construct bases of holomorphic differentials. We then obtain
period matrices by integrating holomorphic differentials along homology bases.
We also study the global conformal mapping between genus zero surfaces and
spheres, and between general meshes and planes. Our method of computing
conformal structures can be applied to tackle fundamental problems in computer
aid design and computer graphics, such as geometry classification and
identification, and surface global parametrization.
</summary>
    <author>
      <name>Xianfeng Gu</name>
    </author>
    <author>
      <name>Shing-Tung Yau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, simplified version, full version upon request</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0212043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5;F.2.2;G.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306031v1</id>
    <updated>2003-06-06T12:34:53Z</updated>
    <published>2003-06-06T12:34:53Z</published>
    <title>The FRED Event Display: an Extensible HepRep Client for GLAST</title>
    <summary>  A new graphics client prototype for the HepRep protocol is presented. Based
on modern toolkits and high level languages (C++ and Ruby), Fred is an
experiment to test applicability of scripting facilities to the high energy
physics event display domain. Its flexible structure, extensibility and the use
of the HepRep protocol are key features for its use in the astroparticle
experiment GLAST.
</summary>
    <author>
      <name>Marco Frailis</name>
    </author>
    <author>
      <name>Riccardo Giannitrapani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 3 eps figures. PSN
  MOLT010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONFC0303241:MOLT010,2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0306031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.2,I.3.3,I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404022v1</id>
    <updated>2004-04-08T14:07:05Z</updated>
    <published>2004-04-08T14:07:05Z</published>
    <title>An Algorithm for Transforming Color Images into Tactile Graphics</title>
    <summary>  This paper presents an algorithm that transforms color visual images, like
photographs or paintings, into tactile graphics. In the algorithm, the edges of
objects are detected and colors of the objects are estimated. Then, the edges
and the colors are encoded into lines and textures in the output tactile image.
Design of the method is substantiated by various qualities of haptic
recognizing of images. Also, means of presentation of the tactile images in
printouts are discussed. Example translated images are shown.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603132v1</id>
    <updated>2006-03-31T19:58:30Z</updated>
    <published>2006-03-31T19:58:30Z</published>
    <title>Graphics Turing Test</title>
    <summary>  We define a Graphics Turing Test to measure graphics performance in a similar
manner to the definition of the traditional Turing Test. To pass the test one
needs to reach a computational scale, the Graphics Turing Scale, for which
Computer Generated Imagery becomes comparatively indistinguishable from real
images while also being interactive. We derive an estimate for this
computational scale which, although large, is within reach of todays
supercomputers. We consider advantages and disadvantages of various computer
systems designed to pass the Graphics Turing Test. Finally we discuss
commercial applications from the creation of such a system, in particular
Interactive Cinema.
</summary>
    <author>
      <name>Michael McGuigan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607050v2</id>
    <updated>2008-03-25T15:53:37Z</updated>
    <published>2006-07-11T19:01:41Z</published>
    <title>Interactive Hatching and Stippling by Example</title>
    <summary>  We describe a system that lets a designer interactively draw patterns of
strokes in the picture plane, then guide the synthesis of similar patterns over
new picture regions. Synthesis is based on an initial user-assisted analysis
phase in which the system recognizes distinct types of strokes (hatching and
stippling) and organizes them according to perceptual grouping criteria. The
synthesized strokes are produced by combining properties (eg. length,
orientation, parallelism, proximity) of the stroke groups extracted from the
input examples. We illustrate our technique with a drawing application that
allows the control of attributes and scale-dependent reproduction of the
synthesized patterns.
</summary>
    <author>
      <name>Pascal Barla</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes / GRAVIR-IMAG</arxiv:affiliation>
    </author>
    <author>
      <name>Simon Breslav</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EECS</arxiv:affiliation>
    </author>
    <author>
      <name>Lee Markosian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EECS</arxiv:affiliation>
    </author>
    <author>
      <name>Joëlle Thollot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes / GRAVIR-IMAG</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0607050v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607050v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608003v2</id>
    <updated>2006-08-01T22:08:19Z</updated>
    <published>2006-08-01T19:25:17Z</published>
    <title>On a solution to display non-filled-in quaternionic Julia sets</title>
    <summary>  During early 1980s, the so-called `escape time' method, developed to display
the Julia sets for complex dynamical systems, was exported to quaternions in
order to draw analogous pictures in this wider numerical field. Despite of the
fine results in the complex plane, where all topological configurations of
Julia sets have been successfully displayed, the `escape time' method fails to
render properly the non-filled-in variety of quaternionic Julia sets. So their
digital visualisation remained an open problem for several years. Both the
solution for extending this old method to non-filled-in quaternionic Julia sets
and its implementation into a program are explained here.
</summary>
    <author>
      <name>Alessandro Rosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 28 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609084v1</id>
    <updated>2006-09-15T07:21:11Z</updated>
    <published>2006-09-15T07:21:11Z</published>
    <title>Non-photorealistic image rendering with a labyrinthine tiling</title>
    <summary>  The paper describes a new image processing for a non-photorealistic
rendering. The algorithm is based on a random generation of gray tones and
competing statistical requirements. The gray tone value of each pixel in the
starting image is replaced selecting among randomly generated tone values,
according to the statistics of nearest-neighbor and next-nearest-neighbor
pixels. Two competing conditions for replacing the tone values - one position
on the local mean value the other on the local variance - produce a peculiar
pattern on the image. This pattern has a labyrinthine tiling aspect. For
certain subjects, the pattern enhances the look of the image.
</summary>
    <author>
      <name>A. Sparavigna</name>
    </author>
    <author>
      <name>B. Montrucchio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610088v1</id>
    <updated>2006-10-14T09:25:52Z</updated>
    <published>2006-10-14T09:25:52Z</published>
    <title>Vector field visualization with streamlines</title>
    <summary>  We have recently developed an algorithm for vector field visualization with
oriented streamlines, able to depict the flow directions everywhere in a dense
vector field and the sense of the local orientations. The algorithm has useful
applications in the visualization of the director field in nematic liquid
crystals. Here we propose an improvement of the algorithm able to enhance the
visualization of the local magnitude of the field. This new approach of the
algorithm is compared with the same procedure applied to the Line Integral
Convolution (LIC) visualization.
</summary>
    <author>
      <name>A. Sparavigna</name>
    </author>
    <author>
      <name>B. Montrucchio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pges, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.1549v1</id>
    <updated>2007-12-10T17:42:48Z</updated>
    <published>2007-12-10T17:42:48Z</published>
    <title>Dynamic Multilevel Graph Visualization</title>
    <summary>  We adapt multilevel, force-directed graph layout techniques to visualizing
dynamic graphs in which vertices and edges are added and removed in an online
fashion (i.e., unpredictably). We maintain multiple levels of coarseness using
a dynamic, randomized coarsening algorithm. To ensure the vertices follow
smooth trajectories, we employ dynamics simulation techniques, treating the
vertices as point particles. We simulate fine and coarse levels of the graph
simultaneously, coupling the dynamics of adjacent levels. Projection from
coarser to finer levels is adaptive, with the projection determined by an
affine transformation that evolves alongside the graph layouts. The result is a
dynamic graph visualizer that quickly and smoothly adapts to changes in a
graph.
</summary>
    <author>
      <name>Todd L. Veldhuizen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.1549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.1549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.0; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1500v1</id>
    <updated>2008-01-09T20:51:02Z</updated>
    <published>2008-01-09T20:51:02Z</published>
    <title>Toward the Graphics Turing Scale on a Blue Gene Supercomputer</title>
    <summary>  We investigate raytracing performance that can be achieved on a class of Blue
Gene supercomputers. We measure a 822 times speedup over a Pentium IV on a 6144
processor Blue Gene/L. We measure the computational performance as a function
of number of processors and problem size to determine the scaling performance
of the raytracing calculation on the Blue Gene. We find nontrivial scaling
behavior at large number of processors. We discuss applications of this
technology to scientific visualization with advanced lighting and high
resolution. We utilize three racks of a Blue Gene/L in our calculations which
is less than three percent of the the capacity of the worlds largest Blue Gene
computer.
</summary>
    <author>
      <name>Michael McGuigan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.1500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3249v1</id>
    <updated>2008-01-21T18:27:09Z</updated>
    <published>2008-01-21T18:27:09Z</published>
    <title>Complex Eigenvalues for Binary Subdivision Schemes</title>
    <summary>  Convergence properties of binary stationary subdivision schemes for curves
have been analyzed using the techniques of z-transforms and eigenanalysis.
Eigenanalysis provides a way to determine derivative continuity at specific
points based on the eigenvalues of a finite matrix. None of the well-known
subdivision schemes for curves have complex eigenvalues. We prove when a
convergent scheme with palindromic mask can have complex eigenvalues and that a
lower limit for the size of the mask exists in this case. We find a scheme with
complex eigenvalues achieving this lower bound. Furthermore we investigate this
scheme numerically and explain from a geometric viewpoint why such a scheme has
not yet been used in computer-aided geometric design.
</summary>
    <author>
      <name>Christian Kuehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.3249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3103v1</id>
    <updated>2008-04-18T21:02:19Z</updated>
    <published>2008-04-18T21:02:19Z</published>
    <title>Size matters: performance declines if your pixels are too big or too
  small</title>
    <summary>  We present a conceptual model that describes the effect of pixel size on
target acquisition. We demonstrate the use of our conceptual model by applying
it to predict and explain the results of an experiment to evaluate users'
performance in a target acquisition task involving three distinct display
sizes: standard desktop, small and large displays. The results indicate that
users are fastest on standard desktop displays, undershoots are the most common
error on small displays and overshoots are the most common error on large
displays. We propose heuristics to maintain usability when changing displays.
Finally, we contribute to the growing body of evidence that amplitude does
affect performance in a display-based pointing task.
</summary>
    <author>
      <name>Vassilis Kostakos</name>
    </author>
    <author>
      <name>Eamonn O'Neill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 13 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1667v1</id>
    <updated>2008-07-10T14:40:35Z</updated>
    <published>2008-07-10T14:40:35Z</published>
    <title>Quasi-Mandelbrot sets for perturbed complex analytic maps: visual
  patterns</title>
    <summary>  We consider perturbations of the complex quadratic map $ z \to z^2 +c$ and
corresponding changes in their quasi-Mandelbrot sets. Depending on particular
perturbation, visual forms of quasi-Mandelbrot set changes either sharply (when
the perturbation reaches some critical value) or continuously. In the latter
case we have a smooth transition from the classical form of the set to some
forms, constructed from mostly linear structures, as it is typical for
two-dimensional real number dynamics. Two examples of continuous evolution of
the quasi-Mandelbrot set are described.
</summary>
    <author>
      <name>A. V. Toporensky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages with 10 JPEG pictures</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.1667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.4121v1</id>
    <updated>2008-11-25T17:12:22Z</updated>
    <published>2008-11-25T17:12:22Z</published>
    <title>String Art: Circle Drawing Using Straight Lines</title>
    <summary>  An algorithm to generate the locus of a circle using the intersection points
of straight lines is proposed. The pixels on the circle are plotted independent
of one another and the operations involved in finding the locus of the circle
from the intersection of straight lines are parallelizable. Integer only
arithmetic and algorithmic optimizations are used for speedup. The proposed
algorithm makes use of an envelope to form a parabolic arc which is consequent
transformed into a circle. The use of parabolic arcs for the transformation
results in higher pixel errors as the radius of the circle to be drawn
increases. At its current state, the algorithm presented may be suitable only
for generating circles for string art.
</summary>
    <author>
      <name>Sankar K</name>
    </author>
    <author>
      <name>Sarad AV</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, code included</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.4121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.4121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.4681v1</id>
    <updated>2008-11-28T16:31:12Z</updated>
    <published>2008-11-28T16:31:12Z</published>
    <title>The Good, the Bad, and the Ugly: three different approaches to break
  their watermarking system</title>
    <summary>  The Good is Blondie, a wandering gunman with a strong personal sense of
honor. The Bad is Angel Eyes, a sadistic hitman who always hits his mark. The
Ugly is Tuco, a Mexican bandit who's always only looking out for himself.
Against the backdrop of the BOWS contest, they search for a watermark in gold
buried in three images. Each knows only a portion of the gold's exact location,
so for the moment they're dependent on each other. However, none are
particularly inclined to share...
</summary>
    <author>
      <name>Gaëtan Le Guelvouit</name>
    </author>
    <author>
      <name>Teddy Furon</name>
    </author>
    <author>
      <name>François Cayre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.703968</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.703968" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IS&amp;T/SPIE Electronic Imaging, vol. 6505, San Jose, CA, Jan.
  2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.4681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.4681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1401v1</id>
    <updated>2010-03-06T16:42:53Z</updated>
    <published>2010-03-06T16:42:53Z</published>
    <title>Macro and micro view on steady states in state space</title>
    <summary>  This paper describes visualization of chaotic attractor and elements of the
singularities in 3D space. 3D view of these effects enables to create a
demonstrative projection about relations of chaos generated by physical
circuit, the Chua's circuit. Via macro views on chaotic attractor is obtained
not only visual space illustration of representative point motion in state
space, but also its relation to planes of singularity elements. Our created
program enables view on chaotic attractor both in 2D and 3D space together with
plane objects visualization -- elements of singularities.
</summary>
    <author>
      <name>Branislav Sobota</name>
    </author>
    <author>
      <name>Milan Guzan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Univ. Sapientiae, Informatica, 2,1 (2010) 90-98</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.1401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94C99, 68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1410v2</id>
    <updated>2013-08-08T18:05:00Z</updated>
    <published>2010-03-06T18:08:12Z</published>
    <title>Local Space-Time Smoothing for Version Controlled Documents</title>
    <summary>  Unlike static documents, version controlled documents are continuously edited
by one or more authors. Such collaborative revision process makes traditional
modeling and visualization techniques inappropriate. In this paper we propose a
new representation based on local space-time smoothing that captures important
revision patterns. We demonstrate the applicability of our framework using
experiments on synthetic and real-world data.
</summary>
    <author>
      <name>Seungyeon Kim</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 23rd International Conference on Computational
  Linguistics (Coling 2010); 2010 Aug 23-27; Beijing, CN</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.1410v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1410v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3163v1</id>
    <updated>2010-05-18T11:54:39Z</updated>
    <published>2010-05-18T11:54:39Z</published>
    <title>Virtual Texturing</title>
    <summary>  In this thesis a rendering system and an accompanying tool chain for Virtual
Texturing is presented. Our tools allow to automatically retexture existing
geometry in order to apply unique texturing on each face. Furthermore we
investigate several techniques that try to minimize visual artifacts in the
case that only a small amount of pages can be streamed per frame. We analyze
the influence of different heuristics that are responsible for the page
selection. Alongside these results we present a measurement method to allow the
comparison of our heuristics.
</summary>
    <author>
      <name>Andreas Neu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RWTH Aachen University, Computer Graphics &amp; Multimedia</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1005.3163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3190v1</id>
    <updated>2010-05-18T13:10:28Z</updated>
    <published>2010-05-18T13:10:28Z</published>
    <title>From granular avalanches to fluid turbulences through oozing pastes. A
  mesoscopic physically-based particle model</title>
    <summary>  In this paper, we describe how we can precisely produce complex and various
dynamic morphological features such as structured and chaotic features which
occur in sand pilings (piles, avalanches, internal collapses, arches) , in
flowing fluids (laminar flowing, Kelvin-Helmholtz and Von Karmann eddies), and
in cohesive pastes (twist-and-turn oozing and packing) using only a single
unified model, called "mesoscopic model". This model is a physically-based
particle model whose behavior depends on only four simple, but easy to
understand, physically-based parameters : elasticity, viscosity and their local
areas of influence. It is fast to compute and easy to understand by
non-physicist users.
</summary>
    <author>
      <name>Annie Luciani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Graphicon 2000, Moscou : Russian Federation (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.3190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4903v2</id>
    <updated>2010-12-30T05:17:12Z</updated>
    <published>2010-06-25T03:11:37Z</published>
    <title>Toric degenerations of Bezier patches</title>
    <summary>  The control polygon of a Bezier curve is well-defined and has geometric
significance---there is a sequence of weights under which the limiting position
of the curve is the control polygon. For a Bezier surface patch, there are many
possible polyhedral control structures, and none are canonical. We propose a
not necessarily polyhedral control structure for surface patches, regular
control surfaces, which are certain C^0 spline surfaces. While not unique,
regular control surfaces are exactly the possible limiting positions of a
Bezier patch when the weights are allowed to vary.
</summary>
    <author>
      <name>Luis David Garcia-Puente</name>
    </author>
    <author>
      <name>Frank Sottile</name>
    </author>
    <author>
      <name>Chungang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, many .eps figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4903v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4903v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 14M25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.2204v1</id>
    <updated>2010-07-13T21:09:11Z</updated>
    <published>2010-07-13T21:09:11Z</published>
    <title>What's wrong with Phong - Designers' appraisal of shading in CAD-systems</title>
    <summary>  The Phong illumination model is still widely used in realtime 3D
visualization systems. The aim of this article is to document problems with the
Phong illumination model that are encountered by an important professional user
group, namely digital designers. This leads to a visual evaluation of Phong
illumination, which at least in this condensed form seems still to be missing
in the literature. It is hoped that by explicating these flaws, awareness about
the limitations and interdependencies of the model will increase, both among
fellow users, and among researchers and developers.
</summary>
    <author>
      <name>Jörg M. Hahn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.2204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.2204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; J.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.0208v2</id>
    <updated>2010-08-03T08:15:53Z</updated>
    <published>2010-08-01T22:58:04Z</published>
    <title>Parametric polynomial minimal surfaces of arbitrary degree</title>
    <summary>  Weierstrass representation is a classical parameterization of minimal
surfaces. However, two functions should be specified to construct the
parametric form in Weierestrass representation. In this paper, we propose an
explicit parametric form for a class of parametric polynomial minimal surfaces
of arbitrary degree. It includes the classical Enneper surface for cubic case.
The proposed minimal surfaces also have some interesting properties such as
symmetry, containing straight lines and self-intersections. According to the
shape properties, the proposed minimal surface can be classified into four
categories with respect to $n=4k-1$ $n=4k+1$, $n=4k$ and $n=4k+2$. The explicit
parametric form of corresponding conjugate minimal surfaces is given and the
isometric deformation is also implemented.
</summary>
    <author>
      <name>Gang Xu</name>
    </author>
    <author>
      <name>Guozhao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1008.0208v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.0208v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1664v1</id>
    <updated>2010-08-10T08:34:46Z</updated>
    <published>2010-08-10T08:34:46Z</published>
    <title>L-systems in Geometric Modeling</title>
    <summary>  We show that parametric context-sensitive L-systems with affine geometry
interpretation provide a succinct description of some of the most fundamental
algorithms of geometric modeling of curves. Examples include the
Lane-Riesenfeld algorithm for generating B-splines, the de Casteljau algorithm
for generating Bezier curves, and their extensions to rational curves. Our
results generalize the previously reported geometric-modeling applications of
L-systems, which were limited to subdivision curves.
</summary>
    <author>
      <name>Przemyslaw Prusinkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Calgary</arxiv:affiliation>
    </author>
    <author>
      <name>Mitra Shirmohammadi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Calgary</arxiv:affiliation>
    </author>
    <author>
      <name>Faramarz Samavati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Calgary</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.31.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.31.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings DCFS 2010, arXiv:1008.1270</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 31, 2010, pp. 3-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.1664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4602v1</id>
    <updated>2010-09-23T13:08:32Z</updated>
    <published>2010-09-23T13:08:32Z</published>
    <title>Geoglyphs of Titicaca as an ancient example of graphic design</title>
    <summary>  The paper proposes an ancient landscape design as an example of graphic
design for an age and place where no written documents existed. It is created
by a network of earthworks, which constitute the remains of an extensive
ancient agricultural system. It can be seen by means of the Google satellite
imagery on the Peruvian region near the Titicaca Lake, as a texture
superimposed to the background landform. In this texture, many drawings
(geoglyphs) can be observed.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Geoglyphs, History of Graphics, Image processing, Satellite
  maps, Archaeology</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.2623v2</id>
    <updated>2010-11-15T10:47:24Z</updated>
    <published>2010-10-13T10:35:23Z</published>
    <title>Surface Curvature Effects on Reflectance from Translucent Materials</title>
    <summary>  Most of the physically based techniques for rendering translucent objects use
the diffusion theory of light scattering in turbid media. The widely used
dipole diffusion model (Jensen et al. 2001) applies the diffusion-theory
formula derived for a planar interface to objects of arbitrary shapes. This
paper presents first results of our investigation of how surface curvature
affects the diffuse reflectance from translucent materials.
</summary>
    <author>
      <name>Konstantin Kolchin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures. The first version of this paper was published in
  the Communication Papers Proceedings of 18th International Conference on
  Computer Graphics, Visualization and Computer Vision 2010 - WSCG2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.2623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.2623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0262v1</id>
    <updated>2010-12-31T14:34:06Z</updated>
    <published>2010-12-31T14:34:06Z</published>
    <title>High Speed and Area Efficient 2D DWT Processor based Image Compression"
  Signal &amp; Image Processing</title>
    <summary>  This paper presents a high speed and area efficient DWT processor based
design for Image Compression applications. In this proposed design, pipelined
partially serial architecture has been used to enhance the speed along with
optimal utilization and resources available on target FPGA. The proposed model
has been designed and simulated using Simulink and System Generator blocks,
synthesized with Xilinx Synthesis tool (XST) and implemented on Spartan 2 and 3
based XC2S100-5tq144 and XC3S500E-4fg320 target device. The results show that
proposed design can operate at maximum frequency 231 MHz in case of Spartan 3
by consuming power of 117mW at 28 degree/c junction temperature. The result
comparison has shown an improvement of 15% in speed.
</summary>
    <author>
      <name>Sugreev Kaur</name>
    </author>
    <author>
      <name>Rajesh Mehra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.0690v1</id>
    <updated>2011-07-04T17:51:00Z</updated>
    <published>2011-07-04T17:51:00Z</published>
    <title>A Framework for Designing 3D Virtual Environments</title>
    <summary>  The process of design and development of virtual environments can be
supported by tools and frameworks, to save time in technical aspects and
focusing on the content. In this paper we present an academic framework which
provides several levels of abstraction to ease this work. It includes
state-of-the-art components we devised or integrated adopting open-source
solutions in order to face specific problems. Its architecture is modular and
customizable, the code is open-source.
</summary>
    <author>
      <name>Salvatore Catanese</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Giacomo Fiumara</name>
    </author>
    <author>
      <name>Francesco Pagano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-30214-5_23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-30214-5_23" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure, Proceedings of the 4th International ICST
  Conference On Intelligent Technologies For Interactive Entertainment, 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes of the Institute for Computer Sciences, Social
  Informatics and Telecommunications Engineering Volume 78, 2012, pp 209-218</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.0690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.0690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0070v1</id>
    <updated>2011-12-30T06:23:54Z</updated>
    <published>2011-12-30T06:23:54Z</published>
    <title>Fast B-spline Curve Fitting by L-BFGS</title>
    <summary>  We propose a novel method for fitting planar B-spline curves to unorganized
data points. In traditional methods, optimization of control points and foot
points are performed in two very time-consuming steps in each iteration: 1)
control points are updated by setting up and solving a linear system of
equations; and 2) foot points are computed by projecting each data point onto a
B-spline curve. Our method uses the L-BFGS optimization method to optimize
control points and foot points simultaneously and therefore it does not need to
perform either matrix computation or foot point projection in every iteration.
As a result, our method is much faster than existing methods.
</summary>
    <author>
      <name>Wenni Zheng</name>
    </author>
    <author>
      <name>Pengbo Bo</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Wenping Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1201.0070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1461v1</id>
    <updated>2012-04-06T12:03:45Z</updated>
    <published>2012-04-06T12:03:45Z</published>
    <title>Efficient computational noise in GLSL</title>
    <summary>  We present GLSL implementations of Perlin noise and Perlin simplex noise that
run fast enough for practical consideration on current generation GPU hardware.
The key benefits are that the functions are purely computational, i.e. they use
neither textures nor lookup tables, and that they are implemented in GLSL
version 1.20, which means they are compatible with all current GLSL-capable
platforms, including OpenGL ES 2.0 and WebGL 1.0. Their performance is on par
with previously presented GPU implementations of noise, they are very
convenient to use, and they scale well with increasing parallelism in present
and upcoming GPU architectures.
</summary>
    <author>
      <name>Ian McEwan</name>
    </author>
    <author>
      <name>David Sheets</name>
    </author>
    <author>
      <name>Stefan Gustavson</name>
    </author>
    <author>
      <name>Mark Richardson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/2151237X.2012.649621</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/2151237X.2012.649621" rel="related"/>
    <link href="http://arxiv.org/abs/1204.1461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3899v1</id>
    <updated>2012-07-17T07:17:27Z</updated>
    <published>2012-07-17T07:17:27Z</published>
    <title>Fast View Frustum Culling of Spatial Object by Analytical Bounding Bin</title>
    <summary>  It is a common sense to apply the VFC (view frustum culling) of spatial
object to bounding cube of the object in 3D graphics. The accuracy of VFC can
not be guaranteed even in cube rotated three-dimensionally. In this paper is
proposed a method which is able to carry out more precise and fast VFC of any
spatial object in the image domain of cube by an analytic mapping, and is
demonstrated the effect of the method for terrain block on global surface.
</summary>
    <author>
      <name>Munsu Ju</name>
    </author>
    <author>
      <name>Yunchol Jong</name>
    </author>
    <link href="http://arxiv.org/abs/1207.3899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1768v2</id>
    <updated>2019-03-04T11:25:07Z</updated>
    <published>2012-11-08T06:50:44Z</published>
    <title>Nearest Neighbor Value Interpolation</title>
    <summary>  This paper presents the nearest neighbor value (NNV) algorithm for high
resolution (H.R.) image interpolation. The difference between the proposed
algorithm and conventional nearest neighbor algorithm is that the concept
applied, to estimate the missing pixel value, is guided by the nearest value
rather than the distance. In other words, the proposed concept selects one
pixel, among four directly surrounding the empty location, whose value is
almost equal to the value generated by the conventional bilinear interpolation
algorithm. The proposed method demonstrated higher performances in terms of
H.R. when compared to the conventional interpolation algorithms mentioned.
</summary>
    <author>
      <name>Olivier Rukundo</name>
    </author>
    <author>
      <name>Hanqiang Cao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2012.030405</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2012.030405" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications(IJACSA),Vol. 3, No. 4, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.1768v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1768v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3297v2</id>
    <updated>2013-08-01T14:12:08Z</updated>
    <published>2012-11-14T13:21:03Z</published>
    <title>Gap Processing for Adaptive Maximal Poisson-Disk Sampling</title>
    <summary>  In this paper, we study the generation of maximal Poisson-disk sets with
varying radii. First, we present a geometric analysis of gaps in such disk
sets. This analysis is the basis for maximal and adaptive sampling in Euclidean
space and on manifolds. Second, we propose efficient algorithms and data
structures to detect gaps and update gaps when disks are inserted, deleted,
moved, or have their radius changed. We build on the concepts of the regular
triangulation and the power diagram. Third, we will show how our analysis can
make a contribution to the state-of-the-art in surface remeshing.
</summary>
    <author>
      <name>Dong-Ming Yan</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages. ACM Transactions on Graphics, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.3297v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3297v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3659v1</id>
    <updated>2012-11-15T17:08:58Z</updated>
    <published>2012-11-15T17:08:58Z</published>
    <title>Color scales that are effective in both color and grayscale</title>
    <summary>  We consider the problem of finding a color scale which performs well when
converted to a grayscale. We assume that each color is converted to a shade of
gray with the same intensity as the color. We also assume that the color scales
have a linear variation of intensity and hue, and find scales which maximize
the average chroma (or "colorfulness") of the colors. We find two classes of
solutions, which traverse the color wheel in opposite directions. The two
classes of scales start with hues near cyan and red. The average chroma of the
scales are 65-77% those of the pure colors.
</summary>
    <author>
      <name>Silas Alben</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.3659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5669v1</id>
    <updated>2012-11-24T12:56:58Z</updated>
    <published>2012-11-24T12:56:58Z</published>
    <title>Analysis-suitable T-splines: characterization, refineability, and
  approximation</title>
    <summary>  We establish several fundamental properties of analysis-suitable T-splines
which are important for design and analysis. First, we characterize T-spline
spaces and prove that the space of smooth bicubic polynomials, defined over the
extended T-mesh of an analysis-suitable T-spline, is contained in the
corresponding analysis-suitable T-spline space. This is accomplished through
the theory of perturbed analysis-suitable T-spline spaces and a simple
topological dimension formula. Second, we establish the theory of
analysis-suitable local refinement and describe the conditions under which two
analysis-suitable T-spline spaces are nested. Last, we demonstrate that these
results can be used to establish basic approximation results which are critical
for analysis.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>M. A. Scott</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Models and Methods in Applied Sciences,Vol. 24, No.
  06, pp. 1141-1164 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.5669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5842v1</id>
    <updated>2012-11-26T02:13:52Z</updated>
    <published>2012-11-26T02:13:52Z</published>
    <title>A Novel Algorithm for Real-time Procedural Generation of Building Floor
  Plans</title>
    <summary>  Real-time generation of natural-looking floor plans is vital in games with
dynamic environments. This paper presents an algorithm to generate suburban
house floor plans in real-time. The algorithm is based on the work presented in
[1]. However, the corridor placement is redesigned to produce floor plans
similar to real houses. Moreover, an optimization stage is added to find a
corridor placement with the minimum used space, an approach that is designed to
mimic the real-life practices to minimize the wasted spaces in the design. The
results show very similar floor plans to the ones designed by an architect.
</summary>
    <author>
      <name>Maysam Mirahmadi</name>
    </author>
    <author>
      <name>Abdallah Shami</name>
    </author>
    <link href="http://arxiv.org/abs/1211.5842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0289v1</id>
    <updated>2012-12-24T17:10:28Z</updated>
    <published>2012-12-24T17:10:28Z</published>
    <title>Reconstructing Self Organizing Maps as Spider Graphs for better visual
  interpretation of large unstructured datasets</title>
    <summary>  Self-Organizing Maps (SOM) are popular unsupervised artificial neural network
used to reduce dimensions and visualize data. Visual interpretation from
Self-Organizing Maps (SOM) has been limited due to grid approach of data
representation, which makes inter-scenario analysis impossible. The paper
proposes a new way to structure SOM. This model reconstructs SOM to show
strength between variables as the threads of a cobweb and illuminate
inter-scenario analysis. While Radar Graphs are very crude representation of
spider web, this model uses more lively and realistic cobweb representation to
take into account the difference in strength and length of threads. This model
allows for visualization of highly unstructured dataset with large number of
dimensions, common in Bigdata sources.
</summary>
    <author>
      <name>Aaditya Prakash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6008v1</id>
    <updated>2013-01-25T11:07:37Z</updated>
    <published>2013-01-25T11:07:37Z</published>
    <title>Immersive VR Visualizations by VFIVE. Part 2: Applications</title>
    <summary>  VFIVE is a scientific visualization application for CAVE-type immersive
virtual reality systems. The source codes are freely available. VFIVE is used
as a research tool in various VR systems. It also lays the groundwork for
developments of new visualization software for CAVEs. In this paper, we pick up
five CAVE systems in four different institutions in Japan. Applications of
VFIVE in each CAVE system are summarized. Special emphases will be placed on
scientific and technical achievements made possible by VFIVE.
</summary>
    <author>
      <name>Akira Kageyama</name>
    </author>
    <author>
      <name>Nobuaki Ohno</name>
    </author>
    <author>
      <name>Shintaro Kawahara</name>
    </author>
    <author>
      <name>Kazuo Kashiyama</name>
    </author>
    <author>
      <name>Hiroaki Ohtani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.6008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.4974v1</id>
    <updated>2013-04-17T21:32:37Z</updated>
    <published>2013-04-17T21:32:37Z</published>
    <title>Fast exact digital differential analyzer for circle generation</title>
    <summary>  In the first part of the paper we present a short review of applications of
digital differential analyzers (DDA) to generation of circles showing that they
can be treated as one-step numerical schemes. In the second part we present and
discuss a novel fast algorithm based on a two-step numerical scheme (explicit
midpoint rule). Although our algorithm is as cheap as the simplest one-step DDA
algoritm (and can be represented in terms of shifts and additions), it
generates circles with maximal accuracy, i.e., it is exact up to round-off
errors.
</summary>
    <author>
      <name>Jan L. Cieśliński</name>
    </author>
    <author>
      <name>Leonid V. Moroz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.4974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.4974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 68U07, 65L12" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; G.1.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7842v1</id>
    <updated>2013-04-30T03:10:31Z</updated>
    <published>2013-04-30T03:10:31Z</published>
    <title>The Logarithmic Curvature Graphs of Generalised Cornu Spirals</title>
    <summary>  The Generalized Cornu Spiral (GCS) was first proposed by Ali et al. in 1995
[9]. Due to the monotonocity of its curvature function, the surface generated
with GCS segments has been considered as a high quality surface and it has
potential applications in surface design [2]. In this paper, the analysis of
GCS segment is carried out by determining its aesthetic value using the log
curvature Graph (LCG) as proposed by Kanaya et al.[10]. The analysis of LCG
supports the claim that GCS is indeed a generalized aesthetic curve.
</summary>
    <author>
      <name>R. U. Gobithaasan</name>
    </author>
    <author>
      <name>J. M. Ali</name>
    </author>
    <author>
      <name>Kenjiro T. Miura</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2012 Punjab University Journal of Mathematics, 44, Pg.1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7848v1</id>
    <updated>2013-04-30T03:33:43Z</updated>
    <published>2013-04-30T03:33:43Z</published>
    <title>Characterization of Planar Cubic Alternative curve</title>
    <summary>  In this paper, we analyze the planar cubic Alternative curve to determine the
conditions for convex, loops, cusps and inflection points. Thus cubic curve is
represented by linear combination of three control points and basis function
that consist of two shape parameters. By using algebraic manipulation, we can
determine the constraint of shape parameters and sufficient conditions are
derived which ensure that the curve is a strictly convex, loops, cusps and
inflection point. We conclude the result in a shape diagram of parameters. The
simplicity of this form makes characterization more intuitive and efficient to
compute.
</summary>
    <author>
      <name>Azhar Ahmad</name>
    </author>
    <author>
      <name>R. Gobithasan</name>
    </author>
    <author>
      <name>Jamaluddin Md. Ali</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2009 Journal of Mathematika, Vol. 25, Nu.2, Pg. 125-134</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7881v1</id>
    <updated>2013-04-30T05:25:31Z</updated>
    <published>2013-04-30T05:25:31Z</published>
    <title>Various Types of Aesthetic Curves</title>
    <summary>  The research on developing planar curves to produce visually pleasing
products (ranges from electric appliances to car body design) and
indentifying/modifying planar curves for special purposes namely for railway
design, highway design and robot trajectories have been progressing since
1970s. The pattern of research in this field of study has branched to five
major groups namely curve synthesis, fairing process, improvement in control of
natural spiral, construction of new type of planar curves and, natural spiral
fitting &amp; approximation techniques. The purpose of is this paper is to briefly
review recent progresses in Computer Aided Geometric Design (CAGD) focusing on
the topics states above.
</summary>
    <author>
      <name>R. U. Gobithaasan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2011 The Proceedings of Seminar Bidang Kepakaran Jabatan Matematik
  2010, Cherating, Pahang. Disember 27th- 30th 2010, Pg.9-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7883v1</id>
    <updated>2013-04-30T05:30:00Z</updated>
    <published>2013-04-30T05:30:00Z</published>
    <title>An Improvised Algorithm to Identify The Beauty of A Planar Curve</title>
    <summary>  An improvised algorithm is proposed based on the work of Yoshimoto and
Harada. The improvised algorithm results a graph which is called LDGC or
Logarithmic Distribution Graph of Curvature. This graph has the capability to
identify the beauty of monotonic planar curves with less effort as compared to
LDDC by Yoshimoto and Harada.
</summary>
    <author>
      <name>R. U. Gobithaasan</name>
    </author>
    <author>
      <name>Jamaludin Md. Ali</name>
    </author>
    <author>
      <name>Kenjiro T. Miura</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2008 The Proceedings of Simposium Kebangsaan Sains Matematik ke-16
  (SKSM16), Kota Bharu, Kelantan. June 3rd-5th 2008, Pg.223-228</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0001v1</id>
    <updated>2013-04-30T05:12:05Z</updated>
    <published>2013-04-30T05:12:05Z</published>
    <title>Perfectly normal type-2 fuzzy interpolation B-spline curve</title>
    <summary>  In this paper, we proposed another new form of type-2 fuzzy data
points(T2FDPs) that is perfectly normal type-2 data points(PNT2FDPs). These
kinds of brand-new data were defined by using the existing type-2 fuzzy set
theory(T2FST) and type-2 fuzzy number(T2FN) concept since we dealt with the
problem of defining complex uncertainty data. Along with this restructuring, we
included the fuzzification(alpha-cut operation), type-reduction and
defuzzification processes against PNT2FDPs. In addition, we used interpolation
B-soline curve function to demonstrate the PNT2FDPs.
</summary>
    <author>
      <name>Rozaimi Zakaria</name>
    </author>
    <author>
      <name>Abd. Fatah Wahab</name>
    </author>
    <author>
      <name>R. U. Gobithaasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1304.7868</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2013 Applied Mathematical Sciences 7 (21-24), Pg.1043-1055</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.0001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1473v1</id>
    <updated>2013-05-07T11:35:49Z</updated>
    <published>2013-05-07T11:35:49Z</published>
    <title>On the variety of planar spirals and their applications in computer
  aided design</title>
    <summary>  In this paper we discuss the variety of planar spiral segments and their
applications in objects in both the real and artificial world. The discussed
curves with monotonic curvature function are well-known in geometric modelling
and computer aided geometric design as fair curves, and they are very
significant in aesthetic shape modelling. Fair curve segments are used for
two-point G1 and G2 Hermite interpolation, as well as for generating aesthetic
splines.
</summary>
    <author>
      <name>Rushan Ziatdinov</name>
    </author>
    <author>
      <name>Kenjiro T. Miura</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Researcher 27(8-2), 1227-1232, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.1473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1737v1</id>
    <updated>2013-05-08T07:59:52Z</updated>
    <published>2013-05-08T07:59:52Z</published>
    <title>MC-curves and aesthetic measurements for pseudospiral curve segments</title>
    <summary>  This article studies families of curves with monotonic curvature function
(MC-curves) and their applications in geometric modelling and aesthetic design.
Aesthetic analysis and assessment of the structure and plastic qualities of
pseudospirals, which are curves with monotonic curvature function, are
conducted for the first time in the field of geometric modelling from the
position of technical aesthetics laws. The example of car body surface
modelling with the use of aesthetics splines is given.
</summary>
    <author>
      <name>Rushan Ziatdinov</name>
    </author>
    <author>
      <name>Rifkat I. Nabiyev</name>
    </author>
    <author>
      <name>Kenjiro T. Miura</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Design &amp; Technical Aesthetics 1(1), 6-17, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.1737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0419v2</id>
    <updated>2014-05-08T18:34:09Z</updated>
    <published>2013-08-02T07:10:47Z</published>
    <title>Inverse Procedural Modeling of Facade Layouts</title>
    <summary>  In this paper, we address the following research problem: How can we generate
a meaningful split grammar that explains a given facade layout? To evaluate if
a grammar is meaningful, we propose a cost function based on the description
length and minimize this cost using an approximate dynamic programming
framework. Our evaluation indicates that our framework extracts meaningful
split grammars that are competitive with those of expert users, while some
users and all competing automatic solutions are less successful.
</summary>
    <author>
      <name>Fuzhang Wu</name>
    </author>
    <author>
      <name>Dong-Ming Yan</name>
    </author>
    <author>
      <name>Weiming Dong</name>
    </author>
    <author>
      <name>Xiaopeng Zhang</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0419v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0419v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0869v1</id>
    <updated>2013-08-05T01:56:59Z</updated>
    <published>2013-08-05T01:56:59Z</published>
    <title>A Spline-based Volumetric Data Modeling Framework and Its Applications</title>
    <summary>  In this dissertation, we concentrate on the challenging research issue of
developing a spline-based modeling framework, which converts the conventional
data (e.g., surface meshes) to tensor-product trivariate splines. This
methodology can represent both boundary/volumetric geometry and real volumetric
physical attributes in a compact and continuous fashion. The regular
tensor-product structure enables our new developed methods to be embedded into
the industry standard seamlessly. These properties make our techniques highly
preferable in many physically-based applications including mechanical analysis,
shape deformation and editing, virtual surgery training, etc.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D thesis, Computer Science Department, Stony Brook University</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1279v9</id>
    <updated>2014-10-30T05:07:47Z</updated>
    <published>2013-08-06T14:15:42Z</published>
    <title>Barycentric Coordinates as Interpolants</title>
    <summary>  Barycentric coordinates are frequently used as interpolants to shade computer
graphics images. A simple equation transforms barycentric coordinates from
screen space into eye space in order to undo the perspective transformation and
permit accurate interpolative shading of texture maps. This technique is
amenable to computation using a block-normalized integer representation.
</summary>
    <author>
      <name>Russell A. Brown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.1279v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1279v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1917v1</id>
    <updated>2013-09-07T23:21:31Z</updated>
    <published>2013-09-07T23:21:31Z</published>
    <title>Zahir: a Object-Oriented Framework for Computer Graphics</title>
    <summary>  In this article we present Zahir, a framework for experimentation in Computer
Graphics that provides a group of object-oriented base components that take
care of common tasks in rendering techniques and algorithms, specially those of
Non Photo-realistic Rendering (NPR). These components allow developers to
implement rendering techniques and algorithms over static and animated meshes.
Currently, Zahir is being used in a Master's Thesis and as support material in
the undergraduate Computer Graphics course in University of Chile.
</summary>
    <author>
      <name>Eduardo Graells-Garrido</name>
    </author>
    <author>
      <name>María Cecilia Rivara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report, 2009, Santiago, Chile</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.1917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3314v1</id>
    <updated>2013-09-12T21:47:37Z</updated>
    <published>2013-09-12T21:47:37Z</published>
    <title>Progressive Compression of 3D Objects with an Adaptive Quantization</title>
    <summary>  This paper presents a new progressive compression method for triangular
meshes. This method, in fact, is based on a schema of irregular
multi-resolution analysis and is centered on the optimization of the
rate-distortion trade-off. The quantization precision is adapted to each vertex
during the encoding / decoding process to optimize the rate-distortion
compromise. The Optimization of the treated mesh geometry improves the
approximation quality and the compression ratio at each level of resolution.
The experimental results show that the proposed algorithm gives competitive
results compared to the previous works dealing with the rate-distortion
compromise.
</summary>
    <author>
      <name>Zeineb Abderrahim</name>
    </author>
    <author>
      <name>Elhem Techini</name>
    </author>
    <author>
      <name>Mohamed Salim Bouhlel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI Volume 10, Issue 2,No 1, March 2013 , Pages 504-511</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.3314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7430v1</id>
    <updated>2013-11-28T21:45:10Z</updated>
    <published>2013-11-28T21:45:10Z</published>
    <title>A local Gaussian filter and adaptive morphology as tools for completing
  partially discontinuous curves</title>
    <summary>  This paper presents a method for extraction and analysis of curve--type
structures which consist of disconnected components. Such structures are found
in electron--microscopy (EM) images of metal nanograins, which are widely used
in the field of nanosensor technology.
  The topography of metal nanograins in compound nanomaterials is crucial to
nanosensor characteristics. The method of completing such templates consists of
three steps. In the first step, a local Gaussian filter is used with different
weights for each neighborhood. In the second step, an adaptive morphology
operation is applied to detect the endpoints of curve segments and connect
them. In the last step, pruning is employed to extract a curve which optimally
fits the template.
</summary>
    <author>
      <name>P. Spurek</name>
    </author>
    <author>
      <name>A. Chaikouskaya</name>
    </author>
    <author>
      <name>J. Tabor</name>
    </author>
    <author>
      <name>E. Zając</name>
    </author>
    <link href="http://arxiv.org/abs/1311.7430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5431v2</id>
    <updated>2014-06-23T12:03:12Z</updated>
    <published>2014-06-20T15:34:06Z</published>
    <title>Consistently Orienting Facets in Polygon Meshes by Minimizing the
  Dirichlet Energy of Generalized Winding Numbers</title>
    <summary>  Jacobson et al. [JKSH13] hypothesized that the local coherency of the
generalized winding number function could be used to correctly determine
consistent facet orientations in polygon meshes. We report on an approach to
consistently orienting facets in polygon meshes by minimizing the Dirichlet
energy of generalized winding numbers. While the energy can be concisely
formulated and efficiently computed, we found that this approach is
fundamentally flawed and is unfortunately not applicable for most handmade
meshes shared on popular mesh repositories such as Google 3D Warehouse.
</summary>
    <author>
      <name>Kenshi Takayama</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>Ladislav Kavan</name>
    </author>
    <author>
      <name>Olga Sorkine-Hornung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.5431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7025v1</id>
    <updated>2014-06-26T20:50:34Z</updated>
    <published>2014-06-26T20:50:34Z</published>
    <title>DASS: Detail Aware Sketch-Based Surface Modeling</title>
    <summary>  We present a sketch-based modeling system suitable for detail editing, based
on a multilevel representation for surfaces. The main advantage of this
representation allowing for the control of local (details) and global changes
of the model. We used an adaptive mesh (4-8 mesh) and developed a label theory
to construct a manifold structure, which is responsible for controlling local
editing of the model. The overall shape and global modifications are defined by
a variational implicit surface (Hermite RBF). Our system assembles the manifold
structures to allow the user to add details without changing the overall shape,
as well as edit the overall shape while repositioning details coherently.
</summary>
    <author>
      <name>Emilio Vital Brazil</name>
    </author>
    <link href="http://arxiv.org/abs/1406.7025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5024v1</id>
    <updated>2014-09-01T00:46:26Z</updated>
    <published>2014-09-01T00:46:26Z</published>
    <title>Comparative Study of Geometric and Image Based Modelling and Rendering
  Techniques</title>
    <summary>  This is a comparative study of the traditional 3D computer graphics technique
of geometric modelling and image-based rendering techniques that were surveyed
and implemented.We have discussed the classifications and representative
methods of both the techniques. The study has shown that there is a strong
continuum between both the techniques and a hybrid of the two is most suitable
for further implementations.This hybridisation study is underway to create
models of real life situations and provide disaster management training.
</summary>
    <author>
      <name>Agrima Seth</name>
    </author>
    <author>
      <name>Deepak Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/1409.5024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03605v1</id>
    <updated>2015-01-15T09:16:26Z</updated>
    <published>2015-01-15T09:16:26Z</published>
    <title>Feature Lines for Illustrating Medical Surface Models: Mathematical
  Background and Survey</title>
    <summary>  This paper provides a tutorial and survey for a specific kind of illustrative
visualization technique: feature lines. We examine different feature line
methods. For this, we provide the differential geometry behind these concepts
and adapt this mathematical field to the discrete differential geometry. All
discrete differential geometry terms are explained for triangulated surface
meshes. These utilities serve as basis for the feature line methods. We provide
the reader with all knowledge to re-implement every feature line method.
Furthermore, we summarize the methods and suggest a guideline for which kind of
surface which feature line algorithm is best suited. Our work is motivated by,
but not restricted to, medical and biological surface models.
</summary>
    <author>
      <name>Kai Lawonn</name>
    </author>
    <author>
      <name>Bernhard Preim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.03605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06364v1</id>
    <updated>2015-01-26T12:38:21Z</updated>
    <published>2015-01-26T12:38:21Z</published>
    <title>GPU Programming - Speeding Up the 3D Surface Generator VESTA</title>
    <summary>  The novel "Volume-Enclosing Surface exTraction Algorithm" (VESTA) generates
triangular isosurfaces from computed tomography volumetric images and/or
three-dimensional (3D) simulation data. Here, we present various benchmarks for
GPU-based code implementations of both VESTA and the current state-of-the-art
Marching Cubes Algorithm (MCA). One major result of this study is that VESTA
runs significantly faster than the MCA.
</summary>
    <author>
      <name>B. R. Schlei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15120/GR-2015-1-FG-GENERAL-42</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15120/GR-2015-1-FG-GENERAL-42" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 page, 1 figure, submitted contribution to the GSI Scientific Report
  2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.06364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04232v1</id>
    <updated>2015-02-14T17:42:02Z</updated>
    <published>2015-02-14T17:42:02Z</published>
    <title>Sketch-based Shape Retrieval using Pyramid-of-Parts</title>
    <summary>  We present a multi-scale approach to sketch-based shape retrieval. It is
based on a novel multi-scale shape descriptor called Pyramidof- Parts, which
encodes the features and spatial relationship of the semantic parts of query
sketches. The same descriptor can also be used to represent 2D projected views
of 3D shapes, allowing effective matching of query sketches with 3D shapes
across multiple scales. Experimental results show that the proposed method
outperforms the state-of-the-art method, whether the sketch segmentation
information is obtained manually or automatically by considering each stroke as
a semantic part.
</summary>
    <author>
      <name>Changqing Zou</name>
    </author>
    <author>
      <name>Zhe Huang</name>
    </author>
    <author>
      <name>Rynson W. H. Lau</name>
    </author>
    <author>
      <name>Jianzhuang Liu</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01379v2</id>
    <updated>2015-07-29T20:57:06Z</updated>
    <published>2015-04-06T15:53:09Z</published>
    <title>Preprint Big City 3D Visual Analysis</title>
    <summary>  This is the preprint version of our paper on EUROGRAPHICS 2015. A big city
visual analysis platform based on Web Virtual Reality Geographical Information
System (WEBVRGIS) is presented. Extensive model editing functions and spatial
analysis functions are available, including terrain analysis, spatial analysis,
sunlight analysis, traffic analysis, population analysis and community
analysis.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Xiaoming Li</name>
    </author>
    <author>
      <name>Baoyun Zhang</name>
    </author>
    <author>
      <name>Weixi Wang</name>
    </author>
    <author>
      <name>Shengzhong Feng</name>
    </author>
    <author>
      <name>Jinxing Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on EUROGRAPHICS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01379v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01379v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02744v1</id>
    <updated>2015-04-10T17:42:14Z</updated>
    <published>2015-04-10T17:42:14Z</published>
    <title>Real-time Tool for Affine Transformations of Two Dimensional IFS
  Fractals</title>
    <summary>  This work introduces a novel tool for interactive, real-time transformations
of two dimensional IFS fractals. We assign barycentric coordinates (relative to
an arbitrary affine basis of $\mathbb{R}^2$) to the points that constitute the
image of a fractal. The tool uses some of the nice properties of the
barycentric coordinates, enabling any affine transformation of the basis, done
by click-and-drag, to be immediately followed by the same affine transformation
of the IFS fractal attractor. In order to have a better control over the
fractal, as affine basis we use a kind of minimal simplex that contains the
attractor. We give theoretical grounds of the tool and then the software
application.
</summary>
    <author>
      <name>Elena Hadzieva</name>
    </author>
    <author>
      <name>Marija Shuminoska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="28A80, 65D17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03151v1</id>
    <updated>2015-04-13T12:39:50Z</updated>
    <published>2015-04-13T12:39:50Z</published>
    <title>Massively Parallel Ray Tracing Algorithm Using GPU</title>
    <summary>  Ray tracing is a technique for generating an image by tracing the path of
light through pixels in an image plane and simulating the effects of
high-quality global illumination at a heavy computational cost. Because of the
high computation complexity, it can't reach the requirement of real-time
rendering. The emergence of many-core architectures, makes it possible to
reduce significantly the running time of ray tracing algorithm by employing the
powerful ability of floating point computation. In this paper, a new GPU
implementation and optimization of the ray tracing to accelerate the rendering
process is presented.
</summary>
    <author>
      <name>Yutong Qin</name>
    </author>
    <author>
      <name>Jianbiao Lin</name>
    </author>
    <author>
      <name>Xiang Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1504.03151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00021v1</id>
    <updated>2015-05-29T20:27:11Z</updated>
    <published>2015-05-29T20:27:11Z</published>
    <title>Variance Analysis for Monte Carlo Integration: A
  Representation-Theoretic Perspective</title>
    <summary>  In this report, we revisit the work of Pilleboue et al. [2015], providing a
representation-theoretic derivation of the closed-form expression for the
expected value and variance in homogeneous Monte Carlo integration. We show
that the results obtained for the variance estimation of Monte Carlo
integration on the torus, the sphere, and Euclidean space can be formulated as
specific instances of a more general theory. We review the related
representation theory and show how it can be used to derive a closed-form
solution.
</summary>
    <author>
      <name>Michael Kazhdan</name>
    </author>
    <author>
      <name>Gurprit Singh</name>
    </author>
    <author>
      <name>Adrien Pilleboue</name>
    </author>
    <author>
      <name>David Coeurjolly</name>
    </author>
    <author>
      <name>Victor Ostromoukhov</name>
    </author>
    <link href="http://arxiv.org/abs/1506.00021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06968v1</id>
    <updated>2015-06-23T12:22:32Z</updated>
    <published>2015-06-23T12:22:32Z</published>
    <title>A Survey on Distributed Visualization Techniques over Clusters of
  Personal Computers</title>
    <summary>  In the last years, Distributed Visualization over Personal Computer (PC)
clusters has become important for research and industrial communities. They
have made large-scale visualizations practical and more accessible. In this
work we survey Distributed Visualization techniques aiming at compiling last
decade's literature on the use of PC clusters as suitable alternatives to
high-end workstations. We review the topic by defining basic concepts,
enumerating system requirements and implementation challenges, and presenting
up-to-date methodologies. Our work fulfills the needs of newcomers and seasoned
professionals as an introductory compilation at the same time that it can help
experienced personnel by organizing ideas.
</summary>
    <author>
      <name>Jose Rodrigues</name>
    </author>
    <author>
      <name>Andre Balan</name>
    </author>
    <author>
      <name>Luciana Zaina</name>
    </author>
    <author>
      <name>Agma Traina</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">INFOCOMP Journal of Computer Science, 2009, ISSN: 1807-4545, vol
  8: 4. 79-90</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.06968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05290v2</id>
    <updated>2016-07-06T01:24:42Z</updated>
    <published>2015-07-19T13:53:47Z</published>
    <title>A concise parametrisation of affine transformation</title>
    <summary>  Good parametrisations of affine transformations are essential to
interpolation, deformation, and analysis of shape, motion, and animation. It
has been one of the central research topics in computer graphics. However,
there is no single perfect method and each one has both advantages and
disadvantages. In this paper, we propose a novel parametrisation of affine
transformations, which is a generalisation to or an improvement of existing
methods. Our method adds yet another choice to the existing toolbox and shows
better performance in some applications. A C++ implementation is available to
make our framework ready to use in various applications.
</summary>
    <author>
      <name>Shizuo Kaji</name>
    </author>
    <author>
      <name>Hiroyuki Ochiai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">errors corrected, a section on Frechet mean removed</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 65D18, 65F60, 15A16" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07593v5</id>
    <updated>2022-04-29T07:02:49Z</updated>
    <published>2015-08-30T16:12:59Z</published>
    <title>The Prose Storyboard Language: A Tool for Annotating and Directing
  Movies</title>
    <summary>  The prose storyboard language is a formal language for describing movies shot
by shot, where each shot is described with a unique sentence. The language uses
a simple syntax and limited vocabulary borrowed from working practices in
traditional movie-making, and is intended to be readable both by machines and
humans. The language is designed to serve as a high-level user interface for
intelligent cinematography and editing systems.
</summary>
    <author>
      <name>Remi Ronfard</name>
    </author>
    <author>
      <name>Vineet Gandhi</name>
    </author>
    <author>
      <name>Laurent Boiron</name>
    </author>
    <author>
      <name>Vaishnavi Ameya Murukutla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, extended version includes new figures and references</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07593v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07593v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07932v1</id>
    <updated>2015-11-25T01:41:08Z</updated>
    <published>2015-11-25T01:41:08Z</published>
    <title>Embedding of Hypercube into Cylinder</title>
    <summary>  Task mapping in modern high performance parallel computers can be modeled as
a graph embedding problem, which simulates the mapping as embedding one graph
into another and try to find the minimum wirelength for the mapping. Though
embedding problems have been considered for several regular graphs, such as
hypercubes into grids, binary trees into grids, et al, it is still an open
problem for hypercubes into cylinders. In this paper, we consider the problem
of embedding hypercubes into cylinders to minimize the wirelength. We obtain
the exact wirelength formula of embedding hypercube $Q^r$ into cylinder
$C_{2^3}\times P_{2^{r-3}}$ with $r\ge3$.
</summary>
    <author>
      <name>Weixing Ji</name>
    </author>
    <author>
      <name>Qinghui Liu</name>
    </author>
    <author>
      <name>Guizhen Wang</name>
    </author>
    <author>
      <name>ZhuoJia Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08118v1</id>
    <updated>2015-11-25T17:02:20Z</updated>
    <published>2015-11-25T17:02:20Z</published>
    <title>SlicerPET: A workflow based software module for PET/CT guided needle
  biopsy</title>
    <summary>  Biopsy is commonly used to confirm cancer diagnosis when radiologically
indicated. Given the ability of PET to localize malignancies in heterogeneous
tumors and tumors that do not have a CT correlate, PET/CT guided biopsy may
improve the diagnostic yield of biopsies. To facilitate PET/CT guided needle
biopsy, we developed a workflow that allows us to bring PET image guidance into
the interventional CT suite. In this abstract, we present SlicerPET, a
user-friendly workflow based module developed using open source software
libraries to guide needle biopsy in the interventional suite.
</summary>
    <author>
      <name>Dženan Zukić</name>
    </author>
    <author>
      <name>Julien Finet</name>
    </author>
    <author>
      <name>Emmanuel Wilson</name>
    </author>
    <author>
      <name>Filip Banovac</name>
    </author>
    <author>
      <name>Giuseppe Esposito</name>
    </author>
    <author>
      <name>Kevin Cleary</name>
    </author>
    <author>
      <name>Andinet Enquobahrie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11548-015-1213-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11548-015-1213-2" rel="related"/>
    <link href="http://arxiv.org/abs/1511.08118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01754v1</id>
    <updated>2016-01-08T02:56:57Z</updated>
    <published>2016-01-08T02:56:57Z</published>
    <title>Anti-commutative Dual Complex Numbers and 2D Rigid Transformation</title>
    <summary>  We introduce a new presentation of the two dimensional rigid transformation
which is more concise and efficient than the standard matrix presentation. By
modifying the ordinary dual number construction for the complex numbers, we
define the ring of the anti-commutative dual complex numbers, which
parametrizes two dimensional rotation and translation all together. With this
presentation, one can easily interpolate or blend two or more rigid
transformations at a low computational cost. We developed a library for C++
with the MIT-licensed source code and demonstrate its facility by an
interactive deformation tool developed for iPad.
</summary>
    <author>
      <name>Genki Matsuda</name>
    </author>
    <author>
      <name>Shizuo Kaji</name>
    </author>
    <author>
      <name>Hiroyuki Ochiai</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04816v1</id>
    <updated>2016-01-19T07:48:57Z</updated>
    <published>2016-01-19T07:48:57Z</published>
    <title>Tetrisation of triangular meshes and its application in shape blending</title>
    <summary>  The As-Rigid-As-Possible (ARAP) shape deformation framework is a versatile
technique for morphing, surface modelling, and mesh editing. We discuss an
improvement of the ARAP framework in a few aspects: 1. Given a triangular mesh
in 3D space, we introduce a method to associate a tetrahedral structure, which
encodes the geometry of the original mesh. 2. We use a Lie algebra based method
to interpolate local transformation, which provides better handling of rotation
with large angle. 3. We propose a new error function to compile local
transformations into a global piecewise linear map, which is rotation invariant
and easy to minimise. We implemented a shape blender based on our algorithm and
its MIT licensed source code is available online.
</summary>
    <author>
      <name>Shizuo Kaji</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01224v2</id>
    <updated>2016-09-16T10:41:05Z</updated>
    <published>2016-02-03T08:24:21Z</published>
    <title>Smooth surface interpolation using patches with rational offsets</title>
    <summary>  We present a new method for the interpolation of given data points and
associated normals with surface parametric patches with rational normal fields.
We give some arguments why a dual approach is the most convenient for these
surfaces, which are traditionally called Pythagorean normal vector (PN)
surfaces. Our construction is based on the isotropic model of the dual space to
which the original data are pushed. Then the bicubic Coons patches are
constructed in the isotropic space and then pulled back to the standard three
dimensional space. As a result we obtain the patch construction which is
completely local and produces surfaces with the global G1~continuity.
</summary>
    <author>
      <name>Miroslav Lávička</name>
    </author>
    <author>
      <name>Zbyněk Šír</name>
    </author>
    <author>
      <name>Jan Vršek</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01224v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01224v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06239v1</id>
    <updated>2016-02-04T15:54:02Z</updated>
    <published>2016-02-04T15:54:02Z</published>
    <title>On a recursive construction of circular paths and the search for $π$
  on the integer lattice $\mathbb{Z}^2$</title>
    <summary>  Digital circles not only play an important role in various technological
settings, but also provide a lively playground for more fundamental
number-theoretical questions. In this paper, we present a new recursive
algorithm for the construction of digital circles on the integer lattice
$\mathbb{Z}^2$, which makes sole use of the signum function. By briefly
elaborating on the nature of discretization of circular paths, we then find
that this algorithm recovers, in a space endowed with $\ell^1$-norm, the
defining constant $\pi$ of a circle in $\mathbb{R}^2$.
</summary>
    <author>
      <name>Michelle Rudolph-Lilith</name>
    </author>
    <link href="http://arxiv.org/abs/1602.06239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97N70, 68R10, 52C05, 11H06" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02013v1</id>
    <updated>2016-04-06T05:20:04Z</updated>
    <published>2016-04-06T05:20:04Z</published>
    <title>Keyboard Based Control of Four Dimensional Rotations</title>
    <summary>  Aiming at applications to the scientific visualization of three dimensional
simulations with time evolution, a keyboard based control method to specify
rotations in four dimensions is proposed. It is known that four dimensional
rotations are generally so-called double rotations, and a double rotation is a
combination of simultaneously applied two simple rotations. The proposed method
can specify both the simple and double rotations by single key typings of the
keyboard. The method is tested in visualizations of a regular pentachoron in
four dimensional space by a hyperplane slicing.
</summary>
    <author>
      <name>Akira Kageyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Journal of Visualization</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02483v3</id>
    <updated>2020-03-15T09:59:41Z</updated>
    <published>2016-04-08T20:59:46Z</published>
    <title>On the Hessian of Shape Matching Energy</title>
    <summary>  In this technical report we derive the analytic form of the Hessian matrix
for shape matching energy. Shape matching is a useful technique for meshless
deformation, which can be easily combined with multiple techniques in real-time
dynamics. Nevertheless, it has been rarely applied in scenarios where implicit
integrators are required, and hence strong viscous damping effect, though
popular in simulation systems nowadays, is forbidden for shape matching. The
reason lies in the difficulty to derive the Hessian matrix of the shape
matching energy. Computing the Hessian matrix correctly, and stably, is the key
to more broadly application of shape matching in implicitly-integrated systems.
</summary>
    <author>
      <name>Yun Fei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02483v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02483v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08848v1</id>
    <updated>2016-04-29T14:26:55Z</updated>
    <published>2016-04-29T14:26:55Z</published>
    <title>Augmented Reality Oculus Rift</title>
    <summary>  This paper covers the whole process of developing an Augmented Reality
Stereoscopig Render Engine for the Oculus Rift. To capture the real world in
form of a camera stream, two cameras with fish-eye lenses had to be installed
on the Oculus Rift DK1 hardware. The idea was inspired by Steptoe
\cite{steptoe2014presence}. After the introduction, a theoretical part covers
all the most neccessary elements to achieve an AR System for the Oculus Rift,
following the implementation part where the code from the AR Stereo Engine is
explained in more detail. A short conclusion section shows some results,
reflects some experiences and in the final chapter some future works will be
discussed. The project can be accessed via the git repository
https://github.com/MaXvanHeLL/ARift.git.
</summary>
    <author>
      <name>Markus Höll</name>
    </author>
    <author>
      <name>Nikolaus Heran</name>
    </author>
    <author>
      <name>Vincent Lepetit</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09451v1</id>
    <updated>2016-05-31T00:33:04Z</updated>
    <published>2016-05-31T00:33:04Z</published>
    <title>Quantitative Analysis of Saliency Models</title>
    <summary>  Previous saliency detection research required the reader to evaluate
performance qualitatively, based on renderings of saliency maps on a few
shapes. This qualitative approach meant it was unclear which saliency models
were better, or how well they compared to human perception. This paper provides
a quantitative evaluation framework that addresses this issue. In the first
quantitative analysis of 3D computational saliency models, we evaluate four
computational saliency models and two baseline models against ground-truth
saliency collected in previous work.
</summary>
    <author>
      <name>Flora Ponjou Tasse</name>
    </author>
    <author>
      <name>Jiří Kosinka</name>
    </author>
    <author>
      <name>Neil Anthony Dodgson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04953v1</id>
    <updated>2016-08-17T13:07:27Z</updated>
    <published>2016-08-17T13:07:27Z</published>
    <title>A Perceptual Aesthetics Measure for 3D Shapes</title>
    <summary>  While the problem of image aesthetics has been well explored, the study of 3D
shape aesthetics has focused on specific manually defined features. In this
paper, we learn an aesthetics measure for 3D shapes autonomously from raw voxel
data and without manually-crafted features by leveraging the strength of deep
learning. We collect data from humans on their aesthetics preferences for
various 3D shape classes. We take a deep convolutional 3D shape ranking
approach to compute a measure that gives an aesthetics score for a 3D shape. We
demonstrate our approach with various types of shapes and for applications such
as aesthetics-based visualization, search, and scene composition.
</summary>
    <author>
      <name>Kapil Dev</name>
    </author>
    <author>
      <name>Manfred Lau</name>
    </author>
    <author>
      <name>Ligang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 8 Figures, Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05344v1</id>
    <updated>2016-09-17T14:43:17Z</updated>
    <published>2016-09-17T14:43:17Z</published>
    <title>Optimisations for Real-Time Volumetric Cloudscapes</title>
    <summary>  Volumetric cloudscapes are prohibitively expensive to render in real time
without extensive optimisations. Previous approaches render the clouds to an
offscreen buffer at one quarter resolution and update a fraction of the pixels
per frame, drawing the remaining pixels by temporal reprojection. We present an
alternative approach, reducing the number of raymarching steps and adding a
randomly jittered offset to the raymarch. We use an analytical integration
technique to make results consistent with a lower number of raymarching steps.
To remove noise from the resulting image we apply a temporal anti-aliasing
implementation. The result is a technique producing visually similar results
with 1/16 the number of steps.
</summary>
    <author>
      <name>Alastair Toft</name>
    </author>
    <author>
      <name>Huw Bowles</name>
    </author>
    <author>
      <name>Daniel Zimmermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02769v1</id>
    <updated>2016-10-10T03:57:40Z</updated>
    <published>2016-10-10T03:57:40Z</published>
    <title>Inverse Diffusion Curves using Shape Optimization</title>
    <summary>  The inverse diffusion curve problem focuses on automatic creation of
diffusion curve images that resemble user provided color fields. This problem
is challenging since the 1D curves have a nonlinear and global impact on
resulting color fields via a partial differential equation (PDE). We introduce
a new approach complementary to previous methods by optimizing curve geometry.
In particular, we propose a novel iterative algorithm based on the theory of
shape derivatives. The resulting diffusion curves are clean and well-shaped,
and the final image closely approximates the input. Our method provides a
user-controlled parameter to regularize curve complexity, and generalizes to
handle input color fields represented in a variety of formats.
</summary>
    <author>
      <name>Shuang Zhao</name>
    </author>
    <author>
      <name>Fredo Durand</name>
    </author>
    <author>
      <name>Changxi Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04281v1</id>
    <updated>2016-10-13T22:32:08Z</updated>
    <published>2016-10-13T22:32:08Z</published>
    <title>Augmented Reality with Hololens: Experiential Architectures Embedded in
  the Real World</title>
    <summary>  Early hands-on experiences with the Microsoft Hololens augmented/mixed
reality device are reported and discussed, with a general aim of exploring
basic 3D visualization. A range of usage cases are tested, including data
visualization and immersive data spaces, in-situ visualization of 3D models and
full scale architectural form visualization. Ultimately, the Hololens is found
to provide a remarkable tool for moving from traditional visualization of 3D
objects on a 2D screen, to fully experiential 3D visualizations embedded in the
real world.
</summary>
    <author>
      <name>Paul Hockett</name>
    </author>
    <author>
      <name>Tim Ingleby</name>
    </author>
    <link href="http://arxiv.org/abs/1610.04281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09988v1</id>
    <updated>2016-10-31T15:56:30Z</updated>
    <published>2016-10-31T15:56:30Z</published>
    <title>Selecting the Best Quadrilateral Mesh for Given Planar Shape</title>
    <summary>  The problem of mesh matching is addressed in this work. For a given n-sided
planar region bounded by one loop of n polylines we are selecting optimal
quadrilateral mesh from existing catalogue of meshes. The formulation of
matching between planar shape and quadrilateral mesh from the catalogue is
based on the problem of finding longest common subsequence (LCS). Theoretical
foundation of mesh matching method is provided. Suggested method represents a
viable technique for selecting best mesh for planar region and stepping stone
for further parametrization of the region.
</summary>
    <author>
      <name>Petra Surynkova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05064v1</id>
    <updated>2016-12-15T13:52:30Z</updated>
    <published>2016-12-15T13:52:30Z</published>
    <title>Orthogonal Edge Routing for the EditLens</title>
    <summary>  The EditLens is an interactive lens technique that supports the editing of
graphs. The user can insert, update, or delete nodes and edges while
maintaining an already existing layout of the graph. For the nodes and edges
that are affected by an edit operation, the EditLens suggests suitable
locations and routes, which the user can accept or adjust. For this purpose,
the EditLens requires an efficient routing algorithm that can compute results
at interactive framerates. Existing algorithms cannot fully satisfy the needs
of the EditLens. This paper describes a novel algorithm that can compute
orthogonal edge routes for incremental edit operations of graphs. Tests
indicate that, in general, the algorithm is better than alternative solutions.
</summary>
    <author>
      <name>Stefan Gladisch</name>
    </author>
    <author>
      <name>Valerius Weigandt</name>
    </author>
    <author>
      <name>Heidrun Schumann</name>
    </author>
    <author>
      <name>Christian Tominski</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07353v1</id>
    <updated>2016-11-17T02:11:59Z</updated>
    <published>2016-11-17T02:11:59Z</published>
    <title>Data-driven Shoulder Inverse Kinematics</title>
    <summary>  This paper proposes a shoulder inverse kinematics (IK) technique. Shoulder
complex is comprised of the sternum, clavicle, ribs, scapula, humerus, and four
joints. The shoulder complex shows specific motion pattern, such as Scapulo
humeral rhythm. As a result, if a motion of the shoulder isgenerated without
the knowledge of kinesiology, it will be seen as un-natural. The proposed
technique generates motion of the shoulder complex about the orientation of the
upper arm by interpolating the measurement data. The shoulder IK method allows
novice animators to generate natural shoulder motions easily. As a result, this
technique improves the quality of character animation.
</summary>
    <author>
      <name>YoungBeom Kim</name>
    </author>
    <author>
      <name>Byung-Ha Park</name>
    </author>
    <author>
      <name>Kwang-Mo Jung</name>
    </author>
    <author>
      <name>JungHyun Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 14 figures, IJCGA</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04383v1</id>
    <updated>2017-01-16T18:20:32Z</updated>
    <published>2017-01-16T18:20:32Z</published>
    <title>Automatic Knot Adjustment Using Dolphin Echolocation Algorithm for
  B-Spline Curve Approximation</title>
    <summary>  In this paper, a new approach to solve the cubic B-spline curve fitting
problem is presented based on a meta-heuristic algorithm called " dolphin
echolocation ". The method minimizes the proximity error value of the selected
nodes that measured using the least squares method and the Euclidean distance
method of the new curve generated by the reverse engineering. The results of
the proposed method are compared with the genetic algorithm. As a result, this
new method seems to be successful.
</summary>
    <author>
      <name>Hasan Ali Akyürek</name>
    </author>
    <author>
      <name>Erkan Ülker</name>
    </author>
    <author>
      <name>Barış Koçer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Journal of MacroTrends in Technology and Innovation, Vol 4. Issue
  1. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05754v1</id>
    <updated>2017-01-20T10:58:20Z</updated>
    <published>2017-01-20T10:58:20Z</published>
    <title>User-guided free-form asset modelling</title>
    <summary>  In this paper a new system for piecewise primitive surface recovery on point
clouds is presented, which allows a novice user to sketch areas of interest in
order to guide the fitting process. The algorithm is demonstrated against a
benchmark technique for autonomous surface fitting, and, contrasted against
existing literature in user guided surface recovery, with empirical evidence.
It is concluded that the system is an improvement to the current documented
literature for its visual quality when modelling objects which are composed of
piecewise primitive shapes, and, in its ability to fill large holes on occluded
surfaces using free-form input.
</summary>
    <author>
      <name>Daniel Beale</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01537v1</id>
    <updated>2017-02-06T09:24:02Z</updated>
    <published>2017-02-06T09:24:02Z</published>
    <title>Conceptual and algorithmic development of Pseudo 3D Graphics and Video
  Content Visualization</title>
    <summary>  The article presents a general concept of the organization of pseudo three
dimension visualization of graphics and video content for three dimension
visualization systems. The steps of algorithms for solving the problem of
synthesis of three dimension stereo images based on two dimension images are
introduced. The features of synthesis organization of standard format of three
dimension stereo frame are presented. Moreover, the performed experimental
simulation for generating complete stereoframes and the results of its time
complexity are shown. Keywords:Three dimension visualization, pseudo three
dimension stereo, a stereo pair, three dimension stereo format, algorithm,
modeling, time complexity.
</summary>
    <author>
      <name>Aladdein M. Amro</name>
    </author>
    <author>
      <name>S. A. Zori</name>
    </author>
    <author>
      <name>Anas M. Al-Oraiqat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Publisher for Advanced Scientific Journals 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.01537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02878v1</id>
    <updated>2017-02-09T16:23:36Z</updated>
    <published>2017-02-09T16:23:36Z</published>
    <title>Bezier developable surfaces</title>
    <summary>  In this paper we address the issue of designing developable surfaces with
Bezier patches. We show that developable surfaces with a polynomial edge of
regression are the set of developable surfaces which can be constructed with
Aumann's algorithm. We also obtain the set of polynomial developable surfaces
which can be constructed using general polynomial curves. The conclusions can
be extended to spline surfaces as well.
</summary>
    <author>
      <name>L. Fernández-Jambrina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cagd.2017.02.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cagd.2017.02.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 10 figures, Computer Aided Geometric Design special number
  in memoriam Professor Gerald Farin</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Aided Geometric Design 55, 15-28 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.02878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04038v2</id>
    <updated>2017-11-10T06:39:53Z</updated>
    <published>2017-04-13T09:03:28Z</published>
    <title>Denoising a Point Cloud for Surface Reconstruction</title>
    <summary>  Surface reconstruction from an unorganized point cloud is an important
problem due to its widespread applications. White noise, possibly clustered
outliers, and noisy perturbation may be generated when a point cloud is sampled
from a surface. Most existing methods handle limited amount of noise. We
develop a method to denoise a point cloud so that the users can run their
surface reconstruction codes or perform other analyses afterwards. Our
experiments demonstrate that our method is computationally efficient and it has
significantly better noise handling ability than several existing surface
reconstruction codes.
</summary>
    <author>
      <name>Siu-Wing Cheng</name>
    </author>
    <author>
      <name>Man-Kit Lau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02422v1</id>
    <updated>2017-05-06T00:25:31Z</updated>
    <published>2017-05-06T00:25:31Z</published>
    <title>On Discrete Conformal Seamless Similarity Maps</title>
    <summary>  An algorithm for the computation of global discrete conformal
parametrizations with prescribed global holonomy signatures for triangle meshes
was recently described in [Campen and Zorin 2017]. In this paper we provide a
detailed analysis of convergence and correctness of this algorithm. We
generalize and extend ideas of [Springborn et al. 2008] to show a connection of
the algorithm to Newton's algorithm applied to solving the system of
constraints on angles in the parametric domain, and demonstrate that this
system can be obtained as a gradient of a convex energy.
</summary>
    <author>
      <name>Marcel Campen</name>
    </author>
    <author>
      <name>Denis Zorin</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05508v1</id>
    <updated>2017-05-16T02:58:44Z</updated>
    <published>2017-05-16T02:58:44Z</published>
    <title>Automated Body Structure Extraction from Arbitrary 3D Mesh</title>
    <summary>  This paper presents an automated method for 3D character skeleton extraction
that can be applied for generic 3D shapes. Our work is motivated by the
skeleton-based prior work on automatic rigging focused on skeleton extraction
and can automatically aligns the extracted structure to fit the 3D shape of the
given 3D mesh. The body mesh can be subsequently skinned based on the extracted
skeleton and thus enables rigging process. In the experiment, we apply public
dataset to drive the estimated skeleton from different body shapes, as well as
the real data obtained from 3D scanning systems. Satisfactory results are
obtained compared to the existing approaches.
</summary>
    <author>
      <name>Yong Khoo</name>
    </author>
    <author>
      <name>Sang Chung</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Imaging and Graphics, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.05508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00267v1</id>
    <updated>2017-05-30T20:29:05Z</updated>
    <published>2017-05-30T20:29:05Z</published>
    <title>On the Design and Invariants of a Ruled Surface</title>
    <summary>  This paper deals with a kind of design of a ruled surface. It combines
concepts from the fields of computer aided geometric design and kinematics. A
dual unit spherical B\'ezier-like curve on the dual unit sphere (DUS) is
obtained with respect the control points by a new method. So, with the aid of
Study [1] transference principle, a dual unit spherical B\'ezier-like curve
corresponds to a ruled surface. Furthermore, closed ruled surfaces are
determined via control points and integral invariants of these surfaces are
investigated. The results are illustrated by examples.
</summary>
    <author>
      <name>Ferhat Taş</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0219887819500932</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0219887819500932" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 4 figures, 1 figure list</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="53A17, 53A25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08262v1</id>
    <updated>2017-06-26T07:40:55Z</updated>
    <published>2017-06-26T07:40:55Z</published>
    <title>Degenerations of NURBS curves while all of weights approaching infinity</title>
    <summary>  NURBS curve is widely used in Computer Aided Design and Computer Aided
Geometric Design. When a single weight approaches infinity, the limit of a
NURBS curve tends to the corresponding control point. In this paper, a kind of
control structure of a NURBS curve, called regular control curve, is defined.
We prove that the limit of the NURBS curve is exactly its regular control curve
when all of weights approach infinity, where each weight is multiplied by a
certain one-parameter function tending to infinity, different for each control
point. Moreover, some representative examples are presented to show this
property and indicate its application for shape deformation.
</summary>
    <author>
      <name>Yue Zhang</name>
    </author>
    <author>
      <name>Chun-Gang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 47 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09123v1</id>
    <updated>2017-07-28T07:03:29Z</updated>
    <published>2017-07-28T07:03:29Z</published>
    <title>Research on Shape Mapping of 3D Mesh Models based on Hidden Markov
  Random Field and EM Algorithm</title>
    <summary>  How to establish the matching (or corresponding) between two different 3D
shapes is a classical problem. This paper focused on the research on shape
mapping of 3D mesh models, and proposed a shape mapping algorithm based on
Hidden Markov Random Field and EM algorithm, as introducing a hidden state
random variable associated with the adjacent blocks of shape matching when
establishing HMRF. This algorithm provides a new theory and method to ensure
the consistency of the edge data of adjacent blocks, and the experimental
results show that the algorithm in this paper has a great improvement on the
shape mapping of 3D mesh models.
</summary>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Huai-yu Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09629v1</id>
    <updated>2017-07-30T15:12:52Z</updated>
    <published>2017-07-30T15:12:52Z</published>
    <title>Kernel Projection of Latent Structures Regression for Facial Animation
  Retargeting</title>
    <summary>  Inspired by kernel methods that have been used extensively in achieving
efficient facial animation retargeting, this paper presents a solution to
retargeting facial animation in virtual character's face model based on the
kernel projection of latent structure (KPLS) regression between semantically
similar facial expressions. Specifically, a given number of corresponding
semantically similar facial expressions are projected into the latent space. By
using the Nonlinear Iterative Partial Least Square method, decomposition of the
latent variables is achieved. Finally, the KPLS is achieved by solving a
kernalized version of the eigenvalue problem. By evaluating our methodology
with other kernel-based solutions, the efficiency of the presented methodology
in transferring facial animation to face models with different morphological
variations is demonstrated.
</summary>
    <author>
      <name>Christos Ouzounis</name>
    </author>
    <author>
      <name>Alex Kilias</name>
    </author>
    <author>
      <name>Christos Mousas</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09839v4</id>
    <updated>2023-05-30T19:33:36Z</updated>
    <published>2017-07-13T18:16:06Z</published>
    <title>Superposition de calques monochromes d'opacités variables</title>
    <summary>  For a monochrome layer $x$ of opacity $0\le o_x\le1 $ placed on another
monochrome layer of opacity 1, the result given by the standard formula is
$$\small\Pi\left({\bf
C}_\varphi\right)=1+\sum_{n=1}^2\left(2-n-(-1)^no_{\chi(\varphi+1)}\right)\left(\chi(n+\varphi-1)-o_{\chi(n+\varphi-1)}\right),$$
the formula being of course explained in detail in this paper. We will
eventually deduce a very simple theorem, generalize it and then see its
validity with alternative formulas to this standard containing the same main
properties here exposed.
</summary>
    <author>
      <name>Alexandre Bali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">poor research and unrigorous</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09839v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09839v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04233v1</id>
    <updated>2017-08-13T23:02:34Z</updated>
    <published>2017-08-13T23:02:34Z</published>
    <title>Fast, large-scale hologram calculation in wavelet domain</title>
    <summary>  We propose a large-scale hologram calculation using WAvelet ShrinkAge-Based
superpositIon (WASABI), a wavelet transform-based algorithm. An image-type
hologram calculated using the WASABI method is printed on a glass substrate
with the resolution of $65,536 \times 65,536$ pixels and a pixel pitch of $1
\mu$m. The hologram calculation time amounts to approximately 354 s on a
commercial CPU, which is approximately 30 times faster than conventional
methods.
</summary>
    <author>
      <name>Tomoyoshi Shimobaba</name>
    </author>
    <author>
      <name>Kyoji Matsushima</name>
    </author>
    <author>
      <name>Takayuki Takahashi</name>
    </author>
    <author>
      <name>Yuki Nagahama</name>
    </author>
    <author>
      <name>Satoki Hasegawa</name>
    </author>
    <author>
      <name>Marie Sano</name>
    </author>
    <author>
      <name>Ryuji Hirayama</name>
    </author>
    <author>
      <name>Takashi Kakue</name>
    </author>
    <author>
      <name>Tomoyoshi Ito</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.optcom.2017.11.066</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.optcom.2017.11.066" rel="related"/>
    <link href="http://arxiv.org/abs/1708.04233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06684v1</id>
    <updated>2017-07-28T10:52:52Z</updated>
    <published>2017-07-28T10:52:52Z</published>
    <title>Fractions, Projective Representation, Duality, Linear Algebra and
  Geometry</title>
    <summary>  This contribution describes relationship between fractions, projective
representation, duality, linear algebra and geometry. Many problems lead to a
system of linear equations. This paper presents equivalence of the
Cross-product operation and solution of a system of linear equations Ax=0 or
Ax=b using projective space representation and homogeneous coordinates. It
leads to conclusion that division operation is not required for a solution of a
system of linear equations, if the projective representation and homogeneous
coordinates are used. An efficient solution on CPU and GPU based architectures
is presented with an application to barycentric coordinates computation as
well.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CZECH-SLOVAK CONFERENCE ON GEOMETRY AND GRAPHICS 2016, ISBN
  978-80-7464-874-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.06684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04752v1</id>
    <updated>2017-09-10T11:24:08Z</updated>
    <published>2017-09-10T11:24:08Z</published>
    <title>The wave method of building color palette and its application in
  computer graphics</title>
    <summary>  This article describes a method of getting a harmonious combination of
colors, developed by us on the basis of the relationship of color and acoustic
waves. Presents a parallel between harmoniously matched colors and the concept
of harmony in music theory (consonance). Describes the physical assumption of
the essence of the phenomenon of harmony (consonance). The article also
provides algorithm of implementation wave method for the sRGB color model.
</summary>
    <author>
      <name>I. I. Sabo</name>
    </author>
    <author>
      <name>H. R. Lagoda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07421v1</id>
    <updated>2017-10-20T06:16:44Z</updated>
    <published>2017-10-20T06:16:44Z</published>
    <title>Quasi-random Agents for Image Transition and Animation</title>
    <summary>  Quasi-random walks show similar features as standard random walks, but with
much less randomness. We utilize this established model from discrete
mathematics and show how agents carrying out quasi-random walks can be used for
image transition and animation. The key idea is to generalize the notion of
quasi-random walks and let a set of autonomous agents perform quasi-random
walks painting an image. Each agent has one particular target image that they
paint when following a sequence of directions for their quasi-random walk. The
sequence can easily be chosen by an artist and allows them to produce a wide
range of different transition patterns and animations.
</summary>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <author>
      <name>Tobias Friedrich</name>
    </author>
    <link href="http://arxiv.org/abs/1710.07421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00432v1</id>
    <updated>2018-01-01T11:51:37Z</updated>
    <published>2018-01-01T11:51:37Z</published>
    <title>A Comparative Study of LOWESS and RBF Approximations for Visualization</title>
    <summary>  Approximation methods are widely used in many fields and many techniques have
been published already. This comparative study presents a comparison of LOWESS
(Locally weighted scatterplot smoothing) and RBF (Radial Basis Functions)
approximation methods on noisy data as they use different approaches. The RBF
approach is generally convenient for high dimensional scattered data sets. The
LOWESS method needs finding a subset of nearest points if data are scattered.
The experiments proved that LOWESS approximation gives slightly better results
than RBF in the case of lower dimension, while in the higher dimensional case
</summary>
    <author>
      <name>Michal Smolik</name>
    </author>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <author>
      <name>Ondrej Nedved</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-42108-7_31</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-42108-7_31" rel="related"/>
    <link href="http://arxiv.org/abs/1801.00432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00441v1</id>
    <updated>2018-01-01T13:13:31Z</updated>
    <published>2018-01-01T13:13:31Z</published>
    <title>A Fast Algorithm for Line Clipping by Convex Polyhedron in E3</title>
    <summary>  A new algorithm for line clipping against convex polyhedron is given. The
suggested algorithm is faster for higher number of facets of the given
polyhedron than the traditional Cyrus-Beck's and others algorithms with
complexity O(N) . The suggested algorithm has O(N) complexity in the worst N
case and expected O(sqrt(N))) complexity. The speed up is achieved because of
'known order' of triangles. Some principal results of comparisons of selected
algorithms are presented and give some imagination how the proposed algorithm
could be used effectively.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01922v2</id>
    <updated>2018-09-05T21:46:14Z</updated>
    <published>2018-01-05T21:26:48Z</published>
    <title>Vectorization of Line Drawings via PolyVector Fields</title>
    <summary>  Image tracing is a foundational component of the workflow in graphic design,
engineering, and computer animation, linking hand-drawn concept images to
collections of smooth curves needed for geometry processing and editing. Even
for clean line drawings, modern algorithms often fail to faithfully vectorize
junctions, or points at which curves meet; this produces vector drawings with
incorrect connectivity. This subtle issue undermines the practical application
of vectorization tools and accounts for hesitance among artists and engineers
to use automatic vectorization software. To address this issue, we propose a
novel image vectorization method based on state-of-the-art mathematical
algorithms for frame field processing. Our algorithm is tailored specifically
to disambiguate junctions without sacrificing quality.
</summary>
    <author>
      <name>Mikhail Bessmeltsev</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <link href="http://arxiv.org/abs/1801.01922v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01922v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03168v1</id>
    <updated>2018-02-09T08:32:29Z</updated>
    <published>2018-02-09T08:32:29Z</published>
    <title>Hierarchical Cloth Simulation using Deep Neural Networks</title>
    <summary>  Fast and reliable physically-based simulation techniques are essential for
providing flexible visual effects for computer graphics content. In this paper,
we propose a fast and reliable hierarchical cloth simulation method, which
combines conventional physically-based simulation with deep neural networks
(DNN). Simulations of the coarsest level of the hierarchical model are
calculated using conventional physically-based simulations, and more detailed
levels are generated by inference using DNN models. We demonstrate that our
method generates reliable and fast cloth simulation results through experiments
under various conditions.
</summary>
    <author>
      <name>Young Jin Oh</name>
    </author>
    <author>
      <name>Tae Min Lee</name>
    </author>
    <author>
      <name>In-Kwon Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07592v1</id>
    <updated>2018-02-07T17:14:01Z</updated>
    <published>2018-02-07T17:14:01Z</published>
    <title>"How to squash a mathematical tomato", Rubic's cube-like surfaces and
  their connection to reversible computation</title>
    <summary>  Here we show how reversible computation processes, like Margolus diffusion,
can be envisioned as physical turning operations on a 2-dimensional rigid
surface that is cut by a regular pattern of intersecting circles. We then
briefly explore the design-space of these patterns, and report on the discovery
of an interesting fractal subdivision of space by iterative circle packings. We
devise two different ways for creating this fractal, both showing interesting
properties, some resembling properties of the dragon curve. The patterns
presented here can have interesting applications to the engineering of modular,
kinetic, active surfaces.
</summary>
    <author>
      <name>Ioannis Tamvakis</name>
    </author>
    <link href="http://arxiv.org/abs/1802.07592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A08, 00A79, 06F30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03979v1</id>
    <updated>2018-04-11T13:42:55Z</updated>
    <published>2018-04-11T13:42:55Z</published>
    <title>Experimental similarity assessment for a collection of fragmented
  artifacts</title>
    <summary>  In the Visual Heritage domain, search engines are expected to support
archaeologists and curators to address cross-correlation and searching across
multiple collections. Archaeological excavations return artifacts that often
are damaged with parts that are fragmented in more pieces or totally missing.
The notion of similarity among fragments cannot simply base on the geometric
shape but style, material, color, decorations, etc. are all important factors
that concur to this concept. In this work, we discuss to which extent the
existing techniques for 3D similarity matching are able to approach fragment
similarity, what is missing and what is necessary to be further developed.
</summary>
    <author>
      <name>Silvia Biasotti</name>
    </author>
    <author>
      <name>Elia Moscoso Thompson</name>
    </author>
    <author>
      <name>Michela Spagnuolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics Workshop on 3D Object Retrieval 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45, 68P20, 68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09293v1</id>
    <updated>2018-04-24T23:31:55Z</updated>
    <published>2018-04-24T23:31:55Z</published>
    <title>Taichi: An Open-Source Computer Graphics Library</title>
    <summary>  An ideal software system in computer graphics should be a combination of
innovative ideas, solid software engineering and rapid development. However, in
reality these requirements are seldom met simultaneously. In this paper, we
present early results on an open-source library named Taichi
(http://taichi.graphics), which alleviates this practical issue by providing an
accessible, portable, extensible, and high-performance infrastructure that is
reusable and tailored for computer graphics. As a case study, we share our
experience in building a novel physical simulation system using Taichi.
</summary>
    <author>
      <name>Yuanming Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.09293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01005v2</id>
    <updated>2018-09-06T07:29:24Z</updated>
    <published>2018-06-04T08:28:26Z</published>
    <title>Path Throughput Importance Weights</title>
    <summary>  Many Monte Carlo light transport simulations use multiple importance sampling
(MIS) to weight between different path sampling strategies. We propose to use
the path throughput to compute the MIS weights instead of the commonly used
probability density per area measure. This new formulation is equivalent to the
previous approach and results in the same weights as well as implementation.
However, it is more intuitive and can help in understanding the effects of
modifications to the weight function. We show some examples of required
modifications which are often neglected in implementations. Also, our new
perspective might help to derive MIS strategies for new samplers in the future.
</summary>
    <author>
      <name>Johannes Jendersie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11K45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.8; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09058v2</id>
    <updated>2018-09-12T06:01:38Z</updated>
    <published>2018-06-24T00:31:35Z</published>
    <title>Golden interpolation</title>
    <summary>  For the classic aesthetic interpolation problem, we propose an entirely new
thought: apply the golden section. For how to apply the golden section to
interpolation methods, we present three examples: the golden step
interpolation, the golden piecewise linear interpolation and the golden curve
interpolation, which respectively deal with the applications of golden section
in the interpolation of degree 0, 1, and 2 in the plane. In each example, we
present our basic ideas, the specific methods, comparative examples and
applications, and relevant criteria. And it is worth mentioning that for
aesthetics, we propose two novel concepts: the golden cuspidal hill and the
golden domed hill. This paper aims to provide the reference for the combination
of golden section and interpolation, and stimulate more and better related
researches.
</summary>
    <author>
      <name>Ying He</name>
    </author>
    <author>
      <name>Jincai Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09070v1</id>
    <updated>2018-06-24T02:33:00Z</updated>
    <published>2018-06-24T02:33:00Z</published>
    <title>Generative Models for Pose Transfer</title>
    <summary>  We investigate nearest neighbor and generative models for transferring pose
between persons. We take in a video of one person performing a sequence of
actions and attempt to generate a video of another person performing the same
actions. Our generative model (pix2pix) outperforms k-NN at both generating
corresponding frames and generalizing outside the demonstrated action set. Our
most salient contribution is determining a pipeline (pose detection, face
detection, k-NN based pairing) that is effective at perform-ing the desired
task. We also detail several iterative improvements and failure modes.
</summary>
    <author>
      <name>Patrick Chao</name>
    </author>
    <author>
      <name>Alexander Li</name>
    </author>
    <author>
      <name>Gokul Swamy</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02921v1</id>
    <updated>2018-07-09T02:52:01Z</updated>
    <published>2018-07-09T02:52:01Z</published>
    <title>Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis</title>
    <summary>  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
</summary>
    <author>
      <name>Paul Rosen</name>
    </author>
    <author>
      <name>Mustafa Hajij</name>
    </author>
    <author>
      <name>Junyi Tu</name>
    </author>
    <author>
      <name>Tanvirul Arafin</name>
    </author>
    <author>
      <name>Les Piegl</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08486v1</id>
    <updated>2018-07-23T08:59:04Z</updated>
    <published>2018-07-23T08:59:04Z</published>
    <title>Conformal Mesh Parameterization Using Discrete Calabi Flow</title>
    <summary>  In this paper, we introduce discrete Calabi flow to the graphics research
community and present a novel conformal mesh parameterization algorithm. Calabi
energy has a succinct and explicit format. Its corresponding flow is conformal
and convergent under certain conditions. Our method is based on the Calabi
energy and Calabi flow with solid theoretical and mathematical base. We
demonstrate our approach on dozens of models and compare it with other related
flow based methods, such as the well-known Ricci flow and CETM. Our experiments
show that the performance of our algorithm is comparably the same with other
methods. The discrete Calabi flow in our method provides another perspective on
conformal flow and conformal parameterization.
</summary>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Xuan Li</name>
    </author>
    <author>
      <name>Huabin Ge</name>
    </author>
    <author>
      <name>Xianfeng Gu</name>
    </author>
    <author>
      <name>Na Lei</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00107v1</id>
    <updated>2018-09-28T22:24:07Z</updated>
    <published>2018-09-28T22:24:07Z</published>
    <title>Superimposition-guided Facial Reconstruction from Skull</title>
    <summary>  We develop a new algorithm to perform facial reconstruction from a given
skull. This technique has forensic application in helping the identification of
skeletal remains when other information is unavailable. Unlike most existing
strategies that directly reconstruct the face from the skull, we utilize a
database of portrait photos to create many face candidates, then perform a
superimposition to get a well matched face, and then revise it according to the
superimposition. To support this pipeline, we build an effective autoencoder
for image-based facial reconstruction, and a generative model for constrained
face inpainting. Our experiments have demonstrated that the proposed pipeline
is stable and accurate.
</summary>
    <author>
      <name>Celong Liu</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.00107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00706v3</id>
    <updated>2018-10-28T18:20:25Z</updated>
    <published>2018-10-01T13:57:05Z</published>
    <title>Designing Volumetric Truss Structures</title>
    <summary>  We present the first algorithm for designing volumetric Michell Trusses. Our
method uses a parametrization approach to generate trusses made of structural
elements aligned with the primary direction of an object's stress field. Such
trusses exhibit high strength-to-weight ratios. We demonstrate the structural
robustness of our designs via a posteriori physical simulation. We believe our
algorithm serves as an important complement to existing structural optimization
tools and as a novel standalone design tool itself.
</summary>
    <author>
      <name>Rahul Arora</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>Timothy R. Langlois</name>
    </author>
    <author>
      <name>Yijiang Huang</name>
    </author>
    <author>
      <name>Caitlin Mueller</name>
    </author>
    <author>
      <name>Wojciech Matusik</name>
    </author>
    <author>
      <name>Ariel Shamir</name>
    </author>
    <author>
      <name>Karan Singh</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3328939.3328999</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3328939.3328999" rel="related"/>
    <link href="http://arxiv.org/abs/1810.00706v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00706v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06884v2</id>
    <updated>2019-12-03T10:07:02Z</updated>
    <published>2018-10-16T08:57:41Z</published>
    <title>Subdivision Directional Fields</title>
    <summary>  We present a novel linear subdivision scheme for face-based tangent
directional fields on triangle meshes. Our subdivision scheme is based on a
novel coordinate-free representation of directional fields as halfedge-based
scalar quantities, bridging the finite-element representation with discrete
exterior calculus. By commuting with differential operators, our subdivision is
structure-preserving: it reproduces curl-free fields precisely, and reproduces
divergence-free fields in the weak sense. Moreover, our subdivision scheme
directly extends to directional fields with several vectors per face by working
on the branched covering space. Finally, we demonstrate how our scheme can be
applied to directional-field design, advection, and robust earth mover's
distance computation, for efficient and robust computation.
</summary>
    <author>
      <name>Bram Custers</name>
    </author>
    <author>
      <name>Amir Vaxman</name>
    </author>
    <link href="http://arxiv.org/abs/1810.06884v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06884v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="graphic" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03510v1</id>
    <updated>2018-11-08T15:58:40Z</updated>
    <published>2018-11-08T15:58:40Z</published>
    <title>Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision
  Surfaces</title>
    <summary>  We present a fast and efficient method for intersecting rays with
Catmull-Clark subdivision surfaces. It takes advantage of the approximation
democratized by OpenSubdiv, in which regular patches are represented by tensor
product B\'ezier surfaces and irregular ones are approximated using Gregory
patches. Our algorithm operates solely on the original patch data and can
process both patch types simultaneously with only a small amount of control
flow divergence. Besides introducing an optimized method to determine axis
aligned bounding boxes of Gregory patches restricted in the parametric domain,
several techniques are introduced that accelerate the recursive subdivision
process including stackless operation, efficient work distribution, and control
flow optimizations. The algorithm is especially useful for quick turnarounds
during patch editing and animation playback.
</summary>
    <author>
      <name>Nikolaus Binder</name>
    </author>
    <author>
      <name>Alexander Keller</name>
    </author>
    <link href="http://arxiv.org/abs/1811.03510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05674v2</id>
    <updated>2019-03-25T01:31:39Z</updated>
    <published>2018-11-14T08:02:10Z</published>
    <title>Total Positivity of A Kind of Generalized Toric-Bernstein Basis</title>
    <summary>  The normalized totally positive bases are widely used in many fields.Based on
the generalized Vandermonde determinant, the normalized total positivity of a
kind of generalized toric-Bernstein basis is proved, which is defined on a set
of real points. By this result, the progressive iterative approximation
property of the generalized toric-B\'{e}zier curve is obtained.
</summary>
    <author>
      <name>Ying-Ying Yu</name>
    </author>
    <author>
      <name>Hui Ma</name>
    </author>
    <author>
      <name>Chun-Gang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.05674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 15B48, 41A15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06600v1</id>
    <updated>2018-11-15T21:48:46Z</updated>
    <published>2018-11-15T21:48:46Z</published>
    <title>Iso-parametric tool path planning for point clouds</title>
    <summary>  The computational consuming and non-robust reconstruction from point clouds
to either meshes or spline surfaces motivates the direct tool path planning for
point clouds. In this paper, a novel approach for planning iso-parametric tool
path from a point cloud is presented. The planning depends on the
parameterization of point clouds. Accordingly, a conformal map is employed to
build the parameterization which leads to a significant simplification of
computing tool path parameters and boundary conformed paths. Then, Tool path is
generated through linear interpolation with the forward and side step computed
against specified chord deviation and scallop height, respectively.
Experimental results are given to illustrate effectiveness of the proposed
methods.
</summary>
    <author>
      <name>Qiang Zou</name>
    </author>
    <author>
      <name>Jibin Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2013.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2013.07.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer-Aided Design 45(11) 2013 1459-1468</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.06600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12464v2</id>
    <updated>2018-12-03T14:08:15Z</updated>
    <published>2018-11-29T20:12:26Z</published>
    <title>Increasing the Capability of Neural Networks for Surface Reconstruction
  from Noisy Point Clouds</title>
    <summary>  This paper builds upon the current methods to increase their capability and
automation for 3D surface construction from noisy and potentially sparse point
clouds. It presents an analysis of an artificial neural network surface
regression and mapping method, describing caveats, improvements and
justification for the different approach.
</summary>
    <author>
      <name>Adam R White</name>
    </author>
    <author>
      <name>Li Bai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, quantitative and qualitative results of comparisons between
  ANN regression methods incorporating Isomap vs LLE, bSpline, multi-depth
  point cloud sampling</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.12464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.02629v1</id>
    <updated>2019-01-09T08:09:01Z</updated>
    <published>2019-01-09T08:09:01Z</published>
    <title>Collaborative 3D modeling system based on blockchain</title>
    <summary>  We propose a collaborative 3D modeling system, which is based on the
blockchain technology. Our approach uses the blockchain to communicate with
modeling tools and to provide them a decentralized database of the mesh
modification history. This approach also provides a server-less version control
system: users can commit their modifications to the blockchain and checkout
others' modifications from the blockchain. As a result, our system enables
users to do collaborative modeling without any central server.
</summary>
    <author>
      <name>Hunmin Park</name>
    </author>
    <author>
      <name>Sung-Eui Yoon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.02629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.02629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.06487v2</id>
    <updated>2019-04-10T10:27:13Z</updated>
    <published>2019-01-19T09:21:57Z</published>
    <title>Automatic normal orientation in point clouds of building interiors</title>
    <summary>  Orienting surface normals correctly and consistently is a fundamental problem
in geometry processing. Applications such as visualization, feature detection,
and geometry reconstruction often rely on the availability of correctly
oriented normals. Many existing approaches for automatic orientation of normals
on meshes or point clouds make severe assumptions on the input data or the
topology of the underlying object which are not applicable to real-world
measurements of urban scenes. In contrast, our approach is specifically
tailored to the challenging case of unstructured indoor point cloud scans of
multi-story, multi-room buildings. We evaluate the correctness and speed of our
approach on multiple real-world point cloud datasets.
</summary>
    <author>
      <name>Sebastian Ochmann</name>
    </author>
    <author>
      <name>Reinhard Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1901.06487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01192v1</id>
    <updated>2019-01-31T12:55:30Z</updated>
    <published>2019-01-31T12:55:30Z</published>
    <title>Advances in the Treatment of Trimmed CAD Models due to Isogeometric
  Analysis</title>
    <summary>  Trimming is a core technique in geometric modeling. Unfortunately, the
resulting objects do not take the requirements of numerical simulations into
account and yield various problems. This paper outlines principal issues of
trimmed models and highlights different analysis-suitable strategies to address
them. It is discussed that these concepts not only provide important
computational tools for isogeometric analysis, but can also improve the
treatment of trimmed models in a design context.
</summary>
    <author>
      <name>Benjamin Marussig</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th World Congress on Computational Mechanics (WCCM XIII) and 2nd
  Pan American Congress on Computational Mechanics (PANACM II), July 22-27,
  2018, New York City, NY, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.01192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.03406v1</id>
    <updated>2019-03-07T04:07:04Z</updated>
    <published>2019-03-07T04:07:04Z</published>
    <title>Computing Three-dimensional Constrained Delaunay Refinement Using the
  GPU</title>
    <summary>  We propose the first GPU algorithm for the 3D triangulation refinement
problem. For an input of a piecewise linear complex $\mathcal{G}$ and a
constant $B$, it produces, by adding Steiner points, a constrained Delaunay
triangulation conforming to $\mathcal{G}$ and containing tetrahedra mostly of
radius-edge ratios smaller than $B$. Our implementation of the algorithm shows
that it can be an order of magnitude faster than the best CPU algorithm while
using a similar amount of Steiner points to produce triangulations of
comparable quality.
</summary>
    <author>
      <name>Zhenghai Chen</name>
    </author>
    <author>
      <name>Tiow-Seng Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.12270v1</id>
    <updated>2019-03-28T21:09:57Z</updated>
    <published>2019-03-28T21:09:57Z</published>
    <title>Implementing Noise with Hash functions for Graphics Processing Units</title>
    <summary>  We propose a modification to Perlin noise which use computable hash functions
instead of textures as lookup tables. We implemented the FNV1, Jenkins and
Murmur hashes on Shader Model 4.0 Graphics Processing Units for noise
generation. Modified versions of the FNV1 and Jenkins hashes provide very close
performance compared to a texture based Perlin noise implementation. Our noise
modification enables noise function evaluation without any texture fetches,
trading computational power for memory bandwidth.
</summary>
    <author>
      <name>Matias Valdenegro-Toro</name>
    </author>
    <author>
      <name>Hector Pincheira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings XXVIII International Conference of the Chilean Computing
  Science Society (SCCC, 2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.12270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.04603v1</id>
    <updated>2019-04-09T11:34:57Z</updated>
    <published>2019-04-09T11:34:57Z</published>
    <title>Developable surface patches bounded by NURBS curves</title>
    <summary>  In this paper we construct developable surface patches which are bounded by
two rational or NURBS curves, though the resulting patch is not a rational or
NURBS surface in general. This is accomplished by reparameterizing one of the
boundary curves. The reparameterization function is the solution of an
algebraic equation. For the relevant case of cubic or cubic spline curves, this
equation is quartic at most, quadratic if the curves are Bezier or splines and
lie on parallel planes, and hence it may be solved either by standard
analytical or numerical methods.
</summary>
    <author>
      <name>Leonardo Fernandez-Jambrina</name>
    </author>
    <author>
      <name>Francisco Perez-Arribas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4208/jcm.1904-m2018-0209</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4208/jcm.1904-m2018-0209" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 17 figures. Accepted for publication in Journal of
  Computational Mathematics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational Mathematics 38, 693-709 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.04603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00523v1</id>
    <updated>2019-07-01T03:07:08Z</updated>
    <published>2019-07-01T03:07:08Z</published>
    <title>Geodesic Centroidal Voronoi Tessellations: Theories, Algorithms and
  Applications</title>
    <summary>  Nowadays, big data of digital media (including images, videos and 3D
graphical models) are frequently modeled as low-dimensional manifold meshes
embedded in a high-dimensional feature space. In this paper, we summarized our
recent work on geodesic centroidal Voronoi tessellations(GCVTs), which are
intrinsic geometric structures on manifold meshes. We show that GCVT can find a
widely range of interesting applications in computer vision and graphics, due
to the efficiency of search, location and indexing inherent in these intrinsic
geometric structures. Then we present the challenging issues of how to build
the combinatorial structures of GCVTs and establish their time and space
complexities, including both theoretical and algorithmic results.
</summary>
    <author>
      <name>Zipeng Ye</name>
    </author>
    <author>
      <name>Ran Yi</name>
    </author>
    <author>
      <name>Minjing Yu</name>
    </author>
    <author>
      <name>Yong-Jin Liu</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <link href="http://arxiv.org/abs/1907.00523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03953v1</id>
    <updated>2019-07-09T03:13:58Z</updated>
    <published>2019-07-09T03:13:58Z</published>
    <title>Efficient Cloth Simulation using Miniature Cloth and Upscaling Deep
  Neural Networks</title>
    <summary>  Cloth simulation requires a fast and stable method for interactively and
realistically visualizing fabric materials using computer graphics. We propose
an efficient cloth simulation method using miniature cloth simulation and
upscaling Deep Neural Networks (DNN). The upscaling DNNs generate the target
cloth simulation from the results of physically-based simulations of a
miniature cloth that has similar physical properties to those of the target
cloth. We have verified the utility of the proposed method through experiments,
and the results demonstrate that it is possible to generate fast and stable
cloth simulations under various conditions.
</summary>
    <author>
      <name>Tae Min Lee</name>
    </author>
    <author>
      <name>Young Jin Oh</name>
    </author>
    <author>
      <name>In-Kwon Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 15 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.03953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07198v3</id>
    <updated>2019-11-01T15:06:21Z</updated>
    <published>2019-07-16T18:01:44Z</published>
    <title>RayTracer.jl: A Differentiable Renderer that supports Parameter
  Optimization for Scene Reconstruction</title>
    <summary>  In this paper, we present RayTracer.jl, a renderer in Julia that is fully
differentiable using source-to-source Automatic Differentiation (AD). This
means that RayTracer not only renders 2D images from 3D scene parameters, but
it can be used to optimize for model parameters that generate a target image in
a Differentiable Programming (DP) pipeline. We interface our renderer with the
deep learning library Flux for use in combination with neural networks. We
demonstrate the use of this differentiable renderer in rendering tasks and in
solving inverse graphics problems.
</summary>
    <author>
      <name>Avik Pal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1442780</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1442780" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the JuliaCon Conferences 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JuliaCon Proceedings, 1 (2020), 37</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.07198v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07198v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10208v1</id>
    <updated>2019-07-24T02:24:57Z</updated>
    <published>2019-07-24T02:24:57Z</published>
    <title>Spectral Visualization Sharpening</title>
    <summary>  In this paper, we propose a perceptually-guided visualization sharpening
technique. We analyze the spectral behavior of an established comprehensive
perceptual model to arrive at our approximated model based on an adapted
weighting of the bandpass images from a Gaussian pyramid. The main benefit of
this approximated model is its controllability and predictability for
sharpening color-mapped visualizations. Our method can be integrated into any
visualization tool as it adopts generic image-based post-processing, and it is
intuitive and easy to use as viewing distance is the only parameter. Using
highly diverse datasets, we show the usefulness of our method across a wide
range of typical visualizations.
</summary>
    <author>
      <name>Liang Zhou</name>
    </author>
    <author>
      <name>Rudolf Netzel</name>
    </author>
    <author>
      <name>Daniel Weiskopf</name>
    </author>
    <author>
      <name>Chris Johnson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3343036.3343133</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3343036.3343133" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Symposium of Applied Perception'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.10208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12343v2</id>
    <updated>2019-07-30T07:22:01Z</updated>
    <published>2019-07-29T11:38:05Z</published>
    <title>Blue-Noise Dithered QMC Hierarchical Russian Roulette</title>
    <summary>  In order to efficiently sample specular-diffuse-glossy and
glossy-diffuse-glossy transport phenomena, Tokuyoshi and Harada introduced
hierarchical Russian roulette, a smart algorithm that allows to compute the
minimum of the random numbers associated to leaves of a tree at each internal
node. The algorithm is used to efficiently cull the connections between the
product set of eye and light vertices belonging to large caches of eye and
light subpaths produced through bidirectional path tracing. The original
version of the algorithm is entirely based on the generation of semi-stratified
pseudo-random numbers. Our paper proposes a novel variant based on
deterministic blue-noise dithered Quasi Monte Carlo samples.
</summary>
    <author>
      <name>Jacopo Pantaleoni</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12343v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12343v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12845v1</id>
    <updated>2019-07-30T11:53:49Z</updated>
    <published>2019-07-30T11:53:49Z</published>
    <title>Overlap-free Drawing of Generalized Pythagoras Trees for Hierarchy
  Visualization</title>
    <summary>  Generalized Pythagoras trees were developed for visualizing hierarchical
data, producing organic, fractal-like representations. However, the drawback of
the original layout algorithm is visual overlap of tree branches. To avoid such
overlap, we introduce an adapted drawing algorithm using ellipses instead of
circles to recursively place tree nodes representing the subhierarchies. Our
technique is demonstrated by resolving overlap in diverse real-world and
generated datasets, while comparing the results to the original approach.
</summary>
    <author>
      <name>Tanja Munz</name>
    </author>
    <author>
      <name>Michael Burch</name>
    </author>
    <author>
      <name>Toon van Benthem</name>
    </author>
    <author>
      <name>Yoeri Poels</name>
    </author>
    <author>
      <name>Fabian Beck</name>
    </author>
    <author>
      <name>Daniel Weiskopf</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01809v1</id>
    <updated>2019-08-05T19:20:20Z</updated>
    <published>2019-08-05T19:20:20Z</published>
    <title>Geometric Sample Reweighting for Monte Carlo Integration</title>
    <summary>  We present a general sample reweighting scheme and its underlying theory for
the integration of an unknown function with low dimensionality. Our method
produces better results than standard weighting schemes for common sampling
strategies, while avoiding bias. Our main insight is to link the weight
derivation to the function reconstruction process during integration. The
implementation of our solution is simple and results in an improved convergence
behavior. We illustrate its benefit by applying our method to multiple Monte
Carlo rendering problems.
</summary>
    <author>
      <name>Jerry Jinfeng Guo</name>
    </author>
    <author>
      <name>Elmar Eisemann</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01906v1</id>
    <updated>2019-08-05T23:56:59Z</updated>
    <published>2019-08-05T23:56:59Z</published>
    <title>Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes
  Using Hardware Accelerated Ray Tracing</title>
    <summary>  Sample based ray marching is an effective method for direct volume rendering
of unstructured meshes. However, sampling such meshes remains expensive, and
strategies to reduce the number of samples taken have received relatively
little attention. In this paper, we introduce a method for rendering
unstructured meshes using a combination of a coarse spatial acceleration
structure and hardware-accelerated ray tracing. Our approach enables efficient
empty space skipping and adaptive sampling of unstructured meshes, and
outperforms a reference ray marcher by up to 7x.
</summary>
    <author>
      <name>Nathan Morrical</name>
    </author>
    <author>
      <name>Will Usher</name>
    </author>
    <author>
      <name>Ingo Wald</name>
    </author>
    <author>
      <name>Valerio Pascucci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, 1 supplemental page, IEEE VIS 2019 Conference
  Short Paper, Author Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.01906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04694v4</id>
    <updated>2019-11-12T06:24:11Z</updated>
    <published>2019-08-10T06:58:55Z</published>
    <title>Channel Decomposition into Painting Actions</title>
    <summary>  This work presents a method to decompose a convolutional layer of the deep
neural network into painting actions. To behave like the human painter, these
actions are driven by the cost simulating the hand movement, the paint color
change, the stroke shape and the stroking style. To help planning, the Mask
R-CNN is applied to detect the object areas and decide the painting order. The
proposed painting system introduces a variety of extensions in artistic styles,
based on the chosen parameters. Further experiments are performed to evaluate
the channel penetration and the channel sensitivity on the strokes.
</summary>
    <author>
      <name>Shih-Chieh Su</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04694v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04694v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06974v2</id>
    <updated>2019-09-26T12:58:29Z</updated>
    <published>2019-08-19T11:26:38Z</published>
    <title>Adding quadric fillets to quador lattice structures</title>
    <summary>  Gupta et al. [1, 2] describe a very beautiful application of algebraic
geometry to lattice structures composed of quadric of revolution (quador)
implicit surfaces. However, the shapes created have concave edges where the
stubs meet, and such edges can be stress-raisers which can cause significant
problems with, for instance, fatigue under cyclic loading. This note describes
a way in which quadric fillets can be added to these models, thus relieving
this problem while retaining their computational simplicity and efficiency.
</summary>
    <author>
      <name>Fehmi Cirak</name>
    </author>
    <author>
      <name>Malcolm Sabin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2019.102754</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2019.102754" rel="related"/>
    <link href="http://arxiv.org/abs/1908.06974v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06974v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02926v3</id>
    <updated>2020-06-27T16:05:43Z</updated>
    <published>2019-10-07T17:32:05Z</published>
    <title>Cubic Stylization</title>
    <summary>  We present a 3D stylization algorithm that can turn an input shape into the
style of a cube while maintaining the content of the original shape. The key
insight is that cubic style sculptures can be captured by the
as-rigid-as-possible energy with an l1-regularization on rotated surface
normals. Minimizing this energy naturally leads to a detail-preserving, cubic
geometry. Our optimization can be solved efficiently without any mesh surgery.
Our method serves as a non-realistic modeling tool where one can incorporate
many artistic controls to create stylized geometries.
</summary>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3355089.3356495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3355089.3356495" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 28 figures, SIGGRAPH Asia 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 38, 6, Article 197 (November 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.02926v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02926v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04386v2</id>
    <updated>2019-10-13T15:40:26Z</updated>
    <published>2019-10-10T06:33:28Z</published>
    <title>Dialog on a canvas with a machine</title>
    <summary>  We propose a new form of human-machine interaction. It is a pictorial game
consisting of interactive rounds of creation between artists and a machine.
They repetitively paint one after the other. At its rounds, the computer
partially completes the drawing using machine learning algorithms, and projects
its additions directly on the canvas, which the artists are free to insert or
modify. Alongside fostering creativity, the process is designed to question the
growing interaction between humans and machines.
</summary>
    <author>
      <name>Vivien Cabannes</name>
    </author>
    <author>
      <name>Thomas Kerdreux</name>
    </author>
    <author>
      <name>Louis Thiry</name>
    </author>
    <author>
      <name>Tina Campana</name>
    </author>
    <author>
      <name>Charly Ferrandes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for poster at creativity workshop NeurIPS 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">creativity workshop NeurIPS 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.04386v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04386v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08462v1</id>
    <updated>2019-10-18T15:15:09Z</updated>
    <published>2019-10-18T15:15:09Z</published>
    <title>Animation Synthesis Triggered by Vocal Mimics</title>
    <summary>  We propose a method leveraging the naturally time-related expressivity of our
voice to control an animation composed of a set of short events. The user
records itself mimicking onomatopoeia sounds such as "Tick", "Pop", or "Chhh"
which are associated with specific animation events. The recorded soundtrack is
automatically analyzed to extract every instant and types of sounds. We finally
synthesize an animation where each event type and timing correspond with the
soundtrack. In addition to being a natural way to control animation timing, we
demonstrate that multiple stories can be efficiently generated by recording
different voice sequences. Also, the use of more than one soundtrack allows us
to control different characters with overlapping actions.
</summary>
    <author>
      <name>Adrien Nivaggioli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX</arxiv:affiliation>
    </author>
    <author>
      <name>Damien Rohmer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3359566.3360067</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3359566.3360067" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Motion, Interaction and Games, Oct 2019, Newcastle, United Kingdom</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.08462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05204v1</id>
    <updated>2019-11-12T23:35:59Z</updated>
    <published>2019-11-12T23:35:59Z</published>
    <title>Locking-free Simulation of Isometric Thin Plates</title>
    <summary>  To efficiently simulate very thin, inextensible materials like cloth or
paper, it is tempting to replace force-based thin-plate dynamics with hard
isometry constraints. Unfortunately, naive formulations of the constraints
induce membrane locking---artificial stiffening of bending modes due to the
inability of discrete kinematics to reproduce exact isometries. We propose a
simple set of meshless isometry constraints, based on moving-least-squares
averaging of the strain tensor, which do not lock, and which can be easily
incorporated into standard constrained Lagrangian dynamics integration.
</summary>
    <author>
      <name>Hsiao-yu Chen</name>
    </author>
    <author>
      <name>Paul Kry</name>
    </author>
    <author>
      <name>Etienne Vouga</name>
    </author>
    <link href="http://arxiv.org/abs/1911.05204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05992v1</id>
    <updated>2019-11-14T08:37:21Z</updated>
    <published>2019-11-14T08:37:21Z</published>
    <title>Efficient Direct Slicing Of Dilated And Eroded 3d Models For Additive
  Manufacturing: Technical Report</title>
    <summary>  In the context of additive manufacturing we present a novel technique for
direct slicing of a dilated or eroded volume, where the input volume boundary
is a triangle mesh. Rather than computing a 3D model of the boundary of the
dilated or eroded volume, our technique directly produces its slices. This
leads to a computationally and memory efficient algorithm, which is
embarrassingly parallel. Contours can be extracted under an arbitrary chord
error, non-uniform dilation or erosion are also possible. Finally, the scheme
is simple and robust to implement.
</summary>
    <author>
      <name>Sylvain Lefebvre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MFX</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1911.05992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09177v1</id>
    <updated>2019-11-09T14:24:56Z</updated>
    <published>2019-11-09T14:24:56Z</published>
    <title>Feature Extraction in Augmented Reality</title>
    <summary>  Augmented Reality (AR) is used for various applications associated with the
real world. In this paper, first, describe characteristics and essential
services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned
in the introductory section. Then, AR Technologies along with its workflow is
depicted, which includes the complete AR Process consisting of the stages of
Image Acquisition, Feature Extraction, Feature Matching, Geometric
Verification, and Associated Information Retrieval. Feature extraction is the
essence of AR hence its details are furnished in the paper.
</summary>
    <author>
      <name>Jekishan K. Parmar</name>
    </author>
    <author>
      <name>Ankit Desai</name>
    </author>
    <link href="http://arxiv.org/abs/1911.09177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10217v3</id>
    <updated>2019-11-27T11:46:20Z</updated>
    <published>2019-11-22T19:02:02Z</published>
    <title>Importance Sampling of Many Lights with Reinforcement Lightcuts Learning</title>
    <summary>  In this manuscript, we introduce a novel technique for sampling and
integrating direct illumination in the presence of many lights. Unlike previous
work, the presented technique importance samples the product distribution of
radiance and visibility while using bounded memory footprint and very low
sampling overhead. This is achieved by learning a compact approximation of the
target distributions over both space and time, allowing to reuse and adapt the
learnt distributions both spatially, within a frame, and temporally, across
multiple frames. Finally, the technique is amenable to massive parallelization
on GPUs and suitable for both offline and real-time rendering.
</summary>
    <author>
      <name>Jacopo Pantaleoni</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10217v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10217v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00321v1</id>
    <updated>2019-12-01T04:59:20Z</updated>
    <published>2019-12-01T04:59:20Z</published>
    <title>A SVBRDF Modeling Pipeline using Pixel Clustering</title>
    <summary>  We present a pipeline for modeling spatially varying BRDFs (svBRDFs) of
planar materials which only requires a mobile phone for data acquisition. With
a minimum of two photos under the ambient and point light source, our pipeline
produces svBRDF parameters, a normal map and a tangent map for the material
sample. The BRDF fitting is achieved via a pixel clustering strategy and an
optimization based scheme. Our method is light-weight, easy-to-use and capable
of producing high-quality BRDF textures.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Jie Feng</name>
    </author>
    <author>
      <name>Bingfeng Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1912.00321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02125v1</id>
    <updated>2019-11-25T17:48:22Z</updated>
    <published>2019-11-25T17:48:22Z</published>
    <title>Towards Sustainable Architecture: 3D Convolutional Neural Networks for
  Computational Fluid Dynamics Simulation and Reverse DesignWorkflow</title>
    <summary>  We present a general and flexible approximation model for near real-time
prediction of steady turbulent flow in a 3D domain based on residual
Convolutional Neural Networks (CNNs). This approach can provide immediate
feedback for real-time iterations at the early stage of architectural design.
This work-flow is then reversed and offers a designer a tool that generates
building volumes based on target wind flow.
</summary>
    <author>
      <name>Josef Musil</name>
    </author>
    <author>
      <name>Jakub Knir</name>
    </author>
    <author>
      <name>Athanasios Vitsas</name>
    </author>
    <author>
      <name>Irene Gallou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS Workshop on Machine Learning for Creativity and Design 3.0,
  33rd Conference on Neural Information Processing Systems (NeurIPS 2019),
  Vancouver, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.02125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10, I.4.0, J.5, J.6" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.4.0; J.5; J.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.04356v1</id>
    <updated>2019-12-09T20:16:48Z</updated>
    <published>2019-12-09T20:16:48Z</published>
    <title>Interactive 3D fluid simulation: steering the simulation in progress
  using Lattice Boltzmann Method</title>
    <summary>  This paper describes a work in progress about software and hardware
architecture to steer and control an ongoing fluid simulation in a context of a
serious game application. We propose to use the Lattice Boltzmann Method as the
simulation approach considering that it can provide fully parallel algorithms
to reach interactive time and because it is easier to change parameters while
the simulation is in progress remaining physically relevant than more classical
simulation approaches. We describe which parameters we can modify and how we
solve technical issues of interactive steering and we finally show an
application of our interactive fluid simulation approach of water dam
phenomena.
</summary>
    <author>
      <name>Mengchen Wang</name>
    </author>
    <author>
      <name>Nicolas Ferey</name>
    </author>
    <author>
      <name>Patrick Bourdot</name>
    </author>
    <author>
      <name>Frederic Magoules</name>
    </author>
    <link href="http://arxiv.org/abs/1912.04356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.04583v2</id>
    <updated>2019-12-16T08:45:00Z</updated>
    <published>2019-12-10T09:04:09Z</published>
    <title>RGB Point Cloud Manipulation with Triangular Structures for Artistic
  Image Recoloring</title>
    <summary>  Usual approaches for image recoloring, such as local filtering by transfer
functions and global histogram remapping, lack of accurate control or miss
small groups of important pixels. In this paper, we introduce a triangle-based
structuring of the colors of an image in the RGB space. We present an analysis
of image colors in the RGB space showing the theoretical motivation of our
triangular abstraction. We illustrate the usefulness of our structure to
recolor images.
</summary>
    <author>
      <name>Baptiste Delos</name>
    </author>
    <author>
      <name>Nicolas Mellado</name>
    </author>
    <author>
      <name>David Vanderhaeghe</name>
    </author>
    <author>
      <name>Remi Cozot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.04583v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04583v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.04898v1</id>
    <updated>2019-12-10T09:10:31Z</updated>
    <published>2019-12-10T09:10:31Z</published>
    <title>Modelling curvature of a bent paper leaf</title>
    <summary>  In this article, we briefly describe various tools and approaches that
algebraic geometry has to offer to straighten bent objects. Throughout this
article we will consider a specific example of a bent or curved piece of paper
which in our case acts very much like an elastica curve. We conclude this
article with a suggestion to algebraic geometry as a viable and fast
performance alternative of neural networks in vision and machine learning. The
purpose of this article is not to build a full blown framework but to show
possibility of using algebraic geometry as an alternative to neural networks
for recognizing or extracting features on manifolds.
</summary>
    <author>
      <name>Sasikanth Raghava Goteti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7287/PEERJ.PREPRINTS.161</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7287/PEERJ.PREPRINTS.161" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages , 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.04898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.05494v1</id>
    <updated>2019-12-09T12:33:17Z</updated>
    <published>2019-12-09T12:33:17Z</published>
    <title>Spectral Domain Decomposition Method for Natural Lighting and Medieval
  Glass Rendering</title>
    <summary>  In this paper, we use an original ray-tracing domain decomposition method to
address image rendering of naturally lighted scenes. This new method allows to
particularly analyze rendering problems on parallel architectures, in the case
of interactions between light-rays and glass material. Numerical experiments,
for medieval glass rendering within the church of the Royaumont abbey,
illustrate the performance of the proposed ray-tracing domain decomposition
method (DDM) on multi-cores and multi-processors architectures. On one hand,
applying domain decomposition techniques increases speedups obtained by
parallelizing the computation. On the other hand, for a fixed number of
parallel processes, we notice that speedups increase as the number of
sub-domains do.
</summary>
    <author>
      <name>Guillaume Gbikpi-Benissan</name>
    </author>
    <author>
      <name>Remi Cerise</name>
    </author>
    <author>
      <name>Patrick Callet</name>
    </author>
    <author>
      <name>Frederic Magoules</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPCC.2014.17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPCC.2014.17" rel="related"/>
    <link href="http://arxiv.org/abs/1912.05494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.05494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08757v1</id>
    <updated>2019-12-18T17:44:47Z</updated>
    <published>2019-12-18T17:44:47Z</published>
    <title>Neural Smoke Stylization with Color Transfer</title>
    <summary>  Artistically controlling fluid simulations requires a large amount of manual
work by an artist. The recently presented transportbased neural style transfer
approach simplifies workflows as it transfers the style of arbitrary input
images onto 3D smoke simulations. However, the method only modifies the shape
of the fluid but omits color information. In this work, we therefore extend the
previous approach to obtain a complete pipeline for transferring shape and
color information onto 2D and 3D smoke simulations with neural networks. Our
results demonstrate that our method successfully transfers colored style
features consistently in space and time to smoke data for different input
textures.
</summary>
    <author>
      <name>Fabienne Christen</name>
    </author>
    <author>
      <name>Byungsoo Kim</name>
    </author>
    <author>
      <name>Vinicius C. Azevedo</name>
    </author>
    <author>
      <name>Barbara Solenthaler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2312/egs.20201015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2312/egs.20201015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Eurographics2020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics 2020 - Short Papers</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.08757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12786v1</id>
    <updated>2019-12-30T02:40:42Z</updated>
    <published>2019-12-30T02:40:42Z</published>
    <title>Adding Custom Intersectors to the C++ Ray Tracing Template Library
  Visionaray</title>
    <summary>  Most ray tracing libraries allow the user to provide custom functionality
that is executed when a potential ray surface interaction was encountered to
determine if the interaction was valid or traversal should be continued. This
is e.g. useful for alpha mask validation and allows the user to reuse existing
ray object intersection routines rather than reimplementing them. Augmenting
ray traversal with custom intersection logic requires some kind of callback
mechanism that injects user code into existing library routines. With template
libraries, this injection can happen statically since the user compiles the
binary code herself. We present an implementation of this "custom intersector"
approach and its integration into the C++ ray tracing template library
Visionaray.
</summary>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <link href="http://arxiv.org/abs/1912.12786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03196v1</id>
    <updated>2020-02-08T16:36:30Z</updated>
    <published>2020-02-08T16:36:30Z</published>
    <title>Correction of Chromatic Aberration from a Single Image Using Keypoints</title>
    <summary>  In this paper, we propose a method to correct for chromatic aberration in a
single photograph. Our method replicates what a user would do in a photo
editing program to account for this defect. We find matching keypoints in each
colour channel then align them as a user would.
</summary>
    <author>
      <name>Benjamin T. Cecchetto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Originally this paper was a project for a course in 2009 and has not
  been published. It has been cited multiple times since then. The LaTeX code
  was lost, so it has been revised in February 2020 to post on ArXiV</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05234v2</id>
    <updated>2020-10-02T17:57:31Z</updated>
    <published>2020-02-12T21:01:00Z</published>
    <title>Visualizing modular forms</title>
    <summary>  We examine several currently used techniques for visualizing complex-valued
functions applied to modular forms. We plot several examples and study the
benefits and limitations of each technique. We then introduce a method of
visualization that can take advantage of colormaps in Python's matplotlib
library, describe an implementation, and give more examples. Much of this
discussion applies to general visualizations of complex-valued functions in the
plane.
</summary>
    <author>
      <name>David Lowry-Duda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, many figures, after first major set of revisions</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05234v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05234v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11109v1</id>
    <updated>2020-02-25T14:29:25Z</updated>
    <published>2020-02-25T14:29:25Z</published>
    <title>$G^1$ hole filling with S-patches made easy</title>
    <summary>  S-patches have been around for 30 years, but they are seldom used, and are
considered more of a mathematical curiosity than a practical surface
representation. In this article a method is presented for automatically
creating S-patches of any degree or any number of sides, suitable for inclusion
in a curve network with tangential continuity to the adjacent surfaces. The
presentation aims at making the implementation straightforward; a few examples
conclude the paper.
</summary>
    <author>
      <name>Péter Salvi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 12th Conference of the Hungarian Association
  for Image Processing and Pattern Recognition, #1, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11111v1</id>
    <updated>2020-02-25T15:09:05Z</updated>
    <published>2020-02-25T15:09:05Z</published>
    <title>On the CAD-compatible conversion of S-patches</title>
    <summary>  S-patches have many nice mathematical properties. It is known since their
first appearance, that any regular S-patch can be exactly converted into a
trimmed rational B\'ezier surface. This is a big advantage compared to other
multi-sided surface representations that have to be approximated for exporting
them into CAD/CAM systems. The actual conversion process, however, remained at
a theoretical level, with bits and pieces scattered in multiple publications.
In this paper we review the entirety of the algorithm, and investigate it from
a practical aspect.
</summary>
    <author>
      <name>Péter Salvi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on the Advances of Information
  Technology, pp. 72-76, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11212v1</id>
    <updated>2020-02-25T22:55:41Z</updated>
    <published>2020-02-25T22:55:41Z</published>
    <title>Computationally efficient transfinite patches with fullness control</title>
    <summary>  Transfinite patches provide a simple and elegant solution to the problem of
representing non-four-sided continuous surfaces, which are useful in a variety
of applications, such as curve network based design. Real-time responsiveness
is essential in this context, and thus reducing the computation cost is an
important concern. The Midpoint Coons (MC) patch presented in this paper is a
fusion of two previous transfinite schemes, combining the speed of one with the
superior control mechanism of the other. This is achieved using a new
constrained parameterization based on generalized barycentric coordinates and
transfinite blending functions.
</summary>
    <author>
      <name>Péter Salvi</name>
    </author>
    <author>
      <name>István Kovács</name>
    </author>
    <author>
      <name>Tamás Várady</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on the Advances of Information
  Technology, pp. 96-100, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08475v1</id>
    <updated>2020-04-17T22:41:01Z</updated>
    <published>2020-04-17T22:41:01Z</published>
    <title>A Simple, General, and GPU Friendly Method for Computing Dual Mesh and
  Iso-Surfaces of Adaptive Mesh Refinement (AMR) Data</title>
    <summary>  We propose a novel approach to extracting crack-free iso-surfaces from
Structured AMR data that is more general than previous techniques, is trivially
simple to implement, requires no information other than the list of AMR cells,
and works, in particular, for different AMR formats including octree AMR,
block-structured AMR with arbitrary level differences at level boundaries, and
AMR data that consist of individual cells without any existing grid structure.
We describe both the technique itself and a CUDA-based GPU implementation of
this technique, and evaluate it on several non-trivial AMR data sets.
</summary>
    <author>
      <name>Ingo Wald</name>
    </author>
    <link href="http://arxiv.org/abs/2004.08475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05386v1</id>
    <updated>2020-05-11T19:07:55Z</updated>
    <published>2020-05-11T19:07:55Z</published>
    <title>Design and visualization of Riemannian metrics</title>
    <summary>  Local and global illumination were recently defined in Riemannian manifolds
to visualize classical Non-Euclidean spaces. This work focuses on Riemannian
metric construction in $\mathbb{R}^3$ to explore special effects like warping,
mirages, and deformations. We investigate the possibility of using graphs of
functions and diffeomorphism to produce such effects. For these, their
Riemannian metrics and geodesics derivations are provided, and ways of
accumulating such metrics. We visualize, in "real-time", the resulting
Riemannian manifolds using a ray tracing implemented on top of Nvidia RTX GPUs.
</summary>
    <author>
      <name>Tiago Novello</name>
    </author>
    <author>
      <name>Vinícius da Silva</name>
    </author>
    <author>
      <name>Luiz Velho</name>
    </author>
    <link href="http://arxiv.org/abs/2005.05386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07702v1</id>
    <updated>2020-05-15T19:26:11Z</updated>
    <published>2020-05-15T19:26:11Z</published>
    <title>Generative Adversarial Networks for photo to Hayao Miyazaki style
  cartoons</title>
    <summary>  This paper takes on the problem of transferring the style of cartoon images
to real-life photographic images by implementing previous work done by
CartoonGAN. We trained a Generative Adversial Network(GAN) on over 60 000
images from works by Hayao Miyazaki at Studio Ghibli. To evaluate our results,
we conducted a qualitative survey comparing our results with two
state-of-the-art methods. 117 survey results indicated that our model on
average outranked state-of-the-art methods on cartoon-likeness.
</summary>
    <author>
      <name>Filip Andersson</name>
    </author>
    <author>
      <name>Simon Arvidsson</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08468v1</id>
    <updated>2020-05-18T06:00:47Z</updated>
    <published>2020-05-18T06:00:47Z</published>
    <title>An error reduced and uniform parameter approximation in fitting of
  B-spline curves to data points</title>
    <summary>  Approximating data points in three or higher dimension space based on cubic
B-spline curve is presented. Representations for planar curves, are merged and
extended to the higher dimension. The curve is fitted to the order of data
points, or uniform parameter values are assumed for the points. Tangents are
assumed at the data points, corresponding to the property used in cardinal
splines, for shape preserving and visually pleasing fit. Control points of
piecewise continuous cubic bezier curves, meeting the boundary conditions of
cardinal spline segments, are used for b-spline curve in corresponding
coordinate planes. Approximation using error computed in the least square
sense, based on a fraction of data points, is also presented.
</summary>
    <author>
      <name>Debashis Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09941v2</id>
    <updated>2020-11-16T14:22:59Z</updated>
    <published>2020-05-20T09:59:57Z</published>
    <title>Non-Uniform Gaussian Blur of Hexagonal Bins in Cartesian Coordinates</title>
    <summary>  In a recent application of the Bokeh Python library for visualizing
physico-chemical properties of chemical entities text-mined from the scientific
literature, we found ourselves facing the task of smoothing hexagonally binned
data in Cartesian coordinates. To the best of our knowledge, no documentation
for how to do this exist in the public domain. This short paper shows how to
accomplish this in general and for Bokeh in particular. We illustrate the
method with a real-world example and discuss some potential advantages of using
hexagonal bins in these and similar applications.
</summary>
    <author>
      <name>Reinier Vleugels</name>
    </author>
    <author>
      <name>Magnus Palmblad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures. Figures 1-5 made in LaTeX and look OK in
  preview. Typos corrected in Version 2</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.09941v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09941v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01481v1</id>
    <updated>2020-07-03T03:38:24Z</updated>
    <published>2020-07-03T03:38:24Z</published>
    <title>Ordinary Facet Angles of a Stroked Path Tessellated by Uniform Tangent
  Angle Steps Are Bounded by Twice the Step Angle</title>
    <summary>  We explain geometrically why ordinary facet angles of a stroked path
tessellated from uniform tangent angle steps are bounded by twice the step
angle. This fact means---excluding a small number of extraordinary facet angles
straddling offset cusps---our polar stroking method bounds the facet angle size
to less than $2 \theta$ where $\theta$ is the tangent step angle.
</summary>
    <author>
      <name>Mark J. Kilgard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, supplemental paper for "Polar Stroking: New Theory and
  Methods for Stroking Paths" (SIGGRAPH 2020) arXiv:2007.00308</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09740v1</id>
    <updated>2020-07-19T18:28:09Z</updated>
    <published>2020-07-19T18:28:09Z</published>
    <title>Octahedral Frames for Feature-Aligned Cross-Fields</title>
    <summary>  We present a method for designing smooth cross fields on surfaces that
automatically align to sharp features of an underlying geometry. Our approach
introduces a novel class of energies based on a representation of cross fields
in the spherical harmonic basis. We provide theoretical analysis of these
energies in the smooth setting, showing that they penalize deviations from
surface creases while otherwise promoting intrinsically smooth fields. We
demonstrate the applicability of our method to quad-meshing and include an
extensive benchmark comparing our fields to other automatic approaches for
generating feature-aligned cross fields on triangle meshes.
</summary>
    <author>
      <name>Paul Zhang</name>
    </author>
    <author>
      <name>Josh Vekhter</name>
    </author>
    <author>
      <name>Edward Chien</name>
    </author>
    <author>
      <name>David Bommes</name>
    </author>
    <author>
      <name>Etienne Vouga</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/pzpzpzp1/CreaseAlignedCrossFields</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.09740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12254v1</id>
    <updated>2020-07-23T21:01:57Z</updated>
    <published>2020-07-23T21:01:57Z</published>
    <title>Anecdotal Survey of Variations in Path Stroking among Real-world
  Implementations</title>
    <summary>  Stroking a path is one of the two basic rendering operations in vector
graphics standards (e.g., PostScript, PDF, SVG). We survey path stroking
rendering results from real-world software implementations of path stroking for
anecdotal evidence that such implementations are prone to rendering variances.
While our survey is limited and informal, the rendering results we gathered
indicate widespread rendering variations for simple-but-problematic stroked
paths first identified decades ago. We conclude that creators of vector
graphics content would benefit from a mathematically grounded standardization
for how a stroked path should be rasterized.
</summary>
    <author>
      <name>Mark J. Kilgard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, supplemental paper for "Polar Stroking: New Theory and
  Methods for Stroking Paths" (SIGGRAPH 2020) arXiv:2007.00308</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.12254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02135v1</id>
    <updated>2020-12-03T18:14:30Z</updated>
    <published>2020-12-03T18:14:30Z</published>
    <title>Simple Methods to Represent Shapes with Sample Spheres</title>
    <summary>  Representing complex shapes with simple primitives in high accuracy is
important for a variety of applications in computer graphics and geometry
processing. Existing solutions may produce suboptimal samples or are complex to
implement. We present methods to approximate given shapes with user-tunable
number of spheres to balance between accuracy and simplicity: touching
medial/scale-axis polar balls and k-means smallest enclosing circles. Our
methods are easy to implement, run efficiently, and can approach quality
similar to manual construction.
</summary>
    <author>
      <name>Li-Yi Wei</name>
    </author>
    <author>
      <name>Arjun V Anand</name>
    </author>
    <author>
      <name>Shally Kumar</name>
    </author>
    <author>
      <name>Tarun Beri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3410700.3425424</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3410700.3425424" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH Asia 2020 Technical Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.02135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05348v1</id>
    <updated>2020-12-09T22:36:29Z</updated>
    <published>2020-12-09T22:36:29Z</published>
    <title>Compressed Bounding Volume Hierarchies for Collision Detection &amp;
  Proximity Query</title>
    <summary>  We present a novel representation of compressed data structure for
simultaneous bounding volume hierarchy (BVH) traversals like they appear for
instance in collision detection &amp; proximity query. The main idea is to compress
bounding volume (BV) descriptors and cluster BVH into a smaller parts 'treelet'
that fit into CPU cache while at the same time maintain random-access and
automatic cache-aware data structure layouts. To do that, we quantify BV and
compress 'treelet' using predictor-corrector scheme with the predictor at a
specific node in the BVH based on the chain of BVs upwards.
</summary>
    <author>
      <name>Toni Tan</name>
    </author>
    <author>
      <name>Rene Weller</name>
    </author>
    <author>
      <name>Gabriel Zachmann</name>
    </author>
    <link href="http://arxiv.org/abs/2012.05348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12614v3</id>
    <updated>2022-09-19T16:27:40Z</updated>
    <published>2020-12-23T11:53:00Z</published>
    <title>On spherical harmonics possessing octahedral symmetry</title>
    <summary>  In this paper, we present the implicit equations for one special class of
real-valued spherical harmonics with octahedral symmetry. Based on this
representation, we construct the rotationally invariant measure of deviation
from the specified symmetry. The spherical harmonics we consider have some
applications in the area of directional fields design due to their ability to
represent mutually orthogonal axes in 3D space, not relative to their order and
orientation.
</summary>
    <author>
      <name>Yuri Nesterenko</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12614v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12614v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02570v1</id>
    <updated>2021-01-07T14:48:42Z</updated>
    <published>2021-01-07T14:48:42Z</published>
    <title>Instanced model simplification using combined geometric and
  appearance-related metric</title>
    <summary>  Evolution of 3D graphics and graphical worlds has brought issues like content
optimization, real-time processing, rendering, and shared storage limitation
under consideration. Generally, different simplification approaches are used to
make 3D meshes viable for rendering. However, many of these approaches ignore
vertex attributes for instanced 3D meshes. In this paper, we implement and
evaluate a simple and improved version to simplify instanced 3D textured
models. The approach uses different vertex attributes in addition to geometry
to simplify mesh instances. The resulting simplified models demonstrate
efficient time-space requirements and better visual quality.
</summary>
    <author>
      <name>Sadia Tariq</name>
    </author>
    <author>
      <name>Anis Ur Rahman</name>
    </author>
    <author>
      <name>Tahir Azim</name>
    </author>
    <author>
      <name>Rehman Gull Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11569v2</id>
    <updated>2021-02-08T13:42:27Z</updated>
    <published>2021-01-27T17:54:11Z</published>
    <title>Closed-form Quadrangulation of N-Sided Patches</title>
    <summary>  We analyze the problem of quadrangulating a $n$-sided patch, each side at its
boundary subdivided into a given number of edges, using a single irregular
vertex (or none, when $n = 4$) that breaks the otherwise fully regular lattice.
We derive, in an analytical closed-form, (1) the necessary and sufficient
conditions that a patch must meet to admit this quadrangulation, and (2) a full
description of the resulting tessellation(s).
</summary>
    <author>
      <name>Marco Tarini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cag.2022.06.015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cag.2022.06.015" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Graphics, Volume 107, Pages 60-65, ISSN 0097-8493,
  2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.11569v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11569v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02309v1</id>
    <updated>2021-03-03T10:39:03Z</updated>
    <published>2021-03-03T10:39:03Z</published>
    <title>Compact Tetrahedralization-based Acceleration Structure for Ray Tracing</title>
    <summary>  We propose a compact and efficient tetrahedral mesh representation to improve
the ray-tracing performance. We reorder tetrahedral mesh data using a
space-filling curve to improve cache locality. Most importantly, we propose an
efficient ray traversal algorithm. We provide details of common ray tracing
operations on tetrahedral meshes and give the GPU implementation of our
traversal method. We demonstrate our findings through a set of comprehensive
experiments. Our method outperforms existing tetrahedral mesh-based traversal
methods and yields comparable results to the traversal methods based on the
state of the art acceleration structures such as k-dimensional (k-d) trees and
Bounding Volume Hierarchies (BVHs).
</summary>
    <author>
      <name>Aytek Aman</name>
    </author>
    <author>
      <name>Serkan Demirci</name>
    </author>
    <author>
      <name>Uğur Güdükbay</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02992v1</id>
    <updated>2021-03-04T12:34:14Z</updated>
    <published>2021-03-04T12:34:14Z</published>
    <title>Clusterplot: High-dimensional Cluster Visualization</title>
    <summary>  We present Clusterplot, a multi-class high-dimensional data visualization
tool designed to visualize cluster-level information offering an intuitive
understanding of the cluster inter-relations. Our unique plots leverage 2D
blobs devised to convey the geometrical and topological characteristics of
clusters within the high-dimensional data, and their pairwise relations, such
that general inter-cluster behavior is easily interpretable in the plot. Class
identity supervision is utilized to drive the measuring of relations among
clusters in high-dimension, particularly, proximity and overlap, which are then
reflected spatially through the 2D blobs. We demonstrate the strength of our
clusterplots and their ability to deliver a clear and intuitive informative
exploration experience for high-dimensional clusters characterized by complex
structure and significant overlap.
</summary>
    <author>
      <name>Or Malkai</name>
    </author>
    <author>
      <name>Min Lu</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04033v1</id>
    <updated>2021-03-06T05:26:54Z</updated>
    <published>2021-03-06T05:26:54Z</published>
    <title>An Effective Approach to Minimize Error in Midpoint Ellipse Drawing
  Algorithm</title>
    <summary>  The present paper deals with the generalization of Midpoint Ellipse Drawing
Algorithm (MPEDA) to minimize the error in the existing MPEDA in cartesian
form. In this method, we consider three different values of h, i.e., 1, 0.5 and
0.1. For h = 1, all the results of MPEDA have been verified. For other values
of h it is observed that as the value of h decreases, the number of iteration
increases but the error between the points generated and the original ellipse
points decreases and vice-versa.
</summary>
    <author>
      <name>M. Javed Idrisi</name>
    </author>
    <author>
      <name>Aayesha Ashraf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 tables and 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Internaational Journal of Engineering Research and Technology,
  Vol. 10, Issue 2, p. 621-625, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.04033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Wxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09988v2</id>
    <updated>2023-03-07T09:58:48Z</updated>
    <published>2021-03-18T02:25:18Z</published>
    <title>Crowdsourcing Autonomous Traffic Simulation</title>
    <summary>  We present an innovative framework, Crowdsourcing Autonomous Traffic
Simulation (CATS) framework, in order to safely implement and realize orderly
traffic flows. We firstly provide a semantic description of the CATS framework
using theories of economics to construct coupling constraints among drivers, in
which drivers monitor each other by making use of transportation resources and
driving credit. We then introduce an emotion-based traffic simulation, which
utilizes the Weber-Fechner law to integrate economic factors into drivers'
behaviors. Simulation results show that the CATS framework can significantly
reduce traffic accidents and improve urban traffic conditions.
</summary>
    <author>
      <name>Hua Wang</name>
    </author>
    <author>
      <name>Wenshan Zhao</name>
    </author>
    <author>
      <name>Zhigang Deng</name>
    </author>
    <author>
      <name>Mingliang Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2103.09988v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09988v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15163v3</id>
    <updated>2022-06-02T23:43:37Z</updated>
    <published>2021-03-28T16:15:39Z</published>
    <title>Countering Racial Bias in Computer Graphics Research</title>
    <summary>  Current computer graphics research practices contain racial biases that have
resulted in investigations into "skin" and "hair" that focus on the hegemonic
visual features of Europeans and East Asians. To broaden our research horizons
to encompass all of humanity, we propose a variety of improvements to
quantitative measures and qualitative practices, and pose novel, open research
problems.
</summary>
    <author>
      <name>Theodore Kim</name>
    </author>
    <author>
      <name>Holly Rushmeier</name>
    </author>
    <author>
      <name>Julie Dorsey</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <author>
      <name>Raqi Syed</name>
    </author>
    <author>
      <name>Wojciech Jarosz</name>
    </author>
    <author>
      <name>A. M. Darke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15163v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15163v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05052v1</id>
    <updated>2021-04-11T16:57:50Z</updated>
    <published>2021-04-11T16:57:50Z</published>
    <title>Fabrication-aware Design for Furniture with Planar Pieces</title>
    <summary>  We propose a computational design tool to enable casual end-users to easily
design, fabricate, and assemble flat-pack furniture with guaranteed
manufacturability. Using our system, users select parameterized components from
a library and constrain their dimensions. Then they abstractly specify
connections among components to define the furniture. Once fabrication
specifications (e.g. materials) designated, the mechanical implementation of
the furniture is automatically handled by leveraging encoded domain expertise.
Afterwards, the system outputs 3D models for visualization and mechanical
drawings for fabrication. We demonstrate the validity of our approach by
designing, fabricating, and assembling a variety of flat-pack (scaled)
furniture on demand.
</summary>
    <author>
      <name>Wenzhong Yan</name>
    </author>
    <author>
      <name>Dawei Zhao</name>
    </author>
    <author>
      <name>Ankur Mehta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S0263574722000443</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S0263574722000443" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 14 figures, submitted to Robotica</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Robotica, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.05052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08016v2</id>
    <updated>2021-04-20T03:14:51Z</updated>
    <published>2021-04-16T10:24:04Z</published>
    <title>A Review of the State-of-the-Art on Tours for Dynamic Visualization of
  High-dimensional Data</title>
    <summary>  This article discusses a high-dimensional visualization technique called the
tour, which can be used to view data in more than three dimensions. We review
the theory and history behind the technique, as well as modern software
developments and applications of the tour that are being found across the
sciences and machine learning.
</summary>
    <author>
      <name>Stuart Lee</name>
    </author>
    <author>
      <name>Dianne Cook</name>
    </author>
    <author>
      <name>Natalia da Silva</name>
    </author>
    <author>
      <name>Ursula Laa</name>
    </author>
    <author>
      <name>Earo Wang</name>
    </author>
    <author>
      <name>Nick Spyrison</name>
    </author>
    <author>
      <name>H. Sherry Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12826v1</id>
    <updated>2021-04-26T19:00:11Z</updated>
    <published>2021-04-26T19:00:11Z</published>
    <title>HodgeNet: Learning Spectral Geometry on Triangle Meshes</title>
    <summary>  Constrained by the limitations of learning toolkits engineered for other
applications, such as those in image processing, many mesh-based learning
algorithms employ data flows that would be atypical from the perspective of
conventional geometry processing. As an alternative, we present a technique for
learning from meshes built from standard geometry processing modules and
operations. We show that low-order eigenvalue/eigenvector computation from
operators parameterized using discrete exterior calculus is amenable to
efficient approximate backpropagation, yielding spectral per-element or
per-mesh features with similar formulas to classical descriptors like the
heat/wave kernel signatures. Our model uses few parameters, generalizes to
high-resolution meshes, and exhibits performance and time complexity on par
with past work.
</summary>
    <author>
      <name>Dmitriy Smirnov</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SIGGRAPH 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13012v1</id>
    <updated>2021-05-27T08:58:51Z</updated>
    <published>2021-05-27T08:58:51Z</published>
    <title>Passing Multi-Channel Material Textures to a 3-Channel Loss</title>
    <summary>  Our objective is to compute a textural loss that can be used to train texture
generators with multiple material channels typically used for physically based
rendering such as albedo, normal, roughness, metalness, ambient occlusion, etc.
Neural textural losses often build on top of the feature spaces of pretrained
convolutional neural networks. Unfortunately, these pretrained models are only
available for 3-channel RGB data and hence limit neural textural losses to this
format. To overcome this limitation, we show that passing random triplets to a
3-channel loss provides a multi-channel loss that can be used to generate
high-quality material textures.
</summary>
    <author>
      <name>Thomas Chambon</name>
    </author>
    <author>
      <name>Eric Heitz</name>
    </author>
    <author>
      <name>Laurent Belcour</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450623.3464685</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450623.3464685" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.00058v2</id>
    <updated>2021-11-28T16:39:28Z</updated>
    <published>2021-08-31T19:55:26Z</published>
    <title>Wanderlust: 3D Impressionism in Human Journeys</title>
    <summary>  The movements of individuals are fundamental to building and maintaining
social connections. This pictorial presents Wanderlust, an experimental
three-dimensional data visualization on the universal visitation pattern
revealed from large-scale mobile phone tracking data. It explores ways of
visualizing recurrent flows and the attractive places they implied. Inspired by
the 19th-century art movement Impressionism, we develop a multi-layered effect,
an impression, of mountains emerging from consolidated flows, to capture the
essence of human journeys and urban spatial structure.
</summary>
    <author>
      <name>Guangyu Du</name>
    </author>
    <author>
      <name>Lei Dong</name>
    </author>
    <author>
      <name>Fabio Duarte</name>
    </author>
    <author>
      <name>Carlo Ratti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VISAP52981.2021.00012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VISAP52981.2021.00012" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VIS Arts Program (VISAP) 2021, pictorial, 14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.00058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.00058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.05805v1</id>
    <updated>2021-09-13T09:30:56Z</updated>
    <published>2021-09-13T09:30:56Z</published>
    <title>Rectangle-based Approximation for Rendering Glossy Interreflections</title>
    <summary>  This study introduces an approximation for rendering one bounce glossy
interreflection in real time. The solution is based on the most representative
point (MRP) and extends to a sampling disk near the MRP. Our algorithm
represents geometry as rectangle proxies and specular reflections using a
spherical Gaussian. The reflected radiance from the disk was efficiently
approximated by selecting a representative attenuation axis in the sampling
disk. We provide an efficient approximation of the glossy interreflection and
can efficiently perform the approximation at runtime. Our method uses forward
rendering (without using GBuffer), which is more suitable for platforms that
favor forward rendering, such as mobile applications and virtual reality.
</summary>
    <author>
      <name>Chunbiao Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2109.05805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.05805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10193v1</id>
    <updated>2021-09-21T14:20:30Z</updated>
    <published>2021-09-21T14:20:30Z</published>
    <title>The Parallel Coordinates Plot Revisited: Visual Extensions from Hive
  Plots, Heterogeneous Correlations, and an Exploration of Covid-19 Data in the
  United States</title>
    <summary>  This paper extends an existing visualization, the Parallel Coordinates Plot
(PCP), specifically its polar coordinate representation, the $\textit{Polar
Parallel Coordinates Plot (P2CP)}$. With the additional incorporation of
techniques borrowed from Hive Plot network visualizations, we demonstrate
improved capabilities to explore multidimensional data in flatland, with a
particular emphasis on the unique ability to represent 3-dimensional data. To
demonstrate these techniques on P2CPs, we consider toy data, the Iris dataset,
and socioeconomic data for counties in the United States. We conclude with an
exploration of Covid-19 data from counties in the contiguous United States.
</summary>
    <author>
      <name>Gary Koplik</name>
    </author>
    <author>
      <name>Ashlee Valente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.10193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00327v1</id>
    <updated>2021-10-01T11:38:52Z</updated>
    <published>2021-10-01T11:38:52Z</published>
    <title>Navigating Higher Dimensional Spaces using Hyperbolic Geometry</title>
    <summary>  Higher-dimensional spaces are ubiquitous in applications of mathematics. Yet,
as we live in a three-dimensional space, visualizing, say, a four-dimensional
space is challenging. We introduce a novel method of interactive visualization
of higher-dimensional grids, based on hyperbolic geometry. In our approach,
visualized objects are adjacent on the screen if and only if they are in
adjacent cells of the grid. Previous attempts do not show the whole
higher-dimensional space at once, put close objects in distant parts of the
screen, or map multiple locations to the same point on the screen; our solution
lacks these disadvantages, making it applicable in data visualization, user
interfaces, and game design.
</summary>
    <author>
      <name>Eryk Kopczyński</name>
    </author>
    <author>
      <name>Dorota Celińska-Kopczyńska</name>
    </author>
    <link href="http://arxiv.org/abs/2110.00327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.12601v1</id>
    <updated>2021-10-25T02:27:24Z</updated>
    <published>2021-10-25T02:27:24Z</published>
    <title>Semantic Resizing of Charts Through Generalization:A Case Study with
  Line Charts</title>
    <summary>  Inspired by cartographic generalization principles, we present a
generalization technique for rendering line charts at different sizes,
preserving the important semantics of the data at that display size. The
algorithm automatically determines the generalization operators to be applied
at that size based on spatial density, distance, and the semantic importance of
the various visualization elements in the line chart. A qualitative evaluation
of the prototype that implemented the algorithm indicates that the generalized
line charts pre-served the general data shape, while minimizing visual clutter.
We identify future opportunities where generalization can be extended and
applied to other chart types and visual analysis authoring tools.
</summary>
    <author>
      <name>Vidya Setlur</name>
    </author>
    <author>
      <name>Haeyong Chung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages (4 + 1 page references), 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VIS 2021 conference (TVCG journal)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.12601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.12601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.12549v1</id>
    <updated>2021-11-24T15:30:36Z</updated>
    <published>2021-11-24T15:30:36Z</published>
    <title>Interpolating Rotations with Non-abelian Kuramoto Model on the 3-Sphere</title>
    <summary>  The paper presents a novel method for interpolating rotations based on the
non-Abelian Kuramoto model on sphere S3. The algorithm, introduced in this
paper, finds the shortest and most direct path between two rotations. We have
discovered that it gives approximately the same results as a Spherical Linear
Interpolation algorithm. Simulation results of our algorithm are visualized on
S2 using Hopf fibration. In addition, in order to gain a better insight, we
have provided one short video illustrating the rotation of an object between
two positions.
</summary>
    <author>
      <name>Zinaid Kapić</name>
    </author>
    <author>
      <name>Aladin Crnkić</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-90055-7_48</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-90055-7_48" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advanced Technologies, Systems, and Applications VI. IAT 2021.
  Lecture Notes in Networks and Systems, vol 316. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.12549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.14934v2</id>
    <updated>2021-12-21T20:20:02Z</updated>
    <published>2021-11-29T20:20:29Z</published>
    <title>Generative Adversarial Networks with Conditional Neural Movement
  Primitives for An Interactive Generative Drawing Tool</title>
    <summary>  Sketches are abstract representations of visual perception and visuospatial
construction. In this work, we proposed a new framework, Generative Adversarial
Networks with Conditional Neural Movement Primitives (GAN-CNMP), that
incorporates a novel adversarial loss on CNMP to increase sketch smoothness and
consistency. Through the experiments, we show that our model can be trained
with few unlabeled samples, can construct distributions automatically in the
latent space, and produces better results than the base model in terms of shape
consistency and smoothness.
</summary>
    <author>
      <name>Suzan Ece Ada</name>
    </author>
    <author>
      <name>M. Yunus Seker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.14934v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.14934v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.04362v2</id>
    <updated>2022-02-23T05:18:20Z</updated>
    <published>2021-12-08T16:15:35Z</published>
    <title>Physics-based Mesh Deformation with Haptic Feedback and Material
  Anisotropy</title>
    <summary>  We present a physics-based framework to simulate porous, deformable materials
and interactive tools with haptic feedback that can reshape it. In order to
allow the material to be moulded non-homogeneously, we propose an algorithm to
change the material properties of the object depending on its water content. We
present a multi-resolution, multi-timescale simulation framework to enable
stable visual and haptic feedback at interactive rates. We test our model for
physical consistency, accuracy, interactivity and appeal through a user study
and quantitative performance evaluation.
</summary>
    <author>
      <name>Avirup Mandal</name>
    </author>
    <author>
      <name>Parag Chaudhuri</name>
    </author>
    <author>
      <name>Subhasis Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.04362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.04362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.09728v1</id>
    <updated>2021-12-17T19:27:43Z</updated>
    <published>2021-12-17T19:27:43Z</published>
    <title>Real-Time Path-Guiding Based on Parametric Mixture Models</title>
    <summary>  Path-Guiding algorithms for sampling scattering directions can drastically
decrease the variance of Monte Carlo estimators of Light Transport Equation,
but their usage was limited to offline rendering because of memory and
computational limitations. We introduce a new robust screen-space technique
that is based on online learning of parametric mixture models for guiding the
real-time path-tracing algorithm. It requires storing of 8 parameters for every
pixel, achieves a reduction of FLIP metric up to 4 times with 1 spp rendering.
Also, it consumes less than 1.5ms on RTX 2070 for 1080p and reduces
path-tracing timings by generating more coherent rays by about 5% on average.
Moreover, it leads to significant bias reduction and a lower level of
flickering of SVGF output.
</summary>
    <author>
      <name>Mikhail Derevyannykh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.09728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.09728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11835v1</id>
    <updated>2022-03-22T16:02:35Z</updated>
    <published>2022-03-22T16:02:35Z</published>
    <title>Rendering Layered Materials with Diffuse Interfaces</title>
    <summary>  In this work, we introduce a novel method to render, in real-time, Lambertian
surfaces with a rough dieletric coating. We show that the appearance of such
configurations is faithfully represented with two microfacet lobes accounting
for direct and indirect interactions respectively. We numerically fit these
lobes based on the first order directional statistics (energy, mean and
variance) of light transport using 5D tables and narrow them down to 2D + 1D
with analytical forms and dimension reduction. We demonstrate the quality of
our method by efficiently rendering rough plastics and ceramics, closely
matching ground truth. In addition, we improve a state-of-the-art layered
material model to include Lambertian interfaces.
</summary>
    <author>
      <name>Héloïse de Dinechin</name>
    </author>
    <author>
      <name>Laurent Belcour</name>
    </author>
    <link href="http://arxiv.org/abs/2203.11835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03237v1</id>
    <updated>2022-04-07T06:04:59Z</updated>
    <published>2022-04-07T06:04:59Z</published>
    <title>Rule-based Procedural Tree Modeling Approach</title>
    <summary>  In some entertainment and virtual reality applications, it is necessary to
model and draw the real world realistically, so as to improve the fidelity of
natural scenes and make users have a better sense of immersion. However, due to
the morphological structure of trees The complexity and variety present many
challenges for photorealistic modeling and rendering of trees. This paper
reviews the progress achieved in photorealistic modeling and rendering of tree
branches, leaves, and bark over the past few decades. The main achievement is
mainly a rule-based procedural tree modeling method.
</summary>
    <author>
      <name>Yinhui Yang</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Yuchi Huo</name>
    </author>
    <link href="http://arxiv.org/abs/2204.03237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06396v1</id>
    <updated>2022-04-13T13:57:51Z</updated>
    <published>2022-04-13T13:57:51Z</published>
    <title>Creating good quality meshes from smooth implicit surfaces</title>
    <summary>  Visualization of implicit surfaces is an actively researched topic. While
raytracing can produce high quality images, it is not well suited for creating
a quick preview of the surface. Indirect algorithms (e.g. Marching Cubes)
create an easily renderable triangle mesh, but the result is often not
sufficiently well-structured for a good approximation of differential surface
quantities (normals, curvatures, etc.). Post-processing methods usually have a
considerable computational overhead, and high quality is not guaranteed. We
propose a tessellation algorithm to create nearly isotropic meshes, using
multi-sided implicit surfaces.
</summary>
    <author>
      <name>Ágostons Sipos</name>
    </author>
    <author>
      <name>Péter Salvi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on the Advances of Information
  Technology, pp. 47-51, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.06396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09783v1</id>
    <updated>2022-04-20T20:48:47Z</updated>
    <published>2022-04-20T20:48:47Z</published>
    <title>TopoEmbedding, a web tool for the interactive analysis of persistent
  homology</title>
    <summary>  Software libraries for Topological Data Analysis (TDA) offer limited support
for interactive visualization. Most libraries only allow to visualize
topological descriptors (e.g., persistence diagrams), and lose the connection
with the original domain of data. This makes it challenging for users to
interpret the results of a TDA pipeline in an exploratory context. In this
paper, we present TopoEmbedding, a web-based tool that simplifies the
interactive visualization and analysis of persistence-based descriptors.
TopoEmbedding allows non-experts in TDA to explore similarities and differences
found by TDA descriptors with simple yet effective visualization techniques.
</summary>
    <author>
      <name>Xueyi Bao</name>
    </author>
    <author>
      <name>Guoxi Liu</name>
    </author>
    <author>
      <name>Federico Iuricich</name>
    </author>
    <link href="http://arxiv.org/abs/2204.09783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10170v1</id>
    <updated>2022-04-21T15:30:22Z</updated>
    <published>2022-04-21T15:30:22Z</published>
    <title>Data Parallel Path Tracing in Object Space</title>
    <summary>  We investigate the concept of rendering production-style content with full
path tracing in a data-distributed fashion -- that is, with multiple
collaborating nodes and/or GPUs that each store only part of the model. In
particular, we propose a new approach to tracing rays across different
nodes/GPUs that improves over traditional spatial partitioning, can support
both object-space and spatial partitioning (or any combination thereof), and
that enables multiple techniques for reducing the number of rays sent across
the network. We show that this approach can handle different kinds of model
partitioning strategies, and can ultimately render non-trivial models with full
path tracing even on quite moderate hardware resources with rather low-end
interconnect.
</summary>
    <author>
      <name>Ingo Wald</name>
    </author>
    <author>
      <name>Steven G Parker</name>
    </author>
    <link href="http://arxiv.org/abs/2204.10170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12301v1</id>
    <updated>2022-04-26T13:30:59Z</updated>
    <published>2022-04-26T13:30:59Z</published>
    <title>Designing Perceptual Puzzles by Differentiating Probabilistic Programs</title>
    <summary>  We design new visual illusions by finding "adversarial examples" for
principled models of human perception -- specifically, for probabilistic
models, which treat vision as Bayesian inference. To perform this search
efficiently, we design a differentiable probabilistic programming language,
whose API exposes MCMC inference as a first-class differentiable function. We
demonstrate our method by automatically creating illusions for three features
of human vision: color constancy, size constancy, and face perception.
</summary>
    <author>
      <name>Kartik Chandra</name>
    </author>
    <author>
      <name>Tzu-Mao Li</name>
    </author>
    <author>
      <name>Joshua Tenenbaum</name>
    </author>
    <author>
      <name>Jonathan Ragan-Kelley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3528233.3530715</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3528233.3530715" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages; 3 figures; SIGGRAPH '22 Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3110v1</id>
    <updated>2011-12-14T03:46:46Z</updated>
    <published>2011-12-14T03:46:46Z</published>
    <title>GPU-based Image Analysis on Mobile Devices</title>
    <summary>  With the rapid advances in mobile technology many mobile devices are capable
of capturing high quality images and video with their embedded camera. This
paper investigates techniques for real-time processing of the resulting images,
particularly on-device utilizing a graphical processing unit. Issues and
limitations of image processing on mobile devices are discussed, and the
performance of graphical processing units on a range of devices measured
through a programmable shader implementation of Canny edge detection.
</summary>
    <author>
      <name>Andrew Ensor</name>
    </author>
    <author>
      <name>Seth Hall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Image and Vision Computing New Zealand 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.3110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.1; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06995v1</id>
    <updated>2015-03-24T11:42:43Z</updated>
    <published>2015-03-24T11:42:43Z</published>
    <title>Interpolation of a spline developable surface between a curve and two
  rulings</title>
    <summary>  In this paper we address the problem of interpolating a spline developable
patch bounded by a given spline curve and the first and the last rulings of the
developable surface. In order to complete the boundary of the patch a second
spline curve is to be given. Up to now this interpolation problem could be
solved, but without the possibility of choosing both endpoints for the rulings.
We circumvent such difficulty here by resorting to degree elevation of the
developable surface. This is useful not only to solve this problem, but also
other problems dealing with triangular developable patches.
</summary>
    <author>
      <name>A. Cantón</name>
    </author>
    <author>
      <name>L. Fernández-Jambrina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1631/FITEE.14a0210</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1631/FITEE.14a0210" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers of Information Technology &amp; Electronic Engineering 16,
  173-190 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.06995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07790v1</id>
    <updated>2017-11-17T13:44:33Z</updated>
    <published>2017-11-17T13:44:33Z</published>
    <title>Solving Poisson's Equation on the Microsoft HoloLens</title>
    <summary>  We present a mixed reality application (HoloFEM) for the Microsoft HoloLens.
The application lets a user define and solve a physical problem governed by
Poisson's equation with the surrounding real world geometry as input data.
Holograms are used to visualise both the problem and the solution. The finite
element method is used to solve Poisson's equation. Solving and visualising
partial differential equations in mixed reality could have potential usage in
areas such as building planning and safety engineering.
</summary>
    <author>
      <name>Anders Logg</name>
    </author>
    <author>
      <name>Carl Lundholm</name>
    </author>
    <author>
      <name>Magne Nordaas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3139131.3141777</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3139131.3141777" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 23rd ACM Symposium on Virtual Reality
  Software and Technology (VRST 2017). ACM, New York, NY, USA, Article 87</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.07790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11326v1</id>
    <updated>2017-11-30T11:38:11Z</updated>
    <published>2017-11-30T11:38:11Z</published>
    <title>High Dynamic Range Imaging Technology</title>
    <summary>  In this lecture note, we describe high dynamic range (HDR) imaging systems;
such systems are able to represent luminances of much larger brightness and,
typically, also a larger range of colors than conventional standard dynamic
range (SDR) imaging systems. The larger luminance range greatly improve the
overall quality of visual content, making it appears much more realistic and
appealing to observers. HDR is one of the key technologies of the future
imaging pipeline, which will change the way the digital visual content is
represented and manipulated today.
</summary>
    <author>
      <name>Alessandro Artusi</name>
    </author>
    <author>
      <name>Thomas Richter</name>
    </author>
    <author>
      <name>Touradj Ebrahimi</name>
    </author>
    <author>
      <name>Rafal K. Mantiuk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSP.2017.2716957</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSP.2017.2716957" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Magazine ( Volume: 34, Issue: 5, Sept. 2017
  )</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.11326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02493v1</id>
    <updated>2018-05-07T13:09:21Z</updated>
    <published>2018-05-07T13:09:21Z</published>
    <title>GeneVis - An interactive visualization tool for combining
  cross-discipline datasets within genetics</title>
    <summary>  GeneVis is a web-based tool to visualize complementary data sets of different
disciplines within the field of genetics. It overlays gene-cluster information,
gene-interaction data and gene-disease association data by means of web-based
interactive graph visualizations. This allows an intuitive and quick assessment
of possible relations between the different datasets. By starting from a
high-level graph abstraction based on gene clusters, which can be selected for
detailed inspection at the gene-interaction level in a separate window, GeneVis
circumvents the common visual clutter problem when using gene datasets with a
high number of gene entries.
</summary>
    <author>
      <name>Casper van Leeuwen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09048v1</id>
    <updated>2018-05-23T10:51:02Z</updated>
    <published>2018-05-23T10:51:02Z</published>
    <title>Area-preserving parameterizations for spherical ellipses</title>
    <summary>  We present new methods for uniformly sampling the solid angle subtended by a
disk. To achieve this, we devise two novel area-preserving mappings from the
unit square $[0,1]^2$ to a spherical ellipse (i.e. the projection of the disk
onto the unit sphere). These mappings allow for low-variance stratified
sampling of direct illumination from disk-shaped light sources. We discuss how
to efficiently incorporate our methods into a production renderer and
demonstrate the quality of our maps, showing significantly lower variance than
previous work.
</summary>
    <author>
      <name>Ibón Guillén</name>
    </author>
    <author>
      <name>Carlos Ureña</name>
    </author>
    <author>
      <name>Alan King</name>
    </author>
    <author>
      <name>Marcos Fajardo</name>
    </author>
    <author>
      <name>Iliyan Georgiev</name>
    </author>
    <author>
      <name>Jorge López-Moreno</name>
    </author>
    <author>
      <name>Adrian Jarabo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.13234</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.13234" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum 36 (2017) 179-187</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.09048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01720v1</id>
    <updated>2018-09-05T20:25:28Z</updated>
    <published>2018-09-05T20:25:28Z</published>
    <title>Extending Mandelbox Fractals with Shape Inversions</title>
    <summary>  The Mandelbox is a recently discovered class of escape-time fractals which
use a conditional combination of reflection, spherical inversion, scaling, and
translation to transform points under iteration. In this paper we introduce a
new extension to Mandelbox fractals which replaces spherical inversion with a
more generalized shape inversion. We then explore how this technique can be
used to generate new fractals in 2D, 3D, and 4D.
</summary>
    <author>
      <name>Gregg Helt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">slightly modified version of paper published in conferences
  proceedings of Bridges 2018: Mathematics, Art, Music, Architecture,
  Education, Culture, "Extending Mandelbox Fractals with Shape Inversions",
  Gregg Helt, July 2018, pp. 547-550</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07009v2</id>
    <updated>2018-09-28T04:09:35Z</updated>
    <published>2018-09-19T04:19:28Z</published>
    <title>Light Field Neural Network</title>
    <summary>  We introduce an optical neural network system made by off-the-shelf
components. In order to test the evaluate the physical property of the proposed
system, we are making a prototype. After further discussions with our
cooperators, we are agreed that the prototype implementation may take longer
time than we expected earlier. Therefore we reach a consensus on withdrawing
the paper until the physical data is available.
</summary>
    <author>
      <name>Yuchi Huo</name>
    </author>
    <author>
      <name>Sung-Eui Yoon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Need some time to produce and test the prototype</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.07009v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07009v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.10515v1</id>
    <updated>2018-12-16T18:15:13Z</updated>
    <published>2018-12-16T18:15:13Z</published>
    <title>Derivation of an Algorithm for Calculation of the Intersection Area of a
  Circle with a Grid with Finite Fill Factor</title>
    <summary>  The problem deals with an exact calculation of the intersection area of a
circle arbitrary placed on a grid of square shaped elements with gaps between
them (finite fill factor). Usually an approximation is used for the calculation
of the intersection area of the circle and the squares of the grid. We analyze
the geometry of the problem and derive an algorithm for the exact computation
of the intersection areas. The results of the analysis are summarized in the
tally sheet. In a real world example this might be a CCD or CMOS chip, or the
tile structure of a floor.
</summary>
    <author>
      <name>Dmitrij Gendler</name>
    </author>
    <author>
      <name>Christian Eisele</name>
    </author>
    <author>
      <name>Dirk Seiffer</name>
    </author>
    <author>
      <name>Norbert Wendelstein</name>
    </author>
    <link href="http://arxiv.org/abs/1812.10515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05710v1</id>
    <updated>2019-06-13T14:14:32Z</updated>
    <published>2019-06-13T14:14:32Z</published>
    <title>RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed
  Joints and Precision-Cut Rods</title>
    <summary>  We present RodSteward, a design-to-assembly system for creating
furniture-scale structures composed of 3D printed joints and precision-cut
rods. The RodSteward systems consists of: RSDesigner, a fabrication-aware
design interface that visualizes accurate geometries during edits and
identifies infeasible designs; physical fabrication of parts via novel fully
automatic construction of solid 3D-printable joint geometries and automatically
generated cutting plans for rods; and RSAssembler, a guided-assembly interface
that prompts the user to place parts in order while showing a focus+context
visualization of the assembly in progress. We demonstrate the effectiveness of
our tools with a number of example constructions of varying complexity, style
and parameter choices.
</summary>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06751v1</id>
    <updated>2019-06-16T18:52:31Z</updated>
    <published>2019-06-16T18:52:31Z</published>
    <title>Pi-surfaces: products of implicit surfaces towards constructive
  composition of 3D objects</title>
    <summary>  Implicit functions provide a fundamental basis to model 3D objects, no matter
they are rigid or deformable, in computer graphics and geometric modeling. This
paper introduces a new constructive scheme of implicitly-defined 3D objects
based on products of implicit functions. This scheme is in contrast with
popular approaches like blobbies, meta balls and soft objects, which rely on
the sum of specific implicit functions to fit a 3D object to a set of spheres.
</summary>
    <author>
      <name>Adriano N. Raposo</name>
    </author>
    <author>
      <name>Abel J. P. Gomes</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WSCG 2019 27. International Conference in Central Europe on
  Computer Graphics, Visualization and Computer Vision</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.06751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11633v1</id>
    <updated>2019-06-26T16:58:46Z</updated>
    <published>2019-06-26T16:58:46Z</published>
    <title>ORRB -- OpenAI Remote Rendering Backend</title>
    <summary>  We present the OpenAI Remote Rendering Backend (ORRB), a system that allows
fast and customizable rendering of robotics environments. It is based on the
Unity3d game engine and interfaces with the MuJoCo physics simulation library.
ORRB was designed with visual domain randomization in mind. It is optimized for
cloud deployment and high throughput operation. We are releasing it to the
public under a liberal MIT license: https://github.com/openai/orrb .
</summary>
    <author>
      <name>Maciek Chociej</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Lilian Weng</name>
    </author>
    <link href="http://arxiv.org/abs/1906.11633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.01456v1</id>
    <updated>2019-09-03T21:15:24Z</updated>
    <published>2019-09-03T21:15:24Z</published>
    <title>Topologically-Guided Color Image Enhancement</title>
    <summary>  Enhancement is an important step in post-processing digital images for
personal use, in medical imaging, and for object recognition. Most existing
manual techniques rely on region selection, similarity, and/or thresholding for
editing, never really considering the topological structure of the image. In
this paper, we leverage the contour tree to extract a hierarchical
representation of the topology of an image. We propose 4 topology-aware
transfer functions for editing features of the image using local topological
properties, instead of global image properties. Finally, we evaluate our
approach with grayscale and color images.
</summary>
    <author>
      <name>Junyi Tu</name>
    </author>
    <author>
      <name>Paul Rosen</name>
    </author>
    <link href="http://arxiv.org/abs/1909.01456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11657v2</id>
    <updated>2020-08-30T08:09:00Z</updated>
    <published>2020-08-26T16:33:42Z</published>
    <title>Test Scene Design for Physically Based Rendering</title>
    <summary>  Physically based rendering is a discipline in computer graphics which aims at
reproducing certain light and material appearances that occur in the real
world. Complex scenes can be difficult to compute for rendering algorithms.
This paper introduces a new comprehensive test database of scenes that treat
different light setups in conjunction with diverse materials and discusses its
design principles. A lot of research is focused on the development of new
algorithms that can deal with difficult light conditions and materials
efficiently. This database delivers a comprehensive foundation for evaluating
existing and newly developed rendering techniques. A final evaluation compares
different results of different rendering algorithms for all scenes.
</summary>
    <author>
      <name>Elias Brugger</name>
    </author>
    <author>
      <name>Christian Freude</name>
    </author>
    <author>
      <name>Michael Wimmer</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02480v2</id>
    <updated>2021-03-15T09:22:30Z</updated>
    <published>2020-09-05T07:05:11Z</published>
    <title>Trimmed Spline Surfaces with Accurate Boundary Control</title>
    <summary>  We introduce trimmed NURBS surfaces with accurate boundary control, briefly
called ABC-surfaces, as a solution to the notorious problem of constructing
watertight or smooth ($G^1$ and $G^2)$ multi-patch surfaces within the function
range of standard CAD/CAM systems and the associated file exchange formats. Our
construction is based on the appropriate blend of a base surface, which traces
out the intended global shape, and a series of reparametrized ribbons, which
dominate the shape near the boundary.
</summary>
    <author>
      <name>Florian Martin</name>
    </author>
    <author>
      <name>Ulrich Reif</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04077v2</id>
    <updated>2020-10-18T16:02:31Z</updated>
    <published>2020-10-08T15:59:52Z</published>
    <title>Temporally-smooth Antialiasing and Lens Distortion with Rasterization
  Map</title>
    <summary>  Current GPU rasterization procedure is limited to narrow views in rectilinear
perspective. While industries demand curvilinear perspective in wide-angle
views, like Virtual Reality and Virtual Film Production industry. This paper
delivers new rasterization method using industry-standard STMaps. Additionally
new antialiasing rasterization method is proposed, which outperforms MSAA in
both quality and performance. It is an improvement upon previous solutions
found in paper Perspective picture from Visual Sphere by yours truly.
</summary>
    <author>
      <name>Jakub Maximilian Fober</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 7 figures, 8 code listings</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.04077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08735v1</id>
    <updated>2020-10-17T07:32:32Z</updated>
    <published>2020-10-17T07:32:32Z</published>
    <title>Real-time High-Quality Rendering of Non-Rotating Black Holes</title>
    <summary>  We propose a real-time method to render high-quality images of a non-rotating
black hole with an accretion disc and background stars. Our method is based on
beam tracing, but uses precomputed tables to find the intersections of each
curved light beam with the scene in constant time per pixel. It also uses a
specific texture filtering scheme to integrate the contribution of the light
sources to each beam. Our method is simple to implement and achieves high frame
rates.
</summary>
    <author>
      <name>Eric Bruneton</name>
    </author>
    <link href="http://arxiv.org/abs/2010.08735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13864v1</id>
    <updated>2020-10-12T10:22:28Z</updated>
    <published>2020-10-12T10:22:28Z</published>
    <title>Diptychs of human and machine perceptions</title>
    <summary>  We propose visual creations that put differences in algorithms and humans
\emph{perceptions} into perspective. We exploit saliency maps of neural
networks and visual focus of humans to create diptychs that are
reinterpretations of an original image according to both machine and human
attentions. Using those diptychs as a qualitative evaluation of perception, we
discuss some crucial issues of current \textit{task-oriented} artificial
intelligence.
</summary>
    <author>
      <name>Vivien Cabannes</name>
    </author>
    <author>
      <name>Thomas Kerdreux</name>
    </author>
    <author>
      <name>Louis Thiry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 36 images</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">creativity workshop NeurIPS 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.13864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05550v1</id>
    <updated>2020-11-11T05:18:53Z</updated>
    <published>2020-11-11T05:18:53Z</published>
    <title>Diffusion Structures for Architectural Stripe Pattern Generation</title>
    <summary>  We present Diffusion Structures, a family of resilient shell structures from
the eigenfunctions of a pair of novel diffusion operators. This approach is
based on Michell's theorem but avoids expensive non-linear optimization with
computation that amounts to constructing and solving two generalized eigenvalue
problems to generate two sets of stripe patterns. This structure family can be
generated quickly, and navigated in real-time using a small number of tuneable
parameters.
</summary>
    <author>
      <name>Abhishek Madan</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09657v1</id>
    <updated>2020-11-19T05:19:25Z</updated>
    <published>2020-11-19T05:19:25Z</published>
    <title>Assembling a Pipeline for 3D Face Interpolation</title>
    <summary>  This paper describes a pipeline built with open source tools for
interpolating 3D facial expressions taken from images. The presented approach
allows anyone to create 3D face animations from 2 input photos: one from the
start face expression, and the other from the final face expression. Given the
input photos, corresponding 3D face models are constructed and texture-mapped
using the photos as textures aligned with facial features. Animations are then
generated by morphing the models by interpolation of the geometries and
textures of the models. This work was performed as a MS project at the
University of California, Merced.
</summary>
    <author>
      <name>Yusuke Niiro</name>
    </author>
    <author>
      <name>Marcelo Kallmann</name>
    </author>
    <link href="http://arxiv.org/abs/2011.09657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10257v1</id>
    <updated>2020-11-20T08:08:00Z</updated>
    <published>2020-11-20T08:08:00Z</published>
    <title>Perceptual Evaluation of Liquid Simulation Methods</title>
    <summary>  This paper proposes a novel framework to evaluate fluid simulation methods
based on crowd-sourced user studies in order to robustly gather large numbers
of opinions. The key idea for a robust and reliable evaluation is to use a
reference video from a carefully selected real-world setup in the user study.
By conducting a series of controlled user studies and comparing their
evaluation results, we observe various factors that affect the perceptual
evaluation. Our data show that the availability of a reference video makes the
evaluation consistent. We introduce this approach for computing scores of
simulation methods as visual accuracy metric. As an application of the proposed
framework, a variety of popular simulation methods are evaluated.
</summary>
    <author>
      <name>Kiwon Um</name>
    </author>
    <author>
      <name>Xiangyu Hu</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3072959.3073633</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3072959.3073633" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Details at: http://ge.in.tum.de/publications/2017-sig-um/</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.10257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.02708v1</id>
    <updated>2021-07-06T16:13:52Z</updated>
    <published>2021-07-06T16:13:52Z</published>
    <title>Exact Analytical Parallel Vectors</title>
    <summary>  This paper demonstrates that parallel vector curves are piecewise cubic
rational curves in 3D piecewise linear vector fields. Parallel vector curves --
loci of points where two vector fields are parallel -- have been widely used to
extract features including ridges, valleys, and vortex core lines in scientific
data. We define the term \emph{generalized and underdetermined eigensystem} in
the form of
$\mathbf{A}\mathbf{x}+\mathbf{a}=\lambda(\mathbf{B}\mathbf{x}+\mathbf{b})$ in
order to derive the piecewise rational representation of 3D parallel vector
curves. We discuss how singularities of the rationals lead to different types
of intersections with tetrahedral cells.
</summary>
    <author>
      <name>Hanqi Guo</name>
    </author>
    <author>
      <name>Tom Peterka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VIS49827.2021.9623310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VIS49827.2021.9623310" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE Visualization Conference (VIS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.02708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04875v1</id>
    <updated>2021-07-10T16:48:14Z</updated>
    <published>2021-07-10T16:48:14Z</published>
    <title>Never 'Drop the Ball' in the Operating Room: An efficient hand-based VR
  HMD controller interpolation algorithm, for collaborative, networked virtual
  environments</title>
    <summary>  In this work, we propose two algorithms that can be applied in the context of
a networked virtual environment to efficiently handle the interpolation of
displacement data for hand-based VR HMDs. Our algorithms, based on the use of
dual-quaternions and multivectors respectively, impact the network consumption
rate and are highly effective in scenarios involving multiple users. We
illustrate convincing results in a modern game engine and a medical VR
collaborative training scenario.
</summary>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Nick Lydatakis</name>
    </author>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures, Initial paper submitted and accepted to CGI2021
  - ENGAGE Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.04875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12900v1</id>
    <updated>2021-07-11T13:24:57Z</updated>
    <published>2021-07-11T13:24:57Z</published>
    <title>Projector Pixel Redirection Using Phase-Only Spatial Light Modulator</title>
    <summary>  In projection mapping from a projector to a non-planar surface, the pixel
density on the surface becomes uneven. This causes the critical problem of
local spatial resolution degradation. We confirmed that the pixel density
uniformity on the surface was improved by redirecting projected rays using a
phase-only spatial light modulator.
</summary>
    <author>
      <name>Haruka Terai</name>
    </author>
    <author>
      <name>Daisuke Iwai</name>
    </author>
    <author>
      <name>Kosuke Sato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Author's version of a paper published at IDW (International Display
  Workshops) 2020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the International Display Workshops, pp.
  663-665, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.12900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.00094v1</id>
    <updated>2022-01-01T02:26:35Z</updated>
    <published>2022-01-01T02:26:35Z</published>
    <title>Wavelet Transparency</title>
    <summary>  Order-independent transparency schemes rely on low-order approximations of
transmittance as a function of depth. We introduce a new wavelet representation
of this function and an algorithm for building and evaluating it efficiently on
a GPU. We then extend the order-independent Phenomenological Transparency
algorithm to our representation and introduce a new phenomenological
approximation of chromatic aberration under refraction. This generates
comparable image quality to reference A-buffering for challenging cases such as
smoke coverage, more realistic refraction, and comparable or better performance
and bandwidth to the state-of-the-art Moment transparency with a simpler
implementation.
</summary>
    <author>
      <name>Maksim Aizenshtein</name>
    </author>
    <author>
      <name>Niklas Smal</name>
    </author>
    <author>
      <name>Morgan McGuire</name>
    </author>
    <link href="http://arxiv.org/abs/2201.00094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08266v4</id>
    <updated>2022-04-27T07:06:50Z</updated>
    <published>2022-01-20T16:16:35Z</published>
    <title>A Real-Time Rendering Method for Light Field Display</title>
    <summary>  A real-time elemental image array (EIA) generation method which does not
sacrifice accuracy nor rely on high-performance hardware is developed, through
raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both
offline and online working flow, experiments will verified the effectiveness.
</summary>
    <author>
      <name>Quanzhen Wan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We are reminded by our supervisors and peers that we have not taken
  many potential influential factors into consideration, which might lead to a
  rather different outcome. If the whole idea will be certified correctly in
  the future, we will resubmit our updated version at that time</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.08266v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08266v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08354v1</id>
    <updated>2022-01-19T16:22:57Z</updated>
    <published>2022-01-19T16:22:57Z</published>
    <title>HPCGen: Hierarchical K-Means Clustering and Level Based Principal
  Components for Scan Path Genaration</title>
    <summary>  In this paper, we present a new approach for decomposing scan paths and its
utility for generating new scan paths. For this purpose, we use the K-Means
clustering procedure to the raw gaze data and subsequently iteratively to find
more clusters in the found clusters. The found clusters are grouped for each
level in the hierarchy, and the most important principal components are
computed from the data contained in them. Using this tree hierarchy and the
principal components, new scan paths can be generated that match the human
behavior of the original data. We show that this generated data is very useful
for generating new data for scan path classification but can also be used to
generate fake scan paths.
</summary>
    <author>
      <name>Wolfgang Fuhl</name>
    </author>
    <link href="http://arxiv.org/abs/2201.08354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.00948v3</id>
    <updated>2022-06-15T13:25:40Z</updated>
    <published>2022-02-02T10:49:08Z</published>
    <title>Eikonal Fields for Refractive Novel-View Synthesis</title>
    <summary>  We tackle the problem of generating novel-view images from collections of 2D
images showing refractive and reflective objects. Current solutions assume
opaque or transparent light transport along straight paths following the
emission-absorption model. Instead, we optimize for a field of 3D-varying Index
of Refraction (IoR) and trace light through it that bends toward the spatial
gradients of said IoR according to the laws of eikonal light transport.
</summary>
    <author>
      <name>Mojtaba Bemana</name>
    </author>
    <author>
      <name>Karol Myszkowski</name>
    </author>
    <author>
      <name>Jeppe Revall Frisvad</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Tobias Ritschel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, project webpage:
  https://eikonalfield.mpi-inf.mpg.de</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.00948v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.00948v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02309v1</id>
    <updated>2022-02-04T18:46:32Z</updated>
    <published>2022-02-04T18:46:32Z</published>
    <title>Neural Collision Detection for Deformable Objects</title>
    <summary>  We propose a neural network-based approach for collision detection with
deformable objects. Unlike previous approaches based on bounding volume
hierarchies, our neural approach does not require an update of the spatial data
structure when the object deforms. Our network is trained on the reduced
degrees of freedom of the object, so that we can use the same network to query
for collisions even when the object deforms. Our approach is simple to use and
implement, and it can readily be employed on the GPU. We demonstrate our
approach with two concrete examples: a haptics application with a finite
element mesh, and cloth simulation with a skinned character.
</summary>
    <author>
      <name>Ryan S. Zesch</name>
    </author>
    <author>
      <name>Bethany R. Witemeyer</name>
    </author>
    <author>
      <name>Ziyan Xiong</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <author>
      <name>Shinjiro Sueda</name>
    </author>
    <link href="http://arxiv.org/abs/2202.02309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12020v1</id>
    <updated>2022-02-24T11:05:06Z</updated>
    <published>2022-02-24T11:05:06Z</published>
    <title>Point Containment Queries on Ray Tracing Cores for AMR Flow
  Visualization</title>
    <summary>  Modern GPUs come with dedicated hardware to perform ray/triangle
intersections and bounding volume hierarchy (BVH) traversal. While the primary
use case for this hardware is photorealistic 3D computer graphics, with careful
algorithm design scientists can also use this special-purpose hardware to
accelerate general-purpose computations such as point containment queries. This
article explains the principles behind these techniques and their application
to vector field visualization of large simulation data using particle tracing.
</summary>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <author>
      <name>Daniel Seifried</name>
    </author>
    <author>
      <name>Nate Morrical</name>
    </author>
    <author>
      <name>Ingo Wald</name>
    </author>
    <author>
      <name>Will Usher</name>
    </author>
    <author>
      <name>Jamie A. P. Law-Smith</name>
    </author>
    <author>
      <name>Stefanie Walch-Gassner</name>
    </author>
    <author>
      <name>André Hinkenjann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCSE.2022.3153677</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCSE.2022.3153677" rel="related"/>
    <link href="http://arxiv.org/abs/2202.12020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12670v1</id>
    <updated>2022-02-25T13:05:39Z</updated>
    <published>2022-02-25T13:05:39Z</published>
    <title>Hex-Mesh Generation and Processing: a Survey</title>
    <summary>  In this article, we provide a detailed survey of techniques for hexahedral
mesh generation. We cover the whole spectrum of alternative approaches to mesh
generation, as well as post processing algorithms for connectivity editing and
mesh optimization. For each technique, we highlight capabilities and
limitations, also pointing out the associated unsolved challenges. Recent
relaxed approaches, aiming to generate not pure-hex but hex-dominant meshes,
are also discussed. The required background, pertaining to geometrical as well
as combinatorial aspects, is introduced along the way.
</summary>
    <author>
      <name>Nico Pietroni</name>
    </author>
    <author>
      <name>Marcel Campen</name>
    </author>
    <author>
      <name>Alla Sheffer</name>
    </author>
    <author>
      <name>Gianmarco Cherchi</name>
    </author>
    <author>
      <name>David Bommes</name>
    </author>
    <author>
      <name>Xifeng Gao</name>
    </author>
    <author>
      <name>Riccardo Scateni</name>
    </author>
    <author>
      <name>Franck Ledoux</name>
    </author>
    <author>
      <name>Jean-Francois Remacle</name>
    </author>
    <author>
      <name>Marco Livesu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3554920</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3554920" rel="related"/>
    <link href="http://arxiv.org/abs/2202.12670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.08028v1</id>
    <updated>2022-05-16T23:58:01Z</updated>
    <published>2022-05-16T23:58:01Z</published>
    <title>Browser-based Hyperbolic Visualization of Graphs</title>
    <summary>  Hyperbolic geometry offers a natural focus + context for data visualization
and has been shown to underlie real-world complex networks. However, current
hyperbolic network visualization approaches are limited to special types of
networks and do not scale to large datasets. With this in mind, we designed,
implemented, and analyzed three methods for hyperbolic visualization of
networks in the browser based on inverse projections, generalized
force-directed algorithms, and hyperbolic multi-dimensional scaling (H-MDS). A
comparison with Euclidean MDS shows that H-MDS produces embeddings with lower
distortion for several types of networks. All three methods can handle
node-link representations and are available in fully functional web-based
systems.
</summary>
    <author>
      <name>Jacob Miller</name>
    </author>
    <author>
      <name>Stephen Kobourov</name>
    </author>
    <author>
      <name>Vahan Huroyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE PacificVis 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.08028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.08028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.15573v1</id>
    <updated>2022-05-31T07:25:46Z</updated>
    <published>2022-05-31T07:25:46Z</published>
    <title>Text/Speech-Driven Full-Body Animation</title>
    <summary>  Due to the increasing demand in films and games, synthesizing 3D avatar
animation has attracted much attention recently. In this work, we present a
production-ready text/speech-driven full-body animation synthesis system. Given
the text and corresponding speech, our system synthesizes face and body
animations simultaneously, which are then skinned and rendered to obtain a
video stream output. We adopt a learning-based approach for synthesizing facial
animation and a graph-based approach to animate the body, which generates
high-quality avatar animation efficiently and robustly. Our results demonstrate
the generated avatar animations are realistic, diverse and highly
text/speech-correlated.
</summary>
    <author>
      <name>Wenlin Zhuang</name>
    </author>
    <author>
      <name>Jinwei Qi</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Bang Zhang</name>
    </author>
    <author>
      <name>Ping Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI-2022 demo track, video see https://youtu.be/MipiwU3Em_8</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.15573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07798v1</id>
    <updated>2022-06-15T20:22:16Z</updated>
    <published>2022-06-15T20:22:16Z</published>
    <title>Gaussian Blue Noise</title>
    <summary>  Among the various approaches for producing point distributions with blue
noise spectrum, we argue for an optimization framework using Gaussian kernels.
We show that with a wise selection of optimization parameters, this approach
attains unprecedented quality, provably surpassing the current state of the art
attained by the optimal transport (BNOT) approach. Further, we show that our
algorithm scales smoothly and feasibly to high dimensions while maintaining the
same quality, realizing unprecedented high-quality high-dimensional blue noise
sets. Finally, we show an extension to adaptive sampling.
</summary>
    <author>
      <name>Abdalla G. M. Ahmed</name>
    </author>
    <author>
      <name>Jing Ren</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <link href="http://arxiv.org/abs/2206.07798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.08422v2</id>
    <updated>2023-05-10T13:34:50Z</updated>
    <published>2022-06-16T19:48:00Z</published>
    <title>Real-time motion amplification on mobile devices</title>
    <summary>  A simple motion amplification algorithm suitable for real-time applications
on mobile devices, including smartphones, is presented. It is based on motion
enhancement by moving average differencing (MEMAD), a temporal high-pass filter
for video streams. MEMAD can amplify small moving objects or subtle motion in
larger objects. It is computationally sufficiently simple to be implemented in
real time on smartphones. In the specific implementation as an Android phone
app, MEMAD is demonstrated on examples chosen such as to motivate applications
in the engineering, biological, and medical sciences.
</summary>
    <author>
      <name>Henning U. Voss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplemental data at https://doi.org/10.6084/m9.figshare.20084981.v2.
  Changes to v1: Inclusion of offline video processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.08422v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.08422v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15236v2</id>
    <updated>2022-09-20T19:29:37Z</updated>
    <published>2022-06-30T12:34:11Z</published>
    <title>Stochastic Poisson Surface Reconstruction</title>
    <summary>  We introduce a statistical extension of the classic Poisson Surface
Reconstruction algorithm for recovering shapes from 3D point clouds. Instead of
outputting an implicit function, we represent the reconstructed shape as a
modified Gaussian Process, which allows us to conduct statistical queries
(e.g., the likelihood of a point in space being on the surface or inside a
solid). We show that this perspective: improves PSR's integration into the
online scanning process, broadens its application realm, and opens the door to
other lines of research such as applying task-specific priors.
</summary>
    <author>
      <name>Silvia Sellán</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/2206.15236v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15236v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.05415v1</id>
    <updated>2022-07-12T09:26:05Z</updated>
    <published>2022-07-12T09:26:05Z</published>
    <title>Rendering along the Hilbert Curve</title>
    <summary>  Based on the seminal work on Array-RQMC methods and rank-1 lattice sequences
by Pierre L'Ecuyer and collaborators, we introduce efficient deterministic
algorithms for image synthesis. Enumerating a low discrepancy sequence along
the Hilbert curve superimposed on the raster of pixels of an image, we achieve
noise characteristics that are desirable with respect to the human visual
system, especially at very low sampling rates. As compared to the state of the
art, our simple algorithms neither require randomization, nor costly
optimization, nor lookup tables. We analyze correlations of space-filling
curves and low discrepancy sequences, and demonstrate the benefits of the new
algorithms in a professional, massively parallel light transport simulation and
rendering system.
</summary>
    <author>
      <name>Alexander Keller</name>
    </author>
    <author>
      <name>Carsten Wächter</name>
    </author>
    <author>
      <name>Nikolaus Binder</name>
    </author>
    <link href="http://arxiv.org/abs/2207.05415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.05415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.07695v1</id>
    <updated>2022-07-15T18:35:15Z</updated>
    <published>2022-07-15T18:35:15Z</published>
    <title>An Exact Bitwise Reversible Integrator</title>
    <summary>  At a fundamental level most physical equations are time reversible. In this
paper we propose an integrator that preserves this property at the discrete
computational level. Our simulations can be run forward and backwards and trace
the same path exactly bitwise. We achieve this by implementing theoretically
reversible integrators using a mix of fixed and floating point arithmetic. Our
main application is in efficiently implementing the reverse step in the adjoint
method used in optimization. Our integrator has applications in differential
simulations and machine learning (backpropagation).
</summary>
    <author>
      <name>Jos Stam</name>
    </author>
    <link href="http://arxiv.org/abs/2207.07695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12011v1</id>
    <updated>2022-07-25T09:39:22Z</updated>
    <published>2022-07-25T09:39:22Z</published>
    <title>RayPC: Interactive Ray Tracing Meets Parallel Coordinates</title>
    <summary>  Large-scale numerical simulations of planetary interiors require dedicated
visualization algorithms that are able to efficiently extract a large amount of
information in an interactive and user-friendly way. Here we present a software
framework for the visualization of mantle convection data. This framework
combines real-time volume rendering, pathline visualization, and parallel
coordinates to explore the fluid dynamics in an interactive way and to identify
correlations between various output variables.
</summary>
    <author>
      <name>Jonathan Fritsch</name>
    </author>
    <author>
      <name>Markus Flatken</name>
    </author>
    <author>
      <name>Simon Schneegans</name>
    </author>
    <author>
      <name>Andreas Gerndt</name>
    </author>
    <author>
      <name>Ana-Catalina Plesa</name>
    </author>
    <author>
      <name>Christian Hüttig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE VIS 2021 as part of the SciVis contest</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.03674v1</id>
    <updated>2022-08-07T08:41:24Z</updated>
    <published>2022-08-07T08:41:24Z</published>
    <title>Projective Geometry, Duality and Plucker Coordinates for Geometric
  Computations with Determinants on GPUs</title>
    <summary>  Many algorithms used are based on geometrical computation. There are several
criteria in selecting appropriate algorithm from already known. Recently, the
fastest algorithms have been preferred. Nowadays, algorithms with a high
stability are preferred. Also technology and computer architecture, like GPU
etc., plays a significant role for large data processing. However, some
algorithms are ill-conditioned due to numerical representation used; result of
the floating point representation. In this paper, relations between projective
representation, duality and Plucker coordinates will be explored with
demonstration on simple geometric examples. The presented approach is
convenient especially for application on GPUs or vector-vector computational
architectures
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1708.06684</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.03674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68xx, 68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.03677v1</id>
    <updated>2022-08-07T08:50:07Z</updated>
    <published>2022-08-07T08:50:07Z</published>
    <title>A New Robust Algorithm for Computation of a Triangle Circumscribed
  Sphere in E3 and a Hypersphere Simplex</title>
    <summary>  There are many applications in which a bounding sphere containing the given
triangle E3 is needed, e.g. fast collision detection, ray-triangle intersecting
in raytracing etc. This is a typical geometrical problem in E3 and it has also
applications in computational problems in general. In this paper a new fast and
robust algorithm of circumscribed sphere computation in the -dimensional space
is presented and specification for the E3 space is given, too. The presented
method is convenient for use on GPU or with SSE or Intel AVX instructions on a
standard CPU
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <link href="http://arxiv.org/abs/2208.03677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68xx, 68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04370v1</id>
    <updated>2022-08-08T19:11:36Z</updated>
    <published>2022-08-08T19:11:36Z</published>
    <title>CLIP-based Neural Neighbor Style Transfer for 3D Assets</title>
    <summary>  We present a method for transferring the style from a set of images to a 3D
object. The texture appearance of an asset is optimized with a differentiable
renderer in a pipeline based on losses using pretrained deep neural networks.
More specifically,
  we utilize a nearest-neighbor feature matching loss with CLIP-ResNet50 to
extract the style from images. We show that a CLIP- based style loss provides a
different appearance over a VGG-based loss by focusing more on texture over
geometric shapes.
  Additionally, we extend the loss to support multiple images and enable
loss-based control over the color palette combined with automatic color palette
extraction from style images.
</summary>
    <author>
      <name>Shailesh Mishra</name>
    </author>
    <author>
      <name>Jonathan Granskog</name>
    </author>
    <link href="http://arxiv.org/abs/2208.04370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04728v1</id>
    <updated>2022-08-07T09:29:33Z</updated>
    <published>2022-08-07T09:29:33Z</published>
    <title>A New Approach to Line-Sphere and Line-Quadrics Intersection Detection
  and Computation</title>
    <summary>  Line intersection with convex and un-convex polygons or polyhedron algorithms
are well known as line clipping algorithms and very often used in computer
graphics. Rendering of geometrical problems often leads to ray tracing
techniques, when an intersection of many lines with spheres or quadrics is a
critical issue due to ray-tracing algorithm complexity. A new formulation of
detection and computation of the intersection of line (ray) with a quadric
surface is presented, which separates geometric properties of the line and
quadrics that enables pre-computation. The presented approach is especially
convenient for implementation with SSE instructions or on GPU
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4913058</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4913058" rel="related"/>
    <link href="http://arxiv.org/abs/2208.04728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68xx, 68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10815v1</id>
    <updated>2022-08-23T08:59:41Z</updated>
    <published>2022-08-23T08:59:41Z</published>
    <title>Parameterization-Independent Importance Sampling of Environment Maps</title>
    <summary>  Environment maps with high dynamic range lighting, such as daylight sky maps,
require importance sampling to keep the balance between noise and number of
samples per pixel manageable. Typically, importance sampling schemes for
environment maps are based directly on the map parameterization, e.g.
equirectangular maps, and do not work with alternative parameterizations that
might provide better sampling quality. In this paper, an importance sampling
scheme based on an equal-area projection of the sphere is proposed that is easy
to implement and works independently of the environment map parameterization or
resolution. This allows to apply the same scheme to equirectangular maps, cube
map variants, or any other map representation, and to adapt the importance
sampling granularity to the requirements of the map contents.
</summary>
    <author>
      <name>Martin Lambers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.10815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04449v1</id>
    <updated>2022-10-10T06:08:24Z</updated>
    <published>2022-10-10T06:08:24Z</published>
    <title>RTSDF: Generating Signed Distance Fields in Real Time for Soft Shadow
  Rendering</title>
    <summary>  Signed Distance Fields (SDFs) for surface representation are commonly
generated offline and subsequently loaded into interactive applications like
games. Since they are not updated every frame, they only provide a rigid
surface representation. While there are methods to generate them quickly on
GPU, the efficiency of these approaches is limited at high resolutions. This
paper showcases a novel technique that combines jump flooding and ray tracing
to generate approximate SDFs in real-time for soft shadow approximation,
achieving prominent shadow penumbras while maintaining interactive frame rates.
</summary>
    <author>
      <name>Yu Wei Tan</name>
    </author>
    <author>
      <name>Nicholas Chua</name>
    </author>
    <author>
      <name>Clarence Koh</name>
    </author>
    <author>
      <name>Anand Bhojan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2312/pg.20201232</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2312/pg.20201232" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pacific Graphics Short Papers, Posters, and Work-in-Progress
  Papers (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.04449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04981v1</id>
    <updated>2022-10-10T19:30:00Z</updated>
    <published>2022-10-10T19:30:00Z</published>
    <title>Hybrid DoF: Ray-Traced and Post-Processed Hybrid Depth of Field Effect
  for Real-Time Rendering</title>
    <summary>  Depth of Field (DoF) in games is usually achieved as a post-process effect by
blurring pixels in the sharp rasterized image based on the defined focus plane.
This paper describes a novel real-time DoF technique that uses ray tracing with
image filtering to achieve more accurate partial occlusion semi-transparencies
on edges of blurry foreground geometry. This hybrid rendering technique
leverages ray tracing hardware acceleration as well as spatio-temporal
reconstruction techniques to achieve interactive frame rates.
</summary>
    <author>
      <name>Yu Wei Tan</name>
    </author>
    <author>
      <name>Nicholas Chua</name>
    </author>
    <author>
      <name>Nathan Biette</name>
    </author>
    <author>
      <name>Anand Bhojan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3388770.3407426</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3388770.3407426" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGGRAPH 2020 Posters</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.04981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.05364v1</id>
    <updated>2022-10-11T11:47:59Z</updated>
    <published>2022-10-11T11:47:59Z</published>
    <title>Hybrid MBlur: Using Ray Tracing to Solve the Partial Occlusion Artifacts
  in Real-Time Rendering of Motion Blur Effect</title>
    <summary>  For a foreground object in motion, details of its background which would
otherwise be hidden are uncovered through its inner blur. This paper presents a
novel hybrid motion blur rendering technique combining post-process image
filtering and hardware-accelerated ray tracing. In each frame, we advance rays
recursively into the scene to retrieve background information for inner blur
regions and apply a post-process filtering pass on the ray-traced background
and rasterized colour before compositing them together. Our approach achieves
more accurate partial occlusion semi-transparencies for moving objects while
maintaining interactive frame rates.
</summary>
    <author>
      <name>Yu Wei Tan</name>
    </author>
    <author>
      <name>Xiaohan Cui</name>
    </author>
    <author>
      <name>Anand Bhojan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3388770.3407436</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3388770.3407436" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGGRAPH 2020 Posters</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.05364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.05364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06158v1</id>
    <updated>2022-10-11T14:24:44Z</updated>
    <published>2022-10-11T14:24:44Z</published>
    <title>A Hybrid System for Real-time Rendering of Depth of Field Effect in
  Games</title>
    <summary>  Real-time depth of field in game cinematics tends to approximate the
semi-transparent silhouettes of out-of-focus objects through post-processing
techniques. We leverage ray tracing hardware acceleration and spatio-temporal
reconstruction to improve the realism of such semi-transparent regions through
hybrid rendering, while maintaining interactive frame rates for immersive
gaming. This paper extends our previous work with a complete presentation of
our technique and details on its design, implementation, and future work.
</summary>
    <author>
      <name>Yu Wei Tan</name>
    </author>
    <author>
      <name>Nicholas Chua</name>
    </author>
    <author>
      <name>Nathan Biette</name>
    </author>
    <author>
      <name>Anand Bhojan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0010839800003124</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0010839800003124" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Best Student Paper Award (GRAPP 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.06158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06160v1</id>
    <updated>2022-10-11T11:47:12Z</updated>
    <published>2022-10-11T11:47:12Z</published>
    <title>RTSDF: Real-time Signed Distance Fields for Soft Shadow Approximation in
  Games</title>
    <summary>  Signed distance fields (SDFs) are a form of surface representation widely
used in computer graphics, having applications in rendering, collision
detection and modelling. In interactive media such as games, high-resolution
SDFs are commonly produced offline and subsequently loaded into the
application, representing rigid meshes only. This work develops a novel
technique that combines jump flooding and ray tracing to generate approximate
SDFs in real-time. Our approach can produce relatively accurate scene
representation for rendering soft shadows while maintaining interactive frame
rates. We extend our previous work with details on the design and
implementation as well as visual quality and performance evaluation of the
technique.
</summary>
    <author>
      <name>Yu Wei Tan</name>
    </author>
    <author>
      <name>Nicholas Chua</name>
    </author>
    <author>
      <name>Clarence Koh</name>
    </author>
    <author>
      <name>Anand Bhojan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0010996200003124</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0010996200003124" rel="related"/>
    <link href="http://arxiv.org/abs/2210.06160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00330v1</id>
    <updated>2022-11-01T08:34:57Z</updated>
    <published>2022-11-01T08:34:57Z</published>
    <title>Real-Time Character Inverse Kinematics using the Gauss-Seidel Iterative
  Approximation Method</title>
    <summary>  We present a realistic, robust, and computationally fast method of solving
highly non-linear inverse kinematic problems with angular limits using the
Gauss-Seidel iterative method. Our method is ideally suited towards character
based interactive applications such as games. To achieve interactive simulation
speeds, numerous acceleration techniques are employed, including spatial
coherent starting approximations and projected angular clamping. The method has
been tested on a continuous range of poses for animated articulated characters
and successfully performed in all cases and produced good visual outcomes.
</summary>
    <author>
      <name>Ben Kenwright</name>
    </author>
    <link href="http://arxiv.org/abs/2211.00330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.04314v1</id>
    <updated>2022-11-08T15:33:04Z</updated>
    <published>2022-11-08T15:33:04Z</published>
    <title>Scalable multi-class sampling via filtered sliced optimal transport</title>
    <summary>  We propose a multi-class point optimization formulation based on continuous
Wasserstein barycenters. Our formulation is designed to handle hundreds to
thousands of optimization objectives and comes with a practical optimization
scheme. We demonstrate the effectiveness of our framework on various sampling
applications like stippling, object placement, and Monte-Carlo integration. We
a derive multi-class error bound for perceptual rendering error which can be
minimized using our optimization. We provide source code at
https://github.com/iribis/filtered-sliced-optimal-transport.
</summary>
    <author>
      <name>Corentin Salaün</name>
    </author>
    <author>
      <name>Iliyan Georgiev</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Gurprit Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3550454.3555484 10.1145/3550454.3555484 10.1145/3550454.3555484
  10.1145/3550454.3555484 10.1145/3550454.3555484 10.1145/3550454.3555484</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3550454.3555484" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.1145/3550454.3555484" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.1145/3550454.3555484" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.1145/3550454.3555484" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.1145/3550454.3555484" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.1145/3550454.3555484" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 17 figures, ACM Trans. Graph., Vol. 41, No. 6, Article 261.
  Publication date: December 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.04314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.07296v1</id>
    <updated>2022-11-14T12:06:28Z</updated>
    <published>2022-11-14T12:06:28Z</published>
    <title>Optimizing Placements of 360-degree Panoramic Cameras in Indoor
  Environments by Integer Programming</title>
    <summary>  We propose a computational approach to find a minimal set of 360-degree
camera placements that together sufficiently cover an indoor environment for
the building documentation problem in the architecture, engineering, and
construction (AEC) industries. Our approach, based on a simple integer
programming (IP) problem formulation, solves very efficiently and globally
optimally. We conducted a study of using panoramas to capture the appearances
of a real-world indoor environment, in which we found that our computed
solutions are better than human solutions decided by both non-professional and
professional users.
</summary>
    <author>
      <name>Syuan-Rong Syu</name>
    </author>
    <author>
      <name>Chi-Han Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Smart Tools and Applications in Graphics (STAG), 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.07296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.07296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.14859v1</id>
    <updated>2022-11-27T15:25:56Z</updated>
    <published>2022-11-27T15:25:56Z</published>
    <title>Quantifying spatial, temporal, angular and spectral structure of
  effective daylight in perceptually meaningful ways</title>
    <summary>  We present a method to capture the 7-dimensional light field structure, and
translate it into perceptually-relevant information. Our spectral cubic
illumination method quantifies objective correlates of perceptually relevant
diffuse and directed light components, including their variations over time,
space, in color and direction, and the environment's response to sky and
sunlight. We applied it 'in the wild', capturing how light on a sunny day
differs between light and shadow, and how light varies over sunny and cloudy
days. We discuss the added value of our method for capturing nuanced lighting
effects on scene and object appearance, such as chromatic gradients.
</summary>
    <author>
      <name>Cehao Yu</name>
    </author>
    <author>
      <name>Maarten Wijntjes</name>
    </author>
    <author>
      <name>Elmar Eisemann</name>
    </author>
    <author>
      <name>Sylvia Pont</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/OE.479715</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/OE.479715" rel="related"/>
    <link href="http://arxiv.org/abs/2211.14859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.14859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07820v1</id>
    <updated>2022-12-14T03:26:38Z</updated>
    <published>2022-12-14T03:26:38Z</published>
    <title>Online Heatmap Generation with Both High and Low Weights</title>
    <summary>  Heatmap is a common geovisualization method that interpolates and visualizes
a set of point observations on a map surface. Most of online web mapping
libraries implement a one-pass heatmap algorithm using HTML5 canvas or WebGL
for efficient heatmap generation. However, such implementation applies additive
operations that accumulate the rendering of point weights on the map surface
grid, making it inappropriate for visualizations that require the highlighting
of both low and high weights. We introduce \textit{hilomap}, an online heatmap
algorithm that highlights surface areas where points with both low and high
trends are located. An HTML5 Canvas-based reference implementation on
OpenLayers is presented and evaluated.
</summary>
    <author>
      <name>Yan Y. Liu</name>
    </author>
    <author>
      <name>Melissa Allen-Dumas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 4th ACM SIGSPATIAL International Workshop on Spatial Gems
  (SpatialGems 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.07820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11527v1</id>
    <updated>2022-12-22T08:08:24Z</updated>
    <published>2022-12-22T08:08:24Z</published>
    <title>Scaffolding Generation using a 3D Physarum Polycephalum Simulation</title>
    <summary>  In this demo, we present a novel technique for approximating topologically
optimal scaffoldings for 3D printed objects using a Monte Carlo algorithm based
on the foraging behavior of the Physarum polycephalum slime mold. As a case
study, we have created a biologically inspired bicycle helmet using this
technique that is designed to be effective in resisting impacts. We have
created a prototype of this helmet and propose further studies that measure the
effectiveness and validity of the design.
</summary>
    <author>
      <name>Drew Ehrlich</name>
    </author>
    <author>
      <name>Milad Hakimshafaei</name>
    </author>
    <author>
      <name>Oskar Elek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3559400.3565590</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3559400.3565590" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in the demo session at the ACM Symposium on Computational
  Fabrication 2022 on October 27, 2022 in Seattle, WA</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.11527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11875v1</id>
    <updated>2022-12-22T17:08:30Z</updated>
    <published>2022-12-22T17:08:30Z</published>
    <title>S-patch: Modification of the Hermite parametric patch</title>
    <summary>  A new modification of the Hermite cubic rectangular patch is proposed: the
S-Patch, which is based on the requirement that diagonal curves must be of
degree 3 instead of degree 6 as it is in the case of the Hermite patch.
Theoretical derivation of conditions is presented and some experimental results
as well. The S-Patch is convenient for applications, where different
tessellation of the u-v domain is needed, boundary and diagonal curves of
different degrees are not acceptable.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <author>
      <name>Vit Ondracka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft of the paper: S-Patch: Modification of the Hermite Parametric
  Patch, ICGG 2010 conference, Kyoto, Japan, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.11875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12502v1</id>
    <updated>2022-12-10T13:37:32Z</updated>
    <published>2022-12-10T13:37:32Z</published>
    <title>Demo: New View on Plasma Fractals -- From the High Point of Array
  Languages</title>
    <summary>  Plasma fractals is a technique to generate random and realistic clouds,
textures and terrains~-- traditionally using recursive subdivision. We
demonstrate a new approach, based on iterative expansion. It gives a family of
algorithms that includes the standard square-diamond algorithm and offers
various interesting ways of extending it, and hence generating nicer pictures.
The approach came about from exploring plasma fractals from the point of view
of an array language (which we implemented as an embedded DSL in OCaml)~-- that
is, from the perspective of declaring whole image transformations rather than
fiddling with individual pixels.
</summary>
    <author>
      <name>Oleg Kiselyov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tohoku University, Japan</arxiv:affiliation>
    </author>
    <author>
      <name>Toshihiro Nakayama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tohoku University, Japan</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Peer-reviewed, accepted for presentation and presented at the ACM
  SIGPLAN FARM 2022 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.12502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.3.6; F.3.3; D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01471v1</id>
    <updated>2023-01-04T07:24:24Z</updated>
    <published>2023-01-04T07:24:24Z</published>
    <title>Freeform Islamic Geometric Patterns</title>
    <summary>  Islamic geometric patterns are a rich and venerable ornamental tradition.
Many classic designs feature periodic arrangements of rosettes: star shapes
surrounded by rings of hexagonal petals. We present a new technique for
generating 'freeform' compositions of rosettes: finite designs that freely mix
rosettes of unusual sizes while retaining the aesthetics of traditional
patterns. We use a circle packing as a scaffolding for developing a patch of
polygons and fill each polygon with a motif based on established constructions
from Islamic art.
</summary>
    <author>
      <name>Rebecca Lin</name>
    </author>
    <author>
      <name>Craig S. Kaplan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.01471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05125v1</id>
    <updated>2023-01-12T16:26:21Z</updated>
    <published>2023-01-12T16:26:21Z</published>
    <title>Adaptive Dynamic Global Illumination</title>
    <summary>  We present an adaptive extension of probe based global illumination solution
that enhances the response to dynamic changes in the scene while while also
enabling an order of magnitude increase in probe count. Our adaptive sampling
strategy carefully places samples in regions where we detect time varying
changes in radiosity either due to a change in lighting, geometry or both. Even
with large number of probes, our technique robustly updates the irradiance and
visibility cache to reflect the most up to date changes without stalling the
overall algorithm. Our bandwidth aware approach is largely an improvement over
the original \textit{Dynamic Diffuse Global Illumination} while also remaining
orthogonal to the recent advancements in the technique.
</summary>
    <author>
      <name>Sayantan Datta</name>
    </author>
    <author>
      <name>Negar Goli</name>
    </author>
    <author>
      <name>Jerry Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://sayan1an.github.io/adgi.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.05125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05262v1</id>
    <updated>2023-01-12T19:16:32Z</updated>
    <published>2023-01-12T19:16:32Z</published>
    <title>Neural Shadow Mapping</title>
    <summary>  We present a neural extension of basic shadow mapping for fast, high quality
hard and soft shadows. We compare favorably to fast pre-filtering shadow
mapping, all while producing visual results on par with ray traced hard and
soft shadows. We show that combining memory bandwidth-aware architecture
specialization and careful temporal-window training leads to a fast, compact
and easy-to-train neural shadowing method. Our technique is memory bandwidth
conscious, eliminates the need for post-process temporal anti-aliasing or
denoising, and supports scenes with dynamic view, emitters and geometry while
remaining robust to unseen objects.
</summary>
    <author>
      <name>Sayantan Datta</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <author>
      <name>Christoph Schied</name>
    </author>
    <author>
      <name>Zhao Dong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3528233.3530700</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3528233.3530700" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://sayan1an.github.io/neuralShadowMapping.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGGRAPH 2022 Conference Proceedings</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.05262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09567v1</id>
    <updated>2023-01-11T20:21:58Z</updated>
    <published>2023-01-11T20:21:58Z</published>
    <title>Rig Inversion by Training a Differentiable Rig Function</title>
    <summary>  Rig inversion is the problem of creating a method that can find the rig
parameter vector that best approximates a given input mesh. In this paper we
propose to solve this problem by first obtaining a differentiable rig function
by training a multi layer perceptron to approximate the rig function. This
differentiable rig function can then be used to train a deep learning model of
rig inversion.
</summary>
    <author>
      <name>Mathieu Marquis Bolduc</name>
    </author>
    <author>
      <name>Hau Nghiep Phan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3550340.3564218</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3550340.3564218" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Siggraph Asia '22 in Daegu, South Korea</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SA '22: SIGGRAPH Asia 2022 Technical Communications, December
  2022, Article No.: 15</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.09567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11006v1</id>
    <updated>2023-01-26T09:39:25Z</updated>
    <published>2023-01-26T09:39:25Z</published>
    <title>From medical imaging to virtual reality for archaeology</title>
    <summary>  The IRMA project aims to design innovative methodologies for research in the
field of historical and archaeological heritage based on a combination of
medical imaging technologies and interactive 3D restitution modalities (virtual
reality, augmented reality, haptics, additive manufacturing). These tools are
based on recent research results from a collaboration between IRISA, Inrap and
the company Image ET and are intended for cultural heritage professionals such
as museums, curators, restorers and archaeologists.
</summary>
    <author>
      <name>Théophane Nicolas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inrap</arxiv:affiliation>
    </author>
    <author>
      <name>Ronan Gaugne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UR1, IRISA, Hybrid</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Arnaldi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSA Rennes, IRISA, Hybrid</arxiv:affiliation>
    </author>
    <author>
      <name>Valérie Gouranton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSA Rennes, IRISA, Hybrid</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CNRS Editions. L'interdisciplinarit{\'e} -- Voyages au-del{\`a}
  des disciplines, CNRS Editions, pp.316, 2023, 978-2-271-13983-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.11006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05002v1</id>
    <updated>2023-02-10T01:16:16Z</updated>
    <published>2023-02-10T01:16:16Z</published>
    <title>FastPoints: A State-of-the-Art Point Cloud Renderer for Unity</title>
    <summary>  In this paper, we introduce FastPoints, a state-of-the-art point cloud
renderer for the Unity game development platform. Our program supports standard
unprocessed point cloud formats with non-programmatic, drag-and-drop support,
and creates an out-of-core data structure for large clouds without requiring an
explicit preprocessing step; instead, the software renders a decimated point
cloud immediately and constructs a shallow octree online, during which time the
Unity editor remains fully interactive.
</summary>
    <author>
      <name>Elias Neuman-Donihue</name>
    </author>
    <author>
      <name>Michael Jarvis</name>
    </author>
    <author>
      <name>Yuhao Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.05002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06087v1</id>
    <updated>2023-02-13T04:10:34Z</updated>
    <published>2023-02-13T04:10:34Z</published>
    <title>Dynamic Simulation of Splashing Fluids</title>
    <summary>  In this paper we describe a method for modeling the dynamic behavior of
splashing fluids. The model simulates the behavior of a fluid when objects
impact or float on its surface. The forces generated by the objects create
waves and splashes on the surface of the fluid. To demonstrate the realism and
limitations of the model, images from a computer-generated animation are
presented and compared with video frames of actual splashes occurring under
similar initial conditions.
</summary>
    <author>
      <name>James F. O'Brien</name>
    </author>
    <author>
      <name>Jessica K. Hodgins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CA.1995.393532</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CA.1995.393532" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Alternative location:
  http://graphics.berkeley.edu/papers/Obrien-DSS-1995-04</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of Computer Animation 95, pages 198-205, April 1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2302.06087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08685v1</id>
    <updated>2023-02-17T04:28:02Z</updated>
    <published>2023-02-17T04:28:02Z</published>
    <title>Creative NFT-Copyrighted AR Face Mask Authoring Using Unity3D Editor</title>
    <summary>  In this paper, we extend well-designed 3D face masks into AR face masks and
demonstrate the possibility of transforming this into an NFT-copyrighted AR
face mask that helps authenticate the ownership of the AR mask user so as to
improve creative control, brand identification, and ID protection. The output
of this project will not only potentially validate the value of the NFT
technology but also explore how to combine the NFT technology with AR
technology so as to be applied to e-commerce and e-business aspects of the
multimedia industry.
</summary>
    <author>
      <name>Mohamed Al Hamzy</name>
    </author>
    <author>
      <name>Shijin Zhang</name>
    </author>
    <author>
      <name>Hong Huang</name>
    </author>
    <author>
      <name>Wanwan Li</name>
    </author>
    <link href="http://arxiv.org/abs/2302.08685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06987v1</id>
    <updated>2023-03-13T10:37:20Z</updated>
    <published>2023-03-13T10:37:20Z</published>
    <title>Challenges of movement quality using motion capture in theatre</title>
    <summary>  We describe1 two case studies of AvatarStaging theatrical mixed reality
framework combining avatars and performers acting in an artistic context. We
outline a qualitative approach toward the condition for stage presence for the
avatars. We describe the motion control solutions we experimented with from the
perspective of building a protocol of avatar direction in a mixed reality
appropriate to live performance.
</summary>
    <author>
      <name>Georges Gagneré</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INREV, UP8, UPL</arxiv:affiliation>
    </author>
    <author>
      <name>Andy Lavender</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INREV, AIAC, UP8, UPL</arxiv:affiliation>
    </author>
    <author>
      <name>Cédric Plessiet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INREV, AIAC, UP8, UPL</arxiv:affiliation>
    </author>
    <author>
      <name>Tim White</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3212721.3212883</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3212721.3212883" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MOCO '18: 5th International Conference on Movement and Computing,
  Jun 2018, Genoa, Italy. pp.1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.06987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.09123v1</id>
    <updated>2023-03-16T07:36:45Z</updated>
    <published>2023-03-16T07:36:45Z</published>
    <title>Emergence and fragility of a research-creation (2000-2007)</title>
    <summary>  My research-creation process coincides with the encounter with the ''digital
paradigm'' and the attempt to incorporate it into the foundation of my scenic
writing. I propose in this paper to give an account from a director point of
view of how I realized my shows between 2000 and 2007 and which researches
influenced the process. I will formulate some remarks on the fragilities that
can arise in a ''technological laboratory of staging''.
</summary>
    <author>
      <name>Georges Gagneré</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INREV, UP8, UPL</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3917/lige.137.0148</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3917/lige.137.0148" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ligeia, dossiers sur l'art, 2015, No. 137-140 (1), pp.148</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.09123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.09123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10551v1</id>
    <updated>2023-03-19T03:33:52Z</updated>
    <published>2023-03-19T03:33:52Z</published>
    <title>Combining Active and Passive Simulations for Secondary Motion</title>
    <summary>  Objects that move in response to the actions of a main character often make
an important contribution to the visual richness of an animated scene. We use
the term "secondary motion" to refer to passive motions generated in response
to the movements of characters and other objects or environmental forces.
Secondary motions aren't normally the mail focus of an animated scene, yet
their absence can distract or disturb the viewer, destroying the illusion of
reality created by the scene. We describe how to generate secondary motion by
coupling physically based simulations of passive objects to actively controlled
characters.
</summary>
    <author>
      <name>James F. O'Brien</name>
    </author>
    <author>
      <name>Victor B. Zordan</name>
    </author>
    <author>
      <name>Jessica K. Hodgins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/38.851756</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/38.851756" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computer Graphics and Applications, 20(4):86-96, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.10551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14831v1</id>
    <updated>2023-03-26T21:51:20Z</updated>
    <published>2023-03-26T21:51:20Z</published>
    <title>Hardware Acceleration of Progressive Refinement Radiosity using Nvidia
  RTX</title>
    <summary>  A vital component of photo-realistic image synthesis is the simulation of
indirect diffuse reflections, which still remain a quintessential hurdle that
modern rendering engines struggle to overcome. Real-time applications typically
pre-generate diffuse lighting information offline using radiosity to avoid
performing costly computations at run-time. In this thesis we present a variant
of progressive refinement radiosity that utilizes Nvidia's novel RTX technology
to accelerate the process of form-factor computation without compromising on
visual fidelity. Through a modern implementation built on DirectX 12 we
demonstrate that offloading radiosity's visibility component to RT cores
significantly improves the lightmap generation process and potentially propels
it into the domain of real-time.
</summary>
    <author>
      <name>Benjamin Kahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">114 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16156v3</id>
    <updated>2024-08-08T13:47:21Z</updated>
    <published>2023-03-27T04:51:30Z</published>
    <title>On the derivatives of rational Bézier curves</title>
    <summary>  By studying the existing higher order derivation formulas of rational
B\'{e}zier curves, we find that they fail when the order of the derivative
exceeds the degree of the curves. In this paper, we present a new derivation
formula for rational B\'{e}zier curves that overcomes this drawback and show
that the $k$th degree derivative of a $n$th degree rational B\'{e}zier curve
can be written in terms of a $(2^kn)$th degree rational B\'{e}zier curve.we
also consider the properties of the endpoints and the bounds of the
derivatives.
</summary>
    <author>
      <name>Mao Shi</name>
    </author>
    <link href="http://arxiv.org/abs/2303.16156v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16156v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.02814v1</id>
    <updated>2023-04-06T01:32:58Z</updated>
    <published>2023-04-06T01:32:58Z</published>
    <title>4D Agnostic Real-Time Facial Animation Pipeline for Desktop Scenarios</title>
    <summary>  We present a high-precision real-time facial animation pipeline suitable for
animators to use on their desktops. This pipeline is about to be launched in
FACEGOOD's Avatary\footnote{https://www.avatary.com/} software, which will
accelerate animators' productivity. The pipeline differs from professional
head-mounted facial capture solutions in that it only requires the use of a
consumer-grade 3D camera on the desk to achieve high-precision real-time facial
capture. The system enables animators to create high-quality facial animations
with ease and speed, while reducing the cost and complexity of traditional
facial capture solutions. Our approach has the potential to revolutionize the
way facial animation is done in the entertainment industry.
</summary>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>HongWei Xu</name>
    </author>
    <author>
      <name>Jelo Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09626v1</id>
    <updated>2023-04-19T13:02:35Z</updated>
    <published>2023-04-19T13:02:35Z</published>
    <title>StyleDEM: a Versatile Model for Authoring Terrains</title>
    <summary>  Many terrain modelling methods have been proposed for the past decades,
providing efficient and often interactive authoring tools. However, they
generally do not include any notion of style, which is a critical aspect for
designers in the entertainment industry. We introduce StyleDEM, a new
generative adversarial network method for terrain synthesis and authoring, with
a versatile toolbox of authoring methods with style. This method starts from an
input sketch or an existing terrain. It outputs a terrain with features that
can be authored using interactive brushes and enhanced with additional tools
such as style manipulation or super-resolution. The strength of our approach
resides in the versatility and interoperability of the toolbox.
</summary>
    <author>
      <name>Simon Perche</name>
    </author>
    <author>
      <name>Adrien Peytavie</name>
    </author>
    <author>
      <name>Bedrich Benes</name>
    </author>
    <author>
      <name>Eric Galin</name>
    </author>
    <author>
      <name>Eric Guérin</name>
    </author>
    <link href="http://arxiv.org/abs/2304.09626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14592v1</id>
    <updated>2023-04-28T01:58:28Z</updated>
    <published>2023-04-28T01:58:28Z</published>
    <title>Ultrasound Visualization using VTK</title>
    <summary>  This project developed a web application using VTK for ultrasound
visualization. The images were enhanced using median and Gaussian filters, and
two algorithms were utilized for data visualization: isosurface extraction and
Delaunay triangulation. Results showed that both algorithms were effective at
reducing Gaussian noise and high-frequency noise, such as speckle noise, which
is common in ultrasound. The web application allows users to select MHA files,
adjust the marching cubes threshold, and switch between the two algorithms in
runtime. This project demonstrates the benefits of ultrasound visualization in
medical applications and the utility of using VTK to achieve high-quality
visualizations. The web application provides healthcare professionals with a
user-friendly platform to interpret ultrasound data, leading to better
diagnosis and treatment decisions
</summary>
    <author>
      <name>Bhavya Sehgal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Zichen</arxiv:affiliation>
    </author>
    <author>
      <name> Gavin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Zichen</arxiv:affiliation>
    </author>
    <author>
      <name> Gui</name>
    </author>
    <author>
      <name>Md Nahid sadik</name>
    </author>
    <link href="http://arxiv.org/abs/2304.14592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.04804v1</id>
    <updated>2023-05-08T16:02:43Z</updated>
    <published>2023-05-08T16:02:43Z</published>
    <title>TauBench 1.1: A Dynamic Benchmark for Graphics Rendering</title>
    <summary>  Many graphics rendering algorithms used in both real-time games and virtual
reality applications can get performance boosts by temporally reusing previous
computations. However, algorithms based on temporal reuse are typically
measured using trivial benchmarks with very limited dynamic features. To this
end, in [1] we presented TauBench 1.0, a benchmark designed to stress temporal
reuse algorithms. Now, we release TauBench version 1.1, which improves the
usability of the original benchmark. In particular, these improvements reduce
the size of the dataset significantly, resulting in faster loading and
rendering times, and in better compatibility with 3D software that impose
strict size limits for the scenes.
</summary>
    <author>
      <name>Erfan Momeni Yazdi</name>
    </author>
    <author>
      <name>Markku Mäkitalo</name>
    </author>
    <author>
      <name>Julius Ikkala</name>
    </author>
    <author>
      <name>Pekka Jääskeläinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The dataset is downloadable at https://zenodo.org/record/7906987</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.04804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.04804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09778v1</id>
    <updated>2023-05-16T20:05:10Z</updated>
    <published>2023-05-16T20:05:10Z</published>
    <title>Shortest Path to Boundary for Self-Intersecting Meshes</title>
    <summary>  We introduce a method for efficiently computing the exact shortest path to
the boundary of a mesh from a given internal point in the presence of
self-intersections. We provide a formal definition of shortest boundary paths
for self-intersecting objects and present a robust algorithm for computing the
actual shortest boundary path. The resulting method offers an effective
solution for collision and self-collision handling while simulating deformable
volumetric objects, using fast simulation techniques that provide no guarantees
on collision resolution. Our evaluation includes complex self-collision
scenarios with a large number of active contacts, showing that our method can
successfully handle them by introducing a relatively minor computational
overhead.
</summary>
    <author>
      <name>He Chen</name>
    </author>
    <author>
      <name>Elie Diaz</name>
    </author>
    <author>
      <name>Cem Yuksel</name>
    </author>
    <link href="http://arxiv.org/abs/2305.09778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10197v1</id>
    <updated>2023-05-17T13:21:45Z</updated>
    <published>2023-05-17T13:21:45Z</published>
    <title>Deep and Fast Approximate Order Independent Transparency</title>
    <summary>  We present a machine learning approach for efficiently computing order
independent transparency (OIT). Our method is fast, requires a small constant
amount of memory (depends only on the screen resolution and not on the number
of triangles or transparent layers), is more accurate as compared to previous
approximate methods, works for every scene without setup and is portable to all
platforms running even with commodity GPUs. Our method requires a rendering
pass to extract all features that are subsequently used to predict the overall
OIT pixel color with a pre-trained neural network. We provide a comparative
experimental evaluation and shader source code of all methods for reproduction
of the experiments.
</summary>
    <author>
      <name>Grigoris Tsopouridis</name>
    </author>
    <author>
      <name>Andreas-Alexandros Vasilakis</name>
    </author>
    <author>
      <name>Ioannis Fudos</name>
    </author>
    <link href="http://arxiv.org/abs/2305.10197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; I.3.6; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.12653v2</id>
    <updated>2023-10-09T03:56:34Z</updated>
    <published>2023-05-22T02:52:29Z</published>
    <title>Estimating Discrete Total Curvature with Per Triangle Normal Variation</title>
    <summary>  We introduce a novel approach for measuring the total curvature at every
triangle of a discrete surface. This method takes advantage of the relationship
between per triangle total curvature and the Dirichlet energy of the Gauss map.
This new tool can be used on both triangle meshes and point clouds and has
numerous applications. In this study, we demonstrate the effectiveness of our
technique by using it for feature-aware mesh decimation, and show that it
outperforms existing curvature-estimation methods from popular libraries such
as Meshlab, Trimesh2, and Libigl. When estimating curvature on point clouds,
our method outperforms popular libraries PCL and CGAL.
</summary>
    <author>
      <name>Crane He Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2305.12653v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.12653v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.13101v1</id>
    <updated>2023-05-22T15:00:59Z</updated>
    <published>2023-05-22T15:00:59Z</published>
    <title>A Convex Optimization Framework for Regularized Geodesic Distances</title>
    <summary>  We propose a general convex optimization problem for computing regularized
geodesic distances. We show that under mild conditions on the regularizer the
problem is well posed. We propose three different regularizers and provide
analytical solutions in special cases, as well as corresponding efficient
optimization algorithms. Additionally, we show how to generalize the approach
to the all pairs case by formulating the problem on the product manifold, which
leads to symmetric distances. Our regularized distances compare favorably to
existing methods, in terms of robustness and ease of calibration.
</summary>
    <author>
      <name>Michal Edelstein</name>
    </author>
    <author>
      <name>Nestor Guillen</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <author>
      <name>Mirela Ben-Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3588432.3591523</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3588432.3591523" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (excluding supplementary material), 14 figures, SIGGRAPH
  2023</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH '23 Conference Proceedings, August 6-10, 2023, Los
  Angeles, CA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.13101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.13101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.20061v1</id>
    <updated>2023-05-31T17:38:06Z</updated>
    <published>2023-05-31T17:38:06Z</published>
    <title>Towards Neural Path Tracing in SRAM</title>
    <summary>  We present an experimental neural path tracer designed to exploit the large
on-chip memory of Graphcore intelligence-processing-units (IPUs). This open
source renderer demonstrates how to map path tracing to the novel software and
hardware architecture and is a useful tool for analysing in-cache
neural-rendering scenarios. Such scenarios will be increasingly important if
rasterisation is replaced by combinations of ray/path tracing, neural-radiance
caching, and AI denoising/up-scaling, for which small neural networks are
already routinely employed. A detailed description of the implementation also
serves as a self-contained resource for more general software design on IPU.
</summary>
    <author>
      <name>Mark Pupilli</name>
    </author>
    <link href="http://arxiv.org/abs/2305.20061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.20061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01973v1</id>
    <updated>2023-06-03T01:00:21Z</updated>
    <published>2023-06-03T01:00:21Z</published>
    <title>Algebraic Smooth Occluding Contours</title>
    <summary>  Computing occluding contours is a key building block of non-photorealistic
rendering, but producing contours with consistent visibility has been
notoriously challenging. This paper describes the first general-purpose smooth
surface construction for which the occluding contours can be computed in closed
form. For a given input mesh and camera viewpoint, we produce a $G^1$
piecewise-quadratic surface approximating the mesh. We show how the image-space
occluding contours of this representation may then be described as piecewise
rational curves. We show that this method produces smooth contours with
consistent visibility much more efficiently than the state-of-the-art.
</summary>
    <author>
      <name>Ryan Capouellez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">New York University</arxiv:affiliation>
    </author>
    <author>
      <name>Jiacheng Dai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">New York University</arxiv:affiliation>
    </author>
    <author>
      <name>Aaron Hertzmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Adobe</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Zorin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">New York University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3588432.3591547</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3588432.3591547" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.01973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.05051v1</id>
    <updated>2023-06-08T09:09:07Z</updated>
    <published>2023-06-08T09:09:07Z</published>
    <title>Real-Time Rendering of Glinty Appearances using Distributed Binomial
  Laws on Anisotropic Grids</title>
    <summary>  In this work, we render in real-time glittery materials caused by discrete
flakes on the surface. To achieve this, one has to count the number of flakes
reflecting the light towards the camera within every texel covered by a given
pixel footprint. To do so, we derive a counting method for arbitrary footprints
that, unlike previous work, outputs the correct statistics. We combine this
counting method with an anisotropic parameterization of the texture space that
reduces the number of texels falling under a pixel footprint. This allows our
method to run with both stable performance and 1.5X to 5X faster than the
state-of-the-art.
</summary>
    <author>
      <name> Deliot</name>
    </author>
    <author>
      <name> Thomas</name>
    </author>
    <author>
      <name> Belcour</name>
    </author>
    <author>
      <name> Laurent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">High-Performance Graphics (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.05051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.05051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.00938v1</id>
    <updated>2023-07-03T11:24:25Z</updated>
    <published>2023-07-03T11:24:25Z</published>
    <title>Interpolation of Point Distributions for Digital Stippling</title>
    <summary>  We present a new way to merge any two point distribution approaches using
distance fields. Our new process allows us to produce digital stippling that
fills areas with stipple dots without visual artifacts as well as includes
clear linear features without fussiness. Our merging thus benefits from past
work that can optimize for either goal individually, yet typically by
sacrificing the other. The new possibility of combining any two distributions
using different distance field functions and their parameters also allows us to
produce a vast range of stippling styles, which we demonstrate as well.
</summary>
    <author>
      <name>Germán Arroyo</name>
    </author>
    <author>
      <name>Domingo Martín</name>
    </author>
    <author>
      <name>Tobias Isenberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 42 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.00938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.00938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97R60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15584v1</id>
    <updated>2023-07-28T14:35:45Z</updated>
    <published>2023-07-28T14:35:45Z</published>
    <title>Quasi-Monte Carlo Algorithms (not only) for Graphics Software</title>
    <summary>  Quasi-Monte Carlo methods have become the industry standard in computer
graphics. For that purpose, efficient algorithms for low discrepancy sequences
are discussed. In addition, numerical pitfalls encountered in practice are
revealed. We then take a look at massively parallel quasi-Monte Carlo
integro-approximation for image synthesis by light transport simulation. Beyond
superior uniformity, low discrepancy points may be optimized with respect to
additional criteria, such as noise characteristics at low sampling rates or the
quality of low-dimensional projections.
</summary>
    <author>
      <name>Alexander Keller</name>
    </author>
    <author>
      <name>Carsten Wächter</name>
    </author>
    <author>
      <name>Nikolaus Binder</name>
    </author>
    <link href="http://arxiv.org/abs/2307.15584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.01633v1</id>
    <updated>2023-08-03T09:09:10Z</updated>
    <published>2023-08-03T09:09:10Z</published>
    <title>LEAVEN -- Lightweight Surface and Volume Mesh Sampling Application for
  Particle-based Simulations</title>
    <summary>  We present an easy-to-use and lightweight surface and volume mesh sampling
standalone application tailored for the needs of particle-based simulation. We
describe the surface and volume sampling algorithms used in LEAVEN in a
beginner-friendly fashion. Furthermore, we describe a novel method of
generating random volume samples that satisfy blue noise criteria by modifying
a surface sampling algorithm. We aim to lower one entry barrier for starting
with particle-based simulations while still pose a benefit to advanced users.
The goal is to provide a useful tool to the community and lowering the need for
heavyweight third-party applications, especially for starters.
</summary>
    <author>
      <name>Alexander Sommer</name>
    </author>
    <author>
      <name>Ulrich Schwanecke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.24132/CSRN.2021.3101.17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.24132/CSRN.2021.3101.17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science Research Notes [CSRN] (2021). Code available at:
  https://github.com/a1ex90/Leaven</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.01633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.01633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.03765v2</id>
    <updated>2024-12-04T17:08:53Z</updated>
    <published>2023-07-23T16:57:50Z</published>
    <title>Real and complexified configuration spaces for spherical 4-bar linkages</title>
    <summary>  This note is a complete library of symbolic parametrized expressions for both
real and complexified configuration spaces of a spherical 4-bar linkage.
Building upon the previous work from Izmestiev, (2016, Section 2), this library
expands on the expressions by incorporating all four folding angles across all
possible linkage length choices, along with the polynomial relation between
diagonals (spherical arcs). Furthermore, a complete MATLAB app script is
included, enabling visualization and parametrization. The derivations are
presented in a detailed manner, ensuring accessibility for researchers across
diverse disciplines.
</summary>
    <author>
      <name>Zeyuan He</name>
    </author>
    <author>
      <name>Kentaro Hayakawa</name>
    </author>
    <author>
      <name>Makoto Ohsaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">65 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:2307.12203</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.03765v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.03765v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.10896v1</id>
    <updated>2023-08-21T17:58:43Z</updated>
    <published>2023-08-21T17:58:43Z</published>
    <title>Differentiable Shadow Mapping for Efficient Inverse Graphics</title>
    <summary>  We show how shadows can be efficiently generated in differentiable rendering
of triangle meshes. Our central observation is that pre-filtered shadow
mapping, a technique for approximating shadows based on rendering from the
perspective of a light, can be combined with existing differentiable
rasterizers to yield differentiable visibility information. We demonstrate at
several inverse graphics problems that differentiable shadow maps are orders of
magnitude faster than differentiable light transport simulation with similar
accuracy -- while differentiable rasterization without shadows often fails to
converge.
</summary>
    <author>
      <name>Markus Worchel</name>
    </author>
    <author>
      <name>Marc Alexa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023, project page:
  https://mworchel.github.io/differentiable-shadow-mapping</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.10896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.10896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.12158v2</id>
    <updated>2023-08-24T15:10:25Z</updated>
    <published>2023-08-23T14:18:56Z</published>
    <title>A Visualization System for Hexahedral Mesh Quality Study</title>
    <summary>  In this paper, we introduce a new 3D hex mesh visual analysis system that
emphasizes poor-quality areas with an aggregated glyph, highlights overlapping
elements, and provides detailed boundary error inspection in three forms. By
supporting multi-level analysis through multiple views, our system effectively
evaluates various mesh models and compares the performance of mesh generation
and optimization algorithms for hexahedral meshes.
</summary>
    <author>
      <name>Lei Si</name>
    </author>
    <author>
      <name>Guoning Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VIS54172.2023.00026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VIS54172.2023.00026" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE VIS 2023 Short Papers and will be published on IEEE
  Xplore. Paper contains 4 pages, and 1 reference page. Supplemental includes 4
  pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.12158v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.12158v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.08387v2</id>
    <updated>2023-11-17T21:12:56Z</updated>
    <published>2023-09-12T16:05:45Z</published>
    <title>Efficient Graphics Representation with Differentiable Indirection</title>
    <summary>  We introduce differentiable indirection -- a novel learned primitive that
employs differentiable multi-scale lookup tables as an effective substitute for
traditional compute and data operations across the graphics pipeline. We
demonstrate its flexibility on a number of graphics tasks, i.e., geometric and
image representation, texture mapping, shading, and radiance field
representation. In all cases, differentiable indirection seamlessly integrates
into existing architectures, trains rapidly, and yields both versatile and
efficient results.
</summary>
    <author>
      <name>Sayantan Datta</name>
    </author>
    <author>
      <name>Carl Marshall</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <author>
      <name>Zhao Dong</name>
    </author>
    <author>
      <name>Zhengqin Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3610548.3618203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3610548.3618203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website: https://sayan1an.github.io/din.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH Asia 2023 Conference Papers (SA Conference Papers '23),
  December 12--15, 2023, Sydney, NSW, Australia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2309.08387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.08387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09631v1</id>
    <updated>2023-09-18T10:08:30Z</updated>
    <published>2023-09-18T10:08:30Z</published>
    <title>Digital analysis of early color photographs taken using regular color
  screen processes</title>
    <summary>  Some early color photographic processes based on special color screen filters
pose specific challenges in their digitization and digital presentation. Those
challenges include dynamic range, resolution, and the difficulty of stitching
geometrically-repeating patterns. We describe a novel method used to digitize
the collection of early color photographs at the National Geographic Society
which makes use of a custom open-source software tool to analyze and precisely
stitch regular color screen processes.
</summary>
    <author>
      <name>Jan Hubička</name>
    </author>
    <author>
      <name>Linda Kimrová</name>
    </author>
    <author>
      <name>Kenzie Klaeser</name>
    </author>
    <author>
      <name>Sara Manco</name>
    </author>
    <author>
      <name>Doug Peterson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23738/RCASB.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23738/RCASB.009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, submitted to the proceedings of XVIII Color
  Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Color and Colorimetry. Multidisciplinary Contributions. Vol. XVIII
  A, 2023, 241-248</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2309.09631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1; I.4.5; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.01445v1</id>
    <updated>2023-10-01T08:30:15Z</updated>
    <published>2023-10-01T08:30:15Z</published>
    <title>Investigation on a Novel Length-Based Local Linear Subdivision Strategy
  for Triangular Meshes</title>
    <summary>  Triangular meshes are a widely used representation in the field of 3D
modeling. In this paper, we present a novel approach for edge length-based
linear subdivision on triangular meshes, along with two auxiliary techniques.
We conduct a comprehensive comparison of different subdivision methods in terms
of computational capabilities and mesh-enhancing abilities. Our proposed
approach demonstrates improved computational efficiency and generates fewer
elements with higher quality compared to existing methods. The improvement in
computational efficiency and mesh augmentation capability of our method is
further enhanced when working with the two auxiliary techniques presented in
this paper. Our novel strategy represents a significant contribution to the
field and has important implications for local mesh refinement, computer-aided
design, and isotropic remeshing.
</summary>
    <author>
      <name>Junyi Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2310.01445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.01445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.02881v1</id>
    <updated>2023-10-04T15:17:36Z</updated>
    <published>2023-10-04T15:17:36Z</published>
    <title>Immersive ExaBrick: Visualizing Large AMR Data in the CAVE</title>
    <summary>  Rendering large adaptive mesh refinement (AMR) data in real-time in virtual
reality (VR) environments is a complex challenge that demands sophisticated
techniques and tools. The proposed solution harnesses the ExaBrick framework
and integrates it as a plugin in COVISE, a robust visualization system equipped
with the VR-centric OpenCOVER render module. This setup enables direct
navigation and interaction within the rendered volume in a VR environment. The
user interface incorporates rendering options and functions, ensuring a smooth
and interactive experience. We show that high-quality volume rendering of AMR
data in VR environments at interactive rates is possible using GPUs.
</summary>
    <author>
      <name>Zhaoyang Wang</name>
    </author>
    <author>
      <name>Stefan Wesner</name>
    </author>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <link href="http://arxiv.org/abs/2310.02881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.02955v1</id>
    <updated>2023-10-04T16:44:38Z</updated>
    <published>2023-10-04T16:44:38Z</published>
    <title>Perceptual error optimization for Monte Carlo animation rendering</title>
    <summary>  Independently estimating pixel values in Monte Carlo rendering results in a
perceptually sub-optimal white-noise distribution of error in image space.
Recent works have shown that perceptual fidelity can be improved significantly
by distributing pixel error as blue noise instead. Most such works have focused
on static images, ignoring the temporal perceptual effects of animation
display. We extend prior formulations to simultaneously consider the spatial
and temporal domains, and perform an analysis to motivate a perceptually better
spatio-temporal error distribution. We then propose a practical error
optimization algorithm for spatio-temporal rendering and demonstrate its
effectiveness in various configurations.
</summary>
    <author>
      <name>Miša Korać</name>
    </author>
    <author>
      <name>Corentin Salaün</name>
    </author>
    <author>
      <name>Iliyan Georgiev</name>
    </author>
    <author>
      <name>Pascal Grittmann</name>
    </author>
    <author>
      <name>Philipp Slusallek</name>
    </author>
    <author>
      <name>Karol Myszkowski</name>
    </author>
    <author>
      <name>Gurprit Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3610548.3618146</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3610548.3618146" rel="related"/>
    <link href="http://arxiv.org/abs/2310.02955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.04080v3</id>
    <updated>2025-04-17T23:07:45Z</updated>
    <published>2023-10-06T08:18:04Z</published>
    <title>Robust Average Networks for Monte Carlo Denoising</title>
    <summary>  We present a method for converting denoising neural networks from spatial
into spatio-temporal ones by modifying the network architecture and loss
function. We insert Robust Average blocks at arbitrary depths in the network
graph. Each block performs latent space interpolation with trainable weights
and works on the sequence of image representations from the preceding spatial
components of the network. The temporal connections are kept live during
training by forcing the network to predict a denoised frame from subsets of the
input sequence. Using temporal coherence for denoising improves image quality
and reduces temporal flickering independent of scene or image complexity.
</summary>
    <author>
      <name>Javor Kalojanov</name>
    </author>
    <author>
      <name>Kimball Thurston</name>
    </author>
    <link href="http://arxiv.org/abs/2310.04080v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.04080v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.17838v2</id>
    <updated>2024-02-15T18:56:41Z</updated>
    <published>2023-10-27T01:36:35Z</published>
    <title>Real-time Animation Generation and Control on Rigged Models via Large
  Language Models</title>
    <summary>  We introduce a novel method for real-time animation control and generation on
rigged models using natural language input. First, we embed a large language
model (LLM) in Unity to output structured texts that can be parsed into diverse
and realistic animations. Second, we illustrate LLM's potential to enable
flexible state transition between existing animations. We showcase the
robustness of our approach through qualitative results on various rigged models
and motions.
</summary>
    <author>
      <name>Han Huang</name>
    </author>
    <author>
      <name>Fernanda De La Torre</name>
    </author>
    <author>
      <name>Cathy Mengying Fang</name>
    </author>
    <author>
      <name>Andrzej Banburski-Fahey</name>
    </author>
    <author>
      <name>Judith Amores</name>
    </author>
    <author>
      <name>Jaron Lanier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NeurIPS Workshop on ML for Creativity and Design 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.17838v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.17838v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19803v1</id>
    <updated>2023-10-04T06:53:49Z</updated>
    <published>2023-10-04T06:53:49Z</published>
    <title>ShanshuiDaDA: An Interactive, Generative System towards Chinese Shanshui
  Painting</title>
    <summary>  Shanshui, which means mountain and water, is an East Asian traditional brush
painting involving natural landscapes. This paper proposes an interactive and
generative system based on a Generative Adversarial Network(GAN), which helps
users draw Shanshui easily. We name this system and installation ShanshuiDaDA.
ShanshuiDaDA is trained with CycleGAN and wrapped with a web-based interface.
When participants scribble lines and sketch the landscape, the ShanshuiDaDA
will assist them in generating and creating a Chinese "Shanshui" painting in
real time.
</summary>
    <author>
      <name>Aven Le Zhou</name>
    </author>
    <author>
      <name>Qiufeng Wang</name>
    </author>
    <author>
      <name>Cheng-Hung Lo</name>
    </author>
    <author>
      <name>Kaizhu Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Machine Learning for Creativity and Design Workshop, the
  32nd Conference on Neural Information Processing Systems (NIPS 2018),
  Montreal, Canada. See:
  https://nips2018creativity.github.io/doc/shanshui_dada.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.19803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.17924v1</id>
    <updated>2023-11-14T13:16:34Z</updated>
    <published>2023-11-14T13:16:34Z</published>
    <title>Unrolling Virtual Worlds for Immersive Experiences</title>
    <summary>  This research pioneers a method for generating immersive worlds, drawing
inspiration from elements of vintage adventure games like Myst and employing
modern text-to-image models. We explore the intricate conversion of 2D
panoramas into 3D scenes using equirectangular projections, addressing the
distortions in perception that occur as observers navigate within the
encompassing sphere. Our approach employs a technique similar to "inpainting"
to rectify distorted projections, enabling the smooth construction of locally
coherent worlds. This provides extensive insight into the interrelation of
technology, perception, and experiential reality within human-computer
interaction.
</summary>
    <author>
      <name>Alexey Tikhonov</name>
    </author>
    <author>
      <name>Anton Repushko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for NeurIPS 2023 Workshop on Machine Learning for Creativity
  and Design</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.17924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.17924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 00A66, 68T45, 91C99, 68U35, 94A08" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.3.7; H.5.1; I.3.3; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.02896v1</id>
    <updated>2024-01-05T16:59:00Z</updated>
    <published>2024-01-05T16:59:00Z</published>
    <title>Particle-Wise Higher-Order SPH Field Approximation for DVR</title>
    <summary>  When employing Direct Volume Rendering (DVR) for visualizing volumetric
scalar fields, classification is generally performed on a piecewise constant or
piecewise linear approximation of the field on a viewing ray. Smoothed Particle
Hydrodynamics (SPH) data sets define volumetric scalar fields as the sum of
individual particle contributions, at highly varying spatial resolution. We
present an approach for approximating SPH scalar fields along viewing rays with
piece-wise polynomial functions of higher order. This is done by approximating
each particle contribution individually and then efficiently summing the
results, thus generating a higher-order representation of the field with a
resolution adapting to the data resolution in the volume.
</summary>
    <author>
      <name>Jonathan Fischer</name>
    </author>
    <author>
      <name>Martin Schulze</name>
    </author>
    <author>
      <name>Paul Rosenthal</name>
    </author>
    <author>
      <name>Lars Linsen</name>
    </author>
    <link href="http://arxiv.org/abs/2401.02896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.02896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13001v1</id>
    <updated>2024-01-22T12:33:11Z</updated>
    <published>2024-01-22T12:33:11Z</published>
    <title>PatternPortrait: Draw Me Like One of Your Scribbles</title>
    <summary>  This paper introduces a process for generating abstract portrait drawings
from pictures. Their unique style is created by utilizing single freehand
pattern sketches as references to generate unique patterns for shading. The
method involves extracting facial and body features from images and
transforming them into vector lines. A key aspect of the research is the
development of a graph neural network architecture designed to learn sketch
stroke representations in vector form, enabling the generation of diverse
stroke variations. The combination of these two approaches creates joyful
abstract drawings that are realized via a pen plotter. The presented process
garnered positive feedback from an audience of approximately 280 participants.
</summary>
    <author>
      <name>Sabine Wieluch</name>
    </author>
    <author>
      <name>Friedhelm Schwenker</name>
    </author>
    <link href="http://arxiv.org/abs/2401.13001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15245v1</id>
    <updated>2024-01-26T23:31:53Z</updated>
    <published>2024-01-26T23:31:53Z</published>
    <title>GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface
  Scattering Representation</title>
    <summary>  This paper presents a plugin that adds a representation of homogeneous and
heterogeneous, optically thick, translucent materials on the Blender 3D
modeling tool. The working principle of this plugin is based on a combination
of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based
subsurface scattering method (GenSSS). The proposed plugin has been implemented
using Mitsuba renderer, which is an open source rendering software. The
proposed plugin has been validated on measured subsurface scattering data. It's
shown that the proposed plugin visualizes homogeneous and heterogeneous
subsurface scattering effects, accurately, compactly and computationally
efficiently.
</summary>
    <author>
      <name>Barış Yıldırım</name>
    </author>
    <author>
      <name>Murat Kurt</name>
    </author>
    <link href="http://arxiv.org/abs/2401.15245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.15245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15664v1</id>
    <updated>2024-01-28T14:01:56Z</updated>
    <published>2024-01-28T14:01:56Z</published>
    <title>Learning Human-like Locomotion Based on Biological Actuation and Rewards</title>
    <summary>  We propose a method of learning a policy for human-like locomotion via deep
reinforcement learning based on a human anatomical model, muscle actuation, and
biologically inspired rewards, without any inherent control rules or reference
motions. Our main ideas involve providing a dense reward using metabolic energy
consumption at every step during the initial stages of learning and then
transitioning to a sparse reward as learning progresses, and adjusting the
initial posture of the human model to facilitate the exploration of locomotion.
Additionally, we compared and analyzed differences in learning outcomes across
various settings other than the proposed method.
</summary>
    <author>
      <name>Minkwan Kim</name>
    </author>
    <author>
      <name>Yoonsang Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3588028.3603646</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3588028.3603646" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH 2023 Posters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH '23: ACM SIGGRAPH 2023 Posters, July 2023, Article No.:
  5, Pages 1-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2401.15664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.15664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.09406v1</id>
    <updated>2023-12-07T08:38:42Z</updated>
    <published>2023-12-07T08:38:42Z</published>
    <title>VR-CAD Framework for Parametric Data Modification with a 3D Shape-based
  Interaction</title>
    <summary>  In this poster, we present a new VR-CAD framework, allowing user to modify
parametric CAD data with 3D interaction in an immersive environment. With this
framework, users can implicitly modify parameter values of CAD data with
co-localized 3D shape-based interaction. This poster describes the system
architecture and the interaction technique based on it.
</summary>
    <author>
      <name>Yujiro Okuya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI, LIMSI, EX-SITU</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Ladeveze</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMSI</arxiv:affiliation>
    </author>
    <author>
      <name>Cédric Fleury</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI, EX-SITU</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick Bourdot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMSI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2402.09406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.09406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.05296v1</id>
    <updated>2024-03-08T13:24:18Z</updated>
    <published>2024-03-08T13:24:18Z</published>
    <title>Cyclic Polygon Plots</title>
    <summary>  In this paper, we introduce the cyclic polygon plot, a representation based
on a novel projection concept for multi-dimensional values. Cyclic polygon
plots combine the typically competing requirements of quantitativeness,
image-space efficiency, and readability. Our approach is complemented with a
placement strategy based on its intrinsic features, resulting in a
dimensionality reduction strategy that is consistent with our overall concept.
As a result, our approach combines advantages from dimensionality reduction
techniques and quantitative plots, supporting a wide range of tasks in
multi-dimensional data analysis. We examine and discuss the overall properties
of our approach, and demonstrate its utility with a user study and selected
examples.
</summary>
    <author>
      <name>Maksim Schreck</name>
    </author>
    <author>
      <name>Peter Albers</name>
    </author>
    <author>
      <name>Filip Sadlo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.05296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.05296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.11141v1</id>
    <updated>2024-03-17T08:45:01Z</updated>
    <published>2024-03-17T08:45:01Z</published>
    <title>The Simplex Projection: Lossless Visualization of 4D Compositional Data
  on a 2D Canvas</title>
    <summary>  The simplex projection expands the capabilities of simplex plots (also known
as ternary plots) to achieve a lossless visualization of 4D compositional data
on a 2D canvas. Previously, this was only possible for 3D compositional data.
We demonstrate how our approach can be applied to individual data points, point
clouds, and continuous probability density functions on simplices. While we
showcase our visualization technique specifically for 4D compositional data, we
offer rigorous proofs that support its extension to compositional data of any
(finite) dimensionality.
</summary>
    <author>
      <name>Marvin Schmitt</name>
    </author>
    <author>
      <name>Yuga Hikida</name>
    </author>
    <author>
      <name>Stefan T Radev</name>
    </author>
    <author>
      <name>Filip Sadlo</name>
    </author>
    <author>
      <name>Paul-Christian Bürkner</name>
    </author>
    <link href="http://arxiv.org/abs/2403.11141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.11141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15902v1</id>
    <updated>2024-03-23T18:12:39Z</updated>
    <published>2024-03-23T18:12:39Z</published>
    <title>Utilizing Motion Matching with Deep Reinforcement Learning for Target
  Location Tasks</title>
    <summary>  We present an approach using deep reinforcement learning (DRL) to directly
generate motion matching queries for long-term tasks, particularly targeting
the reaching of specific locations. By integrating motion matching and DRL, our
method demonstrates the rapid learning of policies for target location tasks
within minutes on a standard desktop, employing a simple reward design.
Additionally, we propose a unique hit reward and obstacle curriculum scheme to
enhance policy learning in environments with moving obstacles.
</summary>
    <author>
      <name>Jeongmin Lee</name>
    </author>
    <author>
      <name>Taesoo Kwon</name>
    </author>
    <author>
      <name>Hyunju Shin</name>
    </author>
    <author>
      <name>Yoonsang Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics 2024 Short Papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.15902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.06359v2</id>
    <updated>2024-08-13T16:36:44Z</updated>
    <published>2024-04-09T14:53:59Z</published>
    <title>Towards Practical Meshlet Compression</title>
    <summary>  We propose a codec specifically designed for meshlet compression, optimized
for rapid data-parallel GPU decompression within a mesh shader. Our compression
strategy orders triangles in optimal generalized triangle strips (GTSs), which
we generate by formulating the creation as a mixed integer linear program
(MILP). Our method achieves index buffer compression rates of 16:1 compared to
the vertex pipeline and crack-free vertex attribute quantization based on user
preference. The 15.5 million triangles of our teaser image decompress and
render in 0.59 ms on an AMD Radeon RX 7900 XTX.
</summary>
    <author>
      <name>Bastian Kuth</name>
    </author>
    <author>
      <name>Max Oberberger</name>
    </author>
    <author>
      <name>Felix Kawala</name>
    </author>
    <author>
      <name>Sander Reitter</name>
    </author>
    <author>
      <name>Sebastian Michel</name>
    </author>
    <author>
      <name>Matthäus Chajdas</name>
    </author>
    <author>
      <name>Quirin Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.06359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.06359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.13364v2</id>
    <updated>2024-06-01T10:17:39Z</updated>
    <published>2024-05-22T05:46:14Z</published>
    <title>LucidRaster: GPU Software Rasterizer for Exact Order-Independent
  Transparency</title>
    <summary>  Transparency rendering is problematic and can be considered an open problem
in real-time graphics. There are many different algorithms currently available,
but handling complex scenes and achieving accurate, glitch-free results is
still costly.
  This paper describes LucidRaster: a software rasterizer running on a GPU
which allows for efficient exact rendering of complex transparent scenes. It
uses a new two-stage sorting technique and sample accumulation method. On
average it's faster than high-quality OIT approximations and only about 3x
slower than hardware alpha blending. It can be very efficient especially when
rendering scenes with high triangle density or high depth complexity.
</summary>
    <author>
      <name>Krzysztof Jakubowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.13364v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.13364v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.13730v1</id>
    <updated>2024-05-22T15:24:06Z</updated>
    <published>2024-05-22T15:24:06Z</published>
    <title>Subspace Mixed-FEM for Real-Time Heterogeneous Elastodynamics</title>
    <summary>  We propose a reduced space mixed finite element method (MFEM) built on a
Skinning Eigenmode subspace and material-aware cubature scheme. Our solver is
well-suited for simulating scenes with large material and geometric
heterogeneities in real-time. This mammoth geometry is composed of 98,175
vertices and 531,565 tetrahedral elements and with a heterogenous composition
of widely varying materials of muscles ($E= 5\times10^5$ Pa), joints
($E=1\times10^5$ Pa), and bone ($E=1\times10^{10}$ Pa). The resulting
simulation runs at 120 frames per second (FPS).
</summary>
    <author>
      <name>Ty Trusty</name>
    </author>
    <author>
      <name>Otman Benchekroun</name>
    </author>
    <author>
      <name>Eitan Grinspun</name>
    </author>
    <author>
      <name>Danny M. Kaufman</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.13730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.13730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.18609v1</id>
    <updated>2024-05-28T21:39:29Z</updated>
    <published>2024-05-28T21:39:29Z</published>
    <title>Actuators À La Mode: Modal Actuations for Soft Body Locomotion</title>
    <summary>  Traditional character animation specializes in characters with a rigidly
articulated skeleton and a bipedal/quadripedal morphology. This assumption
simplifies many aspects for designing physically based animations, like
locomotion, but comes with the price of excluding characters of arbitrary
deformable geometries. To remedy this, our framework makes use of a
spatio-temporal actuation subspace built off of the natural vibration modes of
the character geometry. The resulting actuation is coupled to a reduced fast
soft body simulation, allowing us to formulate a locomotion optimization
problem that is tractable for a wide variety of high resolution deformable
characters.
</summary>
    <author>
      <name>Otman Benchekroun</name>
    </author>
    <author>
      <name>Kaixiang Xie</name>
    </author>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Eitan Grinspun</name>
    </author>
    <author>
      <name>Sheldon Andrews</name>
    </author>
    <author>
      <name>Victor Zordan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.18609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.18609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.08647v1</id>
    <updated>2024-06-12T21:13:52Z</updated>
    <published>2024-06-12T21:13:52Z</published>
    <title>Optimized Dual-Volumes for Tetrahedral Meshes</title>
    <summary>  Constructing well-behaved Laplacian and mass matrices is essential for
tetrahedral mesh processing. Unfortunately, the \emph{de facto} standard linear
finite elements exhibit bias on tetrahedralized regular grids, motivating the
development of finite-volume methods. In this paper, we place existing methods
into a common construction, showing how their differences amount to the choice
of simplex centers. These choices lead to satisfaction or breakdown of
important properties: continuity with respect to vertex positions, positive
semi-definiteness of the implied Dirichlet energy, positivity of the mass
matrix, and unbiased-ness on regular grids. Based on this analysis, we propose
a new method for constructing dual-volumes which explicitly satisfy all of
these properties via convex optimization.
</summary>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SGP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.08647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.08647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09328v2</id>
    <updated>2025-01-10T21:20:30Z</updated>
    <published>2024-06-13T17:07:05Z</published>
    <title>Learnable Fractal Flames</title>
    <summary>  This work presents a differentiable rendering approach that allows latent
fractal flame parameters to be learned from image supervision using gradient
descent optimization. The approach extends the state-of-the-art in
differentiable iterated function system fractal rendering through support for
color images, non-linear generator functions, and multi-fractal compositions.
With this approach, artists can use reference images to quickly and intuitively
control the creation of fractals. We describe the approach and conduct a series
of experiments exploring its use, culminating in the creation of complex and
colorful fractal artwork based on famous paintings.
</summary>
    <author>
      <name>Jordan J. Bannister</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <link href="http://arxiv.org/abs/2406.09328v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09328v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11493v1</id>
    <updated>2024-06-17T12:57:30Z</updated>
    <published>2024-06-17T12:57:30Z</published>
    <title>Two-point Equidistant Projection and Degree-of-interest Filtering for
  Smooth Exploration of Geo-referenced Networks</title>
    <summary>  The visualization and interactive exploration of geo-referenced networks
poses challenges if the network's nodes are not evenly distributed. Our
approach proposes new ways of realizing animated transitions for exploring such
networks from an ego-perspective. We aim to reduce the required screen estate
while maintaining the viewers' mental map of distances and directions. A
preliminary study provides first insights of the comprehensiveness of animated
geographic transitions regarding directional relationships between start and
end point in different projections. Two use cases showcase how ego-perspective
graph exploration can be supported using less screen space than previous
approaches.
</summary>
    <author>
      <name>Max Franke</name>
    </author>
    <author>
      <name>Samuel Beck</name>
    </author>
    <author>
      <name>Steffen Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as short paper to IEEE VIS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.11493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.17360v2</id>
    <updated>2024-06-27T13:05:25Z</updated>
    <published>2024-06-25T08:24:12Z</published>
    <title>Non-Orthogonal Reduction for Rendering Fluorescent Materials in
  Non-Spectral Engines</title>
    <summary>  We propose a method to accurately handle fluorescence in a non-spectral (\eg,
tristimulus) rendering engine, showcasing color-shifting and increased
luminance effects. Core to our method is a principled reduction technique that
encodes the re-radiation into a low-dimensional matrix working in the space of
the renderer's Color Matching Functions (CMFs). Our process is independent of a
specific CMF set and allows for the addition of a non-visible ultraviolet band
during light transport. Our representation visually matches full spectral light
transport for measured fluorescent materials even for challenging illuminants.
</summary>
    <author>
      <name>Alban Fichet</name>
    </author>
    <author>
      <name>Laurent Belcour</name>
    </author>
    <author>
      <name>Pascal Barla</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.15150</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.15150" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.17360v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.17360v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00179v1</id>
    <updated>2024-06-28T18:34:53Z</updated>
    <published>2024-06-28T18:34:53Z</published>
    <title>Standardized Data-Parallel Rendering Using ANARI</title>
    <summary>  We propose and discuss a paradigm that allows for expressing
\emph{data-parallel} rendering with the classically non-parallel ANARI API. We
propose this as a new standard for data-parallel sci-vis rendering, describe
two different implementations of this paradigm, and use multiple sample
integrations into existing apps to show how easy it is to adopt this paradigm,
and what can be gained from doing so.
</summary>
    <author>
      <name>Ingo Wald</name>
    </author>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <author>
      <name>Jefferson Amstutz</name>
    </author>
    <author>
      <name>Qi Wu</name>
    </author>
    <author>
      <name>Kevin Griffin</name>
    </author>
    <author>
      <name>Milan Jaros</name>
    </author>
    <author>
      <name>Stefan Wesner</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.15686v1</id>
    <updated>2024-07-22T14:53:29Z</updated>
    <published>2024-07-22T14:53:29Z</published>
    <title>Differentiable Convex Polyhedra Optimization from Multi-view Images</title>
    <summary>  This paper presents a novel approach for the differentiable rendering of
convex polyhedra, addressing the limitations of recent methods that rely on
implicit field supervision. Our technique introduces a strategy that combines
non-differentiable computation of hyperplane intersection through duality
transform with differentiable optimization for vertex positioning with
three-plane intersection, enabling gradient-based optimization without the need
for 3D implicit fields. This allows for efficient shape representation across a
range of applications, from shape parsing to compact mesh reconstruction. This
work not only overcomes the challenges of previous approaches but also sets a
new standard for representing shapes with convex polyhedra.
</summary>
    <author>
      <name>Daxuan Ren</name>
    </author>
    <author>
      <name>Haiyi Mei</name>
    </author>
    <author>
      <name>Hezi Shi</name>
    </author>
    <author>
      <name>Jianmin Zheng</name>
    </author>
    <author>
      <name>Jianfei Cai</name>
    </author>
    <author>
      <name>Lei Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV2024 https://github.com/kimren227/DiffConvex</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.15686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.15686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.17675v2</id>
    <updated>2024-08-08T23:25:39Z</updated>
    <published>2024-07-24T23:55:23Z</published>
    <title>Drawing ellipses and elliptical arcs with piecewise cubic Bézier curve
  approximations</title>
    <summary>  This tutorial explains how to use piecewise cubic B\'ezier curves to draw
arbitrarily oriented ellipses and elliptical arcs. The geometric principles
discussed here result in strikingly simple interfaces for graphics functions
that can draw (approximate) circles, ellipses, and arcs of circles and
ellipses. C++ source code listings are included for these functions. Their code
size can be relatively small because they are designed to be used with a
graphics library or platform that draws B\'ezier curves, and the library or
platform is tasked with the actual rendering of the curves.
</summary>
    <author>
      <name>Jerry R. Van Aken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.17675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.17675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.4; I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.20868v1</id>
    <updated>2024-07-30T14:43:54Z</updated>
    <published>2024-07-30T14:43:54Z</published>
    <title>A Comparative Study of Neural Surface Reconstruction for Scientific
  Visualization</title>
    <summary>  This comparative study evaluates various neural surface reconstruction
methods, particularly focusing on their implications for scientific
visualization through reconstructing 3D surfaces via multi-view rendering
images. We categorize ten methods into neural radiance fields and neural
implicit surfaces, uncovering the benefits of leveraging distance functions
(i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the
reconstructed surfaces. Our findings highlight the efficiency and quality of
NeuS2 for reconstructing closed surfaces and identify NeUDF as a promising
candidate for reconstructing open surfaces despite some limitations. By sharing
our benchmark dataset, we invite researchers to test the performance of their
methods, contributing to the advancement of surface reconstruction solutions
for scientific visualization.
</summary>
    <author>
      <name>Siyuan Yao</name>
    </author>
    <author>
      <name>Weixi Song</name>
    </author>
    <author>
      <name>Chaoli Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.20868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.20868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.02418v1</id>
    <updated>2024-08-05T12:21:38Z</updated>
    <published>2024-08-05T12:21:38Z</published>
    <title>Demystifying Spatial Dependence: Interactive Visualizations for
  Interpreting Local Spatial Autocorrelation</title>
    <summary>  The Local Moran's I statistic is a valuable tool for identifying localized
patterns of spatial autocorrelation. Understanding these patterns is crucial in
spatial analysis, but interpreting the statistic can be difficult. To simplify
this process, we introduce three novel visualizations that enhance the
interpretation of Local Moran's I results. These visualizations can be
interactively linked to one another, and to established visualizations, to
offer a more holistic exploration of the results. We provide a JavaScript
library with implementations of these new visual elements, along with a web
dashboard that demonstrates their integrated use.
</summary>
    <author>
      <name>Lee Mason</name>
    </author>
    <author>
      <name>Blanaid Hicks</name>
    </author>
    <author>
      <name>Jonas Almeida</name>
    </author>
    <link href="http://arxiv.org/abs/2408.02418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.02418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04066v2</id>
    <updated>2024-08-12T14:50:03Z</updated>
    <published>2024-08-07T20:08:23Z</published>
    <title>Automatic Skinning using the Mixed Finite Element Method</title>
    <summary>  In this work, we show that exploiting additional variables in a mixed finite
element formulation of deformation leads to an efficient physics-based
character skinning algorithm. Taking as input, a user-defined rig, we show how
to efficiently compute deformations of the character mesh which respect
artist-supplied handle positions and orientations, but without requiring
complicated constraints on the physics solver, which can cause poor
performance. Rather we demonstrate an efficient, user controllable skinning
pipeline that can generate compelling character deformations, using a variety
of physics material models.
</summary>
    <author>
      <name>Hongcheng Song</name>
    </author>
    <author>
      <name>Dimitry Kachkovski</name>
    </author>
    <author>
      <name>Shaimaa Monem</name>
    </author>
    <author>
      <name>Abraham Kassauhun Negash</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <link href="http://arxiv.org/abs/2408.04066v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04066v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.05462v1</id>
    <updated>2024-08-10T07:12:26Z</updated>
    <published>2024-08-10T07:12:26Z</published>
    <title>Accelerating In-transit Isosurface Generation With Topology Preserving
  Compression</title>
    <summary>  Data visualization through isosurface generation is critical in various
scientific fields, including computational fluid dynamics, medical imaging, and
geophysics. However, the high cost of data sharing between simulation sources
and visualization resources poses a significant challenge. This paper
introduces a novel framework that leverages lossy compression to accelerate
in-transit isosurface generation. Our approach involves a Compressed
Hierarchical Representation (CHR) and topology-preserving compression to ensure
the fidelity of the isosurface generation. Experimental evaluations demonstrate
that our framework can achieve up to 4x speedup in visualization workflows,
making it a promising solution for real-time scientific data analysis.
</summary>
    <author>
      <name>Yanliang Li</name>
    </author>
    <author>
      <name>Jieyang Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/e-Science62913.2024.10678711</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/e-Science62913.2024.10678711" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2024 IEEE 20th International Conference on e-Science (e-Science)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2408.05462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.05462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18 (Primary) 68P30, 68U10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; I.3.7; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.06944v1</id>
    <updated>2024-08-13T15:01:50Z</updated>
    <published>2024-08-13T15:01:50Z</published>
    <title>Mesh Simplification For Unfolding</title>
    <summary>  We present a computational approach for unfolding 3D shapes isometrically
into the plane as a single patch without overlapping triangles. This is a hard,
sometimes impossible, problem, which existing methods are forced to soften by
allowing for map distortions or multiple patches. Instead, we propose a
geometric relaxation of the problem: we modify the input shape until it admits
an overlap-free unfolding. We achieve this by locally displacing vertices and
collapsing edges, guided by the unfolding process. We validate our algorithm
quantitatively and qualitatively on a large dataset of complex shapes and show
its proficiency by fabricating real shapes from paper.
</summary>
    <author>
      <name>Manas Bhargava</name>
    </author>
    <author>
      <name>Camille Schreck</name>
    </author>
    <author>
      <name>Marco Freire</name>
    </author>
    <author>
      <name>Pierre-Alexandre Hugron</name>
    </author>
    <author>
      <name>Sylvain Lefebvre</name>
    </author>
    <author>
      <name>Silvia Sellán</name>
    </author>
    <author>
      <name>Bernd Bickel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.06944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.06944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.12362v2</id>
    <updated>2025-02-16T00:10:43Z</updated>
    <published>2024-09-18T23:46:25Z</published>
    <title>Rest Shape Optimization for Sag-Free Discrete Elastic Rods</title>
    <summary>  We propose a new rest shape optimization framework to achieve sag-free
simulations of discrete elastic rods. To optimize rest shape parameters, we
formulate a minimization problem based on the kinetic energy with a regularizer
while imposing box constraints on these parameters to ensure the system's
stability. Our method solves the resulting constrained minimization problem via
the Gauss-Newton algorithm augmented with penalty methods. We demonstrate that
the optimized rest shape parameters enable discrete elastic rods to achieve
static equilibrium for a wide range of strand geometries and material
parameters.
</summary>
    <author>
      <name>Tetsuya Takahashi</name>
    </author>
    <author>
      <name>Christopher Batty</name>
    </author>
    <link href="http://arxiv.org/abs/2409.12362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.12362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.15746v1</id>
    <updated>2024-09-24T05:03:05Z</updated>
    <published>2024-09-24T05:03:05Z</published>
    <title>A Differentiable Material Point Method Framework for Shape Morphing</title>
    <summary>  We present a novel, physically-based morphing technique for elastic shapes,
leveraging the differentiable material point method (MPM) with space-time
control through per-particle deformation gradients to accommodate complex
topology changes. This approach, grounded in MPM's natural handling of dynamic
topologies, is enhanced by a chained iterative optimization technique, allowing
for the creation of both succinct and extended morphing sequences that maintain
coherence over time. Demonstrated across various challenging scenarios, our
method is able to produce detailed elastic deformation and topology
transitions, all grounded within our physics-based simulation framework.
</summary>
    <author>
      <name>Michael Xu</name>
    </author>
    <author>
      <name>Chang-Yong Song</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <author>
      <name>David Hyde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.15746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.15746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7.a; I.6.8.a; I.3.5.i" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00270v1</id>
    <updated>2024-09-30T22:45:53Z</updated>
    <published>2024-09-30T22:45:53Z</published>
    <title>Real-time Diverse Motion In-betweening with Space-time Control</title>
    <summary>  In this work, we present a data-driven framework for generating diverse
in-betweening motions for kinematic characters. Our approach injects dynamic
conditions and explicit motion controls into the procedure of motion
transitions. Notably, this integration enables a finer-grained spatial-temporal
control by allowing users to impart additional conditions, such as duration,
path, style, etc., into the in-betweening process. We demonstrate that our
in-betweening approach can synthesize both locomotion and unstructured motions,
enabling rich, versatile, and high-quality animation generation.
</summary>
    <author>
      <name>Yuchen Chu</name>
    </author>
    <author>
      <name>Zeshi Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3677388.3696327</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3677388.3696327" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at The 16th ACM SIGGRAPH Conference on Motion, Interaction,
  and Games (MIG '24)</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.00270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.03123v1</id>
    <updated>2024-10-04T03:39:15Z</updated>
    <published>2024-10-04T03:39:15Z</published>
    <title>Shrinking: Reconstruction of Parameterized Surfaces from Signed Distance
  Fields</title>
    <summary>  We propose a novel method for reconstructing explicit parameterized surfaces
from Signed Distance Fields (SDFs), a widely used implicit neural
representation (INR) for 3D surfaces. While traditional reconstruction methods
like Marching Cubes extract discrete meshes that lose the continuous and
differentiable properties of INRs, our approach iteratively contracts a
parameterized initial sphere to conform to the target SDF shape, preserving
differentiability and surface parameterization throughout. This enables
downstream applications such as texture mapping, geometry processing,
animation, and finite element analysis. Evaluated on the typical geometric
shapes and parts of the ABC dataset, our method achieves competitive
reconstruction quality, maintaining smoothness and differentiability crucial
for advanced computer graphics and geometric deep learning applications.
</summary>
    <author>
      <name>Haotian Yin</name>
    </author>
    <author>
      <name>Przemyslaw Musialski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, accepted by ICMLA</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.03123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.03123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.06330v1</id>
    <updated>2024-10-08T20:11:11Z</updated>
    <published>2024-10-08T20:11:11Z</published>
    <title>Local Surface Parameterizations via Geodesic Splines</title>
    <summary>  We present a general method for computing local parameterizations rooted at a
point on a surface, where the surface is described only through a signed
implicit function and a corresponding projection function. Using a two-stage
process, we compute several points radially emanating from the map origin, and
interpolate between them with a spline surface. The narrow interface of our
method allows it to support several kinds of geometry such as signed distance
functions, general analytic implicit functions, triangle meshes, neural
implicits, and point clouds. We demonstrate the high quality of our generated
parameterizations on a variety of examples, and show applications in local
texturing and surface curve drawing.
</summary>
    <author>
      <name>Abhishek Madan</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.06330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.06330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.13760v1</id>
    <updated>2024-10-17T16:55:14Z</updated>
    <published>2024-10-17T16:55:14Z</published>
    <title>Eyelid Fold Consistency in Facial Modeling</title>
    <summary>  Eyelid shape is integral to identity and likeness in human facial modeling.
Human eyelids are diverse in appearance with varied skin fold and epicanthal
fold morphology between individuals. Existing parametric face models express
eyelid shape variation to an extent, but do not preserve sufficient likeness
across a diverse range of individuals. We propose a new definition of eyelid
fold consistency and implement geometric processing techniques to model diverse
eyelid shapes in a unified topology. Using this method we reprocess data used
to train a parametric face model and demonstrate significant improvements in
face-related machine learning tasks.
</summary>
    <author>
      <name>Lohit Petikam</name>
    </author>
    <author>
      <name>Charlie Hewitt</name>
    </author>
    <author>
      <name>Fatemeh Saleh</name>
    </author>
    <author>
      <name>Tadas Baltrušaitis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3681758.3697987</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3681758.3697987" rel="related"/>
    <link href="http://arxiv.org/abs/2410.13760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.13760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18026v2</id>
    <updated>2024-11-03T19:50:56Z</updated>
    <published>2024-10-23T16:57:51Z</published>
    <title>EON: A practical energy-preserving rough diffuse BRDF</title>
    <summary>  We introduce the "Energy-preserving Oren--Nayar" (EON) model for reflection
from rough surfaces. Unlike the popular qualitative Oren--Nayar model (QON) and
its variants, our model is energy-preserving via analytical energy
compensation. We include self-contained GLSL source code for efficient
evaluation of the new model and importance sampling based on a novel technique
we term "Clipped Linearly Transformed Cosine" (CLTC) sampling.
</summary>
    <author>
      <name>Jamie Portsmouth</name>
    </author>
    <author>
      <name>Peter Kutz</name>
    </author>
    <author>
      <name>Stephen Hill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected uniform_lobe_sample function. Provided link to source code</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02095v2</id>
    <updated>2025-01-09T22:38:13Z</updated>
    <published>2024-11-04T13:59:01Z</published>
    <title>The evolution of volumetric video: A survey of smart transcoding and
  compression approaches</title>
    <summary>  Volumetric video, the capture and display of three-dimensional (3D) imagery,
has emerged as a revolutionary technology poised to transform the media
landscape, enabling immersive experiences that transcend the limitations of
traditional 2D video. One of the key challenges in this domain is the efficient
delivery of these high-bandwidth, data-intensive volumetric video streams,
which requires innovative transcoding and compression techniques. This research
paper explores the state-of-the-art in volumetric video compression and
delivery, with a focus on the potential of AI-driven solutions to address the
unique challenges posed by this emerging medium.
</summary>
    <author>
      <name>Preetish Kakkar</name>
    </author>
    <author>
      <name>Hariharan Ragothaman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2024.14401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2024.14401" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Graphics &amp; Animation (IJCGA)
  2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2411.02095v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02095v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.11244v1</id>
    <updated>2024-11-18T02:25:46Z</updated>
    <published>2024-11-18T02:25:46Z</published>
    <title>gDist: Efficient Distance Computation between 3D Meshes on GPU</title>
    <summary>  Computing maximum/minimum distances between 3D meshes is crucial for various
applications, i.e., robotics, CAD, VR/AR, etc. In this work, we introduce a
highly parallel algorithm (gDist) optimized for Graphics Processing Units
(GPUs), which is capable of computing the distance between two meshes with over
15 million triangles in less than 0.4 milliseconds (Fig. 1). By testing on
benchmarks with varying characteristics, the algorithm achieves remarkable
speedups over prior CPU-based and GPU-based algorithms on a commodity GPU
(NVIDIA GeForce RTX 4090). Notably, the algorithm consistently maintains
high-speed performance, even in challenging scenarios that pose difficulties
for prior algorithms.
</summary>
    <author>
      <name>Peng Fang</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Ruofeng Tong</name>
    </author>
    <author>
      <name>Hailong Li</name>
    </author>
    <author>
      <name>Min Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2411.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.03489v2</id>
    <updated>2024-12-09T09:33:15Z</updated>
    <published>2024-12-04T17:26:21Z</published>
    <title>Higher-order Differentiable Rendering</title>
    <summary>  We derive methods to compute higher order differentials (Hessians and
Hessian-vector products) of the rendering operator. Our approach is based on
importance sampling of a convolution that represents the differentials of
rendering parameters and shows to be applicable to both rasterization and path
tracing. We further suggest an aggregate sampling strategy to importance-sample
multiple dimensions of one convolution kernel simultaneously. We demonstrate
that this information improves convergence when used in higher-order optimizers
such as Newton or Conjugate Gradient relative to a gradient descent baseline in
several inverse rendering tasks.
</summary>
    <author>
      <name>Zican Wang</name>
    </author>
    <author>
      <name>Michael Fischer</name>
    </author>
    <author>
      <name>Tobias Ritschel</name>
    </author>
    <link href="http://arxiv.org/abs/2412.03489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.03489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.06164v1</id>
    <updated>2024-12-09T02:53:28Z</updated>
    <published>2024-12-09T02:53:28Z</published>
    <title>Polyhedral Discretizations for Elliptic PDEs</title>
    <summary>  We study the use of polyhedral discretizations for the solution of heat
diffusion and elastodynamic problems in computer graphics. Polyhedral meshes
are more natural for certain applications than pure triangular or quadrilateral
meshes, which thus received significant interest as an alternative
representation. We consider finite element methods using barycentric
coordinates as basis functions and the modern virtual finite element approach.
We evaluate them on a suite of classical graphics problems to understand their
benefits and limitations compared to standard techniques on simplicial
discretizations. Our analysis provides recommendations and a benchmark for
developing polyhedral meshing techniques and corresponding analysis techniques.
</summary>
    <author>
      <name>Junyu Liu</name>
    </author>
    <author>
      <name>Daniele Panozzo</name>
    </author>
    <author>
      <name>Mario Botsch</name>
    </author>
    <author>
      <name>Teseo Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/2412.06164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.06164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.16461v1</id>
    <updated>2024-12-21T03:26:16Z</updated>
    <published>2024-12-21T03:26:16Z</published>
    <title>Optimizing Parameters for Static Equilibrium of Discrete Elastic Rods
  with Active-Set Cholesky</title>
    <summary>  We propose a parameter optimization method for achieving static equilibrium
of discrete elastic rods. Our method simultaneously optimizes material
stiffness and rest shape parameters under box constraints to exactly enforce
zero net force while avoiding stability issues and violations of physical laws.
For efficiency, we split our constrained optimization problem into primal and
dual subproblems via the augmented Lagrangian method, while handling the dual
subproblem via simple vector updates. To efficiently solve the box-constrained
primal subproblem, we propose a new active-set Cholesky preconditioner. Our
method surpasses prior work in generality, robustness, and speed.
</summary>
    <author>
      <name>Tetsuya Takahashi</name>
    </author>
    <author>
      <name>Christopher Batty</name>
    </author>
    <link href="http://arxiv.org/abs/2412.16461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.16461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.01628v1</id>
    <updated>2025-01-03T04:23:39Z</updated>
    <published>2025-01-03T04:23:39Z</published>
    <title>Data Parallel Visualization and Rendering on the RAMSES Supercomputer
  with ANARI</title>
    <summary>  3D visualization and rendering in HPC are very heterogenous applications,
though fundamentally the tasks involved are well-defined and do not differ much
from application to application. The Khronos Group's ANARI standard seeks to
consolidate 3D rendering across sci-vis applications. This paper makes an
effort to convey challenges of 3D rendering and visualization with ANARI in the
context of HPC, where the data does not fit within a single node or GPU but
must be distributed. It also provides a gentle introduction to parallel
rendering concepts and challenges to practitioners from the field of HPC in
general. Finally, we present a case study showcasing data parallel rendering on
the new supercomputer RAMSES at the University of Cologne.
</summary>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <link href="http://arxiv.org/abs/2501.01628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.01628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.15279v1</id>
    <updated>2025-01-25T17:12:37Z</updated>
    <published>2025-01-25T17:12:37Z</published>
    <title>Polynomial 2D Biharmonic Coordinates for High-order Cages</title>
    <summary>  We derive closed-form expressions of biharmonic coordinates for 2D high-order
cages, enabling the transformation of the input polynomial curves into
polynomial curves of any order. Central to our derivation is the use of the
high-order boundary element method. We demonstrate the practicality and
effectiveness of our method on various 2D deformations. In practice, users can
easily manipulate the Bezier control points to perform the desired intuitive
deformation, as the biharmonic coordinates provide an enriched deformation
space and encourage the alignment between the boundary cage and its interior
geometry.
</summary>
    <author>
      <name>Shibo Liu</name>
    </author>
    <author>
      <name>Ligang Liu</name>
    </author>
    <author>
      <name>Xiao-Ming Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.15279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.15279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.05953v1</id>
    <updated>2025-02-09T16:46:27Z</updated>
    <published>2025-02-09T16:46:27Z</published>
    <title>Mul2MAR: A Multi-Marker Mobile Augmented Reality Application for
  Improved Visual Perception</title>
    <summary>  This paper presents an inexpensive Augmented Reality (AR) application which
is aimed to use with mobile devices. Our application is a marker based AR
application, and it can be used by inexpensive three dimensional (3D) red-cyan
glasses. In our AR application, we combine left and right views without
creating any uncomfortable situation for human eyes. We validate our mobile AR
application on several objects, scenes, and views. We show that 3D AR
perception can be obtained by using our inexpensive AR application [G\"ung\"or
and Kurt 2014].
</summary>
    <author>
      <name>Murat Kurt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.05953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.05953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.06616v3</id>
    <updated>2025-02-20T14:27:15Z</updated>
    <published>2025-02-10T16:12:47Z</published>
    <title>From Code to Canvas</title>
    <summary>  The web-based dynamic geometry software CindyJS is a versatile tool to create
interactive applications for mathematics and other topics. In this workshop, we
will look at a code package that makes the creation of animations in CindyJS
easier and more streamlined. Animations, which can then be embedded into
presentations or be used in (lecture) videos. The focus lies on the creation of
the animations themselves and some of the technical and artistic fundamentals
to do so.
</summary>
    <author>
      <name>Bernhard O. Werner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A workshop paper for the Bridges 2025 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.06616v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.06616v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00816v1</id>
    <updated>2025-03-02T09:45:06Z</updated>
    <published>2025-03-02T09:45:06Z</published>
    <title>Random Walks in Self-supervised Learning for Triangular Meshes</title>
    <summary>  This study addresses the challenge of self-supervised learning for 3D mesh
analysis. It presents an new approach that uses random walks as a form of data
augmentation to generate diverse representations of mesh surfaces. Furthermore,
it employs a combination of contrastive and clustering losses. The contrastive
learning framework maximizes similarity between augmented instances of the same
mesh while minimizing similarity between different meshes. We integrate this
with a clustering loss, enhancing class distinction across training epochs and
mitigating training variance. Our model's effectiveness is evaluated using mean
Average Precision (mAP) scores and a supervised SVM linear classifier on
extracted features, demonstrating its potential for various downstream tasks
such as object classification and shape retrieval.
</summary>
    <author>
      <name>Gal Yefet</name>
    </author>
    <author>
      <name>Ayellet Tal</name>
    </author>
    <link href="http://arxiv.org/abs/2503.00816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08166v1</id>
    <updated>2025-03-11T08:29:41Z</updated>
    <published>2025-03-11T08:29:41Z</published>
    <title>Dynamic Scene Reconstruction: Recent Advance in Real-time Rendering and
  Streaming</title>
    <summary>  Representing and rendering dynamic scenes from 2D images is a fundamental yet
challenging problem in computer vision and graphics. This survey provides a
comprehensive review of the evolution and advancements in dynamic scene
representation and rendering, with a particular emphasis on recent progress in
Neural Radiance Fields based and 3D Gaussian Splatting based reconstruction
methods. We systematically summarize existing approaches, categorize them
according to their core principles, compile relevant datasets, compare the
performance of various methods on these benchmarks, and explore the challenges
and future research directions in this rapidly evolving field. In total, we
review over 170 relevant papers, offering a broad perspective on the state of
the art in this domain.
</summary>
    <author>
      <name>Jiaxuan Zhu</name>
    </author>
    <author>
      <name>Hao Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16747v1</id>
    <updated>2025-03-20T23:29:24Z</updated>
    <published>2025-03-20T23:29:24Z</published>
    <title>SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality</title>
    <summary>  3D Gaussian Splatting (3DGS) has significantly improved the efficiency and
realism of three-dimensional scene visualization in several applications,
ranging from robotics to eXtended Reality (XR). This work presents SAGE
(Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel
framework designed to enhance the user experience by dynamically adapting the
Level of Detail (LOD) of different 3DGS objects identified via a semantic
segmentation. Experimental results demonstrate how SAGE effectively reduces
memory and computational overhead while keeping a desired target visual
quality, thus providing a powerful optimization for interactive XR
applications.
</summary>
    <author>
      <name>Chiara Schiavo</name>
    </author>
    <author>
      <name>Elena Camuffo</name>
    </author>
    <author>
      <name>Leonardo Badia</name>
    </author>
    <author>
      <name>Simone Milani</name>
    </author>
    <link href="http://arxiv.org/abs/2503.16747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01402v1</id>
    <updated>2025-04-02T06:39:50Z</updated>
    <published>2025-04-02T06:39:50Z</published>
    <title>A Survey on Physics-based Differentiable Rendering</title>
    <summary>  Physics-based differentiable rendering has emerged as a powerful technique in
computer graphics and vision, with a broad range of applications in solving
inverse rendering tasks. At its core, differentiable rendering enables the
computation of gradients with respect to scene parameters, allowing
optimization-based approaches to solve various problems. Over the past few
years, significant advancements have been made in both the underlying theory
and the practical implementations of differentiable rendering algorithms. In
this report, we provide a comprehensive overview of the current state of the
art in physics-based differentiable rendering, focusing on recent advances in
general differentiable rendering theory, Monte Carlo sampling strategy, and
computational efficiency.
</summary>
    <author>
      <name>Yunfan Zeng</name>
    </author>
    <author>
      <name>Guangyan Cai</name>
    </author>
    <author>
      <name>Shuang Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.01402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02747v1</id>
    <updated>2025-04-03T16:35:17Z</updated>
    <published>2025-04-03T16:35:17Z</published>
    <title>GEOPARD: Geometric Pretraining for Articulation Prediction in 3D Shapes</title>
    <summary>  We present GEOPARD, a transformer-based architecture for predicting
articulation from a single static snapshot of a 3D shape. The key idea of our
method is a pretraining strategy that allows our transformer to learn plausible
candidate articulations for 3D shapes based on a geometric-driven search
without manual articulation annotation. The search automatically discovers
physically valid part motions that do not cause detachments or collisions with
other shape parts. Our experiments indicate that this geometric pretraining
strategy, along with carefully designed choices in our transformer
architecture, yields state-of-the-art results in articulation inference in the
PartNet-Mobility dataset.
</summary>
    <author>
      <name>Pradyumn Goyal</name>
    </author>
    <author>
      <name>Dmitry Petrov</name>
    </author>
    <author>
      <name>Sheldon Andrews</name>
    </author>
    <author>
      <name>Yizhak Ben-Shabat</name>
    </author>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Evangelos Kalogerakis</name>
    </author>
    <link href="http://arxiv.org/abs/2504.02747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.04564v2</id>
    <updated>2025-04-10T17:36:22Z</updated>
    <published>2025-04-06T17:42:23Z</published>
    <title>GPU Volume Rendering with Hierarchical Compression Using VDB</title>
    <summary>  We propose a compression-based approach to GPU rendering of large volumetric
data using OpenVDB and NanoVDB. We use OpenVDB to create a lossy, fixed-rate
compressed representation of the volume on the host, and use NanoVDB to perform
fast, low-overhead, and on-the-fly decompression during rendering. We show that
this approach is fast, works well even in a (incoherent) Monte Carlo path
tracing context, can significantly reduce the memory requirements of volume
rendering, and can be used as an almost drop-in replacement into existing 3D
texture-based renderers.
</summary>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <author>
      <name>Milan Jaros</name>
    </author>
    <author>
      <name>Jefferson Amstutz</name>
    </author>
    <author>
      <name>Ingo Wald</name>
    </author>
    <link href="http://arxiv.org/abs/2504.04564v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.04564v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.09413v1</id>
    <updated>2025-04-13T03:04:25Z</updated>
    <published>2025-04-13T03:04:25Z</published>
    <title>Scalable Motion In-betweening via Diffusion and Physics-Based Character
  Adaptation</title>
    <summary>  We propose a two-stage framework for motion in-betweening that combines
diffusion-based motion generation with physics-based character adaptation. In
Stage 1, a character-agnostic diffusion model synthesizes transitions from
sparse keyframes on a canonical skeleton, allowing the same model to generalize
across diverse characters. In Stage 2, a reinforcement learning-based
controller adapts the canonical motion to the target character's morphology and
dynamics, correcting artifacts and enhancing stylistic realism. This design
supports scalable motion generation across characters with diverse skeletons
without retraining the entire model. Experiments on standard benchmarks and
stylized characters demonstrate that our method produces physically plausible,
style-consistent motions under sparse and long-range constraints.
</summary>
    <author>
      <name>Jia Qin</name>
    </author>
    <link href="http://arxiv.org/abs/2504.09413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.09413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10620v1</id>
    <updated>2025-04-14T18:20:47Z</updated>
    <published>2025-04-14T18:20:47Z</published>
    <title>SPreV</title>
    <summary>  SPREV, short for hyperSphere Reduced to two-dimensional Regular Polygon for
Visualisation, is a novel dimensionality reduction technique developed to
address the challenges of reducing dimensions and visualizing labeled datasets
that exhibit a unique combination of three characteristics: small class size,
high dimensionality, and low sample size. SPREV is designed not only to uncover
but also to visually represent hidden patterns within such datasets. Its
distinctive integration of geometric principles, adapted for discrete
computational environments, makes it an indispensable tool in the modern data
science toolkit, enabling users to identify trends, extract insights, and
navigate complex data efficiently and effectively.
</summary>
    <author>
      <name>Srivathsan Amruth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 Pages, 7 Figures, 3 Tables, 9 Algorithms, Opensource</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12237v1</id>
    <updated>2025-04-16T16:42:13Z</updated>
    <published>2025-04-16T16:42:13Z</published>
    <title>Stereoscopic Cylindrical Screen (SCS) Projection</title>
    <summary>  We present a technique for Stereoscopic Cylindrical Screen (SCS) Projection
of a world scene to a 360-degree canvas for viewing with 3D glasses. To
optimize the rendering pipeline, we render the scene to four cubemaps, before
sampling relevant cubemaps onto the canvas. For an interactive user experience,
we perform stereoscopic view rendering and off-axis projection to anchor the
image to the viewer. This technique is being used to project virtual worlds at
CMU ETC, and is a step in creating immersive viewing experiences.
</summary>
    <author>
      <name>Lim Ngian Xin Terry</name>
    </author>
    <author>
      <name>Adrian Xuan Wei Lim</name>
    </author>
    <author>
      <name>Ezra Hill</name>
    </author>
    <author>
      <name>Lynnette Hui Xian Ng</name>
    </author>
    <link href="http://arxiv.org/abs/2504.12237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13436v1</id>
    <updated>2025-04-18T03:21:34Z</updated>
    <published>2025-04-18T03:21:34Z</published>
    <title>RT-HDIST: Ray-Tracing Core-based Hausdorff Distance Computation</title>
    <summary>  The Hausdorff distance is a fundamental metric with widespread applications
across various fields. However, its computation remains computationally
expensive, especially for large-scale datasets. In this work, we present
RT-HDIST, the first Hausdorff distance algorithm accelerated by ray-tracing
cores (RT-cores). By reformulating the Hausdorff distance problem as a series
of nearest-neighbor searches and introducing a novel quantized index space,
RT-HDIST achieves significant reductions in computational overhead while
maintaining exact results. Extensive benchmarks demonstrate up to a
two-order-of-magnitude speedup over prior state-of-the-art methods,
underscoring RT-HDIST's potential for real-time and large-scale applications.
</summary>
    <author>
      <name>YoungWoo Kim</name>
    </author>
    <author>
      <name>Jaehong Lee</name>
    </author>
    <author>
      <name>Duksu Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.14216v1</id>
    <updated>2025-04-19T07:43:34Z</updated>
    <published>2025-04-19T07:43:34Z</published>
    <title>PyFRep: Shape Modeling with Differentiable Function Representation</title>
    <summary>  We propose a framework for performing differentiable geometric modeling based
on the Function Representation (FRep). The framework is built on top of modern
libraries for performing automatic differentiation allowing us to obtain
derivatives w.r.t. space or shape parameters. We demonstrate possible
applications of this framework: Curvature estimation for shape interrogation,
signed distance function computation and approximation and fitting shape
parameters of a parametric model to data. Our framework is released as
open-source.
</summary>
    <author>
      <name>Pierre-Alain Fayolle</name>
    </author>
    <author>
      <name>Evgenii Maltsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 10 figures. Code available at
  https://github.com/fayolle/PyFRep</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.14216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.14216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15657v1</id>
    <updated>2025-04-22T07:28:28Z</updated>
    <published>2025-04-22T07:28:28Z</published>
    <title>Neural Kinematic Bases for Fluids</title>
    <summary>  We propose mesh-free fluid simulations that exploit a kinematic neural basis
for velocity fields represented by an MLP. We design a set of losses that
ensures that these neural bases satisfy fundamental physical properties such as
orthogonality, divergence-free, boundary alignment, and smoothness. Our neural
bases can then be used to fit an input sketch of a flow, which will inherit the
same fundamental properties from the bases. We then can animate such flow in
real-time using standard time integrators. Our neural bases can accommodate
different domains and naturally extend to three dimensions.
</summary>
    <author>
      <name>Yibo Liu</name>
    </author>
    <author>
      <name>Paul Kry</name>
    </author>
    <author>
      <name>Kenny Erleben</name>
    </author>
    <author>
      <name>Noam Aigerman</name>
    </author>
    <author>
      <name>Sune Darkner</name>
    </author>
    <author>
      <name>Teseo Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02061v1</id>
    <updated>2025-05-04T10:39:41Z</updated>
    <published>2025-05-04T10:39:41Z</published>
    <title>Diffeomorphic Reconstruction Of A 2D Simple Non Parametric Manifold From
  Level Set Data Via Shape Gradients</title>
    <summary>  A variational approach to the reconstruction of a shape (2D simple manifolds)
as triangulated surface from given level set using shape gradients is
presented. It involves an energy functional that depends on the local shape
characteristics of the surface. Minimization of the energy through an iterative
procedure using the gradient descent method yields a triangulated surface mesh
which matches the boundary of the object of interest and this model ensures the
smoothness of the boundary.
</summary>
    <author>
      <name>Shafeequdheen P</name>
    </author>
    <author>
      <name>Jyotiranjan Nayak</name>
    </author>
    <author>
      <name>Vijayakrishna Rowthu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 105 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09350v1</id>
    <updated>2025-05-14T12:57:17Z</updated>
    <published>2025-05-14T12:57:17Z</published>
    <title>Procedural Low-Poly Terrain Generation with Terracing for Computer Games</title>
    <summary>  In computer games, traditional procedural terrain generation relies on a grid
of vertices, with each point representing terrain elevation. For each square in
the grid, two triangles are created by connecting fixed vertex indices,
resulting in a continuous 3D surface. While this method is efficient for
modelling smooth terrain, the grid-like structure lacks the distinct, chaotic
appearance of low-poly objects and is not suitable to be used for our purposes.
The technique presented in this paper aims to solve the following problem:
Generate random, low-poly looking terraced terrain with different biomes and
add vegetation to create an interesting environment.
</summary>
    <author>
      <name>Richard Tivolt</name>
    </author>
    <link href="http://arxiv.org/abs/2505.09350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11799v1</id>
    <updated>2025-05-17T03:01:21Z</updated>
    <published>2025-05-17T03:01:21Z</published>
    <title>Generating Digital Models Using Text-to-3D and Image-to-3D Prompts:
  Critical Case Study</title>
    <summary>  In the world of technology and AI, digital models play an important role in
our lives and are an essential part of the digital twins of real-world objects.
They can be created by designers, artists, or game developers using spline
curves and surfaces, meshes, and voxels, but making such models is too
time-consuming. With the growth of AI tools, there is interest in the automated
generation of 3D models, such as generative design approaches, which can save
creators valuable time. This paper reviews several online 3D model generators
and critically analyses the results, hoping to see higher-quality results from
different prompts.
</summary>
    <author>
      <name>Rushan Ziatdinov</name>
    </author>
    <author>
      <name>Rifkat Nabiyev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SmartIndustryCon65166.2025.10986113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SmartIndustryCon65166.2025.10986113" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2025 International Russian Smart Industry Conference
  (SmartIndustryCon)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2505.11799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.2819v1</id>
    <updated>2010-08-17T05:04:20Z</updated>
    <published>2010-08-17T05:04:20Z</published>
    <title>A symmetric motion picture of the twist-spun trefoil</title>
    <summary>  With the aid of a computer, we provide a motion picture of the twist-spun
trefoil which exhibits the periodicity well.
</summary>
    <author>
      <name>Ayumu Inoue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 21 figures, and 5 movies</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.2819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.2819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 57Q45, Secondary 68U05, 68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5211v1</id>
    <updated>2014-07-19T18:14:20Z</updated>
    <published>2014-07-19T18:14:20Z</published>
    <title>Development &amp; Implementation of a PyMOL 'putty' Representation</title>
    <summary>  The PyMOL molecular graphics program has been modified to introduce a new
'putty' cartoon representation, akin to the 'sausage'-style representation of
the MOLMOL molecular visualization (MolVis) software package. This document
outlines the development and implementation of the putty representation.
</summary>
    <author>
      <name>Cameron Mura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.5211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6649v1</id>
    <updated>2014-12-20T12:51:24Z</updated>
    <published>2014-12-20T12:51:24Z</published>
    <title>Qualitative shape representation based on the qualitative relative
  direction and distance calculus eOPRAm</title>
    <summary>  This document serves as a brief technical report, detailing the processes
used to represent and reconstruct simplified polygons using qualitative spatial
descriptions, as defined by the eOPRAm qualitative spatial calculus.
</summary>
    <author>
      <name>Christopher H. Dorr</name>
    </author>
    <author>
      <name>Reinhard Moratz</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08873v1</id>
    <updated>2020-04-22T22:01:47Z</updated>
    <published>2020-04-22T22:01:47Z</published>
    <title>Knot Morphing Algorithm for Quantum `Fragile Topology'</title>
    <summary>  A knot theoretic algorithm is proposed to model `fragile topology' of quantum
physics.
</summary>
    <author>
      <name>Kirk E. Jordan</name>
    </author>
    <author>
      <name>Ji Li</name>
    </author>
    <author>
      <name>Thomas J. Peters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures, submitted to Physics Letters A</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="57M25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09244v1</id>
    <updated>2021-11-17T17:22:10Z</updated>
    <published>2021-11-17T17:22:10Z</published>
    <title>Evaluations of The Hierarchical Subspace Iteration Method</title>
    <summary>  This document contains additional experiments concerned with the evaluation
of the Hierarchical Subspace Iteration Method, which is introduced
in~\cite{Nasikun2021}}
</summary>
    <author>
      <name>Ahmad Nasikun</name>
    </author>
    <author>
      <name>Klaus Hildebrandt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.09244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; G.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09300v1</id>
    <updated>2022-01-23T15:57:11Z</updated>
    <published>2022-01-23T15:57:11Z</published>
    <title>Poncelet Spatio-Temporal Surfaces and Tangles</title>
    <summary>  We explore geometric and topological properties of 3d surfaces swept by
Poncelet triangles, as well as tangles formed by associated points.
</summary>
    <author>
      <name>Claudio Esperança</name>
    </author>
    <author>
      <name>Ronaldo Garcia</name>
    </author>
    <author>
      <name>Dan Reznik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures, 4 live apps</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.09300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="57M25, 97R60, 51H25, 37A10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.12822v1</id>
    <updated>2023-09-27T18:12:23Z</updated>
    <published>2023-09-27T18:12:23Z</published>
    <title>A Note on Ribbon-based Biharmonic Surface Patches</title>
    <summary>  In this short note we describe a simple adaptation of biharmonic surfaces to
interpolate boundary cross-derivatives given in ribbon form, and compare with
the recently proposed Generalized B-spline patches.
</summary>
    <author>
      <name>Márton Vaitkus</name>
    </author>
    <link href="http://arxiv.org/abs/2311.12822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3974v2</id>
    <updated>2010-01-23T00:51:58Z</updated>
    <published>2010-01-22T12:57:59Z</published>
    <title>Modelacion y Visualizacion Tridimensional Interactiva de Variables
  Electricas en Celdas de Electro-Obtencion con Electrodos Bipolares</title>
    <summary>  The use of floating bipolar electrodes in electrowinning cells of copper
constitutes a nonconventional technology that promises economic and operational
impacts. This paper presents a computational tool for the simulation and
analysis of such electrochemical cells. A new model is developed for floating
electrodes and a method of finite difference is used to obtain the
threedimensional distribution of the potential and the field of current density
inside the cell. The analysis of the results is based on a technique for the
interactive visualization of three-dimensional vectorial fields as lines of
flow.
</summary>
    <author>
      <name>César Mena Labraña</name>
    </author>
    <author>
      <name>Ricardo Sánchez Schulz</name>
    </author>
    <author>
      <name>Lautaro Salazar Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, in Spanish. See also arXiv:1001.4002v1 [cs.GR].
  The only change in V2 is the updating of a reference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Anales del XIV Congreso de la Asociacion Chilena de Control
  Automatico, ACCA, 2000, pp. 362-367</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3974v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3974v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4002v1</id>
    <updated>2010-01-22T18:23:27Z</updated>
    <published>2010-01-22T18:23:27Z</published>
    <title>Aplicacion Grafica para el estudio de un Modelo de Celda Electrolitica
  usando Tecnicas de Visualizacion de Campos Vectoriales</title>
    <summary>  The use of floating bipolar electrodes in electrowinning cells of copper
constitutes a nonconventional technology that promises economic and operational
impacts. This thesis presents a computational tool for the simulation and
analysis of such electrochemical cells. A new model is developed for floating
electrodes and a method of finite difference is used to obtain the
threedimensional distribution of the potential and the field of current density
inside the cell. The analysis of the results is based on a technique for the
interactive visualization of three-dimensional vectorial fields as lines of
flow.
</summary>
    <author>
      <name>César Mena Labraña</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.1291.4082</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.1291.4082" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Engineer Thesis, Universidad de Concepcion, 2000, 105
  pages, 22 figures, in Spanish. The main results are also available in
  arXiv:1001.3974v1 [cs.GR]</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.04496v1</id>
    <updated>2025-03-06T14:44:25Z</updated>
    <published>2025-03-06T14:44:25Z</published>
    <title>Learning Object Placement Programs for Indoor Scene Synthesis with
  Iterative Self Training</title>
    <summary>  Data driven and autoregressive indoor scene synthesis systems generate indoor
scenes automatically by suggesting and then placing objects one at a time.
Empirical observations show that current systems tend to produce incomplete
next object location distributions. We introduce a system which addresses this
problem. We design a Domain Specific Language (DSL) that specifies functional
constraints. Programs from our language take as input a partial scene and
object to place. Upon execution they predict possible object placements. We
design a generative model which writes these programs automatically. Available
3D scene datasets do not contain programs to train on, so we build upon
previous work in unsupervised program induction to introduce a new program
bootstrapping algorithm. In order to quantify our empirical observations we
introduce a new evaluation procedure which captures how well a system models
per-object location distributions. We ask human annotators to label all the
possible places an object can go in a scene and show that our system produces
per-object location distributions more consistent with human annotators. Our
system also generates indoor scenes of comparable quality to previous systems
and while previous systems degrade in performance when training data is sparse,
our system does not degrade to the same degree.
</summary>
    <author>
      <name>Adrian Chang</name>
    </author>
    <author>
      <name>Kai Wang</name>
    </author>
    <author>
      <name>Yuanbo Li</name>
    </author>
    <author>
      <name>Manolis Savva</name>
    </author>
    <author>
      <name>Angel X. Chang</name>
    </author>
    <author>
      <name>Daniel Ritchie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 20 figures Subjects: Graphics (cs.GR), Computer Vision and
  Pattern Recognition (cs.CV), Machine Learning (cs.LG)</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.04496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.04496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001017v1</id>
    <updated>2000-01-21T10:02:49Z</updated>
    <published>2000-01-21T10:02:49Z</published>
    <title>Bezier Curves Intersection Using Relief Perspective</title>
    <summary>  Presented paper describes the method for finding the intersection of class
space rational Bezier curves. The problem curve/curve intersection belongs
among basic geometric problems and the aim of this article is to describe the
new technique to solve the problem using relief perspective and Bezier
clipping.
</summary>
    <author>
      <name>Radoslav Hlusek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, to appear in Proceedings of WSCG'2000 in Plzen,
  Czech Republic</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0001017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.3.5; J.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.0660v1</id>
    <updated>2007-08-05T05:25:45Z</updated>
    <published>2007-08-05T05:25:45Z</published>
    <title>Network synchronizability analysis: the theory of subgraphs and
  complementary graphs</title>
    <summary>  In this paper, subgraphs and complementary graphs are used to analyze the
network synchronizability. Some sharp and attainable bounds are provided for
the eigenratio of the network structural matrix, which characterizes the
network synchronizability, especially when the network's corresponding graph
has cycles, chains, bipartite graphs or product graphs as its subgraphs.
</summary>
    <author>
      <name>Zhisheng Duan</name>
    </author>
    <author>
      <name>Chao Liu</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physd.2007.12.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physd.2007.12.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.0660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2368v1</id>
    <updated>2010-06-11T19:05:05Z</updated>
    <published>2010-06-11T19:05:05Z</published>
    <title>L2-optimal image interpolation and its applications to medical imaging</title>
    <summary>  Digital medical images are always displayed scaled to fit particular view.
Interpolation is responsible for this scaling, and if not done properly, can
significantly degrade diagnostic image quality. However, theoretically-optimal
interpolation algorithms may also be the most time-consuming and impractical.
We propose a new approach, adapted to the needs of digital medical imaging, to
combine high interpolation speed and superior L2-optimal image quality.
</summary>
    <author>
      <name>Oleg Pianykh</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.3661v1</id>
    <updated>2010-06-18T10:40:23Z</updated>
    <published>2010-06-18T10:40:23Z</published>
    <title>Fractal Basins and Boundaries in 2D Maps inspired in Discrete Population
  Models</title>
    <summary>  Two-dimensional maps can model interactions between populations. Despite
their simplicity, these dynamical systems can show some complex situations, as
multistability or fractal boundaries between basins that lead to remarkable
pictures. Some of them are shown and explained here for three different 2D
discrete models.
</summary>
    <author>
      <name>Daniele Fournier-Prunaret</name>
    </author>
    <author>
      <name>Ricardo Lopez-Ruiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.3661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.3661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6336v1</id>
    <updated>2013-01-27T09:35:33Z</updated>
    <published>2013-01-27T09:35:33Z</published>
    <title>Approximation of Polyhedral Surface Uniformization</title>
    <summary>  We present a constructive approach for approximating the conformal map
(uniformization) of a polyhedral surface to a canonical domain in the plane.
The main tool is a characterization of convex spaces of quasiconformal
simplicial maps and their approximation properties. As far as we are aware,
this is the first algorithm proved to approximate the uniformization of general
polyhedral surfaces.
</summary>
    <author>
      <name>Yaron Lipman</name>
    </author>
    <link href="http://arxiv.org/abs/1301.6336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2276v1</id>
    <updated>2013-05-10T07:58:06Z</updated>
    <published>2013-05-10T07:58:06Z</published>
    <title>The effects of computer assisted and distance learning of geometric
  modelling</title>
    <summary>  The effects of computer-assisted and distance learning of geometric modeling
and computer aided geometric design are studied. It was shown that computer
algebra systems and dynamic geometric environments can be considered as
excellent tools for teaching mathematical concepts of mentioned areas, and
distance education technologies would be indispensable for consolidation of
successfully passed topics.
</summary>
    <author>
      <name>Omer Faruk Sozcu</name>
    </author>
    <author>
      <name>Rushan Ziatdinov</name>
    </author>
    <author>
      <name>Ismail Ipek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Researcher 39(1-2), 175-181, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.2276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00754v1</id>
    <updated>2016-09-02T21:47:47Z</updated>
    <published>2016-09-02T21:47:47Z</published>
    <title>A heuristic extending the Squarified treemapping algorithm</title>
    <summary>  A heuristic extending the Squarified Treemap technique for the representation
of hierarchical information as treemaps is presented. The original technique
gives high quality treemap views, since items are laid out with rectangles that
approximate squares, allowing easy comparison and selection operations. New key
steps, with a low computational impact, have been introduced to yield treemaps
with even better aspect ratios and higher homogeneity among items.
</summary>
    <author>
      <name>Antonio Cesarano</name>
    </author>
    <author>
      <name>FIlomena Ferrucci</name>
    </author>
    <author>
      <name>Mario Torre</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01499v2</id>
    <updated>2017-03-15T14:55:52Z</updated>
    <published>2017-03-04T17:37:32Z</published>
    <title>A Machine-Learning Framework for Design for Manufacturability</title>
    <summary>  this is a duplicate submission(original is arXiv:1612.02141). Hence want to
withdraw it
</summary>
    <author>
      <name>Aditya Balu</name>
    </author>
    <author>
      <name>Sambit Ghadai</name>
    </author>
    <author>
      <name>Gavin Young</name>
    </author>
    <author>
      <name>Soumik Sarkar</name>
    </author>
    <author>
      <name>Adarsh Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">this is a duplicate submission. Hence want to withdraw it</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03497v1</id>
    <updated>2017-06-12T08:04:42Z</updated>
    <published>2017-06-12T08:04:42Z</published>
    <title>A filter based approach for inbetweening</title>
    <summary>  We present a filter based approach for inbetweening. We train a convolutional
neural network to generate intermediate frames. This network aim to generate
smooth animation of line drawings. Our method can process scanned images
directly. Our method does not need to compute correspondence of lines and
topological changes explicitly. We experiment our method with real animation
production data. The results show that our method can generate intermediate
frames partially.
</summary>
    <author>
      <name>Yuichi Yagi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in Japanese</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01597v1</id>
    <updated>2017-08-20T19:24:17Z</updated>
    <published>2017-08-20T19:24:17Z</published>
    <title>Multi-color image compression-encryption algorithm based on chaotic
  system and fuzzy transform</title>
    <summary>  In this paper an algorithm for multi-color image compression-encryption is
introduced. For compression step fuzzy transform based on exponential b-spline
function is used. In encryption step, a novel combination chaotic system based
on Sine and Tent systems is proposed. Also in the encryption algorithm, 3D
shift based on chaotic system is introduced. The simulation results and
security analysis show that the proposed algorithm is secure and efficient.
</summary>
    <author>
      <name>M. Zarebnia</name>
    </author>
    <author>
      <name>R. Kianfar</name>
    </author>
    <author>
      <name>R. Parvaz</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02222v1</id>
    <updated>2018-07-06T02:20:16Z</updated>
    <published>2018-07-06T02:20:16Z</published>
    <title>Digital Geometry, a Survey</title>
    <summary>  This paper provides an overview of modern digital geometry and topology
through mathematical principles, algorithms, and measurements. It also covers
recent developments in the applications of digital geometry and topology
including image processing, computer vision, and data science. Recent research
strongly showed that digital geometry has made considerable contributions to
modelings and algorithms in image segmentation, algorithmic analysis, and
BigData analytics.
</summary>
    <author>
      <name>Li Chen</name>
    </author>
    <author>
      <name>David Coeurjolly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages ; 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00369v1</id>
    <updated>2020-02-02T11:32:30Z</updated>
    <published>2020-02-02T11:32:30Z</published>
    <title>Non-Euclidean Virtual Reality IV: Sol</title>
    <summary>  This article presents virtual reality software designed to explore the Sol
geometry. The simulation is available on 3-dimensional.space/sol.html
</summary>
    <author>
      <name>Rémi Coulon</name>
    </author>
    <author>
      <name>Elisabetta A. Matsumoto</name>
    </author>
    <author>
      <name>Henry Segerman</name>
    </author>
    <author>
      <name>Steve Trettel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A09, 00A66, 53A35, 57K35, 51-04, 68U05, 37D40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.06139v2</id>
    <updated>2021-03-27T04:41:26Z</updated>
    <published>2021-03-09T10:52:51Z</published>
    <title>On the Complexity of the CSG Tree Extraction Problem</title>
    <summary>  In this short note, we discuss the complexity of the search space for the
problem of finding a CSG expression (or CSG tree) corresponding to an input
point-cloud and a list of fitted solid primitives.
</summary>
    <author>
      <name>Markus Friedrich</name>
    </author>
    <author>
      <name>Pierre-Alain Fayolle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Add references for the programming language based approaches and the
  construction of the intersection graph</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.06139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.06139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.15116v1</id>
    <updated>2021-04-30T16:57:13Z</updated>
    <published>2021-04-30T16:57:13Z</published>
    <title>Towards Flying through Modular Forms</title>
    <summary>  Modular forms are highly self-symmetric functions studied in number theory,
with connections to several areas of mathematics. But they are rarely
visualized. We discuss ongoing work to compute and visualize modular forms as
3D surfaces and to use these techniques to make videos flying around the peaks
and canyons of these "modular terrains." Our goal is to make beautiful
visualizations exposing the symmetries of these functions.
</summary>
    <author>
      <name>David Lowry-Duda</name>
    </author>
    <author>
      <name>Adam Sakareassen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, for Bridges</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.15116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.15116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11F03" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15179v1</id>
    <updated>2021-06-29T08:51:23Z</updated>
    <published>2021-06-29T08:51:23Z</published>
    <title>Wrong Colored Vermeer: Color-Symmetric Image Distortion</title>
    <summary>  Color symmetry implies that the colors of geometrical objects are assigned
according to their symmetry properties. It is defined by associating the
elements of the symmetry group with a color permutation. I use this concept for
generative art and apply symmetry-consistent color distortions to images of
paintings by Johannes Vermeer. The color permutations are realized as mappings
of the HSV color space onto itself.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15306v1</id>
    <updated>2021-06-08T14:57:25Z</updated>
    <published>2021-06-08T14:57:25Z</published>
    <title>Artificial Intelligence in Minimally Invasive Interventional Treatment</title>
    <summary>  Minimally invasive image guided treatment procedures often employ advanced
image processing algorithms. The recent developments of artificial intelligence
algorithms harbor potential to further enhance this domain. In this article we
explore several application areas within the minimally invasive treatment space
and discuss the deployment of artificial intelligence within these areas.
</summary>
    <author>
      <name>Daniel Ruijters</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; I.2.10; I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.02545v3</id>
    <updated>2022-09-29T00:21:31Z</updated>
    <published>2021-12-05T11:23:50Z</published>
    <title>New Properties and Invariants of Harmonic Polygons</title>
    <summary>  Via simulation, we discover and prove curious new Euclidean properties and
invariants of the Poncelet family of harmonic polygons.
</summary>
    <author>
      <name>Ronaldo Garcia</name>
    </author>
    <author>
      <name>Dan Reznik</name>
    </author>
    <author>
      <name>Pedro Roitman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures, 3 tables, 8 videos</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.02545v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.02545v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="51M04, 51N20, 51N35, 68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.01934v1</id>
    <updated>2020-11-03T10:50:05Z</updated>
    <published>2020-11-03T10:50:05Z</published>
    <title>Palette diagram: A Python package for visualization of collective
  categorical data</title>
    <summary>  Categorical data, wherein a numerical quantity is assigned to each category
(nominal variable), are ubiquitous in data science. A palette diagram is a
visualization tool for a large number of categorical datasets, each comprising
several categories.
</summary>
    <author>
      <name>Chihiro Noguchi</name>
    </author>
    <author>
      <name>Tatsuro Kawamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.01934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.01934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07532v1</id>
    <updated>2020-11-15T14:00:27Z</updated>
    <published>2020-11-15T14:00:27Z</published>
    <title>Aquanims -- Area-Preserving Animated Transitions based on a Hydraulic
  Metaphor</title>
    <summary>  We propose "Aquanims" as new design metaphors for animated transitions that
preserve displayed areas during the transformation. As liquids are
incompressible fluids, we use a hydraulic metaphor to convey the sense of area
preservation during animated transitions. We study the design space of Aquanims
for rectangle-based charts.
</summary>
    <author>
      <name>Michael Aupetit</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.11134v1</id>
    <updated>2020-11-22T23:22:49Z</updated>
    <published>2020-11-22T23:22:49Z</published>
    <title>Differentiable Computational Geometry for 2D and 3D machine learning</title>
    <summary>  With the growth of machine learning algorithms with geometry primitives, a
high-efficiency library with differentiable geometric operators are desired. We
present an optimized Differentiable Geometry Algorithm Library (DGAL) loaded
with implementations of differentiable operators for geometric primitives like
lines and polygons. The library is a header-only templated C++ library with GPU
support. We discuss the internal design of the library and benchmark its
performance on some tasks with other implementations.
</summary>
    <author>
      <name>Yuanxin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08210v1</id>
    <updated>2021-12-23T11:29:49Z</updated>
    <published>2021-12-23T11:29:49Z</published>
    <title>Comparative Study of Cloud and Non-Cloud Gaming Platform: Apercu</title>
    <summary>  Nowadays game engines are imperative for building 3D applications and games.
This is for the reason that the engines appreciably reduce resources for
employing obligatory but intricate utilities. This paper elucidates about a
game engine and its foremost elements. It portrays a number of special kinds of
contemporary game engines by way of their aspects, procedure and deliberates
their stipulations with comparison.
</summary>
    <author>
      <name>Prerna Mishra</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 Figures, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.08210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02551v3</id>
    <updated>2022-05-14T14:57:07Z</updated>
    <published>2022-02-05T13:27:34Z</published>
    <title>Exploring the Dynamics of the Circumcenter Map</title>
    <summary>  Using experimental techniques, we study properties of the "circumcenter map",
which, upon $n$ iterations sends an $n$-gon to a scaled and rotated copy of
itself. We also explore the topology of area-expanding and area-contracting
regions induced by this map.
</summary>
    <author>
      <name>Nicholas McDonald</name>
    </author>
    <author>
      <name>Ronaldo Garcia</name>
    </author>
    <author>
      <name>Dan Reznik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.02551v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02551v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="51N20, 37B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00407v1</id>
    <updated>2022-05-01T06:31:50Z</updated>
    <published>2022-05-01T06:31:50Z</published>
    <title>Quality-Aware Real-Time Augmented Reality Visualization under Delay
  Constraints</title>
    <summary>  Augmented reality (AR) is one of emerging applications in modern multimedia
systems research. Due to intensive time-consuming computations for AR
visualization in mobile devices, quality-aware real-time computing under delay
constraints is essentially required. Inspired by Lyapunov optimization
framework, this paper proposes a time-average quality maximization method for
the AR visualization under delay considerations.
</summary>
    <author>
      <name>Rhoan Lee</name>
    </author>
    <author>
      <name>Soohyun Park</name>
    </author>
    <author>
      <name>Soyi Jung</name>
    </author>
    <author>
      <name>Joongheon Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2205.00407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00934v1</id>
    <updated>2022-05-02T14:32:59Z</updated>
    <published>2022-05-02T14:32:59Z</published>
    <title>Assessing unconstrained surgical cuttings in VR using CNNs</title>
    <summary>  We present a Convolutional Neural Network (CNN) suitable to assess
unconstrained surgical cuttings, trained on a dataset created with a data
augmentation technique.
</summary>
    <author>
      <name>Ilias Chrysovergis</name>
    </author>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Mike Kentros</name>
    </author>
    <author>
      <name>Dimitris Angelis</name>
    </author>
    <author>
      <name>Antonis Protopsaltis</name>
    </author>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, Submitted to the Siggraph '22 Poster Session
  (Vancouver, 8-11 Aug 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06873v1</id>
    <updated>2022-05-13T20:12:12Z</updated>
    <published>2022-05-13T20:12:12Z</published>
    <title>Using Augmented Face Images to Improve Facial Recognition Tasks</title>
    <summary>  We present a framework that uses GAN-augmented images to complement certain
specific attributes, usually underrepresented, for machine learning model
training. This allows us to improve inference quality over those attributes for
the facial recognition tasks.
</summary>
    <author>
      <name>Shuo Cheng</name>
    </author>
    <author>
      <name>Guoxian Song</name>
    </author>
    <author>
      <name>Wan-Chun Ma</name>
    </author>
    <author>
      <name>Chao Wang</name>
    </author>
    <author>
      <name>Linjie Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2022 Workshop: AI-Generated Characters: Putting Deepfakes to Good
  Use</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.06873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.01984v1</id>
    <updated>2023-09-05T06:34:04Z</updated>
    <published>2023-09-05T06:34:04Z</published>
    <title>Focal Surface Projection: Extending Projector Depth-of-Field Using a
  Phase-Only Spatial Light Modulator</title>
    <summary>  We present a focal surface projection to solve the narrow depth-of-field
problem in projection mapping applications. We apply a phase-only spatial light
modulator to realize nonuniform focusing distances, whereby the projected
contents appear focused on a surface with considerable depth variations. The
feasibility of the proposed technique was validated through a physical
experiment.
</summary>
    <author>
      <name>Fumitaka Ueda</name>
    </author>
    <author>
      <name>Yuta Kageyama</name>
    </author>
    <author>
      <name>Daisuke Iwai</name>
    </author>
    <author>
      <name>Kosuke Sato</name>
    </author>
    <link href="http://arxiv.org/abs/2309.01984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.01984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.13477v1</id>
    <updated>2023-09-23T21:02:17Z</updated>
    <published>2023-09-23T21:02:17Z</published>
    <title>Spectral boundary conditions for volumetric frame fields design</title>
    <summary>  Using the 4th and the 3rd degree spherical harmonics as the representations
for volumetric frames, we describe a simple algebraic technique for combining
multiple frame orientation constraints into a single quadratic penalty
function. This technique allows to solve volumetric frame fields design
problems using a coarse-to-fine strategy on hierarchical grids with immersed
boundaries. These results were presented for the first time at the FRAMES 2023
European workshop on meshing.
</summary>
    <author>
      <name>Yuri Nesterenko</name>
    </author>
    <link href="http://arxiv.org/abs/2309.13477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.13477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.16987v1</id>
    <updated>2023-12-28T12:15:58Z</updated>
    <published>2023-12-28T12:15:58Z</published>
    <title>Image Quality, Uniformity and Computation Improvement of Compressive
  Light Field Displays with U-Net</title>
    <summary>  We apply the U-Net model for compressive light field synthesis. Compared to
methods based on stacked CNN and iterative algorithms, this method offers
better image quality, uniformity and less computation.
</summary>
    <author>
      <name>Chen Gao</name>
    </author>
    <author>
      <name>Haifeng Li</name>
    </author>
    <author>
      <name>Xu Liu</name>
    </author>
    <author>
      <name>Xiaodi Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.16987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.16987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="78-06" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.07125v1</id>
    <updated>2024-09-20T21:41:57Z</updated>
    <published>2024-09-20T21:41:57Z</published>
    <title>A Simplified Positional Cell Type Visualization using Spatially
  Aggregated Clusters</title>
    <summary>  We introduce a novel method for overlaying cell type proportion data onto
tissue images. This approach preserves spatial context while avoiding visual
clutter or excessively obscuring the underlying slide. Our proposed technique
involves clustering the data and aggregating neighboring points of the same
cluster into polygons.
</summary>
    <author>
      <name>Lee Mason</name>
    </author>
    <author>
      <name>Jonas Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For the Bio+MedVis 2024 redesign challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.07125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07413v2</id>
    <updated>2025-02-12T09:29:21Z</updated>
    <published>2025-02-11T09:46:15Z</published>
    <title>Multi-directional Backlighting Compressive Light Field Displays</title>
    <summary>  We propose a compressive light field display of a wide viewing angle with a
multi-directional backlight. Displayed layer images of sub-viewing zones are
synchronized with the multi-directional backlight. Viewers can perceive a
three-dimensional scene with a large viewing angle based on the persistence of
vision.
</summary>
    <author>
      <name>Chen Gao</name>
    </author>
    <author>
      <name>Sheng Xu</name>
    </author>
    <author>
      <name>Yun Ye</name>
    </author>
    <author>
      <name>Enguo Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.07413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301002v1</id>
    <updated>2003-01-06T20:57:51Z</updated>
    <published>2003-01-06T20:57:51Z</published>
    <title>Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated
  Rendering</title>
    <summary>  Twenty-five years ago, Crow published the shadow volume approach for
determining shadowed regions in a scene. A decade ago, Heidmann described a
hardware-accelerated stencil buffer-based shadow volume algorithm.
  Unfortunately hardware-accelerated stenciled shadow volume techniques have
not been widely adopted by 3D games and applications due in large part to the
lack of robustness of described techniques. This situation persists despite
widely available hardware support. Specifically what has been lacking is a
technique that robustly handles various "hard" situations created by near or
far plane clipping of shadow volumes.
  We describe a robust, artifact-free technique for hardware-accelerated
rendering of stenciled shadow volumes. Assuming existing hardware, we resolve
the issues otherwise caused by shadow volume near and far plane clipping
through a combination of (1) placing the conventional far clip plane "at
infinity", (2) rasterization with infinite shadow volume polygons via
homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth
clamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves
existing depth precision by not requiring the far plane to be placed at
infinity. We also propose two-sided stencil testing to improve the efficiency
of rendering stenciled shadow volumes.
</summary>
    <author>
      <name>Cass Everitt</name>
    </author>
    <author>
      <name>Mark J. Kilgard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0301002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; I.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305057v1</id>
    <updated>2003-05-29T22:06:27Z</updated>
    <published>2003-05-29T22:06:27Z</published>
    <title>The Persint visualization program for the ATLAS experiment</title>
    <summary>  The Persint program is designed for the three-dimensional representation of
objects and for the interfacing and access to a variety of independent
applications, in a fully interactive way. Facilities are provided for the
spatial navigation and the definition of the visualization properties, in order
to interactively set the viewing and viewed points, and to obtain the desired
perspective. In parallel, applications may be launched through the use of
dedicated interfaces, such as the interactive reconstruction and display of
physics events. Recent developments have focalized on the interfacing to the
XML ATLAS General Detector Description AGDD, making it a widely used tool for
XML developers. The graphics capabilities of this program were exploited in the
context of the ATLAS 2002 Muon Testbeam where it was used as an online event
display, integrated in the online software framework and participating in the
commissioning and debug of the detector system.
</summary>
    <author>
      <name>D. Pomarede</name>
    </author>
    <author>
      <name>M. Virchaux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures, proceedings of CHEP2003</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONFC0303241:MOLT009,2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0305057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307065v1</id>
    <updated>2003-07-29T13:40:12Z</updated>
    <published>2003-07-29T13:40:12Z</published>
    <title>Application of interactive parallel visualization for commodity-based
  clusters using visualization APIs</title>
    <summary>  We present an efficient and inexpensive to develop application for
interactive high-performance parallel visualization. We extend popular APIs
such as Open Inventor and VTK to support commodity-based cluster visualization.
Our implementation follows a standard master/slave concept: the general idea is
to have a ``Master'' node, which will intercept a sequential graphical user
interface (GUI) and broadcast it to the ``Slave'' nodes. The interactions
between the nodes are implemented using MPI. The parallel remote rendering uses
Chromium. This paper is mainly the report of our implementation experiences. We
present in detail the proposed model and key aspects of its implementation.
Also, we present performance measurements, we benchmark and quantitatively
demonstrate the dependence of the visualization speed on the data size and the
network bandwidth, and we identify the singularities and draw conclusions on
Chromium's sort-first rendering architecture. The most original part of this
work is the combined use of Open Inventor and Chromium.
</summary>
    <author>
      <name>Stanimire Tomov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Bennett</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Michael McGuigan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Arnold Peskin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Gordon Smith</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>John Spiletic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; I.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310002v2</id>
    <updated>2003-10-07T18:36:30Z</updated>
    <published>2003-10-05T06:30:56Z</published>
    <title>The Graphics Card as a Streaming Computer</title>
    <summary>  Massive data sets have radically changed our understanding of how to design
efficient algorithms; the streaming paradigm, whether it in terms of number of
passes of an external memory algorithm, or the single pass and limited memory
of a stream algorithm, appears to be the dominant method for coping with large
data.
  A very different kind of massive computation has had the same effect at the
level of the CPU. The most prominent example is that of the computations
performed by a graphics card. The operations themselves are very simple, and
require very little memory, but require the ability to perform many
computations extremely fast and in parallel to whatever degree possible. What
has resulted is a stream processor that is highly optimized for stream
computations. An intriguing side effect of this is the growing use of a
graphics card as a general purpose stream processing engine. In an
ever-increasing array of applications, researchers are discovering that
performing a computation on a graphics card is far faster than performing it on
a CPU, and so are using a GPU as a stream co-processor.
</summary>
    <author>
      <name>Suresh Venkatasubramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages: corrected missing bibliographic references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In SIGMOD Workshop on Management and Processing of Massive Data
  (June 2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0310002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.2;F.1.1;I.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311034v1</id>
    <updated>2003-11-22T18:17:26Z</updated>
    <published>2003-11-22T18:17:26Z</published>
    <title>Visualization of variations in human brain morphology using
  differentiating reflection functions</title>
    <summary>  Conventional visualization media such as MRI prints and computer screens are
inherently two dimensional, making them incapable of displaying true 3D volume
data sets. By applying only transparency or intensity projection, and ignoring
light-matter interaction, results will likely fail to give optimal results.
Little research has been done on using reflectance functions to visually
separate the various segments of a MRI volume. We will explore if applying
specific reflectance functions to individual anatomical structures can help in
building an intuitive 2D image from a 3D dataset. We will test our hypothesis
by visualizing a statistical analysis of the genetic influences on variations
in human brain morphology because it inherently contains complex and many
different types of data making it a good candidate for our approach
</summary>
    <author>
      <name>Gibby Koldenhof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, keywords: MRI, Medical Visualization, Volume rendering,
  BRDF, Specular reflection overlap</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0311034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7;I.4.8;I.4.10;I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312006v1</id>
    <updated>2003-12-02T15:47:19Z</updated>
    <published>2003-12-02T15:47:19Z</published>
    <title>Benchmarking and Implementation of Probability-Based Simulations on
  Programmable Graphics Cards</title>
    <summary>  The latest Graphics Processing Units (GPUs) are reported to reach up to
  200 billion floating point operations per second (200 Gflops) and to have
price performance of 0.1 cents per M flop. These facts raise great interest in
the plausibility of extending the GPUs' use to non-graphics applications, in
particular numerical simulations on structured grids (lattice).
  We review previous work on using GPUs for non-graphics applications,
implement probability-based simulations on the GPU, namely the
  Ising and percolation models, implement vector operation benchmarks for the
GPU, and finally compare the CPU's and GPU's performance.
  A general conclusion from the results obtained is that moving computations
from the CPU to the GPU is feasible, yielding good time and price performance,
for certain lattice computations.
  Preliminary results also show that it is feasible to use them in parallel
</summary>
    <author>
      <name>S. Tomov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</arxiv:affiliation>
    </author>
    <author>
      <name>M. McGuigan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</arxiv:affiliation>
    </author>
    <author>
      <name>R. Bennett</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</arxiv:affiliation>
    </author>
    <author>
      <name>G. Smith</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</arxiv:affiliation>
    </author>
    <author>
      <name>J. Spiletic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.3.1; B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401023v1</id>
    <updated>2004-01-26T21:51:04Z</updated>
    <published>2004-01-26T21:51:04Z</published>
    <title>Surface Triangulation -- The Metric Approach</title>
    <summary>  We embark in a program of studying the problem of better approximating
surfaces by triangulations(triangular meshes) by considering the approximating
triangulations as finite metric spaces and the target smooth surface as their
Haussdorff-Gromov limit. This allows us to define in a more natural way the
relevant elements, constants and invariants s.a. principal directions and
principal values, Gaussian and Mean curvature, etc. By a "natural way" we mean
an intrinsic, discrete, metric definitions as opposed to approximating or
paraphrasing the differentiable notions. In this way we hope to circumvent
computational errors and, indeed, conceptual ones, that are often inherent to
the classical, "numerical" approach. In this first study we consider the
problem of determining the Gaussian curvature of a polyhedral surface, by using
the {\em embedding curvature} in the sense of Wald (and Menger). We present two
modalities of employing these definitions for the computation of Gaussian
curvature.
</summary>
    <author>
      <name>Emil Saucan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 13 figures Preliminary version</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.2; I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405048v1</id>
    <updated>2004-05-14T18:18:04Z</updated>
    <published>2004-05-14T18:18:04Z</published>
    <title>Interactive visualization of higher dimensional data in a multiview
  environment</title>
    <summary>  We develop multiple view visualization of higher dimensional data. Our work
was chiefly motivated by the need to extract insight from four dimensional
Quantum Chromodynamic (QCD) data. We develop visualization where multiple
views, generally views of 3D projections or slices of a higher dimensional
data, are tightly coupled not only by their specific order but also by a view
synchronizing interaction style, and an internally defined interaction
language. The tight coupling of the different views allows a fast and
well-coordinated exploration of the data. In particular, the visualization
allowed us to easily make consistency checks of the 4D QCD data and to infer
the correctness of particle properties calculations. The software developed was
also successfully applied in material studies, in particular studies of
meteorite properties. Our implementation uses the VTK API. To handle a large
number of views (slices/projections) and to still maintain good resolution, we
use IBM T221 display (3840 X 2400 pixels).
</summary>
    <author>
      <name>Stanimire Tomov</name>
    </author>
    <author>
      <name>Michael McGuigan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; I.3.8; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505043v2</id>
    <updated>2012-06-27T11:49:17Z</updated>
    <published>2005-05-16T02:20:19Z</published>
    <title>Estimacao Temporal da Deformacao entre Objectos utilizando uma
  Metodologia Fisica</title>
    <summary>  In this paper, it is presented a methodology to estimate the deformation
involved between two objects attending to its physical properties. This
methodology can be used, for example, in Computational Vision or Computer
Graphics applications, and consists in physically modeling the objects, by
means of the Finite Elements Method, establishing correspondences between some
of its data points, by using Modal Matching, and finally, determining the
displacement field, that is the intermediate shapes, through the resolution of
the Lagrange Dynamic Equilibrium Equation. As in many of the possible
applications of the methodology to present, it is necessary to quantify the
existing deformation, as well as to estimate only the non rigid component of
the involved global deformation. The solutions adopted to satisfy such
intentions will be also presented.
</summary>
    <author>
      <name>Joao Manuel R. S. Tavares</name>
    </author>
    <author>
      <name>Raquel R. Pinho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INFOCOMP Journal of Computer Science. This paper has been withdrawn
  due to journal politics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">INFOCOMP Journal of Computer Science, 4(1), 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0505043v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505043v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0507012v1</id>
    <updated>2005-07-05T19:48:09Z</updated>
    <published>2005-07-05T19:48:09Z</published>
    <title>Lattice Gas Cellular Automata for Computational Fluid Animation</title>
    <summary>  The past two decades showed a rapid growing of physically-based modeling of
fluids for computer graphics applications. In this area, a common top down
approach is to model the fluid dynamics by Navier-Stokes equations and apply a
numerical techniques such as Finite Differences or Finite Elements for the
simulation. In this paper we focus on fluid modeling through Lattice Gas
Cellular Automata (LGCA) for computer graphics applications. LGCA are discrete
models based on point particles that move on a lattice, according to suitable
and simple rules in order to mimic a fully molecular dynamics. By
Chapman-Enskog expansion, a known multiscale technique in this area, it can be
demonstrated that the Navier-Stokes model can be reproduced by the LGCA
technique. Thus, with LGCA we get a fluid model that does not require solution
of complicated equations. Therefore, we combine the advantage of the low
computational cost of LGCA and its ability to mimic the realistic fluid
dynamics to develop a new animating framework for computer graphics
applications. In this work, we discuss the theoretical elements of our proposal
and show experimental results.
</summary>
    <author>
      <name>Gilson A. Giraldi</name>
    </author>
    <author>
      <name>Adilson V. Xavier</name>
    </author>
    <author>
      <name>Antonio L. Apolinario Jr</name>
    </author>
    <author>
      <name>Paulo S. Rodrigues</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0507012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0507012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511032v1</id>
    <updated>2005-11-08T14:40:47Z</updated>
    <published>2005-11-08T14:40:47Z</published>
    <title>Spatiotemporal sensistivity and visual attention for efficient rendering
  of dynamic environments</title>
    <summary>  We present a method to accelerate global illumination computation in dynamic
environments by taking advantage of limitations of the human visual system. A
model of visual attention is used to locate regions of interest in a scene and
to modulate spatiotemporal sensitivity. The method is applied in the form of a
spatiotemporal error tolerance map. Perceptual acceleration combined with good
sampling protocols provide a global illumination solution feasible for use in
animation. Results indicate an order of magnitude improvement in computational
speed. The method is adaptable and can also be used in image-based rendering,
geometry level of detail selection, realistic image synthesis, video telephony
and video compression.
</summary>
    <author>
      <name>Yang Li Hector Yee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics, 20(1), January 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0511032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606098v1</id>
    <updated>2006-06-22T15:35:56Z</updated>
    <published>2006-06-22T15:35:56Z</published>
    <title>Outlier Robust ICP for Minimizing Fractional RMSD</title>
    <summary>  We describe a variation of the iterative closest point (ICP) algorithm for
aligning two point sets under a set of transformations. Our algorithm is
superior to previous algorithms because (1) in determining the optimal
alignment, it identifies and discards likely outliers in a statistically robust
manner, and (2) it is guaranteed to converge to a locally optimal solution. To
this end, we formalize a new distance measure, fractional root mean squared
distance (frmsd), which incorporates the fraction of inliers into the distance
function. We lay out a specific implementation, but our framework can easily
incorporate most techniques and heuristics from modern registration algorithms.
We experimentally validate our algorithm against previous techniques on 2 and 3
dimensional data exposed to a variety of outlier types.
</summary>
    <author>
      <name>Jeff M. Phillips</name>
    </author>
    <author>
      <name>Ran Liu</name>
    </author>
    <author>
      <name>Carlo Tomasi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 7 Figures, 9 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702026v1</id>
    <updated>2007-02-05T10:11:58Z</updated>
    <published>2007-02-05T10:11:58Z</published>
    <title>Shape preservation behavior of spline curves</title>
    <summary>  Shape preservation behavior of a spline consists of criterial conditions for
preserving convexity, inflection, collinearity, torsion and coplanarity shapes
of data polgonal arc. We present our results which acts as an improvement in
the definitions of and provide geometrical insight into each of the above shape
preservation criteria. We also investigate the effect of various results from
the literature on various shape preservation criteria. These results have not
been earlier refered in the context of shape preservation behaviour of splines.
We point out that each curve segment need to satisfy more than one shape
preservation criteria. We investigate the conflict between different shape
preservation criteria 1)on each curve segment and 2)of adjacent curve segments.
We derive simplified formula for shape preservation criteria for cubic curve
segments. We study the shape preservation behavior of cubic Catmull-Rom splines
and see that, though being very simple spline curve, it indeed satisfy all the
shape preservation criteria.
</summary>
    <author>
      <name>Ravi Shankar Gautam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.4224v1</id>
    <updated>2007-06-28T13:19:04Z</updated>
    <published>2007-06-28T13:19:04Z</published>
    <title>User driven applications - new design paradigm</title>
    <summary>  Programs for complicated engineering and scientific tasks always have to deal
with a problem of showing numerous graphical results. The limits of the screen
space and often opposite requirements from different users are the cause of the
infinite discussions between designers and users, but the source of this
ongoing conflict is not in the level of interface design, but in the basic
principle of current graphical output: user may change some views and details,
but in general the output view is absolutely defined and fixed by the
developer. Author was working for several years on the algorithm that will
allow eliminating this problem thus allowing stepping from designer-driven
applications to user-driven. Such type of applications in which user is
deciding what, when and how to show on the screen, is the dream of scientists
and engineers working on the analysis of the most complicated tasks. The new
paradigm is based on movable and resizable graphics, and such type of graphics
can be widely used not only for scientific and engineering applications.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <link href="http://arxiv.org/abs/0706.4224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.4224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.0712v1</id>
    <updated>2007-08-06T07:42:56Z</updated>
    <published>2007-08-06T07:42:56Z</published>
    <title>Virtual Environments for Training: From Individual Learning to
  Collaboration with Humanoids</title>
    <summary>  The next generation of virtual environments for training is oriented towards
collaborative aspects. Therefore, we have decided to enhance our platform for
virtual training environments, adding collaboration opportunities and
integrating humanoids. In this paper we put forward a model of humanoid that
suits both virtual humans and representations of real users, according to
collaborative training activities. We suggest adaptations to the scenario model
of our platform making it possible to write collaborative procedures. We
introduce a mechanism of action selection made up of a global repartition and
an individual choice. These models are currently being integrated and validated
in GVT, a virtual training tool for maintenance of military equipments,
developed in collaboration with the French company NEXTER-Group.
</summary>
    <author>
      <name>Stéphanie Gerbaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Mollet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Arnaldi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Edutainment (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.0712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3553v1</id>
    <updated>2007-09-22T00:21:08Z</updated>
    <published>2007-09-22T00:21:08Z</published>
    <title>Design of moveable and resizable graphics</title>
    <summary>  We are communicating with computers on two different levels. On upper level
we have a very flexible system of windows: we can move them, resize, overlap or
put side by side. At any moment we decide what would be the best view and
reorganize the whole view easily. Then we start any application, go to the
inner level, and everything changes. Here we are stripped of all the
flexibility and can work only inside the scenario, developed by the designer of
the program. Interface will allow us to change some tiny details, but in
general everything is fixed: graphics is neither moveable, nor resizable, and
the same with controls. Author designed an extremely powerful mechanism of
turning graphical objects and controls into moveable and resizable. This can
not only significantly improve the existing applications, but this will bring
the applications to another level. (To estimate the possible difference, try to
imagine the Windows system without its flexibility and compare it with the
current one.) This article explains in details the construction and use of
moveable and resizable graphical objects.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.3553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4093v2</id>
    <updated>2011-07-27T23:29:19Z</updated>
    <published>2008-09-24T05:50:56Z</published>
    <title>Perspective Drawing of Surfaces with Line Hidden Line Elimination,
  Dibujando Superficies En Perspectiva Con Eliminacion De Lineas Ocultas</title>
    <summary>  An efficient computer algorithm is described for the perspective drawing of a
wide class of surfaces. The class includes surfaces corresponding lo
single-valued, continuous functions which are defined over rectangular domains.
The algorithm automatically computes and eliminates hidden lines. The number of
computations in the algorithm grows linearly with the number of sample points
on the surface to be drawn. An analysis of the algorithm is presented, and
extensions lo certain multi-valued functions are indicated. The algorithm is
implemented and tested on .Net 2.0 platform that left interactive use. Running
times are found lo be exceedingly efficient for visualization, where
interaction on-line and view-point control, enables effective and rapid
examination of a surfaces from many perspectives.
</summary>
    <author>
      <name>Ignacio Vega-Paez</name>
    </author>
    <author>
      <name>Jose Angel Ortega</name>
    </author>
    <author>
      <name>Georgina G. Pulido</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings in Technical Memory, XI Congreso Nacional de
  Ingenieria Electromecanica y de Sistemas, pp. 136-144, Mexico, DF., Nov 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.4093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.2021v1</id>
    <updated>2008-10-13T12:53:57Z</updated>
    <published>2008-10-13T12:53:57Z</published>
    <title>Visualization Optimization : Application to the RoboCup Rescue Domain</title>
    <summary>  In this paper we demonstrate the use of intelligent optimization
methodologies on the visualization optimization of virtual / simulated
environments. The problem of automatic selection of an optimized set of views,
which better describes an on-going simulation over a virtual environment is
addressed in the context of the RoboCup Rescue Simulation domain. A generic
architecture for optimization is proposed and described. We outline the
possible extensions of this architecture and argue on how several problems
within the fields of Interactive Rendering and Visualization can benefit from
it.
</summary>
    <author>
      <name>Pedro Miguel Moreira</name>
    </author>
    <author>
      <name>Luís Paulo Reis</name>
    </author>
    <author>
      <name>António Augusto de Sousa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1+4 pages, 3 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings SIACG 2006 - Ibero American Symposyum in Computer
  Graphics, Santiago de Compostela, Spain, 5-7 July 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0810.2021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.2021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2055v2</id>
    <updated>2008-11-18T20:31:15Z</updated>
    <published>2008-11-13T09:34:42Z</published>
    <title>GPU-Based Interactive Visualization of Billion Point Cosmological
  Simulations</title>
    <summary>  Despite the recent advances in graphics hardware capabilities, a brute force
approach is incapable of interactively displaying terabytes of data. We have
implemented a system that uses hierarchical level-of-detailing for the results
of cosmological simulations, in order to display visually accurate results
without loading in the full dataset (containing over 10 billion points). The
guiding principle of the program is that the user should not be able to
distinguish what they are seeing from a full rendering of the original data.
Furthermore, by using a tree-based system for levels of detail, the size of the
underlying data is limited only by the capacity of the IO system containing it.
</summary>
    <author>
      <name>Tamas Szalay</name>
    </author>
    <author>
      <name>Volker Springel</name>
    </author>
    <author>
      <name>Gerard Lemson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2008 Microsoft eScience conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.2055v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2055v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1119v1</id>
    <updated>2008-12-05T12:16:53Z</updated>
    <published>2008-12-05T12:16:53Z</published>
    <title>An analysis of a random algorithm for estimating all the matchings</title>
    <summary>  Counting the number of all the matchings on a bipartite graph has been
transformed into calculating the permanent of a matrix obtained from the
extended bipartite graph by Yan Huo, and Rasmussen presents a simple approach
(RM) to approximate the permanent, which just yields a critical ratio
O($n\omega(n)$) for almost all the 0-1 matrices, provided it's a simple
promising practical way to compute this #P-complete problem. In this paper, the
performance of this method will be shown when it's applied to compute all the
matchings based on that transformation. The critical ratio will be proved to be
very large with a certain probability, owning an increasing factor larger than
any polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence,
RM fails to work well when counting all the matchings via computing the
permanent of the matrix. In other words, we must carefully utilize the known
methods of estimating the permanent to count all the matchings through that
transformation.
</summary>
    <author>
      <name>Jinshan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.1119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1647v1</id>
    <updated>2008-12-09T10:12:36Z</updated>
    <published>2008-12-09T10:12:36Z</published>
    <title>Polyomino-Based Digital Halftoning</title>
    <summary>  In this work, we present a new method for generating a threshold structure.
This kind of structure can be advantageously used in various halftoning
algorithms such as clustered-dot or dispersed-dot dithering, error diffusion
with threshold modulation, etc. The proposed method is based on rectifiable
polyominoes -- a non-periodic hierarchical structure, which tiles the Euclidean
plane with no gaps. Each polyomino contains a fixed number of discrete
threshold values. Thanks to its inherent non-periodic nature combined with
off-line optimization of threshold values, our polyomino-based threshold
structure shows blue-noise spectral properties. The halftone images produced
with this threshold structure have high visual quality. Although the proposed
method is general, and can be applied on any polyomino tiling, we consider one
particular case: tiling with G-hexominoes. We compare our polyomino-based
threshold structure with the best known state-of-the-art methods for generation
threshold matrices, and conclude considerable improvement achieved with our
method.
</summary>
    <author>
      <name>David Vanderhaeghe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes / LJK Laboratoire Jean Kuntzmann, LJK</arxiv:affiliation>
    </author>
    <author>
      <name>Victor Ostromoukhov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DIRO</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://artis.imag.fr/Publications/2008/VO08/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IADIS International Conference on Computer Graphics and
  Visualization 2008 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.1647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.1226v1</id>
    <updated>2009-06-05T22:57:04Z</updated>
    <published>2009-06-05T22:57:04Z</published>
    <title>On the Complexity of Smooth Spline Surfaces from Quad Meshes</title>
    <summary>  This paper derives strong relations that boundary curves of a smooth complex
of patches have to obey when the patches are computed by local averaging. These
relations restrict the choice of reparameterizations for geometric continuity.
In particular, when one bicubic tensor-product B-spline patch is associated
with each facet of a quadrilateral mesh with n-valent vertices and we do not
want segments of the boundary curves forced to be linear, then the relations
dictate the minimal number and multiplicity of knots: For general data, the
tensor-product spline patches must have at least two internal double knots per
edge to be able to model a G^1-conneced complex of C^1 splines. This lower
bound on the complexity of any construction is proven to be sharp by suitably
interpreting an existing surface construction. That is, we have a tight bound
on the complexity of smoothing quad meshes with bicubic tensor-product B-spline
patches.
</summary>
    <author>
      <name>Jorg Peters</name>
    </author>
    <author>
      <name>Jianhua Fan</name>
    </author>
    <link href="http://arxiv.org/abs/0906.1226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.1226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0902v1</id>
    <updated>2009-11-04T18:11:30Z</updated>
    <published>2009-11-04T18:11:30Z</published>
    <title>Digital Image Watermarking for Arbitrarily Shaped Objects Based On
  SA-DWT</title>
    <summary>  Many image watermarking schemes have been proposed in recent years, but they
usually involve embedding a watermark to the entire image without considering
only a particular object in the image, which the image owner may be interested
in. This paper proposes a watermarking scheme that can embed a watermark to an
arbitrarily shaped object in an image. Before embedding, the image owner
specifies an object of arbitrary shape that is of a concern to him. Then the
object is transformed into the wavelet domain using in place lifting shape
adaptive DWT(SADWT) and a watermark is embedded by modifying the wavelet
coefficients. In order to make the watermark robust and transparent, the
watermark is embedded in the average of wavelet blocks using the visual model
based on the human visual system. Wavelet coefficients n least significant bits
(LSBs) are adjusted in concert with the average. Simulation results shows that
the proposed watermarking scheme is perceptually invisible and robust against
many attacks such as lossy compression (e.g.JPEG, JPEG2000), scaling, adding
noise, filtering, etc.
</summary>
    <author>
      <name>A. Essaouabi</name>
    </author>
    <author>
      <name>E. Ibnelhaj</name>
    </author>
    <author>
      <name>F. Fegragui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, Volume 5, pp1-8,
  October 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A.Essaouabi, E.Ibnelhaj and F.regragui, "Digital Image
  Watermarking for Arbitrarily Shaped Objects Based On SA-DWT", International
  Journal of Computer Science Issues, Volume 5, pp1-8, October 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.0902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5157v3</id>
    <updated>2011-04-27T16:54:22Z</updated>
    <published>2009-11-26T22:47:37Z</published>
    <title>Analyzing Midpoint Subdivision</title>
    <summary>  Midpoint subdivision generalizes the Lane-Riesenfeld algorithm for uniform
tensor product splines and can also be applied to non regular meshes. For
example, midpoint subdivision of degree 2 is a specific Doo-Sabin algorithm and
midpoint subdivision of degree 3 is a specific Catmull-Clark algorithm. In
2001, Zorin and Schroeder were able to prove C1-continuity for midpoint
subdivision surfaces analytically up to degree 9. Here, we develop general
analysis tools to show that the limiting surfaces under midpoint subdivision of
any degree &gt;= 2 are C1-continuous at their extraordinary points.
</summary>
    <author>
      <name>Hartmut Prautzsch</name>
    </author>
    <author>
      <name>Qi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper was improved by adding more explanations and by adding an
  illustration of how the statements depend on each other. We combined a few
  theorems to simplify the structure of the paper and better described the
  meaning of the statements and how they fit into the overall proof. 24 pages,
  10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.5157v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5157v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3923v1</id>
    <updated>2009-12-19T18:47:39Z</updated>
    <published>2009-12-19T18:47:39Z</published>
    <title>Secure Watermarking Scheme for Color Image Using Intensity of Pixel and
  LSB Substitution</title>
    <summary>  In this paper a novel spatial domain LSB based watermarking scheme for color
Images is proposed. The proposed scheme is of type blind and invisible
watermarking. Our scheme introduces the concept of storing variable number of
bits in each pixel based on the actual color value of pixel. Equal or higher
the color value of channels with respect to intensity of pixel stores higher
number of watermark bits. The Red, Green and Blue channel of the color image
has been used for watermark embedding. The watermark is embedded into selected
channels of pixel. The proposed method supports high watermark embedding
capacity, which is equivalent to the size of cover image. The security of
watermark is preserved by permuting the watermark bits using secret key. The
proposed scheme is found robust to various image processing operations such as
image compression, blurring, salt and pepper noise, filtering and cropping.
</summary>
    <author>
      <name>Nagaraj V. Dharwadkar</name>
    </author>
    <author>
      <name>B. B. Amberker</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 1, Issue 1, pp 1-6, December 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.3923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.5380v1</id>
    <updated>2009-12-30T18:07:56Z</updated>
    <published>2009-12-30T18:07:56Z</published>
    <title>Computing Principal Components Dynamically</title>
    <summary>  In this paper we present closed-form solutions for efficiently updating the
principal components of a set of $n$ points, when $m$ points are added or
deleted from the point set. For both operations performed on a discrete point
set in $\mathbb{R}^d$, we can compute the new principal components in $O(m)$
time for fixed $d$. This is a significant improvement over the commonly used
approach of recomputing the principal components from scratch, which takes
$O(n+m)$ time. An important application of the above result is the dynamical
computation of bounding boxes based on principal component analysis. PCA
bounding boxes are very often used in many fields, among others in computer
graphics for collision detection and fast rendering. We have implemented and
evaluated few algorithms for computing dynamically PCA bounding boxes in
$\mathbb{R}^3$. In addition, we present closed-form solutions for computing
dynamically principal components of continuous point sets in $\mathbb{R}^2$ and
$\mathbb{R}^3$. In both cases, discrete and continuous, to compute the new
principal components, no additional data structures or storage are needed.
</summary>
    <author>
      <name>Darko Dimitrov</name>
    </author>
    <author>
      <name>Mathias Holst</name>
    </author>
    <author>
      <name>Christian Knauer</name>
    </author>
    <author>
      <name>Klaus Kriegel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.5380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.5380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.5494v1</id>
    <updated>2009-12-30T17:53:18Z</updated>
    <published>2009-12-30T17:53:18Z</published>
    <title>Teaching Physical Based Animation via OpenGL Slides</title>
    <summary>  This work expands further our earlier poster presentation and integration of
the OpenGL Slides Framework (OGLSF) - to make presentations with real-time
animated graphics where each slide is a scene with tidgets - and physical based
animation of elastic two-, three-layer softbody objects. The whole project is
very interactive, and serves dual purpose - delivering the teaching material in
a classroom setting with real running animated examples as well as releasing
the source code to the students to show how the actual working things are made.
</summary>
    <author>
      <name>Miao Song</name>
    </author>
    <author>
      <name>Serguei A. Mokhov</name>
    </author>
    <author>
      <name>Peter Grogono</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-90-481-9112-3_82 10.1145/1557626.1557647</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-90-481-9112-3_82" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.1145/1557626.1557647" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; 7 figures; the poster is presented at C32SE'09 and the
  paper at CISSE'09 at http://conference.cisse2009.org/proceedings.aspx ; there
  are an executable demo and its source code</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.5494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.5494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.3.6; I.4.9; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.4006v1</id>
    <updated>2010-02-21T19:46:12Z</updated>
    <published>2010-02-21T19:46:12Z</published>
    <title>Text/Graphics Separation and Skew Correction of Text Regions of Business
  Card Images for Mobile Devices</title>
    <summary>  Separation of the text regions from background texture and graphics is an
important step of any optical character recognition system for the images
containing both texts and graphics. In this paper, we have presented a novel
text/graphics separation technique and a method for skew correction of text
regions extracted from business card images captured with a cell-phone camera.
At first, the background is eliminated at a coarse level based on intensity
variance. This makes the foreground components distinct from each other. Then
the non-text components are removed using various characteristic features of
text and graphics. Finally, the text regions are skew corrected for further
processing. Experimenting with business card images of various resolutions, we
have found an optimum performance of 98.25% (recall) with 0.75 MP images, that
takes 0.17 seconds processing time and 1.1 MB peak memory on a moderately
powerful computer (DualCore 1.73 GHz Processor, 1 GB RAM, 1 MB L2 Cache). The
developed technique is computationally efficient and consumes low memory so as
to be applicable on mobile devices.
</summary>
    <author>
      <name>Ayatullah Faruk Mollah</name>
    </author>
    <author>
      <name>Subhadip Basu</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 2, February 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.4006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4036v1</id>
    <updated>2010-03-21T23:32:56Z</updated>
    <published>2010-03-21T23:32:56Z</published>
    <title>A Very Simple Approach for 3-D to 2-D Mapping</title>
    <summary>  Many times we need to plot 3-D functions e.g., in many scientificc
experiments. To plot this 3-D functions on 2-D screen it requires some kind of
mapping. Though OpenGL, DirectX etc 3-D rendering libraries have made this job
very simple, still these libraries come with many complex pre- operations that
are simply not intended, also to integrate these libraries with any kind of
system is often a tough trial. This article presents a very simple method of
mapping from 3D to 2D, that is free from any complex pre-operation, also it
will work with any graphics system where we have some primitive 2-D graphics
function. Also we discuss the inverse transform and how to do basic computer
graphics transformations using our coordinate mapping system.
</summary>
    <author>
      <name>Sandipan Dey</name>
    </author>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 5 Figures,</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Image Processing and Communications ,
  Poland, Editor-in-Chief: R. S. Choras; Volume 11, No. 2, pp. 75 - 82, 2007.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.0766v1</id>
    <updated>2010-04-06T03:54:27Z</updated>
    <published>2010-04-06T03:54:27Z</published>
    <title>Text/Graphics Separation for Business Card Images for Mobile Devices</title>
    <summary>  Separation of the text regions from background texture and graphics is an
important step of any optical character recognition sytem for the images
containg both texts and graphics. In this paper, we have presented a novel
text/graphics separation technique for business card images captured with a
cell-phone camera. At first, the background is eliminated at a coarse level
based on intensity variance. This makes the foreground components distinct from
each other. Then the non-text components are removed using various
characteristic features of text and graphics. Finally, the text regions are
skew corrected and binarized for further processing. Experimenting with
business card images of various resolutions, we have found an optimum
performance of 98.54% with 0.75 MP images, that takes 0.17 seconds processing
time and 1.1 MB peak memory on a moderately powerful computer (DualCore 1.73
GHz Processor, 1 GB RAM, 1 MB L2 Cache). The developed technique is
computationally efficient and consumes low memory so as to be applicable on
mobile devices.
</summary>
    <author>
      <name>Ayatullah Faruk Mollah</name>
    </author>
    <author>
      <name>Subhadip Basu</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Dipak Kumar Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IAPR International Workshop on Graphics Recognition (2009)
  263-270</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.2447v1</id>
    <updated>2010-04-14T16:40:41Z</updated>
    <published>2010-04-14T16:40:41Z</published>
    <title>Autoplot: A browser for scientific data on the web</title>
    <summary>  Autoplot is software developed for the Virtual Observatories in Heliophysics
to provide intelligent and automated plotting capabilities for many typical
data products that are stored in a variety of file formats or databases.
Autoplot has proven to be a flexible tool for exploring, accessing, and viewing
data resources as typically found on the web, usually in the form of a
directory containing data files with multiple parameters contained in each
file. Data from a data source is abstracted into a common internal data model
called QDataSet. Autoplot is built from individually useful components, and can
be extended and reused to create specialized data handling and analysis
applications and is being used in a variety of science visualization and
analysis applications. Although originally developed for viewing
heliophysics-related time series and spectrograms, its flexible and generic
data representation model makes it potentially useful for the Earth sciences.
</summary>
    <author>
      <name>J. Faden</name>
    </author>
    <author>
      <name>R. S. Weigel</name>
    </author>
    <author>
      <name>J. Merka</name>
    </author>
    <author>
      <name>R. H. W. Friedel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12145-010-0049-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12145-010-0049-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.2447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.2447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3181v1</id>
    <updated>2010-05-18T13:01:46Z</updated>
    <published>2010-05-18T13:01:46Z</published>
    <title>Multi-sensorial interaction with a nano-scale phenomenon : the force
  curve</title>
    <summary>  Using Atomic Force Microscopes (AFM) to manipulate nano-objects is an actual
challenge for surface scientists. Basic haptic interfacesbetween the AFM and
experimentalists have already been implemented. Themulti-sensory renderings
(seeing, hearing and feeling) studied from acognitive point of view increase
the efficiency of the actual interfaces. Toallow the experimentalist to feel
and touch the nano-world, we add mixedrealities between an AFM and a force
feedback device, enriching thus thedirect connection by a modeling engine. We
present in this paper the firstresults from a real-time remote-control handling
of an AFM by our ForceFeedback Gestural Device through the example of the
approach-retract curve.
</summary>
    <author>
      <name>Sylvain Marliere</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEPES</arxiv:affiliation>
    </author>
    <author>
      <name>Daniela Urma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Loup Florens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <author>
      <name>Florence Marchi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEPES</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EuroHaptics 2004, Munich : Germany (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.3181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4563v1</id>
    <updated>2010-05-25T13:14:11Z</updated>
    <published>2010-05-25T13:14:11Z</published>
    <title>Physically-based particle simulation and visualization of pastes and
  gels</title>
    <summary>  This paper is focused on the question of simulation and visualiza- tion of 3D
gel and paste dynamic effects. In a first part, we introduce a 3D physically
based particle (or mass-interaction) model, with a small number of masses and
few powerful interaction parameters, which is able to generate the dynamic
features of both gels and pastes. This model proves that the 3D
mass-interaction method is relevant for the simulation of such phenomena,
without an explicit knowledge of their underly- ing physics. In a second part,
we expose an original rendering process, the Flow Structuring Method that
enhances the dynamic properties of the simulation and offers a realistic
visualization. This process ignores all the properties of the underlying
physical model. It leads to a reconstruction of the spatial structure of the
gel (or paste) flow only through an analysis of the output of the simula- tion
which is a set of unorganized points moving in a 3D space. Finally, the paper
presents realistic renderings obtained by using implicit surfaces and
ray-tracing techniques on the Structured Flow previously obtained.
</summary>
    <author>
      <name>Claire Guilbaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Annie Luciani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Castagné</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Graphicon 2003, Moscou : Russie, F\'ed\'eration De (2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.4563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5183v1</id>
    <updated>2010-09-27T08:16:36Z</updated>
    <published>2010-09-27T08:16:36Z</published>
    <title>A Framework for an Ego-centered and Time-aware Visualization of
  Relations in Arbitrary Data Repositories</title>
    <summary>  Understanding constellations in large data collections has become a common
task. One obstacle a user has to overcome is the internal complexity of these
repositories. For example, extracting connected data from a normalized
relational database requires knowledge of the table structure which might not
be available for the casual user. In this paper we present a visualization
framework which presents the collection as a set of entities and relations (on
the data level). Using rating functions, we divide large relation networks into
small graphs which resemble ego-centered networks. These graphs are connected
so the user can browse from one to another. To further assist the user, we
present two views which embed information on the evolution of the relations
into the graphs. Each view emphasizes another aspect of temporal development.
The framework can be adapted to any repository by a flexible data interface and
a graph configuration file. We present some first web-based applications
including a visualization of the DBLP data set. We use the DBLP visualization
to evaluate our approach.
</summary>
    <author>
      <name>Florian Reitz</name>
    </author>
    <link href="http://arxiv.org/abs/1009.5183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0395v1</id>
    <updated>2011-01-02T10:09:11Z</updated>
    <published>2011-01-02T10:09:11Z</published>
    <title>Improving the Performance of K-Means for Color Quantization</title>
    <summary>  Color quantization is an important operation with many applications in
graphics and image processing. Most quantization methods are essentially based
on data clustering algorithms. However, despite its popularity as a general
purpose clustering algorithm, k-means has not received much respect in the
color quantization literature because of its high computational requirements
and sensitivity to initialization. In this paper, we investigate the
performance of k-means as a color quantizer. We implement fast and exact
variants of k-means with several initialization schemes and then compare the
resulting quantizers to some of the most popular quantizers in the literature.
Experiments on a diverse set of images demonstrate that an efficient
implementation of k-means with an appropriate initialization strategy can in
fact serve as a very effective color quantizer.
</summary>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.imavis.2010.10.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.imavis.2010.10.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 4 figures, 13 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Image and Vision Computing 29 (2011) 260-271</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1101.0395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0663v1</id>
    <updated>2011-01-04T06:53:36Z</updated>
    <published>2011-01-04T06:53:36Z</published>
    <title>The Role of Computer Graphics in Documentary Film Production</title>
    <summary>  We discuss a topic on the role of computer graphics in the production of
documentaries, which is often ignored in favor of other topics. Typically,
except for some rare occasions, documentary producers and computer scientists
or digital artists that do computer graphics are relatively far apart in their
domains and rarely intercommunicate to have a joint production; yet it happens,
and perhaps more so in the present and the future.
  We attempt to classify the documentaries on the amount and techniques of
computer graphics used for documentaries. We come up with the initial
categories such as "plain" (no graphics), "in-between", "all-out" -- nearly
100% of the documentary consisting of computer-generated imagery. Computer
graphics can be used to enhance the scenery, fill in the gaps in the missing
storyline pieces, or animate between scenes. It can incorporate stereoscopic
effects for higher viewer impression as well as interactivity aspects. It can
also be used simply in old archived image and film restoration.
</summary>
    <author>
      <name>Miao Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages; 7 figures; an April 2009 research paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1240v1</id>
    <updated>2010-12-13T18:45:13Z</updated>
    <published>2010-12-13T18:45:13Z</published>
    <title>Chameleon: A Color-Adaptive Web Browser for Mobile OLED Displays</title>
    <summary>  Displays based on organic light-emitting diode (OLED) technology are
appearing on many mobile devices. Unlike liquid crystal displays (LCD), OLED
displays consume dramatically different power for showing different colors. In
particular, OLED displays are inefficient for showing bright colors. This has
made them undesirable for mobile devices because much of the web content is of
bright colors.
  To tackle this problem, we present the motivational studies, design, and
realization of Chameleon, a color adaptive web browser that renders web pages
with power-optimized color schemes under user-supplied constraints. Driven by
the findings from our motivational studies, Chameleon provides end users with
important options, offloads tasks that are not absolutely needed in real-time,
and accomplishes real-time tasks by carefully enhancing the codebase of a
browser engine. According to measure-ments with OLED smartphones, Chameleon is
able to re-duce average system power consumption for web browsing by 41% and
reduce display power consumption by 64% without introducing any noticeable
delay.
</summary>
    <author>
      <name>Mian Dong</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/1101.1240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5490v1</id>
    <updated>2011-01-28T09:41:43Z</updated>
    <published>2011-01-28T09:41:43Z</published>
    <title>Ray-Based Reflectance Model for Diffraction</title>
    <summary>  We present a novel method of simulating wave effects in graphics using
ray--based renderers with a new function: the Wave BSDF (Bidirectional
Scattering Distribution Function). Reflections from neighboring surface patches
represented by local BSDFs are mutually independent. However, in many surfaces
with wavelength-scale microstructures, interference and diffraction requires a
joint analysis of reflected wavefronts from neighboring patches. We demonstrate
a simple method to compute the BSDF for the entire microstructure, which can be
used independently for each patch. This allows us to use traditional ray--based
rendering pipelines to synthesize wave effects of light and sound. We exploit
the Wigner Distribution Function (WDF) to create transmissive, reflective, and
emissive BSDFs for various diffraction phenomena in a physically accurate way.
In contrast to previous methods for computing interference, we circumvent the
need to explicitly keep track of the phase of the wave by using BSDFs that
include positive as well as negative coefficients. We describe and compare the
theory in relation to well understood concepts in rendering and demonstrate a
straightforward implementation. In conjunction with standard raytracers, such
as PBRT, we demonstrate wave effects for a range of scenarios such as
multi--bounce diffraction materials, holograms and reflection of high frequency
surfaces.
</summary>
    <author>
      <name>Tom Cuypers</name>
    </author>
    <author>
      <name>Se Baek Oh</name>
    </author>
    <author>
      <name>Tom Haber</name>
    </author>
    <author>
      <name>Philippe Bekaert</name>
    </author>
    <author>
      <name>Ramesh Raskar</name>
    </author>
    <link href="http://arxiv.org/abs/1101.5490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.0634v1</id>
    <updated>2011-02-03T10:00:15Z</updated>
    <published>2011-02-03T10:00:15Z</published>
    <title>Glioblastoma Multiforme Segmentation in MRI Data with a Balloon
  Inflation Approach</title>
    <summary>  Gliomas are the most common primary brain tumors, evolving from the cerebral
supportive cells. For clinical follow-up, the evaluation of the preoperative
tumor volume is essential. Volumetric assessment of tumor volume with manual
segmentation of its outlines is a time-consuming process that can be overcome
with the help of computer-assisted segmentation methods. In this paper, a
semi-automatic approach for World Health Organization (WHO) grade IV glioma
segmentation is introduced that uses balloon inflation forces, and relies on
the detection of high-intensity tumor boundaries that are coupled by using
contrast agent gadolinium. The presented method is evaluated on 27 magnetic
resonance imaging (MRI) data sets and the ground truth data of the tumor
boundaries - for evaluation of the results - are manually extracted by
neurosurgeons.
</summary>
    <author>
      <name>Dženan Zukić</name>
    </author>
    <author>
      <name>Jan Egger</name>
    </author>
    <author>
      <name>Miriam H. A. Bauer</name>
    </author>
    <author>
      <name>Daniela Kuhnt</name>
    </author>
    <author>
      <name>Barbara Carl</name>
    </author>
    <author>
      <name>Bernd Freisleben</name>
    </author>
    <author>
      <name>Andreas Kolb</name>
    </author>
    <author>
      <name>Christopher Nimsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, Proc. of the 6th Russian-Bavarian Conference on
  Bio-Medical Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.0634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.0634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.2652v1</id>
    <updated>2011-02-14T01:09:24Z</updated>
    <published>2011-02-14T01:09:24Z</published>
    <title>Rule-based transformations for geometric modelling</title>
    <summary>  The context of this paper is the use of formal methods for topology-based
geometric modelling. Topology-based geometric modelling deals with objects of
various dimensions and shapes. Usually, objects are defined by a graph-based
topological data structure and by an embedding that associates each topological
element (vertex, edge, face, etc.) with relevant data as their geometric shape
(position, curve, surface, etc.) or application dedicated data (e.g. molecule
concentration level in a biological context). We propose to define
topology-based geometric objects as labelled graphs. The arc labelling defines
the topological structure of the object whose topological consistency is then
ensured by labelling constraints. Nodes have as many labels as there are
different data kinds in the embedding. Labelling constraints ensure then that
the embedding is consistent with the topological structure. Thus,
topology-based geometric objects constitute a particular subclass of a category
of labelled graphs in which nodes have multiple labels.
</summary>
    <author>
      <name>Thomas Bellet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Poitiers</arxiv:affiliation>
    </author>
    <author>
      <name>Agnès Arnould</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Poitiers</arxiv:affiliation>
    </author>
    <author>
      <name>Pascale Le Gall</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ecole Centrale Paris</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.48.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.48.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TERMGRAPH 2011, arXiv:1102.2268</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 48, 2011, pp. 20-37</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.2652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.2652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.3328v1</id>
    <updated>2011-02-16T13:04:18Z</updated>
    <published>2011-02-16T13:04:18Z</published>
    <title>An Efficient and Integrated Algorithm for Video Enhancement in
  Challenging Lighting Conditions</title>
    <summary>  We describe a novel integrated algorithm for real-time enhancement of video
acquired under challenging lighting conditions. Such conditions include low
lighting, haze, and high dynamic range situations. The algorithm automatically
detects the dominate source of impairment, then depending on whether it is low
lighting, haze or others, a corresponding pre-processing is applied to the
input video, followed by the core enhancement algorithm. Temporal and spatial
redundancies in the video input are utilized to facilitate real-time processing
and to improve temporal and spatial consistency of the output. The proposed
algorithm can be used as an independent module, or be integrated in either a
video encoder or a video decoder for further optimizations.
</summary>
    <author>
      <name>Xuan Dong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Gene</arxiv:affiliation>
    </author>
    <author>
      <name> Jiangtao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Gene</arxiv:affiliation>
    </author>
    <author>
      <name> Wen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amy</arxiv:affiliation>
    </author>
    <author>
      <name>Weixin Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amy</arxiv:affiliation>
    </author>
    <author>
      <name> Yi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amy</arxiv:affiliation>
    </author>
    <author>
      <name> Pang</name>
    </author>
    <author>
      <name>Guan Wang</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Wei Meng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.3328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.3328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.4992v1</id>
    <updated>2011-02-24T13:54:29Z</updated>
    <published>2011-02-24T13:54:29Z</published>
    <title>Mathematics of Human Motion: from Animation towards Simulation (A View
  form the Outside)</title>
    <summary>  Simulation of human motion is the subject of study in a number of
disciplines: Biomechanics, Robotics, Computer Animation, Control Theory,
Neurophysiology, Medicine, Ergonomics. Since the author has never visited any
of these fields, this review is indeed a passer-by's impression. On the other
hand, he happens to be a human (who occasionally is moving) and, as everybody
else, rates himself an expert in Applied Common Sense. Thus the author hopes
that this view from the {\em outside} will be of some interest not only for the
strangers like himself, but for those who are {\em inside} as well.
  Two flaws of the text that follows are inevitable. First, some essential
issues that are too familar to the specialists to discuss them may be missing.
Second, the author probably failed to provide the uniform "level-of-detail" for
this wide range of topics.
</summary>
    <author>
      <name>A. I. Zhmakin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appendix: "The Animator's Eleven Commandments"</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.4992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.4992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4271v2</id>
    <updated>2011-06-02T18:11:55Z</updated>
    <published>2011-03-22T14:16:07Z</published>
    <title>Rendering of 3D Dynamic Virtual Environments</title>
    <summary>  In this paper we present a framework for the rendering of dynamic 3D virtual
environments which can be integrated in the development of videogames. It
includes methods to manage sounds and particle effects, paged static
geometries, the support of a physics engine and various input systems. It has
been designed with a modular structure to allow future expansions. We exploited
some open-source state-of-the-art components such as OGRE, PhysX,
ParticleUniverse, etc.; all of them have been properly integrated to obtain
peculiar physical and environmental effects. The stand-alone version of the
application is fully compatible with Direct3D and OpenGL APIs and adopts OpenAL
APIs to manage audio cards. Concluding, we devised a showcase demo which
reproduces a dynamic 3D environment, including some particular effects: the
alternation of day and night infuencing the lighting of the scene, the
rendering of terrain, water and vegetation, the reproduction of sounds and
atmospheric agents.
</summary>
    <author>
      <name>Salvatore Catanese</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Giacomo Fiumara</name>
    </author>
    <author>
      <name>Francesco Pagano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4108/icst.simutools.2011.245524</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4108/icst.simutools.2011.245524" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures, Proceedings of the 4th International ICST
  Conference on Simulation Tools and Techniques (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.4271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; I.2.1; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2877v1</id>
    <updated>2011-06-15T05:35:51Z</updated>
    <published>2011-06-15T05:35:51Z</published>
    <title>Injectivity of 2D Toric Bézier Patches</title>
    <summary>  Rational B\'{e}zier functions are widely used as mapping functions in surface
reparameterization, finite element analysis, image warping and morphing. The
injectivity (one-to-one property) of a mapping function is typically necessary
for these applications. Toric B\'{e}zier patches are generalizations of
classical patches (triangular, tensor product) which are defined on the convex
hull of a set of integer lattice points. We give a geometric condition on the
control points that we show is equivalent to the injectivity of every 2D toric
B\'{e}zier patch with those control points for all possible choices of weights.
This condition refines that of Craciun, et al., which only implied injectivity
on the interior of a patch.
</summary>
    <author>
      <name>Frank Sottile</name>
    </author>
    <author>
      <name>Chungang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, extended abstract, to be publised in Proceedings of
  CAD/Graphis 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 14M25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6073v1</id>
    <updated>2011-09-28T01:44:43Z</updated>
    <published>2011-09-28T01:44:43Z</published>
    <title>Evaluation of a Bundling Technique for Parallel Coordinates</title>
    <summary>  We describe a technique for bundled curve representations in
parallel-coordinates plots and present a controlled user study evaluating their
effectiveness. Replacing the traditional C^0 polygonal lines by C^1 continuous
piecewise Bezier curves makes it easier to visually trace data points through
each coordinate axis. The resulting Bezier curves can then be bundled to
visualize data with given cluster structures. Curve bundles are efficient to
compute, provide visual separation between data clusters, reduce visual
clutter, and present a clearer overview of the dataset. A controlled user study
with 14 participants confirmed the effectiveness of curve bundling for
parallel-coordinates visualization: 1) compared to polygonal lines, it is
equally capable of revealing correlations between neighboring data attributes;
2) its geometric cues can be effective in displaying cluster information. For
some datasets curve bundling allows the color perceptual channel to be applied
to other data attributes, while for complex cluster patterns, bundling and
color can represent clustering far more clearly than either alone.
</summary>
    <author>
      <name>Julian Heinrich</name>
    </author>
    <author>
      <name>Yuan Luo</name>
    </author>
    <author>
      <name>Arthur E. Kirkpatrick</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <author>
      <name>Daniel Weiskopf</name>
    </author>
    <link href="http://arxiv.org/abs/1109.6073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6494v1</id>
    <updated>2011-09-29T11:50:29Z</updated>
    <published>2011-09-29T11:50:29Z</published>
    <title>A Survey of Ocean Simulation and Rendering Techniques in Computer
  Graphics</title>
    <summary>  This paper presents a survey of ocean simulation and rendering methods in
computer graphics. To model and animate the ocean's surface, these methods
mainly rely on two main approaches: on the one hand, those which approximate
ocean dynamics with parametric, spectral or hybrid models and use empirical
laws from oceanographic research. We will see that this type of methods
essentially allows the simulation of ocean scenes in the deep water domain,
without breaking waves. On the other hand, physically-based methods use
Navier-Stokes Equations (NSE) to represent breaking waves and more generally
ocean surface near the shore. We also describe ocean rendering methods in
computer graphics, with a special interest in the simulation of phenomena such
as foam and spray, and light's interaction with the ocean surface.
</summary>
    <author>
      <name>Emmanuelle Darles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">XLIM</arxiv:affiliation>
    </author>
    <author>
      <name>Benoît Crespin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">XLIM</arxiv:affiliation>
    </author>
    <author>
      <name>Djamchid Ghazanfarpour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">XLIM</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Christophe Gonzato</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Bordeaux - Sud-Ouest, LaBRI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1467-8659.2010.01828.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1467-8659.2010.01828.x" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum 30, 1 (2011) 43-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4734v1</id>
    <updated>2012-04-20T20:05:46Z</updated>
    <published>2012-04-20T20:05:46Z</published>
    <title>Numerical Analysis of Diagonal-Preserving, Ripple-Minimizing and
  Low-Pass Image Resampling Methods</title>
    <summary>  Image resampling is a necessary component of any operation that changes the
size of an image or its geometry.
  Methods tuned for natural image upsampling (roughly speaking, image
enlargement) are analyzed and developed with a focus on their ability to
preserve diagonal features and suppress overshoots. Monotone, locally bounded
and almost monotone "direct" interpolation and filtering methods, as well as
face split and vertex split surface subdivision methods, alone or in
combination, are studied. Key properties are established by way of proofs and
counterexamples as well as numerical experiments involving 1D curve and 2D
diagonal data resampling.
  In addition, the Remez minimax method for the computation of low-cost
polynomial approximations of low-pass filter kernels tuned for natural image
downsampling (roughly speaking, image reduction) is refactored for relative
error minimization in the presence of roots in the interior of the interval of
approximation and so that even and odd functions are approximated with like
polynomials. The accuracy and frequency response of the approximations are
tabulated and plotted against the original, establishing their rapid
convergence.
</summary>
    <author>
      <name>Chantal Racette</name>
    </author>
    <link href="http://arxiv.org/abs/1204.4734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6216v2</id>
    <updated>2012-09-12T16:53:47Z</updated>
    <published>2012-04-24T20:26:58Z</published>
    <title>Geodesics in Heat</title>
    <summary>  We introduce the heat method for computing the shortest geodesic distance to
a specified subset (e.g., point or curve) of a given domain. The heat method is
robust, efficient, and simple to implement since it is based on solving a pair
of standard linear elliptic problems. The method represents a significant
breakthrough in the practical computation of distance on a wide variety of
geometric domains, since the resulting linear systems can be prefactored once
and subsequently solved in near-linear time. In practice, distance can be
updated via the heat method an order of magnitude faster than with
state-of-the-art methods while maintaining a comparable level of accuracy. We
provide numerical evidence that the method converges to the exact geodesic
distance in the limit of refinement; we also explore smoothed approximations of
distance suitable for applications where more regularity is required.
</summary>
    <author>
      <name>Keenan Crane</name>
    </author>
    <author>
      <name>Clarisse Weischedel</name>
    </author>
    <author>
      <name>Max Wardetzky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2516971.2516977</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2516971.2516977" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 32 (5), 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.6216v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6216v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5204v1</id>
    <updated>2012-05-23T15:29:16Z</updated>
    <published>2012-05-23T15:29:16Z</published>
    <title>Visualizing 2D Flows with Animated Arrow Plots</title>
    <summary>  Flow fields are often represented by a set of static arrows to illustrate
scientific vulgarization, documentary film, meteorology, etc. This simple
schematic representation lets an observer intuitively interpret the main
properties of a flow: its orientation and velocity magnitude. We propose to
generate dynamic versions of such representations for 2D unsteady flow fields.
Our algorithm smoothly animates arrows along the flow while controlling their
density in the domain over time. Several strategies have been combined to lower
the unavoidable popping artifacts arising when arrows appear and disappear and
to achieve visually pleasing animations. Disturbing arrow rotations in low
velocity regions are also handled by continuously morphing arrow glyphs to
semi-transparent discs. To substantiate our method, we provide results for
synthetic and real velocity field datasets.
</summary>
    <author>
      <name>Bruno Jobard</name>
    </author>
    <author>
      <name>Nicolas Ray</name>
    </author>
    <author>
      <name>Dmitry Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1148v2</id>
    <updated>2012-08-07T12:40:29Z</updated>
    <published>2012-06-06T08:38:27Z</published>
    <title>From individual to population: Challenges in Medical Visualization</title>
    <summary>  In this paper, we first give a high-level overview of medical visualization
development over the past 30 years, focusing on key developments and the trends
that they represent. During this discussion, we will refer to a number of key
papers that we have also arranged on the medical visualization research
timeline. Based on the overview and our observations of the field, we then
identify and discuss the medical visualization research challenges that we
foresee for the coming decade.
</summary>
    <author>
      <name>Charl P. Botha</name>
    </author>
    <author>
      <name>Bernhard Preim</name>
    </author>
    <author>
      <name>Arie Kaufman</name>
    </author>
    <author>
      <name>Shigeo Takahashi</name>
    </author>
    <author>
      <name>Anders Ynnerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improvements based on comments by reviewers: Typos and layout issues
  fixed. Added two more multi-modal volume rendering references to 2.1. Added
  more detail on Virtual Colonoscopy to 2.2</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1428v2</id>
    <updated>2012-08-07T12:19:28Z</updated>
    <published>2012-06-07T09:17:34Z</published>
    <title>Visualization in Connectomics</title>
    <summary>  Connectomics is a field of neuroscience that analyzes neuronal connections. A
connectome is a complete map of a neuronal system, comprising all neuronal
connections between its structures. The term "connectome" is close to the word
"genome" and implies completeness of all neuronal connections, in the same way
as a genome is a complete listing of all nucleotide sequences. The goal of
connectomics is to create a complete representation of the brain's wiring. Such
a representation is believed to increase our understanding of how functional
brain states emerge from their underlying anatomical structure. Furthermore, it
can provide important information for the cure of neuronal dysfunctions like
schizophrenia or autism. In this paper, we review the current state-of-the-art
of visualization and image processing techniques in the field of connectomics
and describe some remaining challenges.
</summary>
    <author>
      <name>Hanspeter Pfister</name>
    </author>
    <author>
      <name>Verena Kaynig</name>
    </author>
    <author>
      <name>Charl P. Botha</name>
    </author>
    <author>
      <name>Stefan Bruckner</name>
    </author>
    <author>
      <name>Vincent J. Dercksen</name>
    </author>
    <author>
      <name>Hans-Christian Hege</name>
    </author>
    <author>
      <name>Jos B. T. M. Roerdink</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved definition of diffusion PDF. Integrated reviewer comments:
  Added figures showing DTI tractography and glyphs, fMRI connectivity vis, EM
  reconstruction of neuronal structures, Brainbow image. Typos and grammar
  errors fixed. Description of connectivity matrix added</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3975v1</id>
    <updated>2012-06-18T16:05:47Z</updated>
    <published>2012-06-18T16:05:47Z</published>
    <title>The Ultrasound Visualization Pipeline - A Survey</title>
    <summary>  Ultrasound is one of the most frequently used imaging modality in medicine.
The high spatial resolution, its interactive nature and non-invasiveness makes
it the first choice in many examinations. Image interpretation is one of
ultrasound's main challenges. Much training is required to obtain a confident
skill level in ultrasound-based diagnostics. State-of-the-art graphics
techniques is needed to provide meaningful visualizations of ultrasound in
real-time. In this paper we present the process-pipeline for ultrasound
visualization, including an overview of the tasks performed in the specific
steps. To provide an insight into the trends of ultrasound visualization
research, we have selected a set of significant publications and divided them
into a technique-based taxonomy covering the topics pre-processing,
segmentation, registration, rendering and augmented reality. For the different
technique types we discuss the difference between ultrasound-based techniques
and techniques for other modalities.
</summary>
    <author>
      <name>Åsmund Birkeland</name>
    </author>
    <author>
      <name>Veronika Solteszova</name>
    </author>
    <author>
      <name>Dieter Hönigmann</name>
    </author>
    <author>
      <name>Odd Helge Gilja</name>
    </author>
    <author>
      <name>Svein Brekke</name>
    </author>
    <author>
      <name>Timo Ropinski</name>
    </author>
    <author>
      <name>Ivan Viola</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-1-4471-6497-5_24</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-1-4471-6497-5_24" rel="related"/>
    <link href="http://arxiv.org/abs/1206.3975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6850v1</id>
    <updated>2012-06-27T16:24:29Z</updated>
    <published>2012-06-27T16:24:29Z</published>
    <title>Visualization of Collaborative Data</title>
    <summary>  Collaborative data consist of ratings relating two distinct sets of objects:
users and items. Much of the work with such data focuses on filtering:
predicting unknown ratings for pairs of users and items. In this paper we focus
on the problem of visualizing the information. Given all of the ratings, our
task is to embed all of the users and items as points in the same Euclidean
space. We would like to place users near items that they have rated (or would
rate) high, and far away from those they would give a low rating. We pose this
problem as a real-valued non-linear Bayesian network and employ Markov chain
Monte Carlo and expectation maximization to find an embedding. We present a
metric by which to judge the quality of a visualization and compare our results
to local linear embedding and Eigentaste on three real-world datasets.
</summary>
    <author>
      <name>Guobiao Mei</name>
    </author>
    <author>
      <name>Christian R. Shelton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3351v1</id>
    <updated>2012-07-13T13:13:27Z</updated>
    <published>2012-07-13T13:13:27Z</published>
    <title>Combining Brain-Computer Interfaces and Haptics: Detecting Mental
  Workload to Adapt Haptic Assistance</title>
    <summary>  In this paper we introduce the combined use of Brain-Computer Interfaces
(BCI) and Haptic interfaces. We propose to adapt haptic guides based on the
mental activity measured by a BCI system. This novel approach is illustrated
within a proof-of-concept system: haptic guides are toggled during a
path-following task thanks to a mental workload index provided by a BCI. The
aim of this system is to provide haptic assistance only when the user's brain
activity reflects a high mental workload. A user study conducted with 8
participants shows that our proof-of-concept is operational and exploitable.
Results show that activation of haptic guides occurs in the most difficult part
of the path-following task. Moreover it allows to increase task performance by
53% by activating assistance only 59% of the time. Taken together, these
results suggest that BCI could be used to determine when the user needs
assistance during haptic interaction and to enable haptic guides accordingly.
</summary>
    <author>
      <name>Laurent George</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Maud Marchal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Loeïz Glondu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Anatole Lécuyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-31401-8_12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-31401-8_12" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EuroHaptics (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3794v1</id>
    <updated>2012-08-18T23:40:24Z</updated>
    <published>2012-08-18T23:40:24Z</published>
    <title>General Midpoint Subdivision</title>
    <summary>  In this paper, we introduce two generalizations of midpoint subdivision and
analyze the smoothness of the resulting subdivision surfaces at regular and
extraordinary points.
  The smoothing operators used in midpoint and mid-edge subdivision connect the
midpoints of adjacent faces or of adjacent edges, respectively. An arbitrary
combination of these two operators and the refinement operator that splits each
face with m vertices into m quadrilateral subfaces forms a general midpoint
subdivision operator. We analyze the smoothness of the resulting subdivision
surfaces by estimating the norm of a special second order difference scheme and
by using established methods for analyzing midpoint subdivision. The surfaces
are smooth at their regular points and they are also smooth at extraordinary
points for a certain subclass of general midpoint subdivision schemes.
  Generalizing the smoothing rules of non general midpoint subdivision schemes
around extraordinary and regular vertices or faces results in a class of
subdivision schemes, which includes the Catmull-Clark algorithm with restricted
parameters. We call these subdivision schemes generalized Catmull-Clark schemes
and we analyze their smoothness properties.
</summary>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Hartmut Prautzsch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18, 65D17, 68U07, 68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.6560v1</id>
    <updated>2012-09-28T16:05:37Z</updated>
    <published>2012-09-28T16:05:37Z</published>
    <title>Sparse Modeling of Intrinsic Correspondences</title>
    <summary>  We present a novel sparse modeling approach to non-rigid shape matching using
only the ability to detect repeatable regions. As the input to our algorithm,
we are given only two sets of regions in two shapes; no descriptors are
provided so the correspondence between the regions is not know, nor we know how
many regions correspond in the two shapes. We show that even with such scarce
information, it is possible to establish very accurate correspondence between
the shapes by using methods from the field of sparse modeling, being this, the
first non-trivial use of sparse models in shape correspondence. We formulate
the problem of permuted sparse coding, in which we solve simultaneously for an
unknown permutation ordering the regions on two shapes and for an unknown
correspondence in functional representation. We also propose a robust variant
capable of handling incomplete matches. Numerically, the problem is solved
efficiently by alternating the solution of a linear assignment and a sparse
coding problem. The proposed methods are evaluated qualitatively and
quantitatively on standard benchmarks containing both synthetic and scanned
objects.
</summary>
    <author>
      <name>J. Pokrass</name>
    </author>
    <author>
      <name>A. M. Bronstein</name>
    </author>
    <author>
      <name>M. M. Bronstein</name>
    </author>
    <author>
      <name>P. Sprechmann</name>
    </author>
    <author>
      <name>G. Sapiro</name>
    </author>
    <link href="http://arxiv.org/abs/1209.6560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4490v1</id>
    <updated>2012-12-18T12:52:26Z</updated>
    <published>2012-12-18T12:52:26Z</published>
    <title>Sketch-to-Design: Context-based Part Assembly</title>
    <summary>  Designing 3D objects from scratch is difficult, especially when the user
intent is fuzzy without a clear target form. In the spirit of
modeling-by-example, we facilitate design by providing reference and
inspiration from existing model contexts. We rethink model design as navigating
through different possible combinations of part assemblies based on a large
collection of pre-segmented 3D models. We propose an interactive
sketch-to-design system, where the user sketches prominent features of parts to
combine. The sketched strokes are analyzed individually and in context with the
other parts to generate relevant shape suggestions via a design gallery
interface. As the session progresses and more parts get selected, contextual
cues becomes increasingly dominant and the system quickly converges to a final
design. As a key enabler, we use pre-learned part-based contextual information
to allow the user to quickly explore different combinations of parts. Our
experiments demonstrate the effectiveness of our approach for efficiently
designing new variations from existing shapes.
</summary>
    <author>
      <name>Xiaohua Xie</name>
    </author>
    <author>
      <name>Kai Xu</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <author>
      <name>Baoquan Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.12200</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.12200" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages; Executable: see project webpage</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.4490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6048v1</id>
    <updated>2012-12-25T13:23:32Z</updated>
    <published>2012-12-25T13:23:32Z</published>
    <title>Discrete Surface Modeling Based on Google Earth: A Case Study</title>
    <summary>  Google Earth (GE) has become a powerful tool for geological, geophysical and
geographical modeling; yet GE can be accepted to acquire elevation data of
terrain. In this paper, we present a real study case of building the discrete
surface model (DSM) at Haut-Barr Castle in France based on the elevation data
of terrain points extracted from GE using the COM API. We first locate the
position of Haut-Barr Castle and determine the region of the study area, then
extract elevation data of terrain at Haut-Barr, and thirdly create a planar
triangular mesh that covers the study area and finally generate the desired DSM
by calculating the elevation of vertices in the planar mesh via interpolating
with Universal Kriging (UK) and Inverse Distance Weighting (IDW). The generated
DSM can reflect the features of the ground surface at Haut-Barr well, and can
be used for constructingthe Sealed Engineering Geological Model (SEGM) in
further step.
</summary>
    <author>
      <name>Gang Mei</name>
    </author>
    <author>
      <name>John C. Tipper</name>
    </author>
    <author>
      <name>Nengxiong Xu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCSNT.2012.6526125</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCSNT.2012.6526125" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IEEE Conference, ICCSNT 2012, in Press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2012 2nd International Conference on , vol., no., pp.1137,1141,
  29-31 Dec. 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.6048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6923v1</id>
    <updated>2012-12-31T16:41:07Z</updated>
    <published>2012-12-31T16:41:07Z</published>
    <title>The Geant4 Visualisation System - a multi-driver graphics system</title>
    <summary>  From the beginning the Geant4 Visualisation System was designed to support
several simultaneous graphics systems written to common abstract interfaces.
Today it has matured into a powerful diagnostic and presentational tool. It
comes with a library of models that may be added to the current scene and which
include the representation of the Geant4 geometry hierarchy, simulated
trajectories and user-written hits and digitisations. The workhorse is the
OpenGL suite of drivers for X, Xm, Qt and Win32. There is an Open Inventor
driver. Scenes can be exported in special graphics formats for offline viewing
in the DAWN, VRML, HepRApp and gMocren browsers. PostScript can be generated
through OpenGL, Open Inventor, DAWN and HepRApp. Geant4's own tracking
algorithms are used by the Ray Tracer. Not all drivers support all features but
all drivers bring added functionality of some sort. This paper describes the
interfaces and details the individual drivers.
</summary>
    <author>
      <name>John Allison</name>
    </author>
    <author>
      <name>Laurent Garnier</name>
    </author>
    <author>
      <name>Akinori Kimura</name>
    </author>
    <author>
      <name>Joseph Perl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 15 figures. Submitted to the International Journal of
  Modeling, Simulation, and Scientific Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3455v1</id>
    <updated>2013-01-15T19:14:06Z</updated>
    <published>2013-01-15T19:14:06Z</published>
    <title>3D Geological Modeling and Visualization of Rock Masses Based on Google
  Earth: A Case Study</title>
    <summary>  Google Earth (GE) has become a powerful tool for geological modeling and
visualization. An interesting and useful feature of GE, Google Street View, can
allow the GE users to view geological structure such as layers of rock masses
at a field site. In this paper, we introduce a practical solution for building
3D geological models for rock masses based on the data acquired by use with GE.
A real study case at Haut-Barr, France is presented to demonstrate our
solution. We first locate the position of Haut-Barr in GE, and then determine
the shape and scale of the rock masses in the study area, and thirdly acquire
the layout of layers of rock masses in the Google Street View, and finally
create the approximate 3D geological models by extruding and intersecting. The
generated 3D geological models can simply reflect the basic structure of the
rock masses at Haut-Barr, and can be used for visualizing the rock bodies
interactively.
</summary>
    <author>
      <name>Gang Mei</name>
    </author>
    <author>
      <name>John C. Tipper</name>
    </author>
    <author>
      <name>Nengxiong Xu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CSIT.2013.6588781</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CSIT.2013.6588781" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in the Proceeding of IEEE Conference CSIT2013, in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science and Information Technology (CSIT), 2013 5th
  International Conference on, 2013, pp. 210-213</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.3455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6809v2</id>
    <updated>2014-06-25T14:49:18Z</updated>
    <published>2013-01-29T00:18:00Z</published>
    <title>Skeletal Representations and Applications</title>
    <summary>  When representing a solid object there are alternatives to the use of
traditional explicit (surface meshes) or implicit (zero crossing of implicit
functions) methods. Skeletal representations encode shape information in a
mixed fashion: they are composed of a set of explicit primitives, yet they are
able to efficiently encode the shape's volume as well as its topology. I will
discuss, in two dimensions, how symmetry can be used to reduce the
dimensionality of the data (from a 2D solid to a 1D curve), and how this
relates to the classical definition of skeletons by Medial Axis Transform.
While the medial axis of a 2D shape is composed of a set of curves, in 3D it
results in a set of sheets connected in a complex fashion. Because of this
complexity, medial skeletons are difficult to use in practical applications.
Curve skeletons address this problem by strictly requiring their geometry to be
one dimensional, resulting in an intuitive yet powerful shape representation.
In this report I will define both medial and curve skeletons and discuss their
mutual relationship. I will also present several algorithms for their
computation and a variety of scenarios where skeletons are employed, with a
special focus on geometry processing and shape analysis.
</summary>
    <author>
      <name>Andrea Tagliasacchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, SFU Depth Exam</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.6809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.2024v1</id>
    <updated>2013-02-08T13:11:39Z</updated>
    <published>2013-02-08T13:11:39Z</published>
    <title>User Interface for Volume Rendering in Virtual Reality Environments</title>
    <summary>  Volume Rendering applications require sophisticated user interaction for the
definition and refinement of transfer functions. Traditional 2D desktop user
interface elements have been developed to solve this task, but such concepts do
not map well to the interaction devices available in Virtual Reality
environments.
  In this paper, we propose an intuitive user interface for Volume Rendering
specifically designed for Virtual Reality environments. The proposed interface
allows transfer function design and refinement based on intuitive two-handed
operation of Wand-like controllers. Additional interaction modes such as
navigation and clip plane manipulation are supported as well.
  The system is implemented using the Sony PlayStation Move controller system.
This choice is based on controller device capabilities as well as application
and environment constraints.
  Initial results document the potential of our approach.
</summary>
    <author>
      <name>Jonathan Klein</name>
    </author>
    <author>
      <name>Dennis Reuling</name>
    </author>
    <author>
      <name>Jan Grimm</name>
    </author>
    <author>
      <name>Andreas Pfau</name>
    </author>
    <author>
      <name>Damien Lefloch</name>
    </author>
    <author>
      <name>Martin Lambers</name>
    </author>
    <author>
      <name>Andreas Kolb</name>
    </author>
    <link href="http://arxiv.org/abs/1302.2024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.2024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4110v1</id>
    <updated>2013-03-17T22:00:15Z</updated>
    <published>2013-03-17T22:00:15Z</published>
    <title>On Linear Spaces of Polyhedral Meshes</title>
    <summary>  Polyhedral meshes (PM) - meshes having planar faces - have enjoyed a rise in
popularity in recent years due to their importance in architectural and
industrial design. However, they are also notoriously difficult to generate and
manipulate. Previous methods start with a smooth surface and then apply
elaborate meshing schemes to create polyhedral meshes approximating the
surface. In this paper, we describe a reverse approach: given the topology of a
mesh, we explore the space of possible planar meshes with that topology.
  Our approach is based on a complete characterization of the maximal linear
spaces of polyhedral meshes contained in the curved manifold of polyhedral
meshes with a given topology. We show that these linear spaces can be described
as nullspaces of differential operators, much like harmonic functions are
nullspaces of the Laplacian operator. An analysis of this operator provides
tools for global and local design of a polyhedral mesh, which fully expose the
geometric possibilities and limitations of the given topology.
</summary>
    <author>
      <name>Roi Poranne</name>
    </author>
    <author>
      <name>Renjie Chen</name>
    </author>
    <author>
      <name>Craig Gotsman</name>
    </author>
    <link href="http://arxiv.org/abs/1303.4110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7845v1</id>
    <updated>2013-04-30T03:28:07Z</updated>
    <published>2013-04-30T03:28:07Z</published>
    <title>G2 Transition curve using Quartic Bezier Curve</title>
    <summary>  A method to construct transition curves using a family of the quartic Bezier
spiral is described. The transition curves discussed are S-shape and C-shape of
contact, between two separated circles. A spiral is a curve of monotone
increasing or monotone decreasing curvature of one sign. Thus, a spiral cannot
have an inflection point or curvature extreme. The family of quartic Bezier
spiral form which is introduced has more degrees of freedom and will give a
better approximation. It is proved that the methods of constructing transition
curves can be simplified by the transformation process and the ratio of two
radii has no restriction, which extends the application area, and it gives a
family of transition curves that allow more flexible curve designs.
</summary>
    <author>
      <name>Azhar Ahmad</name>
    </author>
    <author>
      <name>R. Gobithasan</name>
    </author>
    <author>
      <name>Jamaluddin Md. Ali</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2007 Computer Graphics, Imaging and Visualization CGIV 2007, Pg.
  223-228</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7852v1</id>
    <updated>2013-04-30T03:41:31Z</updated>
    <published>2013-04-30T03:41:31Z</published>
    <title>Variational Formulation of the Log-Aesthetic Surface and Development of
  Discrete Surface Filters</title>
    <summary>  The log-aesthetic curves include the logarithmic (equiangular) spiral,
clothoid, and involute curves. Although most of them are expressed only by an
integral form of the tangent vector, it is possible to interactively generate
and deform them and they are expected to be utilized for practical use of
industrial and graphical design. The discrete log-aesthetic filter based on the
formulation of the log-aesthetic curve has successfully been introduced not to
impose strong constraints on the designer's activity, to let him/her design
freely and to embed the properties of the log-aesthetic curves for complicated
ones with both increasing and decreasing curvature. In this paper, in order to
define the log-aesthetic surface and develop surface filters based on its
formulation, at first we reformulate the log-aesthetic curve with variational
principle. Then we propose several new functionals to be minimized for
free-form surfaces and define the log-aesthetic surface. Furthermore we propose
new discrete surface filters based on the log-aesthetic surface formulation
</summary>
    <author>
      <name>K. T. Miura</name>
    </author>
    <author>
      <name>R. Shirahata</name>
    </author>
    <author>
      <name>S. Agari</name>
    </author>
    <author>
      <name>S. Usuki</name>
    </author>
    <author>
      <name>R. U. Gobithaasan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2012 Computer Aided Design &amp; Application, Vol.9 (6), Pg.901-914</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7868v1</id>
    <updated>2013-04-30T04:27:35Z</updated>
    <published>2013-04-30T04:27:35Z</published>
    <title>Normal type-2 Fuzzy Rational B-Spline Curve</title>
    <summary>  In this paper, we proposed a new form of type-2 fuzzy data points(T2FDPs)
that is normal type-2 data points(NT2FDPs). These brand-new forms of data were
defined by using the definition of normal type-2 triangular fuzzy
number(NT2TFN). Then, we applied fuzzification(alpha-cut) and type-reduction
processes towards NT2FDPs after they had been redefined based on the situation
of NT2FDPs. Furthermore, we redefine the defuzzification definition along with
the new definitions of fuzzification process and type-reduction method to
obtain crisp type-2 fuzzy solution data points. For all these processes from
the defining the NT2FDPs to defuzzification of NT2FDPs, we demonstrate through
curve representation by using the rational B-spline curve function as the
example form modeling these NT2FDPs.
</summary>
    <author>
      <name>Rozaimi Zakaria</name>
    </author>
    <author>
      <name>Abd. Fatah Wahab</name>
    </author>
    <author>
      <name>R. U. Gobithaasan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2013 Int. Journal of Math. Analysis, 7(16), Pg.789-806</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3971v1</id>
    <updated>2013-05-17T03:13:28Z</updated>
    <published>2013-05-17T03:13:28Z</published>
    <title>Sparse Norm Filtering</title>
    <summary>  Optimization-based filtering smoothes an image by minimizing a fidelity
function and simultaneously preserves edges by exploiting a sparse norm penalty
over gradients. It has obtained promising performance in practical problems,
such as detail manipulation, HDR compression and deblurring, and thus has
received increasing attentions in fields of graphics, computer vision and image
processing. This paper derives a new type of image filter called sparse norm
filter (SNF) from optimization-based filtering. SNF has a very simple form,
introduces a general class of filtering techniques, and explains several
classic filters as special implementations of SNF, e.g. the averaging filter
and the median filter. It has advantages of being halo free, easy to implement,
and low time and memory costs (comparable to those of the bilateral filter).
Thus, it is more generic than a smoothing operator and can better adapt to
different tasks. We validate the proposed SNF by a wide variety of applications
including edge-preserving smoothing, outlier tolerant filtering, detail
manipulation, HDR compression, non-blind deconvolution, image segmentation, and
colorization.
</summary>
    <author>
      <name>Chengxi Ye</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <author>
      <name>Mingli Song</name>
    </author>
    <author>
      <name>David W. Jacobs</name>
    </author>
    <author>
      <name>Min Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1305.3971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.4583v2</id>
    <updated>2013-11-03T21:39:13Z</updated>
    <published>2013-05-20T17:27:29Z</published>
    <title>Parallel Coordinates Guided High Dimensional Transfer Function Design</title>
    <summary>  High-dimensional transfer function design is widely used to provide
appropriate data classification for direct volume rendering of various
datasets. However, its design is a complicated task. Parallel coordinate plot
(PCP), as a powerful visualization tool, can efficiently display
high-dimensional geometry and accurately analyze multivariate data. In this
paper, we propose to combine parallel coordinates with dimensional reduction
methods to guide high-dimensional transfer function design. Our pipeline has
two major advantages: (1) combine and display extracted high-dimensional
features in parameter space; and (2) select appropriate high-dimensional
parameters, with the help of dimensional reduction methods, to obtain
sophisticated data classification as transfer function for volume rendering. In
order to efficiently design high-dimensional transfer functions, the
combination of both parallel coordinate components and dimension reduction
results is necessary to generate final visualization results. We demonstrate
the capability of our method for direct volume rendering using various CT and
MRI datasets.
</summary>
    <author>
      <name>Xin Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures. This paper has been withdrawn by the author due
  to publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.4583v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.4583v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0118v3</id>
    <updated>2014-05-10T19:56:31Z</updated>
    <published>2013-06-29T15:32:23Z</published>
    <title>Computing a Compact Spline Representation of the Medial Axis Transform
  of a 2D Shape</title>
    <summary>  We present a full pipeline for computing the medial axis transform of an
arbitrary 2D shape. The instability of the medial axis transform is overcome by
a pruning algorithm guided by a user-defined Hausdorff distance threshold. The
stable medial axis transform is then approximated by spline curves in 3D to
produce a smooth and compact representation. These spline curves are computed
by minimizing the approximation error between the input shape and the shape
represented by the medial axis transform. Our results on various 2D shapes
suggest that our method is practical and effective, and yields faithful and
compact representations of medial axis transforms of 2D shapes.
</summary>
    <author>
      <name>Yanshu Zhu</name>
    </author>
    <author>
      <name>Feng Sun</name>
    </author>
    <author>
      <name>Yi-King Choi</name>
    </author>
    <author>
      <name>Bert Jüttler</name>
    </author>
    <author>
      <name>Wenping Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.gmod.2014.03.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.gmod.2014.03.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GMP14 (Geometric Modeling and Processing)</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.0118v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0118v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0247v1</id>
    <updated>2013-06-30T21:54:33Z</updated>
    <published>2013-06-30T21:54:33Z</published>
    <title>Progressive Blue Surfels</title>
    <summary>  In this paper we describe a new technique to generate and use surfels for
rendering of highly complex, polygonal 3D scenes in real time. The basic idea
is to approximate complex parts of the scene by rendering a set of points
(surfels). The points are computed in a preprocessing step and offer two
important properties: They are placed only on the visible surface of the
scene's geometry and they are distributed and sorted in such a way, that every
prefix of points is a good visual representation of the approximated part of
the scene. An early evaluation of the method shows that it is capable of
rendering scenes consisting of several billions of triangles with high image
quality.
</summary>
    <author>
      <name>Claudius Jähn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Please note that this paper represents an early working draft, which
  will be subsequently replaced by refined versions! 3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.0247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6360v1</id>
    <updated>2013-07-24T09:54:22Z</updated>
    <published>2013-07-24T09:54:22Z</published>
    <title>Electronic Visualisation in Chemistry: From Alchemy to Art</title>
    <summary>  Chemists now routinely use software as part of their work. For example,
virtual chemistry allows chemical reactions to be simulated. In particular, a
selection of software is available for the visualisation of complex
3-dimensional molecular structures. Many of these are very beautiful in their
own right. As well as being included as illustrations in academic papers, such
visualisations are often used on the covers of chemistry journals as
artistically decorative and attractive motifs. Chemical images have also been
used as the basis of artworks in exhibitions. This paper explores the
development of the relationship of chemistry, art, and IT. It covers some of
the increasingly sophisticated software used to generate these projections
(e.g., UCSF Chimera) and their progressive use as a visual art form.
</summary>
    <author>
      <name>Karl Harrison</name>
    </author>
    <author>
      <name>Jonathan P. Bowen</name>
    </author>
    <author>
      <name>Alice M. Bowen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 27 figures, EVA London 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EVA London 2013 Conference Proceedings, Electronic Workshops in
  Computing (eWiC), British Computer Society, 29-31 July 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.m; J.3; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0375v1</id>
    <updated>2013-08-01T23:08:03Z</updated>
    <published>2013-08-01T23:08:03Z</published>
    <title>A New 3D Geometric Approach to Focus and Context Lens Effect Simulation</title>
    <summary>  We present a novel methodology based on geometric approach to simulate
magnification lens effects. Our aim is to promote new applications of powerful
geometric modeling techniques in visual computing. Conventional image
processing/visualization methods are computed in two dimensional space (2D). We
examine this conventional 2D manipulation from a completely innovative
perspective of 3D geometric processing. Compared with conventional optical lens
design, 3D geometric method are much more capable of preserving shape features
and minimizing distortion. We magnify an area of interest to better visualize
the interior details, while keeping the rest of area without perceivable
distortion. We flatten the mesh back into 2D space for viewing, and further
applications in the screen space. In both steps, we devise an iterative
deformation scheme to minimize distortion around both focus and context region,
while avoiding the noncontinuous transition region between the focus and
context areas. Particularly, our method allows the user to flexibly modify the
ROI shapes to accommodate complex feature. The user can also easily specify a
spectrum of metrics for different visual effects. Various experimental results
demonstrate the effectiveness, robustness, and efficiency of our framework.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Xin Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Poster for I3D</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0867v1</id>
    <updated>2013-08-05T01:27:36Z</updated>
    <published>2013-08-05T01:27:36Z</published>
    <title>A Survey of Spline-based Volumetric Data Modeling Framework and Its
  Applications</title>
    <summary>  The rapid advances in 3D scanning and acquisition techniques have given rise
to the explosive increase of volumetric digital models in recent years. This
dissertation systematically trailblazes a novel volumetric modeling framework
to represent 3D solids. The need to explore more efficient and robust 3D
modeling framework has gained the prominence. Although the traditional surface
representation (e.g., triangle mesh) has many attractive properties, it is
incapable of expressing the interior space and materials. Such a serious
drawback overshadows many potential modeling and analysis applications.
Consequently volumetric modeling techniques become the well-known solution to
this problem. Nevertheless, many unsolved research issues remain when
developing an efficient modeling paradigm for existing 3D models: complex
geometry (fine details and extreme concaveness), arbitrary topology,
heterogenous materials, large-scale data storage and processing, etc.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master Thesis, Computer Science Department, Stony Brook University</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3917v1</id>
    <updated>2013-08-19T03:34:21Z</updated>
    <published>2013-08-19T03:34:21Z</published>
    <title>Medial Meshes for Volume Approximation</title>
    <summary>  Volume approximation is an important problem found in many applications of
computer graphics, vision, and image processing. The problem is about computing
an accurate and compact approximate representation of 3D volumes using some
simple primitives. In this study, we propose a new volume representation,
called medial meshes, and present an efficient method for its computation.
Specifically, we use the union of a novel type of simple volume primitives,
which are spheres and the convex hulls of two or three spheres, to approximate
a given 3D shape. We compute such a volume approximation based on a new method
for medial axis simplification guided by Hausdorff errors. We further
demonstrate the superior efficiency and accuracy of our method over existing
methods for medial axis simplification.
</summary>
    <author>
      <name>Feng Sun</name>
    </author>
    <author>
      <name>Yi-King Choi</name>
    </author>
    <author>
      <name>Yizhou Yu</name>
    </author>
    <author>
      <name>Wenping Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5843v1</id>
    <updated>2013-08-27T12:44:54Z</updated>
    <published>2013-08-27T12:44:54Z</published>
    <title>Affordable Virtual Reality System Architecture for Representation of
  Implicit Object Properties</title>
    <summary>  A flexible, scalable and affordable virtual reality software system
architecture is proposed. This solution can be easily implemented on different
hardware configurations: on a single computer or on a computer cluster. The
architecture is aimed to be integrated in the workflow for solving engineering
tasks and oriented towards presenting implicit object properties through
multiple sensorial channels (visual, audio and haptic). Implicit properties
represent hidden object features (i.e. magnetization, radiation, humidity,
toxicity, etc.) which cannot be perceived by the observer through his or her
senses but require specialized equipment in order to expand the sensory ability
of the observer. Our approach extends the underlying general scene graph
structure incorporating additional effects nodes for implicit properties
representation.
</summary>
    <author>
      <name>Stoyan Maleshkov</name>
    </author>
    <author>
      <name>Dimo Chotrov</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.5843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5847v1</id>
    <updated>2013-08-27T12:47:34Z</updated>
    <published>2013-08-27T12:47:34Z</published>
    <title>Post-processing of Engineering Analysis Results for Visualization in VR
  Systems</title>
    <summary>  The applicability of Virtual Reality for evaluating engineering analysis
results is beginning to receive increased appreciation in the last years. The
problem many engineers are still facing is how to import their model together
with the analysis results in a virtual reality environment for exploration and
results validation. In this paper we propose an algorithm for transforming
model data and results from finite element analysis (FEA) solving application
to a format easily interpretable by a virtual reality application. The
algorithm includes also steps for reducing the face-count of the resulting mesh
by eliminating faces from the inner part of the model in the cases when only
the surfaces of the model is analyzed. We also describe a possibility for
simultaneously assessing multiple analysis results relying on multimodal
results presentation by stimulating different senses of the operator.
</summary>
    <author>
      <name>Stoyan Maleshkov</name>
    </author>
    <author>
      <name>Dimo Chotrov</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 2, March 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.5847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.4413v2</id>
    <updated>2013-09-18T03:49:02Z</updated>
    <published>2013-09-17T18:13:01Z</published>
    <title>Mobile augmented reality survey: a bottom-up approach</title>
    <summary>  Augmented Reality (AR) is becoming mobile. Mobile devices have many
constraints but also rich new features that traditional desktop computers do
not have. There are several survey papers on AR, but none is dedicated to
Mobile Augmented Reality (MAR). Our work serves the purpose of closing this
gap. The contents are organized with a bottom-up approach. We first present the
state-of-the-art in system components including hardware platforms, software
frameworks and display devices, follows with enabling technologies such as
tracking and data management. We then survey the latest technologies and
methods to improve run-time performance and energy efficiency for practical
implementation. On top of these, we further introduce the application fields
and several typical MAR applications. Finally we conclude the survey with
several challenge problems, which are under exploration and require great
research efforts in the future.
</summary>
    <author>
      <name>Zhanpeng Huang</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <author>
      <name>Christoph Peylo</name>
    </author>
    <author>
      <name>Dimitris Chatzopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.4413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0041v1</id>
    <updated>2013-09-30T20:31:10Z</updated>
    <published>2013-09-30T20:31:10Z</published>
    <title>Gradient-Domain Processing for Large EM Image Stacks</title>
    <summary>  We propose a new gradient-domain technique for processing registered EM image
stacks to remove the inter-image discontinuities while preserving intra-image
detail. To this end, we process the image stack by first performing anisotropic
diffusion to smooth the data along the slice axis and then solving a
screened-Poisson equation within each slice to re-introduce the detail. The
final image stack is both continuous across the slice axis (facilitating the
tracking of information between slices) and maintains sharp details within each
slice (supporting automatic feature detection). To support this editing, we
describe the implementation of the first multigrid solver designed for
efficient gradient domain processing of large, out-of-core, voxel grids.
</summary>
    <author>
      <name>Michael Kazhdan</name>
    </author>
    <author>
      <name>Randal Burns</name>
    </author>
    <author>
      <name>Bobby Kasthuri</name>
    </author>
    <author>
      <name>Jeff Lichtman</name>
    </author>
    <author>
      <name>Jacob Vogelstein</name>
    </author>
    <author>
      <name>Joshua Vogelstein</name>
    </author>
    <link href="http://arxiv.org/abs/1310.0041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.4459v1</id>
    <updated>2013-10-16T17:39:34Z</updated>
    <published>2013-10-16T17:39:34Z</published>
    <title>Matching LBO eigenspace of non-rigid shapes via high order statistics</title>
    <summary>  A fundamental tool in shape analysis is the virtual embedding of the
Riemannian manifold describing the geometry of a shape into Euclidean space.
Several methods have been proposed to embed isometric shapes in flat domains
while preserving distances measured on the manifold. Recently, attention has
been given to embedding shapes into the eigenspace of the Lapalce-Beltrami
operator. The Laplace-Beltrami eigenspace preserves the diffusion distance, and
is invariant under isometric transformations. However, Laplace-Beltrami
eigenfunctions computed independently for different shapes are often
incompatible with each other. Applications involving multiple shapes, such as
pointwise correspondence, would greatly benefit if their respective
eigenfunctions were somehow matched. Here, we introduce a statistical approach
for matching eigenfunctions. We consider the values of the eigenfunctions over
the manifold as sampling of random variables, and try to match their
multivariate distributions. Comparing distributions is done indirectly, using
high order statistics. We show that the permutation and sign ambiguities of low
order eigenfunctions, can be inferred by minimizing the difference of their
third order moments. The sign ambiguities of antisymmetric eigenfunctions can
be resolved by exploiting isometric invariant relations between the gradients
of the eigenfunctions and the surface normal. We present experiments
demonstrating the success of the proposed method applied to feature point
correspondence.
</summary>
    <author>
      <name>Alon Shtern</name>
    </author>
    <author>
      <name>Ron Kimmel</name>
    </author>
    <link href="http://arxiv.org/abs/1310.4459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.4459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0955v2</id>
    <updated>2013-12-07T04:17:37Z</updated>
    <published>2013-11-05T03:25:38Z</published>
    <title>A Dual-Beam Method-of-Images 3D Searchlight BSSRDF</title>
    <summary>  We present a novel BSSRDF for rendering translucent materials. Angular
effects lacking in previous BSSRDF models are incorporated by using a dual-beam
formulation. We employ a Placzek's Lemma interpretation of the method of images
and discard diffusion theory. Instead, we derive a plane-parallel
transformation of the BSSRDF to form the associated BRDF and optimize the image
confiurations such that the BRDF is close to the known analytic solutions for
the associated albedo problem. This ensures reciprocity, accurate colors, and
provides an automatic level-of-detail transition for translucent objects that
appear at various distances in an image. Despite optimizing the subsurface
fluence in a plane-parallel setting, we find that this also leads to fairly
accurate fluence distributions throughout the volume in the original 3D
searchlight problem. Our method-of-images modifications can also improve the
accuracy of previous BSSRDFs.
</summary>
    <author>
      <name>Eugene d'Eon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2614106.2614140</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2614106.2614140" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added clarifying text and 1 figure to illustrate the method</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0955v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0955v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5018v1</id>
    <updated>2013-11-20T11:30:03Z</updated>
    <published>2013-11-20T11:30:03Z</published>
    <title>On the impact of explicit or semi-implicit integration methods over the
  stability of real-time numerical simulations</title>
    <summary>  Physics-based animation of soft or rigid bodies for real-time applications
often suffers from numerical instabilities. We analyse one of the most common
sources of unwanted behaviour: the numerical integration strategy. To assess
the impact of popular integration methods, we consider a scenario where soft
and hard constraints are added to a custom designed deformable linear object.
Since the goal for this class of simulation methods is to attain interactive
frame-rates, we present the drawbacks of using explicit integration methods
over inherently stable, implicit integrators. To help numerical solver
designers better understand the impact of an integrator on a certain simulated
world, we have conceived a method of benchmarking the efficiency of an
integrator with respect to its speed, stability and symplecticity.
</summary>
    <author>
      <name>Teodor Cioaca</name>
    </author>
    <author>
      <name>Horea Caramizaru</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the ROMAI Journal of Applied Mathematics. Presented at
  the CAIM 2013 Conference on Applied and Industrial Mathematics</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.5018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.6.8; G.1.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6811v1</id>
    <updated>2013-11-26T18:41:48Z</updated>
    <published>2013-11-26T18:41:48Z</published>
    <title>Digitize Your Body and Action in 3-D at Over 10 FPS: Real Time Dense
  Voxel Reconstruction and Marker-less Motion Tracking via GPU Acceleration</title>
    <summary>  In this paper, we present an approach to reconstruct 3-D human motion from
multi-cameras and track human skeleton using the reconstructed human 3-D point
(voxel) cloud. We use an improved and more robust algorithm, probabilistic
shape from silhouette to reconstruct human voxel. In addition, the annealed
particle filter is applied for tracking, where the measurement is computed
using the reprojection of reconstructed voxel. We use two different ways to
accelerate the approach. For the CPU only acceleration, we leverage Intel TBB
to speed up the hot spot of the computational overhead and reached an
accelerating ratio of 3.5 on a 4-core CPU. Moreover, we implement an
intensively paralleled version via GPU acceleration without TBB. Taking account
all data transfer and computing time, the GPU version is about 400 times faster
than the original CPU implementation, leading the approach to run at a
real-time speed.
</summary>
    <author>
      <name>Jian Song</name>
    </author>
    <author>
      <name>Yatao Bian</name>
    </author>
    <author>
      <name>Junchi Yan</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Yuncai Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7194v1</id>
    <updated>2013-11-28T03:17:03Z</updated>
    <published>2013-11-28T03:17:03Z</published>
    <title>Real-time High Resolution Fusion of Depth Maps on GPU</title>
    <summary>  A system for live high quality surface reconstruction using a single moving
depth camera on a commodity hardware is presented. High accuracy and real-time
frame rate is achieved by utilizing graphics hardware computing capabilities
via OpenCL and by using sparse data structure for volumetric surface
representation. Depth sensor pose is estimated by combining serial texture
registration algorithm with iterative closest points algorithm (ICP) aligning
obtained depth map to the estimated scene model. Aligned surface is then fused
into the scene. Kalman filter is used to improve fusion quality. Truncated
signed distance function (TSDF) stored as block-based sparse buffer is used to
represent surface. Use of sparse data structure greatly increases accuracy of
scanned surfaces and maximum scanning area. Traditional GPU implementation of
volumetric rendering and fusion algorithms were modified to exploit sparsity to
achieve desired performance. Incorporation of texture registration for sensor
pose estimation and Kalman filter for measurement integration improved accuracy
and robustness of scanning process.
</summary>
    <author>
      <name>Dmitry Trifonov</name>
    </author>
    <link href="http://arxiv.org/abs/1311.7194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7462v1</id>
    <updated>2013-11-29T03:08:37Z</updated>
    <published>2013-11-29T03:08:37Z</published>
    <title>Continuous Collision Detection for Composite Quadric Models</title>
    <summary>  A composite quadric model (CQM) is an object modeled by piecewise linear or
quadric patches. We study the continuous detection problem of a special type of
CQM objects which are commonly used in CAD/CAM, that is, the boundary surfaces
of such a CQM intersect only in straight line segments or conic curve segments.
We present a framework for continuous collision detection (CCD) of this special
type of CQM (which we also call CQM for brevity) in motion. We derive algebraic
formulations and compute numerically the first contact time instants and the
contact points of two moving CQMs in $\mathbb R^3$. Since it is difficult to
process CCD of two CQMs in a direct manner because they are composed of
semi-algebraic varieties, we break down the problem into subproblems of solving
CCD of pairs of boundary elements of the CQMs. We present procedures to solve
CCD of different types of boundary element pairs in different dimensions. Some
CCD problems are reduced to their equivalents in a lower dimensional setting,
where they can be solved more efficiently.
</summary>
    <author>
      <name>Yi-King Choi</name>
    </author>
    <author>
      <name>Wenping Wang</name>
    </author>
    <author>
      <name>Bernard Mourrain</name>
    </author>
    <author>
      <name>Changhe Tu</name>
    </author>
    <author>
      <name>Xiaohong Jia</name>
    </author>
    <author>
      <name>Feng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.7462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7535v2</id>
    <updated>2017-10-09T11:33:12Z</updated>
    <published>2013-11-29T12:03:00Z</published>
    <title>Compact Part-Based Shape Spaces for Dense Correspondences</title>
    <summary>  We consider the problem of establishing dense correspondences within a set of
related shapes of strongly varying geometry. For such input, traditional shape
matching approaches often produce unsatisfactory results. We propose an
ensemble optimization method that improves given coarse correspondences to
obtain dense correspondences. Following ideas from minimum description length
approaches, it maximizes the compactness of the induced shape space to obtain
high-quality correspondences. We make a number of improvements that are
important for computer graphics applications: Our approach handles meshes of
general topology and handles partial matching between input of varying
topology. To this end we introduce a novel part-based generative statistical
shape model. We develop a novel analysis algorithm that learns such models from
training shapes of varying topology. We also provide a novel synthesis method
that can generate new instances with varying part layouts and subject to
generic variational constraints. In practical experiments, we obtain a
substantial improvement in correspondence quality over state-of-the-art
methods. As example application, we demonstrate a system that learns shape
families as assemblies of deformable parts and permits real-time editing with
continuous and discrete variability.
</summary>
    <author>
      <name>Oliver Burghard</name>
    </author>
    <author>
      <name>Alexander Berner</name>
    </author>
    <author>
      <name>Michael Wand</name>
    </author>
    <author>
      <name>Niloy Mitra</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Reinhard Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1311.7535v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7535v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.7034v2</id>
    <updated>2014-06-28T20:57:37Z</updated>
    <published>2013-12-26T00:22:31Z</published>
    <title>A Topologically-informed Hyperstreamline Seeding Method for Alignment
  Tensor Fields</title>
    <summary>  A topologically-informed method is presented for seeding of hyperstreamlines
for visualization of alignment tensor fields. The method is inspired by and
applied to visualization of nematic liquid crystal (LC) reorientation dynamics
simulations. The method distributes hyperstreamlines along domain boundaries
and edges of a nearest-neighbor graph whose vertices are degenerate regions of
the alignment tensor field, which correspond to orientational defects in a
nematic LC domain. This is accomplished without iteration while conforming to a
user-specified spacing between hyperstreamlines and avoids possible failure
modes associated with hyperstreamline integration in the vicinity of
degeneracies of alignment (orientational defects). It is shown that the
presented seeding method enables automated hyperstreamline-based visualization
of a broad range of alignment tensor fields which enhances the ability of
researchers to interpret these fields and provides an alternative to using
glyph-based techniques.
</summary>
    <author>
      <name>Fred Fu</name>
    </author>
    <author>
      <name>Nasser Mohieddin Abukhdeir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2014.2363828</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2014.2363828" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.7034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.7034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0113v3</id>
    <updated>2014-06-17T14:31:06Z</updated>
    <published>2013-12-31T08:50:03Z</published>
    <title>Connectivity-preserving Geometry Images</title>
    <summary>  We propose connectivity-preserving geometry images (CGIMs), which map a
three-dimensional mesh onto a rectangular regular array of an image, such that
the reconstructed mesh produces no sampling errors, but merely round-off
errors. We obtain a V-matrix with respect to the original mesh, whose elements
are vertices of the mesh, which intrinsically preserves the vertex-set and the
connectivity of the original mesh in the sense of allowing round-off errors. We
generate a CGIM array by using the Cartesian coordinates of corresponding
vertices of the V-matrix. To reconstruct a mesh, we obtain a vertex-set and an
edge-set by collecting all the elements with different pixels, and all
different pairwise adjacent elements from the CGIM array respectively. Compared
with traditional geometry images, CGIMs achieve minimum reconstruction errors
with an efficient parametrization-free algorithm via elementary permutation
techniques. We apply CGIMs to lossy compression of meshes, and the experimental
results show that CGIMs perform well in reconstruction precision and detail
preservation.
</summary>
    <author>
      <name>Shaofan Wang</name>
    </author>
    <author>
      <name>Dehui Kong</name>
    </author>
    <author>
      <name>Juan Xue</name>
    </author>
    <author>
      <name>Weijia Zhu</name>
    </author>
    <author>
      <name>Min Xu</name>
    </author>
    <author>
      <name>Baocai Yin</name>
    </author>
    <author>
      <name>Hubert Roth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.0113v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0113v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1488v1</id>
    <updated>2014-01-07T20:14:55Z</updated>
    <published>2014-01-07T20:14:55Z</published>
    <title>Forward and Inverse Kinematics Seamless Matching Using Jacobian</title>
    <summary>  In this paper the problem of matching Forward Kinematics (FK) motion of a 3
Dimensional (3D) joint chain to the Inverse Kinematics (IK) movement and vice
versa has been addressed. The problem lies at the heart of animating a 3D
character having controller and manipulator based rig for animation within any
3D modeling and animation software. The seamless matching has been achieved
through the use of pseudo-inverse of Jacobian Matrix. The Jacobian Matrix is
used to determine the rotation values of each joint of character body part such
as arms, between the inverse kinematics and forward kinematics motion. Then
moving the corresponding kinematic joint system to the desired place,
automatically eliminating the jumping or popping effect which would reduce the
complexity of the system.
</summary>
    <author>
      <name>Zeeshan Bhatti</name>
    </author>
    <author>
      <name>Asadullah Shah</name>
    </author>
    <author>
      <name>Farruh Shahidi</name>
    </author>
    <author>
      <name>Mostafa Karbasi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sindh University Research Journal (SURJ) Volume 45 (2), 8/2013,
  pp:387-392, Sindh University Press</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.1488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2363v1</id>
    <updated>2014-02-11T04:05:12Z</updated>
    <published>2014-02-11T04:05:12Z</published>
    <title>Animation of 3D Human Model Using Markerless Motion Capture Applied To
  Sports</title>
    <summary>  Markerless motion capture is an active research in 3D virtualization. In
proposed work we presented a system for markerless motion capture for 3D human
character animation, paper presents a survey on motion and skeleton tracking
techniques which are developed or are under development. The paper proposed a
method to transform the motion of a performer to a 3D human character (model),
the 3D human character performs similar movements as that of a performer in
real time. In the proposed work, human model data will be captured by Kinect
camera, processed data will be applied on 3D human model for animation. 3D
human model is created using open source software (MakeHuman). Anticipated
dataset for sport activity is considered as input which can be applied to any
HCI application.
</summary>
    <author>
      <name>Ashish Shingade</name>
    </author>
    <author>
      <name>Archana Ghotkar</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5440v1</id>
    <updated>2014-02-21T22:31:26Z</updated>
    <published>2014-02-21T22:31:26Z</published>
    <title>Ergonomic-driven Geometric Exploration and Reshaping</title>
    <summary>  The paper addresses the following problem: given a set of man-made shapes,
e.g., chairs, can we quickly rank and explore the set of shapes with respect to
a given avatar pose? Answering this question requires identifying which shapes
are more suitable for the defined avatar and pose; and moreover, to provide
fast preview of how to alter the input geometry to better fit the deformed
shapes to the given avatar pose? The problem naturally links physical
proportions of human body and its interaction with object shapes in an attempt
to connect ergonomics with shape geometry. We designed an interaction system
that allows users to explore shape collections using the deformation of human
characters while at the same time providing interactive previews of how to
alter the shapes to better fit the user-specified character. We achieve this by
first mapping ergonomics guidelines into a set of simultaneous multi-part
constraints based on target contacts; and then, proposing a novel contact-based
deformation model to realize multi-contact constraints. We evaluate our
framework on various chair models and validate the results via a small user
study.
</summary>
    <author>
      <name>Youyi Zheng</name>
    </author>
    <author>
      <name>Julie Dorsey</name>
    </author>
    <author>
      <name>Niloy Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/1402.5440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0917v1</id>
    <updated>2014-03-04T19:50:00Z</updated>
    <published>2014-03-04T19:50:00Z</published>
    <title>An Extension Of Weiler-Atherton Algorithm To Cope With The
  Self-intersecting Polygon</title>
    <summary>  In this paper a new algorithm has been proposed which can fix the problem of
Weiler Atherton algorithm. The problem of Weiler Atherton algorithm lies in
clipping self intersecting polygon. Clipping self intersecting polygon is not
considered in Weiler Atherton algorithm and hence it is also a main
disadvantage of this algorithm. In our new algorithm a self intersecting
polygon has been divided into non self intersecting contours and then perform
the Weiler Atherton clipping algorithm on those sub polygons. For holes we have
to store the edges that is not own boundary of hole contour from recently
clipped polygon. Thus if both contour is hole then we have to store all the
edges of the recently clipped polygon. Finally the resultant polygon has been
produced by eliminating all the stored edges.
</summary>
    <author>
      <name>Anurag Chakraborty</name>
    </author>
    <link href="http://arxiv.org/abs/1403.0917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6566v2</id>
    <updated>2014-08-21T06:16:01Z</updated>
    <published>2014-03-26T03:29:25Z</published>
    <title>Image Retargeting by Content-Aware Synthesis</title>
    <summary>  Real-world images usually contain vivid contents and rich textural details,
which will complicate the manipulation on them. In this paper, we design a new
framework based on content-aware synthesis to enhance content-aware image
retargeting. By detecting the textural regions in an image, the textural image
content can be synthesized rather than simply distorted or cropped. This method
enables the manipulation of textural &amp; non-textural regions with different
strategy since they have different natures. We propose to retarget the textural
regions by content-aware synthesis and non-textural regions by fast
multi-operators. To achieve practical retargeting applications for general
images, we develop an automatic and fast texture detection method that can
detect multiple disjoint textural regions. We adjust the saliency of the image
according to the features of the textural regions. To validate the proposed
method, comparisons with state-of-the-art image targeting techniques and a user
study were conducted. Convincing visual results are shown to demonstrate the
effectiveness of the proposed method.
</summary>
    <author>
      <name>Weiming Dong</name>
    </author>
    <author>
      <name>Fuzhang Wu</name>
    </author>
    <author>
      <name>Yan Kong</name>
    </author>
    <author>
      <name>Xing Mei</name>
    </author>
    <author>
      <name>Tong-Yee Lee</name>
    </author>
    <author>
      <name>Xiaopeng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1403.6566v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6566v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.8105v1</id>
    <updated>2014-03-31T17:54:34Z</updated>
    <published>2014-03-31T17:54:34Z</published>
    <title>Flux-Limited Diffusion for Multiple Scattering in Participating Media</title>
    <summary>  For the rendering of multiple scattering effects in participating media,
methods based on the diffusion approximation are an extremely efficient
alternative to Monte Carlo path tracing. However, in sufficiently transparent
regions, classical diffusion approximation suffers from non-physical radiative
fluxes which leads to a poor match to correct light transport. In particular,
this prevents the application of classical diffusion approximation to
heterogeneous media, where opaque material is embedded within transparent
regions. To address this limitation, we introduce flux-limited diffusion, a
technique from the astrophysics domain. This method provides a better
approximation to light transport than classical diffusion approximation,
particularly when applied to heterogeneous media, and hence broadens the
applicability of diffusion-based techniques. We provide an algorithm for
flux-limited diffusion, which is validated using the transport theory for a
point light source in an infinite homogeneous medium. We further demonstrate
that our implementation of flux-limited diffusion produces more accurate
renderings of multiple scattering in various heterogeneous datasets than
classical diffusion approximation, by comparing both methods to ground truth
renderings obtained via volumetric path tracing.
</summary>
    <author>
      <name>David Koerner</name>
    </author>
    <author>
      <name>Jamie Portsmouth</name>
    </author>
    <author>
      <name>Filip Sadlo</name>
    </author>
    <author>
      <name>Thomas Ertl</name>
    </author>
    <author>
      <name>Bernd Eberhardt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.12342</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.12342" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Computer Graphics Forum</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.8105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.8105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0981v1</id>
    <updated>2014-04-03T15:48:55Z</updated>
    <published>2014-04-03T15:48:55Z</published>
    <title>New Julia and Mandelbrot Sets for Jungck Ishikawa Iterates</title>
    <summary>  The generation of fractals and study of the dynamics of polynomials is one of
the emerging and interesting field of research nowadays. We introduce in this
paper the dynamics of polynomials z^ n - z + c = 0 for n&gt;=2 and applied Jungck
Ishikawa Iteration to generate new Relative Superior Mandelbrot sets and
Relative Superior Julia sets. In order to solve this function by Jungck type
iterative schemes, we write it in the form of Sz = Tz, where the function T, S
are defined as Tz = z^ n + c and Sz = z. Only mathematical explanations are
derived by applying Jungck Ishikawa Iteration for polynomials in the literature
but in this paper we have generated Relative Mandelbrot sets and Relative Julia
sets.
</summary>
    <author>
      <name>Suman Joshi</name>
    </author>
    <author>
      <name>Dr. Yashwant Singh Chauhan</name>
    </author>
    <author>
      <name>Dr. Ashish Negi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,18 tables,18 charts,18 figures Published with International
  Journal of Computer Trends and Technology (IJCTT)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.0981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2053v1</id>
    <updated>2014-04-08T09:23:05Z</updated>
    <published>2014-04-08T09:23:05Z</published>
    <title>Expression driven Trignometric based Procedural Animation of Quadrupeds</title>
    <summary>  This research paper addresses the problem of generating involuntary and
precise animation of quadrupeds with automatic rigging system of various
character types. The technique proposed through this research is based on a two
tier animation control curve with base simulation being driven through dynamic
mathematical model using procedural algorithm and the top layer with a custom
user controlled animation provided with intuitive Graphical User Interface
(GUI). The character rig is based on forward and inverse kinematics driven
through trigonometric based motion equations. The User is provided with various
manipulators and attributes to control and handle the locomotion gaits of the
characters and choose between various types of simulated motions from walking,
running, trotting, ambling and galloping with complete custom controls to
easily extend the base simulation as per requirements.
</summary>
    <author>
      <name>Zeeshan Bhatti</name>
    </author>
    <author>
      <name>Asadullah Shah</name>
    </author>
    <author>
      <name>Mustafa Karabasi</name>
    </author>
    <author>
      <name>Waheed Mahesar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICICM.2013.25</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICICM.2013.25" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, Conference paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the International Conference on Informatics and
  Creative Multimedia 2103 (ICICM'13), pp.104,109, 4-6 Sept. 2013 IEEEXplore.
  UTM, KUALA LUMPUR (3-6 Sept)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.2053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2728v2</id>
    <updated>2014-04-22T02:26:25Z</updated>
    <published>2014-04-10T08:25:20Z</published>
    <title>Real-time Decolorization using Dominant Colors</title>
    <summary>  Decolorization is the process to convert a color image or video to its
grayscale version, and it has received great attention in recent years. An
ideal decolorization algorithm should preserve the original color contrast as
much as possible. Meanwhile, it should provide the final decolorized result as
fast as possible. However, most of the current methods are suffering from
either unsatisfied color information preservation or high computational cost,
limiting their application value. In this paper, a simple but effective
technique is proposed for real-time decolorization. Based on the typical
rgb2gray() color conversion model, which produces a grayscale image by linearly
combining R, G, and B channels, we propose a dominant color hypothesis and a
corresponding distance measurement metric to evaluate the quality of grayscale
conversion. The local optimum scheme provides several "good" candidates in a
confidence interval, from which the "best" result can be extracted.
Experimental results demonstrate that remarkable simplicity of the proposed
method facilitates the process of high resolution images and videos in
real-time using a common CPU.
</summary>
    <author>
      <name>Wei Hu</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Fan Zhang</name>
    </author>
    <author>
      <name>Qian Du</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some errors in
  equation 9 related descriptions</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.2728v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2728v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3363v3</id>
    <updated>2015-05-07T13:37:13Z</updated>
    <published>2014-04-13T09:52:15Z</published>
    <title>Interactive Isogeometric Volume Visualization with Pixel-Accurate
  Geometry</title>
    <summary>  A recent development, called isogeometric analysis, provides a unified
approach for design, analysis and optimization of functional products in
industry. Traditional volume rendering methods for inspecting the results from
the numerical simulations cannot be applied directly to isogeometric models. We
present a novel approach for interactive visualization of isogeometric analysis
results, ensuring correct, i.e., pixel-accurate geometry of the volume
including its bounding surfaces. The entire OpenGL pipeline is used in a
multi-stage algorithm leveraging techniques from surface rendering,
order-independent transparency, as well as theory and numerical methods for
ordinary differential equations. We showcase the efficiency of our approach on
different models relevant to industry, ranging from quality inspection of the
parametrization of the geometry, to stress analysis in linear elasticity, to
visualization of computational fluid dynamics results.
</summary>
    <author>
      <name>Franz G. Fuchs</name>
    </author>
    <author>
      <name>Jon M. Hjelmervik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2015.2430337</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2015.2430337" rel="related"/>
    <link href="http://arxiv.org/abs/1404.3363v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3363v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.9.h; G.1.1.e; G.1.5; G.1.7; G.1.0.g" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6293v2</id>
    <updated>2015-01-30T00:13:07Z</updated>
    <published>2014-04-25T00:04:20Z</published>
    <title>Piko: A Design Framework for Programmable Graphics Pipelines</title>
    <summary>  We present Piko, a framework for designing, optimizing, and retargeting
implementations of graphics pipelines on multiple architectures. Piko
programmers express a graphics pipeline by organizing the computation within
each stage into spatial bins and specifying a scheduling preference for these
bins. Our compiler, Pikoc, compiles this input into an optimized implementation
targeted to a massively-parallel GPU or a multicore CPU.
  Piko manages work granularity in a programmable and flexible manner, allowing
programmers to build load-balanced parallel pipeline implementations, to
exploit spatial and producer-consumer locality in a pipeline implementation,
and to explore tradeoffs between these considerations. We demonstrate that Piko
can implement a wide range of pipelines, including rasterization, Reyes, ray
tracing, rasterization/ray tracing hybrid, and deferred rendering. Piko allows
us to implement efficient graphics pipelines with relative ease and to quickly
explore design alternatives by modifying the spatial binning configurations and
scheduling preferences for individual stages, all while delivering real-time
performance that is within a factor six of state-of-the-art rendering systems.
</summary>
    <author>
      <name>Anjul Patney</name>
    </author>
    <author>
      <name>Stanley Tzeng</name>
    </author>
    <author>
      <name>Kerry A. Seitz Jr.</name>
    </author>
    <author>
      <name>John D. Owens</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2766973</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2766973" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, updated for 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics 34, 4 (July 2015), 147:1-147:13</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.6293v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6293v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.1; I.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4734v1</id>
    <updated>2014-04-30T22:05:15Z</updated>
    <published>2014-04-30T22:05:15Z</published>
    <title>A General Framework for Bilateral and Mean Shift Filtering</title>
    <summary>  We present a generalization of the bilateral filter that can be applied to
feature-preserving smoothing of signals on images, meshes, and other domains
within a single unified framework. Our discretization is competitive with
state-of-the-art smoothing techniques in terms of both accuracy and speed, is
easy to implement, and has parameters that are straightforward to understand.
Unlike previous bilateral filters developed for meshes and other irregular
domains, our construction reduces exactly to the image bilateral on rectangular
domains and comes with a rigorous foundation in both the smooth and discrete
settings. These guarantees allow us to construct unconditionally convergent
mean-shift schemes that handle a variety of extremely noisy signals. We also
apply our framework to geometric edge-preserving effects like feature
enhancement and show how it is related to local histogram techniques.
</summary>
    <author>
      <name>Justin Solomon</name>
    </author>
    <author>
      <name>Keenan Crane</name>
    </author>
    <author>
      <name>Adrian Butscher</name>
    </author>
    <author>
      <name>Chris Wojtan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7338v1</id>
    <updated>2014-06-28T00:47:30Z</updated>
    <published>2014-06-28T00:47:30Z</published>
    <title>Order-Independent Texture Synthesis</title>
    <summary>  Search-based texture synthesis algorithms are sensitive to the order in which
texture samples are generated; different synthesis orders yield different
textures. Unfortunately, most polygon rasterizers and ray tracers do not
guarantee the order with which surfaces are sampled. To circumvent this
problem, textures are synthesized beforehand at some maximum resolution and
rendered using texture mapping.
  We describe a search-based texture synthesis algorithm in which samples can
be generated in arbitrary order, yet the resulting texture remains identical.
The key to our algorithm is a pyramidal representation in which each texture
sample depends only on a fixed number of neighboring samples at each level of
the pyramid. The bottom (coarsest) level of the pyramid consists of a noise
image, which is small and predetermined. When a sample is requested by the
renderer, all samples on which it depends are generated at once. Using this
approach, samples can be generated in any order. To make the algorithm
efficient, we propose storing texture samples and their dependents in a
pyramidal cache. Although the first few samples are expensive to generate,
there is substantial reuse, so subsequent samples cost less. Fortunately, most
rendering algorithms exhibit good coherence, so cache reuse is high.
</summary>
    <author>
      <name>Li-Yi Wei</name>
    </author>
    <author>
      <name>Marc Levoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a combination of Stanford Computer Science Department
  Technical Report 2002-01 and a subsequent submission to SIGGRAPH 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.7338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3145v1</id>
    <updated>2014-07-11T13:14:16Z</updated>
    <published>2014-07-11T13:14:16Z</published>
    <title>SketchBio: A Scientist's 3D Interface for Molecular Modeling and
  Animation</title>
    <summary>  Background: Because of the difficulties involved in learning and using 3D
modeling and rendering software, many scientists hire programmers or animators
to create models and animations. This both slows the discovery process and
provides opportunities for miscommunication. Working with multiple
collaborators, we developed a set of design goals for a tool that would enable
them to directly construct models and animations. Results: We present
SketchBio, a tool that incorporates state-of-the-art bimanual interaction and
drop shadows to enable rapid construction of molecular structures and
animations. It includes three novel features: crystal by example, pose-mode
physics, and spring-based layout that accelerate operations common in the
formation of molecular models. We present design decisions and their
consequences, including cases where iterative design was required to produce
effective approaches. Conclusions: The design decisions, novel features, and
inclusion of state-of-the-art techniques enabled SketchBio to meet all of its
design goals. These features and decisions can be incorporated into existing
and new tools to improve their effectiveness
</summary>
    <author>
      <name>Shawn M. Waldon</name>
    </author>
    <author>
      <name>Peter M. Thompson</name>
    </author>
    <author>
      <name>Patrick J. Hahn</name>
    </author>
    <author>
      <name>Russell M. Taylor II</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BioVis 2014 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0677v1</id>
    <updated>2014-08-04T13:27:17Z</updated>
    <published>2014-08-04T13:27:17Z</published>
    <title>A Moving Least Squares Based Approach for Contour Visualization of
  Multi-Dimensional Data</title>
    <summary>  Analysis of high dimensional data is a common task. Often, small multiples
are used to visualize 1 or 2 dimensions at a time, such as in a scatterplot
matrix. Associating data points between different views can be difficult
though, as the points are not fixed. Other times, dimensional reduction
techniques are employed to summarize the whole dataset in one image, but
individual dimensions are lost in this view. In this paper, we present a means
of augmenting a dimensional reduction plot with isocontours to reintroduce the
original dimensions. By applying this to each dimension in the original data,
we create multiple views where the points are consistent, which facilitates
their comparison. Our approach employs a combination of a novel, graph-based
projection technique with a GPU accelerated implementation of moving least
squares to interpolate space between the points. We also present evaluations of
this approach both with a case study and with a user study.
</summary>
    <author>
      <name>Chris W. Muelder</name>
    </author>
    <author>
      <name>Nick Leaf</name>
    </author>
    <author>
      <name>Carmen Sigovan</name>
    </author>
    <author>
      <name>Kwan-Liu Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1408.0677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3326v1</id>
    <updated>2014-08-14T16:05:29Z</updated>
    <published>2014-08-14T16:05:29Z</published>
    <title>Regularized Harmonic Surface Deformation</title>
    <summary>  Harmonic surface deformation is a well-known geometric modeling method that
creates plausible deformations in an interactive manner. However, this method
is susceptible to artifacts, in particular close to the deformation handles.
These artifacts often correlate with strong gradients of the deformation
energy.In this work, we propose a novel formulation of harmonic surface
deformation, which incorporates a regularization of the deformation energy. To
do so, we build on and extend a recently introduced generic linear
regularization approach. It can be expressed as a change of norm for the linear
optimization problem, i.e., the regularization is baked into the optimization.
This minimizes the implementation complexity and has only a small impact on
runtime. Our results show that a moderate use of regularization suppresses many
deformation artifacts common to the well-known harmonic surface deformation
method, without introducing new artifacts.
</summary>
    <author>
      <name>Yeara Kozlov</name>
    </author>
    <author>
      <name>Janick Martinez Esturo</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Tino Weinkauf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.3326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6591v1</id>
    <updated>2014-08-26T14:23:57Z</updated>
    <published>2014-08-26T14:23:57Z</published>
    <title>Voronoi Grid-Shell Structures</title>
    <summary>  We introduce a framework for the generation of grid-shell structures that is
based on Voronoi diagrams and allows us to design tessellations that achieve
excellent static performances. We start from an analysis of stress on the input
surface and we use the resulting tensor field to induce an anisotropic
non-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation
under the same metric. The resulting mesh is hex-dominant and made of cells
with a variable density, which depends on the amount of stress, and anisotropic
shape, which depends on the direction of maximum stress. This mesh is further
optimized taking into account symmetry and regularity of cells to improve
aesthetics. We demonstrate that our grid-shells achieve better static
performances with respect to quad-based grid shells, while offering an
innovative and aesthetically pleasing look.
</summary>
    <author>
      <name>Nico Pietroni</name>
    </author>
    <author>
      <name>Davide Tonelli</name>
    </author>
    <author>
      <name>Enrico Puppo</name>
    </author>
    <author>
      <name>Maurizio Froli</name>
    </author>
    <author>
      <name>Roberto Scopigno</name>
    </author>
    <author>
      <name>Paolo Cignoni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.6591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2081v1</id>
    <updated>2014-09-07T04:34:11Z</updated>
    <published>2014-09-07T04:34:11Z</published>
    <title>History-free Collision Response for Deformable Surfaces</title>
    <summary>  Continuous collision detection (CCD) and response methods are widely adopted
in dynamics simulation of deformable models. They are history-based, as their
success is strictly based on an assumption of a collision-free state at the
start of each time interval. On the other hand, in many applications surfaces
have normals defined to designate their orientation (i.e. front- and
back-face), yet CCD methods are totally blind to such orientation
identification (thus are orientation-free). We notice that if such information
is utilized, many penetrations can be untangled. In this paper we present a
history-free method for separation of two penetrating meshes, where at least
one of them has clarified surface orientation. This method first computes all
edge-face (E-F) intersections with discrete collision detection (DCD), and then
builds a number of penetration stencils. On response, the stencil vertices are
relocated into a penetration-free state, via a global displacement minimizer.
Our method is very effective for handling penetration between two meshes, being
it an initial configuration or in the middle of physics simulation. The major
limitation is that it is not applicable to self-collision within one mesh at
the time being.
</summary>
    <author>
      <name>Juntao Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">originally 4 pages, submitted as Technical Brief to Siggraph Asia
  2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.2081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2235v3</id>
    <updated>2014-09-13T23:59:10Z</updated>
    <published>2014-09-08T08:19:21Z</published>
    <title>Tracing Analytic Ray Curves for Light and Sound Propagation in
  Non-linear Media</title>
    <summary>  The physical world consists of spatially varying media, such as the
atmosphere and the ocean, in which light and sound propagates along non-linear
trajectories. This presents a challenge to existing ray-tracing based methods,
which are widely adopted to simulate propagation due to their efficiency and
flexibility, but assume linear rays. We present a novel algorithm that traces
analytic ray curves computed from local media gradients, and utilizes the
closed-form solutions of both the intersections of the ray curves with planar
surfaces, and the travel distance. By constructing an adaptive unstructured
mesh, our algorithm is able to model general media profiles that vary in three
dimensions with complex boundaries consisting of terrains and other scene
objects such as buildings. We trace the analytic ray curves using the adaptive
unstructured mesh, which considerably improves the efficiency over prior
methods. We highlight the algorithm's application on simulation of sound and
visual propagation in outdoor scenes.
</summary>
    <author>
      <name>Qi Mo</name>
    </author>
    <author>
      <name>Hengchin Yeh</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2235v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2235v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7256v1</id>
    <updated>2014-09-09T10:39:46Z</updated>
    <published>2014-09-09T10:39:46Z</published>
    <title>Reactive Programming for Interactive Graphics</title>
    <summary>  One of the big challenges of developing interactive statistical applications
is the management of the data pipeline, which controls transformations from
data to plot. The user's interactions needs to be propagated through these
modules and reflected in the output representation at a fast pace. Each
individual module may be easy to develop and manage, but the dependency
structure can be quite challenging. The MVC (Model/View/Controller) pattern is
an attempt to solve the problem by separating the user's interaction from the
representation of the data. In this paper we discuss the paradigm of reactive
programming in the framework of the MVC architecture and show its applicability
to interactive graphics. Under this paradigm, developers benefit from the
separation of user interaction from the graphical representation, which makes
it easier for users and developers to extend interactive applications. We show
the central role of reactive data objects in an interactive graphics system,
implemented as the R package cranvas, which is freely available on GitHub and
the main developers include the authors of this paper.
</summary>
    <author>
      <name>Yihui Xie</name>
    </author>
    <author>
      <name>Heike Hofmann</name>
    </author>
    <author>
      <name>Xiaoyue Cheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS477</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS477" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/14-STS477 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2014, Vol. 29, No. 2, 201-213</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.7256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7724v1</id>
    <updated>2014-07-23T19:17:11Z</updated>
    <published>2014-07-23T19:17:11Z</published>
    <title>Using 3D Printing to Visualize Social Media Big Data</title>
    <summary>  Big data volume continues to grow at unprecedented rates. One of the key
features that makes big data valuable is the promise to find unknown patterns
or correlations that may be able to improve the quality of processes or
systems. Unfortunately, with the exponential growth in data, users often have
difficulty in visualizing the often-unstructured, non-homogeneous data coming
from a variety of sources. The recent growth in popularity of 3D printing has
ushered in a revolutionary way to interact with big data. Using a 3D printed
mockup up a physical or notional environment, one can display data on the
mockup to show real-time data patterns. In this poster and demonstration, we
describe the process of 3D printing and demonstrate an application of
displaying Twitter data on a 3D mockup of the Massachusetts Institute of
Technology (MIT) campus, known as LuminoCity.
</summary>
    <author>
      <name>Zachary Weber</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1130v1</id>
    <updated>2014-10-05T07:26:44Z</updated>
    <published>2014-10-05T07:26:44Z</published>
    <title>Real-time animation of human characters with fuzzy controllers</title>
    <summary>  The production of animation is a resource intensive process in game
companies. Therefore, techniques to synthesize animations have been developed.
However, these procedural techniques offer limited adaptability by animation
artists. In order to solve this, a fuzzy neural network model of the animation
is proposed, where the parameters can be tuned either by machine learning
techniques that use motion capture data as training data or by the animation
artist himself. This paper illustrates how this real time procedural animation
system can be developed, taking the human gait on flat terrain and inclined
surfaces as example. Currently, the parametric model is capable of synthesizing
animations for various limb sizes and step sizes.
</summary>
    <author>
      <name>Koen Samyn</name>
    </author>
    <author>
      <name>Sofie Van Hoecke</name>
    </author>
    <author>
      <name>Bart Pieters</name>
    </author>
    <author>
      <name>Charles Hollemeersch</name>
    </author>
    <author>
      <name>Aljosha Demeulemeester</name>
    </author>
    <author>
      <name>Rik van de Walle</name>
    </author>
    <link href="http://arxiv.org/abs/1410.1130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2259v1</id>
    <updated>2014-09-14T11:10:06Z</updated>
    <published>2014-09-14T11:10:06Z</published>
    <title>Image compression overview</title>
    <summary>  Compression plays a significant role in a data storage and a transmission. If
we speak about a generall data compression, it has to be a lossless one. It
means, we are able to recover the original data 1:1 from the compressed file.
Multimedia data (images, video, sound...), are a special case. In this area, we
can use something called a lossy compression. Our main goal is not to recover
data 1:1, but only keep them visually similar. This article is about an image
compression, so we will be interested only in image compression. For a human
eye, it is not a huge difference, if we recover RGB color with values
[150,140,138] instead of original [151,140,137]. The magnitude of a difference
determines the loss rate of the compression. The bigger difference usually
means a smaller file, but also worse image quality and noticable differences
from the original image. We want to cover compression techniques mainly from
the last decade. Many of them are variations of existing ones, only some of
them uses new principes.
</summary>
    <author>
      <name>Martin Prantl</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3018v1</id>
    <updated>2014-10-11T18:21:05Z</updated>
    <published>2014-10-11T18:21:05Z</published>
    <title>A mathematical design and evaluation of Bernstein-Bezier curves' shape
  features using the laws of technical aesthetics</title>
    <summary>  We present some notes on the definition of mathematical design as well as on
the methods of mathematical modeling which are used in the process of the
artistic design of the environment and its components. For the first time in
the field of geometric modeling, we perform an aesthetic analysis of planar
Bernstein-Bezier curves from the standpoint of the laws of technical
aesthetics. The shape features of the curve segments' geometry were evaluated
using the following criteria: conciseness-integrity, expressiveness,
proportional consistency, compositional balance, structural organization,
imagery, rationality, dynamism, scale, flexibility and harmony. In the
non-Russian literature, Bernstein-Bezier curves using a monotonic curvature
function (i.e., a class A Bezier curve) are considered to be fair (i.e.,
beautiful) curves, but their aesthetic analysis has never been performed. The
aesthetic analysis performed by the authors of this work means that this is no
longer the case. To confirm the conclusions of the authors' research, a survey
of the "aesthetic appropriateness" of certain Bernstein-Bezier curve segments
was conducted among 240 children, aged 14-17. The results of this survey have
shown themselves to be in full accordance with the authors' results.
</summary>
    <author>
      <name>Rifkat I. Nabiyev</name>
    </author>
    <author>
      <name>Rushan Ziatdinov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13187/md.2014.2.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13187/md.2014.2.6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Design &amp; Technical Aesthetics, 2014, Vol. 2, No. 1,
  pp. 6-13</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.3018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1906v1</id>
    <updated>2014-11-07T13:29:06Z</updated>
    <published>2014-11-07T13:29:06Z</published>
    <title>Footprint-Driven Locomotion Composition</title>
    <summary>  One of the most efficient ways of generating goal-directed walking motions is
synthesising the final motion based on footprints. Nevertheless, current
implementations have not examined the generation of continuous motion based on
footprints, where different behaviours can be generated automatically.
Therefore, in this paper a flexible approach for footprint-driven locomotion
composition is presented. The presented solution is based on the ability to
generate footprint-driven locomotion, with flexible features like jumping,
running, and stair stepping. In addition, the presented system examines the
ability of generating the desired motion of the character based on predefined
footprint patterns that determine which behaviour should be performed. Finally,
it is examined the generation of transition patterns based on the velocity of
the root and the number of footsteps required to achieve the target behaviour
smoothly and naturally.
</summary>
    <author>
      <name>Christos Mousas</name>
    </author>
    <author>
      <name>Paul Newbury</name>
    </author>
    <author>
      <name>Christos-Nikolaos Anagnostopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2014.4403</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2014.4403" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol.4, No.4, pp. 27-42, October 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.1906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3632v1</id>
    <updated>2014-11-13T17:50:51Z</updated>
    <published>2014-11-13T17:50:51Z</published>
    <title>Mesh2Fab: Reforming Shapes for Material-specific Fabrication</title>
    <summary>  As humans, we regularly associate shape of an object with its built material.
In the context of geometric modeling, however, this interrelation between form
and material is rarely explored. In this work, we propose a novel data-driven
reforming (i.e., reshaping) algorithm that adapts an input multi-component
model for a target fabrication material. The algorithm adapts both the part
geometry and the inter-part topology of the input shape to better align with
material specific fabrication requirements. As output, we produce the reshaped
model along with respective part dimensions and inter-part junction
specifications. We evaluate our algorithm on a range of man-made models and
demonstrate non-trivial model reshaping examples focusing only on metal and
wooden materials. We also appraise the output of our algorithm using a user
study.
</summary>
    <author>
      <name>Yong-Liang Yang</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/1411.3632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5993v1</id>
    <updated>2014-11-21T19:37:27Z</updated>
    <published>2014-11-21T19:37:27Z</published>
    <title>Reverse Engineering Point Clouds to Fit Tensor Product B-Spline Surfaces
  by Blending Local Fits</title>
    <summary>  Being able to reverse engineer from point cloud data to obtain 3D models is
important in modeling. As our main contribution, we present a new method to
obtain a tensor product B-spline representation from point cloud data by
fitting surfaces to appropriately segmented data. By blending multiple local
fits our method is more efficient than existing techniques, with the ability to
deal with more detail by efficiently introducing a high number of knots.
Further point cloud data obtained by digitizing 3D data, typically presents
many associated complications like noise and missing data. As our second
contribution, we propose an end-to-end framework for smoothing, hole filling,
parameterization, knot selection and B-spline fitting that addresses these
issues, works robustly with large irregularly shaped data containing holes and
is straightforward to implement.
</summary>
    <author>
      <name>Lavanya Sita Tekumalla</name>
    </author>
    <author>
      <name>Elaine Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Masters Thesis, Lavanya Sita Tekumalla, School of Computing,
  University of Utah</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.5993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3841v6</id>
    <updated>2015-10-18T20:50:07Z</updated>
    <published>2014-12-11T22:11:44Z</published>
    <title>Merging of Bézier curves with box constraints</title>
    <summary>  In this paper, we present a novel approach to the problem of merging of
B\'ezier curves with respect to the $L_2$-norm. We give illustrative examples
to show that the solution of the conventional merging problem may not be
suitable for further modification and applications. As in the case of the
degree reduction problem, we apply the so-called restricted area approach --
proposed recently in (P. Gospodarczyk, Computer-Aided Design 62 (2015),
143--151) -- to avoid certain defects and make the resulting curve more useful.
A method of solving the new problem is based on box-constrained quadratic
programming approach.
</summary>
    <author>
      <name>Przemysław Gospodarczyk</name>
    </author>
    <author>
      <name>Paweł Woźny</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cam.2015.10.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cam.2015.10.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational and Applied Mathematics 296, 265-274
  (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.3841v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3841v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4246v1</id>
    <updated>2014-12-13T15:24:38Z</updated>
    <published>2014-12-13T15:24:38Z</published>
    <title>A Canonical Representation of Data-Linear Visualization Algorithms</title>
    <summary>  We introduce linear-state dataflows, a canonical model for a large set of
visualization algorithms that we call data-linear visualizations. Our model
defines a fixed dataflow architecture: partitioning and subpartitioning of
input data, ordering, graphic primitives, and graphic attributes generation.
Local variables and accumulators are specific concepts that extend the
expressiveness of the dataflow to support features of visualization algorithms
that require state handling. We first show the flexibility of our model: it
enables the declarative construction of many common algorithms with just a few
mappings. Furthermore, the model enables easy mixing of visual mappings, such
as creating treemaps of histograms and 2D plots, plots of histograms...
Finally, we introduce our model in a more formal way and present some of its
important properties. We have implemented this model in a visualization
framework built around the concept of linear-state dataflows.
</summary>
    <author>
      <name>Thomas Baudel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, extended version of the original technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7780v1</id>
    <updated>2014-12-25T03:20:34Z</updated>
    <published>2014-12-25T03:20:34Z</published>
    <title>Interactive Visual Exploration of Halos in Large Scale Cosmology
  Simulation</title>
    <summary>  Halo is one of the most important basic elements in cosmology simulation,
which merges from small clumps to ever larger objects. The processes of the
birth and merging of the halos play a fundamental role in studying the
evolution of large scale cosmological structures. In this paper, a visual
analysis system is developed to interactively identify and explore the
evolution histories of thousands of halos. In this system, an intelligent
structure-aware selection method in What You See Is What You Get manner is
designed to efficiently define the interesting region in 3D space with 2D
hand-drawn lasso input. Then the exact information of halos within this 3D
region is identified by data mining in the merger tree files. To avoid visual
clutter, all the halos are projected in 2D space with a MDS method. Through the
linked view of 3D View and 2D graph, Users can interactively explore these
halos, including the tracing path and evolution history tree.
</summary>
    <author>
      <name>Guihua Shan</name>
    </author>
    <author>
      <name>Maojin Xie</name>
    </author>
    <author>
      <name>FengAn Li</name>
    </author>
    <author>
      <name>Yang Gao</name>
    </author>
    <author>
      <name>Xuebin Chi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12650-014-0206-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12650-014-0206-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9pages, 14figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Visualization 17(3):145-156(2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.7780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03032v3</id>
    <updated>2015-03-09T09:42:07Z</updated>
    <published>2015-01-13T15:13:35Z</published>
    <title>$G^{k,l}$-constrained multi-degree reduction of Bézier curves</title>
    <summary>  We present a new approach to the problem of $G^{k,l}$-constrained ($k,l \leq
3$) multi-degree reduction of B\'{e}zier curves with respect to the least
squares norm. First, to minimize the least squares error, we consider two
methods of determining the values of geometric continuity parameters. One of
them is based on quadratic and nonlinear programming, while the other uses some
simplifying assumptions and solves a system of linear equations. Next, for
prescribed values of these parameters, we obtain control points of the
multi-degree reduced curve, using the properties of constrained dual Bernstein
basis polynomials. Assuming that the input and output curves are of degree $n$
and $m$, respectively, we determine these points with the complexity $O(mn)$,
which is significantly less than the cost of other known methods. Finally, we
give several examples to demonstrate the effectiveness of our algorithms.
</summary>
    <author>
      <name>Przemysław Gospodarczyk</name>
    </author>
    <author>
      <name>Stanisław Lewanowicz</name>
    </author>
    <author>
      <name>Paweł Woźny</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11075-015-9988-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11075-015-9988-3" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Numerical Algorithms 71, 121-137 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.03032v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03032v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01954v1</id>
    <updated>2015-02-06T16:59:20Z</updated>
    <published>2015-02-06T16:59:20Z</published>
    <title>Interactive 3D Face Stylization Using Sculptural Abstraction</title>
    <summary>  Sculptors often deviate from geometric accuracy in order to enhance the
appearance of their sculpture. These subtle stylizations may emphasize anatomy,
draw the viewer's focus to characteristic features of the subject, or symbolize
textures that might not be accurately reproduced in a particular sculptural
medium, while still retaining fidelity to the unique proportions of an
individual. In this work we demonstrate an interactive system for enhancing
face geometry using a class of stylizations based on visual decomposition into
abstract semantic regions, which we call sculptural abstraction. We propose an
interactive two-scale optimization framework for stylization based on
sculptural abstraction, allowing real-time adjustment of both global and local
parameters. We demonstrate this system's effectiveness in enhancing physical 3D
prints of scans from various sources.
</summary>
    <author>
      <name>Jan Jachnik</name>
    </author>
    <author>
      <name>Dan B Goldman</name>
    </author>
    <author>
      <name>Linjie Luo</name>
    </author>
    <author>
      <name>Andrew J. Davison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 16 figures. For associated video and to download some of the
  results, see the project webpage at
  http://wp.doc.ic.ac.uk/robotvision/project/face-stylization/</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.01954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02139v1</id>
    <updated>2015-02-07T13:13:51Z</updated>
    <published>2015-02-07T13:13:51Z</published>
    <title>Marching Surfaces: Isosurface Approximation using G$^1$ Multi-Sided
  Surfaces</title>
    <summary>  Marching surfaces is a method for isosurface extraction and approximation
based on a $G^1$ multi-sided patch interpolation scheme. Given a 3D grid of
scalar values, an underlying curve network is formed using second order
information and cubic Hermite splines. Circular arc fitting defines the tangent
vectors for the Hermite curves at specified isovalues. Once the boundary curve
network is formed, a loop of curves is determined for each grid cell and then
interpolated with multi-sided surface patches, which are $G^1$ continuous at
the joins. The data economy of the method and its continuity preserving
properties provide an effective compression scheme, ideal for indirect volume
rendering on mobile devices, or collaborating on the Internet, while enhancing
visual fidelity. The use of multi-sided patches enables a more natural way to
approximate the isosurfaces than using a fixed number of sides or polygons as
is proposed in the literature. This assertion is supported with comparisons to
the traditional Marching Cubes algorithm and other $G^1$ methods.
</summary>
    <author>
      <name>Gustavo Chávez</name>
    </author>
    <author>
      <name>Alyn Rockwood</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02961v1</id>
    <updated>2015-02-10T16:03:37Z</updated>
    <published>2015-02-10T16:03:37Z</published>
    <title>Avatar-independent scripting for real-time gesture animation</title>
    <summary>  When animation of a humanoid figure is to be generated at run-time, instead
of by replaying pre-composed motion clips, some method is required of
specifying the avatar's movements in a form from which the required motion data
can be automatically generated. This form must be of a more abstract nature
than raw motion data: ideally, it should be independent of the particular
avatar's proportions, and both writable by hand and suitable for automatic
generation from higher-level descriptions of the required actions.
  We describe here the development and implementation of such a scripting
language for the particular area of sign languages of the deaf, called SiGML
(Signing Gesture Markup Language), based on the existing HamNoSys notation for
sign languages.
  We conclude by suggesting how this work may be extended to more general
animation for interactive virtual reality applications.
</summary>
    <author>
      <name>Richard Kennaway</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 12 figures. Last revised November 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06419v1</id>
    <updated>2015-02-14T11:29:26Z</updated>
    <published>2015-02-14T11:29:26Z</published>
    <title>Analysis of Design Principles and Requirements for Procedural Rigging of
  Bipeds and Quadrupeds Characters with Custom Manipulators for Animation</title>
    <summary>  Character rigging is a process of endowing a character with a set of custom
manipulators and controls making it easy to animate by the animators. These
controls consist of simple joints, handles, or even separate character
selection windows.This research paper present an automated rigging system for
quadruped characters with custom controls and manipulators for animation.The
full character rigging mechanism is procedurally driven based on various
principles and requirements used by the riggers and animators. The automation
is achieved initially by creating widgets according to the character type.
These widgets then can be customized by the rigger according to the character
shape, height and proportion. Then joint locations for each body parts are
calculated and widgets are replaced programmatically.Finally a complete and
fully operational procedurally generated character control rig is created and
attached with the underlying skeletal joints. The functionality and feasibility
of the rig was analyzed from various source of actual character motion and a
requirements criterion was met. The final rigged character provides an
efficient and easy to manipulate control rig with no lagging and at high frame
rate.
</summary>
    <author>
      <name>Zeeshan Bhatti</name>
    </author>
    <author>
      <name>Asadullah Shah</name>
    </author>
    <author>
      <name>Ahmad Waqas</name>
    </author>
    <author>
      <name>Nadeem Mahmood</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2015.5104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2015.5104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 24 figures, 4 Algorithms, Journal Paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol.5, No.1, January 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.06419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06686v1</id>
    <updated>2015-02-24T04:30:43Z</updated>
    <published>2015-02-24T04:30:43Z</published>
    <title>Data-Driven Shape Analysis and Processing</title>
    <summary>  Data-driven methods play an increasingly important role in discovering
geometric, structural, and semantic relationships between 3D shapes in
collections, and applying this analysis to support intelligent modeling,
editing, and visualization of geometric data. In contrast to traditional
approaches, a key feature of data-driven approaches is that they aggregate
information from a collection of shapes to improve the analysis and processing
of individual shapes. In addition, they are able to learn models that reason
about properties and relationships of shapes without relying on hard-coded
rules or explicitly programmed instructions. We provide an overview of the main
concepts and components of these techniques, and discuss their application to
shape classification, segmentation, matching, reconstruction, modeling and
exploration, as well as scene analysis and synthesis, through reviewing the
literature and relating the existing works with both qualitative and numerical
comparisons. We conclude our report with ideas that can inspire future research
in data-driven shape analysis and processing.
</summary>
    <author>
      <name>Kai Xu</name>
    </author>
    <author>
      <name>Vladimir G. Kim</name>
    </author>
    <author>
      <name>Qixing Huang</name>
    </author>
    <author>
      <name>Evangelos Kalogerakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02687v1</id>
    <updated>2015-04-10T14:13:39Z</updated>
    <published>2015-04-10T14:13:39Z</published>
    <title>3D Density Histograms for Criteria-driven Edge Bundling</title>
    <summary>  This paper presents a graph bundling algorithm that agglomerates edges taking
into account both spatial proximity as well as user-defined criteria in order
to reveal patterns that were not perceivable with previous bundling techniques.
Each edge belongs to a group that may either be an input of the problem or
found by clustering one or more edge properties such as origin, destination,
orientation, length or domain-specific properties. Bundling is driven by a
stack of density maps, with each map capturing both the edge density of a given
group as well as interactions with edges from other groups. Density maps are
efficiently calculated by smoothing 2D histograms of edge occurrence using
repeated averaging filters based on integral images.
  A CPU implementation of the algorithm is tested on several graphs, and
different grouping criteria are used to illustrate how the proposed technique
can render different visualizations of the same data. Bundling performance is
much higher than on previous approaches, being particularly noticeable on large
graphs, with millions of edges being bundled in seconds.
</summary>
    <author>
      <name>Daniel C. Moura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a conference (under review)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04565v1</id>
    <updated>2015-04-17T16:45:35Z</updated>
    <published>2015-04-17T16:45:35Z</published>
    <title>Real-time correction of panoramic images using hyperbolic Möbius
  transformations</title>
    <summary>  Wide-angle images gained a huge popularity in the last years due to the
development of computational photography and imaging technological advances.
They present the information of a scene in a way which is more natural for the
human eye but, on the other hand, they introduce artifacts such as bent lines.
These artifacts become more and more unnatural as the field of view increases.
  In this work, we present a technique aimed to improve the perceptual quality
of panorama visualization. The main ingredients of our approach are, on one
hand, considering the viewing sphere as a Riemann sphere, what makes natural
the application of M\"obius (complex) transformations to the input image, and,
on the other hand, a projection scheme which changes in function of the field
of view used.
  We also introduce an implementation of our method, compare it against images
produced with other methods and show that the transformations can be done in
real-time, which makes our technique very appealing for new settings, as well
as for existing interactive panorama applications.
</summary>
    <author>
      <name>Luis Peñaranda</name>
    </author>
    <author>
      <name>Luiz Velho</name>
    </author>
    <author>
      <name>Leonardo Sacht</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11554-015-0502-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11554-015-0502-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 13 figures, 1 table, 1 algorithm. In "Real-time Image
  Processing"</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.04565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00073v1</id>
    <updated>2015-05-01T02:47:12Z</updated>
    <published>2015-05-01T02:47:12Z</published>
    <title>Bijective Deformations in $\mathbb{R}^n$ via Integral Curve Coordinates</title>
    <summary>  We introduce Integral Curve Coordinates, which identify each point in a
bounded domain with a parameter along an integral curve of the gradient of a
function $f$ on that domain; suitable functions have exactly one critical
point, a maximum, in the domain, and the gradient of the function on the
boundary points inward. Because every integral curve intersects the boundary
exactly once, Integral Curve Coordinates provide a natural bijective mapping
from one domain to another given a bijection of the boundary. Our approach can
be applied to shapes in any dimension, provided that the boundary of the shape
(or cage) is topologically equivalent to an $n$-sphere. We present a simple
algorithm for generating a suitable function space for $f$ in any dimension. We
demonstrate our approach in 2D and describe a practical (simple and robust)
algorithm for tracing integral curves on a (piecewise-linear) triangulated
regular grid.
</summary>
    <author>
      <name>Lisa Huynh</name>
    </author>
    <author>
      <name>Yotam Gingold</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37E30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01214v1</id>
    <updated>2015-05-05T22:59:32Z</updated>
    <published>2015-05-05T22:59:32Z</published>
    <title>Learning Style Similarity for Searching Infographics</title>
    <summary>  Infographics are complex graphic designs integrating text, images, charts and
sketches. Despite the increasing popularity of infographics and the rapid
growth of online design portfolios, little research investigates how we can
take advantage of these design resources. In this paper we present a method for
measuring the style similarity between infographics. Based on human perception
data collected from crowdsourced experiments, we use computer vision and
machine learning algorithms to learn a style similarity metric for infographic
designs. We evaluate different visual features and learning algorithms and find
that a combination of color histograms and Histograms-of-Gradients (HoG)
features is most effective in characterizing the style of infographics. We
demonstrate our similarity metric on a preliminary image retrieval test.
</summary>
    <author>
      <name>Babak Saleh</name>
    </author>
    <author>
      <name>Mira Dontcheva</name>
    </author>
    <author>
      <name>Aaron Hertzmann</name>
    </author>
    <author>
      <name>Zhicheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, to appear in the 41st annual conference on Graphics
  Interface (GI) 2015,</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03615v1</id>
    <updated>2015-05-14T04:08:53Z</updated>
    <published>2015-05-14T04:08:53Z</published>
    <title>A Connectivity-Aware Multi-level Finite-Element System for Solving
  Laplace-Beltrami Equations</title>
    <summary>  Recent work on octree-based finite-element systems has developed a multigrid
solver for Poisson equations on meshes. While the idea of defining a regularly
indexed function space has been successfully used in a number of applications,
it has also been noted that the richness of the function space is limited
because the function values can be coupled across locally disconnected regions.
In this work, we show how to enrich the function space by introducing functions
that resolve the coupling while still preserving the nesting hierarchy that
supports multigrid. A spectral analysis reveals the superior quality of the
resulting Laplace-Beltrami operator and applications to surface flow
demonstrate that our new solver more efficiently converges to the correct
solution.
</summary>
    <author>
      <name>Ming Chuang</name>
    </author>
    <author>
      <name>Michael Kazhdan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was done when the first author was a PhD student at Johns
  Hopkins University</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03977v2</id>
    <updated>2016-01-27T16:50:48Z</updated>
    <published>2015-05-15T07:38:46Z</published>
    <title>A wide diversity of 3D surfaces Generator using a new implicit function</title>
    <summary>  We present in this paper a new family of implicit function for synthesizing a
wide variety of 3D surfaces. The basis of this family consists of the usual
functions that are: the function rectangular pulses, the function saw-tooth
pulses, the function of triangular pulses, the staircase function and the power
function. By combining these common functions, named constituent functions, in
one implicit function and by varying some parameters of this function we can
synthesize a wide variety of 3D surfaces with the possibility to set their
deformations.
</summary>
    <author>
      <name>Jelloul Elmesbahi</name>
    </author>
    <author>
      <name>Ahmed Errami</name>
    </author>
    <author>
      <name>Mohammed Khaldoun</name>
    </author>
    <author>
      <name>Omar Bouattane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This second submission replaces the first following submission at:
  arXiv:1505.03977 It completes this later by adding a second model to complete
  the first one. It corresponds to particular surfaces (Lego cubes) and the
  other remained results (about 500 3D surfaces). This is to avoid huge file
  size for our submission and to proof the effectiveness of our model</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03977v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03977v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06022v1</id>
    <updated>2015-05-22T10:41:45Z</updated>
    <published>2015-05-22T10:41:45Z</published>
    <title>Implementing a Photorealistic Rendering System using GLSL</title>
    <summary>  Ray tracing on GPUs is becoming quite common these days. There are many
publicly available documents on how to implement basic ray tracing on GPUs for
spheres and implicit surfaces. We even have some general frameworks for ray
tracing on GPUs. We however hardly find details on how to implement more
complex ray tracing algorithms themselves that are commonly used for
photorealistic rendering. This paper explains an implementation of a
stand-alone rendering system on GPUs which supports the bounding volume
hierarchy and stochastic progressive photon mapping. The key characteristic of
the system is that it uses only GLSL shaders without relying on any platform
dependent feature. The system can thus run on many platforms that support
OpenGL, making photorealistic rendering on GPUs widely accessible. This paper
also sketches practical ideas for stackless traversal and pseudorandom number
generation which both fit well with the limited system configuration.
</summary>
    <author>
      <name>Toshiya Hachisuka</name>
    </author>
    <link href="http://arxiv.org/abs/1505.06022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07079v2</id>
    <updated>2016-05-13T12:25:30Z</updated>
    <published>2015-05-26T19:03:54Z</published>
    <title>A survey on Information Visualization in light of Vision and Cognitive
  sciences</title>
    <summary>  Information Visualization techniques are built on a context with many factors
related to both vision and cognition, making it difficult to draw a clear
picture of how data visually turns into comprehension. In the intent of
promoting a better picture, here, we survey concepts on vision, cognition, and
Information Visualization organized in a theorization named Visual Expression
Process. Our theorization organizes the basis of visualization techniques with
a reduced level of complexity; still, it is complete enough to foster
discussions related to design and analytical tasks. Our work introduces the
following contributions: (1) a Theoretical compilation of vision, cognition,
and Information Visualization; (2) Discussions supported by vast literature;
and (3) Reflections on visual-cognitive aspects concerning use and design. We
expect our contributions will provide further clarification about how users and
designers think about InfoVis, leveraging the potential of systems and
techniques.
</summary>
    <author>
      <name>Jose Rodrigues-Jr</name>
    </author>
    <author>
      <name>Luciana Zaina</name>
    </author>
    <author>
      <name>Maria Oliveira</name>
    </author>
    <author>
      <name>Bruno Brandoli</name>
    </author>
    <author>
      <name>Agma Traina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, Elsevier Journal preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.07079v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07079v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07804v1</id>
    <updated>2015-05-28T19:14:44Z</updated>
    <published>2015-05-28T19:14:44Z</published>
    <title>The Spatial-Perceptual Design Space: a new comprehension for Data
  Visualization</title>
    <summary>  We revisit the design space of visualizations aiming at identifying and
relating its components. In this sense, we establish a model to examine the
process through which visualizations become expressive for users. This model
has leaded us to a taxonomy oriented to the human visual perception, a
conceptualization that provides natural criteria in order to delineate a novel
understanding for the visualization design space. The new organization of
concepts that we introduce is our main contribution: a grammar for the
visualization design based on the review of former works and of classical and
state-of-the-art techniques. Like so, the paper is presented as a survey whose
structure introduces a new conceptualization for the space of techniques
concerning visual analysis.
</summary>
    <author>
      <name>Jose F. Rodrigues Jr</name>
    </author>
    <author>
      <name>Agma J. M. Traina</name>
    </author>
    <author>
      <name>Maria C. F. Oliveira</name>
    </author>
    <author>
      <name>Caetano Traina Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1057/palgrave.ivs.9500161</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1057/palgrave.ivs.9500161" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Visualization 6: 4. 261-279 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.07804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00967v2</id>
    <updated>2016-01-06T11:07:51Z</updated>
    <published>2015-06-02T17:35:47Z</published>
    <title>Geometric elements and classification of quadrics in rational Bézier
  form</title>
    <summary>  In this paper we classify and derive closed formulas for geometric elements
of quadrics in rational B\'ezier triangular form (such as the center, the conic
at infinity, the vertex and the axis of paraboloids and the principal planes),
using just the control vertices and the weights for the quadric patch. The
results are extended also to quadric tensor product patches. Our results rely
on using techniques from projective algebraic geometry to find suitable
bilinear forms for the quadric in a coordinate-free fashion, considering a
pencil of quadrics that are tangent to the given quadric along a conic. Most of
the information about the quadric is encoded in one coefficient, involving the
weights of the patch, which allows us to tell apart oval from ruled quadrics.
This coefficient is also relevant to determine the affine type of the quadric.
Spheres and quadrics of revolution are characterised within this framework.
</summary>
    <author>
      <name>A. Cantón</name>
    </author>
    <author>
      <name>L. Fernández-Jambrina</name>
    </author>
    <author>
      <name>M. E. Rosado María</name>
    </author>
    <author>
      <name>M. J. Vázquez-Gallo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cam.2016.01.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cam.2016.01.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures. Minor changes from previous version. To appear
  in Journal of Computational and Applied Mathematics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational and Applied Mathematics 300, 400-419
  (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.00967v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00967v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02079v1</id>
    <updated>2015-06-05T22:35:31Z</updated>
    <published>2015-06-05T22:35:31Z</published>
    <title>Gradient-Domain Fusion for Color Correction in Large EM Image Stacks</title>
    <summary>  We propose a new gradient-domain technique for processing registered EM image
stacks to remove inter-image discontinuities while preserving intra-image
detail. To this end, we process the image stack by first performing anisotropic
smoothing along the slice axis and then solving a Poisson equation within each
slice to re-introduce the detail. The final image stack is continuous across
the slice axis and maintains sharp details within each slice. Adapting existing
out-of-core techniques for solving the linear system, we describe a parallel
algorithm with time complexity that is linear in the size of the data and space
complexity that is sub-linear, allowing us to process datasets as large as five
teravoxels with a 600 MB memory footprint.
</summary>
    <author>
      <name>Michael Kazhdan</name>
    </author>
    <author>
      <name>Kunal Lillaney</name>
    </author>
    <author>
      <name>William Roncal</name>
    </author>
    <author>
      <name>Davi Bock</name>
    </author>
    <author>
      <name>Joshua Vogelstein</name>
    </author>
    <author>
      <name>Randal Burns</name>
    </author>
    <link href="http://arxiv.org/abs/1506.02079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02400v2</id>
    <updated>2016-01-15T15:09:27Z</updated>
    <published>2015-06-08T08:47:52Z</published>
    <title>Pushing the Limits of 3D Color Printing: Error Diffusion with
  Translucent Materials</title>
    <summary>  Accurate color reproduction is important in many applications of 3D printing,
from design prototypes to 3D color copies or portraits. Although full color is
available via other technologies, multi-jet printers have greater potential for
graphical 3D printing, in terms of reproducing complex appearance properties.
However, to date these printers cannot produce full color, and doing so poses
substantial technical challenges, from the shear amount of data to the
translucency of the available color materials. In this paper, we propose an
error diffusion halftoning approach to achieve full color with multi-jet
printers, which operates on multiple isosurfaces or layers within the object.
We propose a novel traversal algorithm for voxel surfaces, which allows the
transfer of existing error diffusion algorithms from 2D printing. The resulting
prints faithfully reproduce colors, color gradients and fine-scale details.
</summary>
    <author>
      <name>Alan Brunton</name>
    </author>
    <author>
      <name>Can Ates Arikan</name>
    </author>
    <author>
      <name>Philipp Urban</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2832905</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2832905" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 14 figures; includes supplemental figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics, 35(1), Article 4, December 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.02400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02976v1</id>
    <updated>2015-06-09T16:08:19Z</updated>
    <published>2015-06-09T16:08:19Z</published>
    <title>Reviewing Data Visualization: an Analytical Taxonomical Study</title>
    <summary>  This paper presents an analytical taxonomy that can suitably describe, rather
than simply classify, techniques for data presentation. Unlike previous works,
we do not consider particular aspects of visualization techniques, but their
mechanisms and foundational vision perception. Instead of just adjusting
visualization research to a classification system, our aim is to better
understand its process. For doing so, we depart from elementary concepts to
reach a model that can describe how visualization techniques work and how they
convey meaning.
</summary>
    <author>
      <name>Jose F. Rodrigues Jr.</name>
    </author>
    <author>
      <name>Agma J. M. Traina</name>
    </author>
    <author>
      <name>Maria Cristina F. de Oliveira</name>
    </author>
    <author>
      <name>Caetano Traina Jr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Proceedings of the Information Visualization
  Conference as Jose Rodrigues, Agma Traina, Maria Oliveira, Caetano Traina,
  Reviewing Data Visualization: an Analytical Taxonomical Study In: 10th
  International Conference on Information Visualization, 2006, 713-720 IEEE
  Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04480v2</id>
    <updated>2015-10-29T07:27:16Z</updated>
    <published>2015-06-15T05:29:06Z</published>
    <title>A Clustering Based Approach for Realistic and Efficient Data-Driven
  Crowd Simulation</title>
    <summary>  In this paper, we present a data-driven approach to generate realistic
steering behaviors for virtual crowds in crowd simulation. We take advantage of
both rule-based models and data-driven models by applying the interaction
patterns discovered from crowd videos. Unlike existing example-based models in
which current states are matched to states extracted from crowd videos
directly, our approach adopts a hierarchical mechanism to generate the steering
behaviors of agents. First, each agent is classified into one of the
interaction patterns that are automatically discovered from crowd video before
simulation. Then the most matched action is selected from the associated
interaction pattern to generate the steering behaviors of the agent. By doing
so, agents can avoid performing a simple state matching as in the traditional
example-based approaches, and can perform a wider variety of steering behaviors
as well as mimic the cognitive process of pedestrians. Simulation results on
scenarios with different crowd densities and main motion directions demonstrate
that our approach performs better than two state-of-the-art simulation models,
in terms of prediction accuracy. Besides, our approach is efficient enough to
run at interactive rates in real time simulation.
</summary>
    <author>
      <name>Mingbi Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial error in
  equation 6</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06636v2</id>
    <updated>2015-06-23T12:11:32Z</updated>
    <published>2015-06-22T14:46:58Z</published>
    <title>3D Geometric Analysis of Tubular Objects based on Surface Normal
  Accumulation</title>
    <summary>  This paper proposes a simple and efficient method for the reconstruction and
extraction of geometric parameters from 3D tubular objects. Our method
constructs an image that accumulates surface normal information, then peaks
within this image are located by tracking. Finally, the positions of these are
optimized to lie precisely on the tubular shape centerline. This method is very
versatile, and is able to process various input data types like full or partial
mesh acquired from 3D laser scans, 3D height map or discrete volumetric images.
The proposed algorithm is simple to implement, contains few parameters and can
be computed in linear time with respect to the number of surface faces. Since
the extracted tube centerline is accurate, we are able to decompose the tube
into rectilinear parts and torus-like parts. This is done with a new linear
time 3D torus detection algorithm, which follows the same principle of a
previous work on 2D arc circle recognition. Detailed experiments show the
versatility, accuracy and robustness of our new method.
</summary>
    <author>
      <name>Bertrand Kerautret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Adrien Krähenbühl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Isabelle Debled-Rennesson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Jacques-Olivier Lachaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 18th International Conference on Image Analysis and Processing,
  Sep 2015, Genova, Italy. 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06636v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06636v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06855v1</id>
    <updated>2015-06-23T04:40:30Z</updated>
    <published>2015-06-23T04:40:30Z</published>
    <title>Modeling and Correspondence of Topologically Complex 3D Shapes</title>
    <summary>  3D shape creation and modeling remains a challenging task especially for
novice users. Many methods in the field of computer graphics have been proposed
to automate the often repetitive and precise operations needed during the
modeling of detailed shapes. This report surveys different approaches of shape
modeling and correspondence especially for shapes exhibiting topological
complexity. We focus on methods designed to help generate or process shapes
with large number of interconnected components often found in man-made shapes.
We first discuss a variety of modeling techniques, that leverage existing
shapes, in easy to use creative modeling systems. We then discuss possible
correspondence strategies for topologically different shapes as it is a
requirement for such systems. Finally, we look at different shape
representations and tools that facilitate the modification of shape topology
and we focus on those particularly useful in free-form 3D modeling.
</summary>
    <author>
      <name>Ibraheem Alhashim</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08459v1</id>
    <updated>2015-06-28T22:08:10Z</updated>
    <published>2015-06-28T22:08:10Z</published>
    <title>On the Approximation Theory of Linear Variational Subspace Design</title>
    <summary>  Solving large-scale optimization on-the-fly is often a difficult task for
real-time computer graphics applications. To tackle this challenge, model
reduction is a well-adopted technique. Despite its usefulness, model reduction
often requires a handcrafted subspace that spans a domain that hypothetically
embodies desirable solutions. For many applications, obtaining such subspaces
case-by-case either is impossible or requires extensive human labors, hence
does not readily have a scalable solution for growing number of tasks. We
propose linear variational subspace design for large-scale constrained
quadratic programming, which can be computed automatically without any human
interventions. We provide meaningful approximation error bound that
substantiates the quality of calculated subspace, and demonstrate its empirical
success in interactive deformable modeling for triangular and tetrahedral
meshes.
</summary>
    <author>
      <name>Jianbo Ye</name>
    </author>
    <author>
      <name>Zhixin Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08956v2</id>
    <updated>2015-07-16T23:30:45Z</updated>
    <published>2015-06-30T06:39:19Z</published>
    <title>Lens Factory: Automatic Lens Generation Using Off-the-shelf Components</title>
    <summary>  Custom optics is a necessity for many imaging applications. Unfortunately,
custom lens design is costly (thousands to tens of thousands of dollars), time
consuming (10-12 weeks typical lead time), and requires specialized optics
design expertise. By using only inexpensive, off-the-shelf lens components the
Lens Factory automatic design system greatly reduces cost and time. Design,
ordering of parts, delivery, and assembly can be completed in a few days, at a
cost in the low hundreds of dollars. Lens design constraints, such as focal
length and field of view, are specified in terms familiar to the graphics
community so no optics expertise is necessary. Unlike conventional lens design
systems, which only use continuous optimization methods, Lens Factory adds a
discrete optimization stage. This stage searches the combinatorial space of
possible combinations of lens elements to find novel designs, evolving simple
canonical lens designs into more complex, better designs. Intelligent pruning
rules make the combinatorial search feasible. We have designed and built
several high performance optical systems which demonstrate the practicality of
the system.
</summary>
    <author>
      <name>Libin Sun</name>
    </author>
    <author>
      <name>Brian Guenter</name>
    </author>
    <author>
      <name>Neel Joshi</name>
    </author>
    <author>
      <name>Patrick Therien</name>
    </author>
    <author>
      <name>James Hays</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02766v1</id>
    <updated>2015-07-10T02:24:41Z</updated>
    <published>2015-07-10T02:24:41Z</published>
    <title>A Hybrid Graph-drawing Algorithm for Large, Naturally-clustered,
  Disconnected Graphs</title>
    <summary>  In this paper, we present a hybrid graph-drawing algorithm (GDA) for
layouting large, naturally-clustered, disconnected graphs. We called it a
hybrid algorithm because it is an implementation of a series of already known
graph-drawing and graph-theoretic procedures. We remedy in this hybrid the
problematic nature of the current force-based GDA which has the inability to
scale to large, naturally-clustered, and disconnected graphs. These kinds of
graph usually model the complex inter-relationships among entities in social,
biological, natural, and artificial networks. Obviously, the hybrid runs longer
than the current GDAs. By using two extreme cases of graphs as inputs, we
present in this paper the derivation of the time complexity of the hybrid which
we found to be $O(|\V|^3)$.
</summary>
    <author>
      <name>Toni-Jan Keith P. Monserrat</name>
    </author>
    <author>
      <name>Jaderick P. Pabico</name>
    </author>
    <author>
      <name>Eliezer A. Albacea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures, originally appeared in H.N. Adorna and J.M.
  Samaniego (eds.) Proceedings of the 5th National Symposium on Mathematical
  Aspects of Computer Science (SMACS 2010), University of the Philippines Los
  Ba\~nos, College, Laguna, 3-4 December 2010, pp. 32-38</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Asia Pacific Journal of Multidisciplinary Research 2(4):119-126</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.02766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02800v1</id>
    <updated>2015-07-10T08:12:22Z</updated>
    <published>2015-07-10T08:12:22Z</published>
    <title>Meshfree C^2-Weighting for Shape Deformation</title>
    <summary>  Handle-driven deformation based on linear blending is widely used in many
applications because of its merits in intuitiveness, efficiency and easiness of
implementation. We provide a meshfree method to compute the smooth weights of
linear blending for shape deformation. The C2-continuity of weighting is
guaranteed by the carefully formulated basis functions, with which the
computation of weights is in a closed-form. Criteria to ensure the quality of
deformation are preserved by the basis functions after decomposing the shape
domain according to the Voronoi diagram of handles. The cost of inserting a new
handle is only the time to evaluate the distances from the new handle to all
sample points in the space of deformation. Moreover, a virtual handle insertion
algorithm has been developed to allow users freely placing handles while
preserving the criteria on weights. Experimental examples for real-time 2D/3D
deformations are shown to demonstrate the effectiveness of this method.
</summary>
    <author>
      <name>Chuhua Xian</name>
    </author>
    <author>
      <name>Shuo Jin</name>
    </author>
    <author>
      <name>Charlie C. L. Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.02800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02860v1</id>
    <updated>2015-07-10T11:41:04Z</updated>
    <published>2015-07-10T11:41:04Z</published>
    <title>A Closed-Form Formulation of HRBF-Based Surface Reconstruction</title>
    <summary>  The Hermite radial basis functions (HRBFs) implicits have been used to
reconstruct surfaces from scattered Hermite data points. In this work, we
propose a closed-form formulation to construct HRBF-based implicits by a
quasi-solution approximating the exact solution. A scheme is developed to
automatically adjust the support sizes of basis functions to hold the error
bound of a quasi-solution. Our method can generate an implicit function from
positions and normals of scattered points without taking any global operation.
Working together with an adaptive sampling algorithm, the HRBF-based implicits
can also reconstruct surfaces from point clouds with non-uniformity and noises.
Robust and efficient reconstruction has been observed in our experimental tests
on real data captured from a variety of scenes.
</summary>
    <author>
      <name>Shengjun Liu</name>
    </author>
    <author>
      <name>Charlie C. L. Wang</name>
    </author>
    <author>
      <name>Guido Brunnett</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages with 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.02860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03351v1</id>
    <updated>2015-07-13T08:07:19Z</updated>
    <published>2015-07-13T08:07:19Z</published>
    <title>On Smooth 3D Frame Field Design</title>
    <summary>  We analyze actual methods that generate smooth frame fields both in 2D and in
3D. We formalize the 2D problem by representing frames as functions (as it was
done in 3D), and show that the derived optimization problem is the one that
previous work obtain via "representation vectors." We show (in 2D) why this non
linear optimization problem is easier to solve than directly minimizing the
rotation angle of the field, and observe that the 2D algorithm is able to find
good fields.
  Now, the 2D and the 3D optimization problems are derived from the same
formulation (based on representing frames by functions). Their energies share
some similarities from an optimization point of view (smoothness, local minima,
bounds of partial derivatives, etc.), so we applied the 2D resolution mechanism
to the 3D problem. Our evaluation of all existing 3D methods suggests to
initialize the field by this new algorithm, but possibly use another method for
further smoothing.
</summary>
    <author>
      <name>Nicolas Ray</name>
    </author>
    <author>
      <name>Dmitry Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/1507.03351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04110v4</id>
    <updated>2015-11-23T10:30:39Z</updated>
    <published>2015-07-15T07:57:26Z</published>
    <title>A de Casteljau Algorithm for Bernstein type Polynomials based on
  (p,q)-integers</title>
    <summary>  In this paper, a de Casteljau algorithm to compute (p,q)-Bernstein Bezier
curves based on (p,q)-integers is introduced. We study the nature of degree
elevation and degree reduction for (p,q)-Bezier Bernstein functions. The new
curves have some properties similar to q-Bezier curves. Moreover, we construct
the corresponding tensor product surfaces over the rectangular domain (u, v)
\in [0, 1] \times [0, 1] depending on four parameters. We also study the de
Casteljau algorithm and degree evaluation properties of the surfaces for these
generalization over the rectangular domain. Furthermore, some fundamental
properties for (p,q)-Bernstein Bezier curves are discussed. We get q-Bezier
curves and surfaces for (u, v) \in [0, 1] \times [0, 1] when we set the
parameter p1 = p2 = 1.
</summary>
    <author>
      <name>Khalid Khan</name>
    </author>
    <author>
      <name>D. K. Lobiyal</name>
    </author>
    <author>
      <name>Adem Kilicman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures, basis function revised. arXiv admin note:
  substantial text overlap with arXiv:1505.01810</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04110v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04110v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 41A10, 41A25, 41A36" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04571v3</id>
    <updated>2015-11-10T08:36:25Z</updated>
    <published>2015-07-16T13:45:36Z</published>
    <title>GPU-based visualization of domain-coloured algebraic Riemann surfaces</title>
    <summary>  We examine an algorithm for the visualization of domain-coloured Riemann
surfaces of plane algebraic curves. The approach faithfully reproduces the
topology and the holomorphic structure of the Riemann surface. We discuss how
the algorithm can be implemented efficiently in OpenGL with geometry shaders,
and (less efficiently) even in WebGL with multiple render targets and floating
point textures. While the generation of the surface takes noticeable time in
both implementations, the visualization of a cached Riemann surface mesh is
possible with interactive performance. This allows us to visually explore
otherwise almost unimaginable mathematical objects. As examples, we look at the
complex square root and the folium of Descartes. For the folium of Descartes,
the visualization reveals features of the algebraic curve which are not obvious
from its equation.
</summary>
    <author>
      <name>Stefan Kranich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revised version; 21 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04571v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04571v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02826v1</id>
    <updated>2015-08-12T07:02:19Z</updated>
    <published>2015-08-12T07:02:19Z</published>
    <title>Inappropriate use of L-BFGS, Illustrated on frame field design</title>
    <summary>  L-BFGS is a hill climbing method that is guarantied to converge only for
convex problems. In computer graphics, it is often used as a black box solver
for a more general class of non linear problems, including problems having many
local minima. Some works obtain very nice results by solving such difficult
problems with L-BFGS. Surprisingly, the method is able to escape local minima:
our interpretation is that the approximation of the Hessian is smoother than
the real Hessian, making it possible to evade the local minima. We analyse the
behavior of L-BFGS on the design of 2D frame fields. It involves an energy
function that is infinitly continuous, strongly non linear and having many
local minima. Moreover, the local minima have a clear visual interpretation:
they corresponds to differents frame field topologies. We observe that the
performances of LBFGS are almost unpredictables: they are very competitive when
the field is sampled on the primal graph, but really poor when they are sampled
on the dual graph.
</summary>
    <author>
      <name>Nicolas Ray</name>
    </author>
    <author>
      <name>Dmitry Sokolov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03590v2</id>
    <updated>2015-12-07T09:12:02Z</updated>
    <published>2015-05-04T12:17:09Z</published>
    <title>Light-field Microscopy with a Consumer Light-field Camera</title>
    <summary>  We explore the use of inexpensive consumer light- field camera technology for
the purpose of light-field mi- croscopy. Our experiments are based on the Lytro
(first gen- eration) camera. Unfortunately, the optical systems of the Lytro
and those of microscopes are not compatible, lead- ing to a loss of light-field
information due to angular and spatial vignetting when directly recording
microscopic pic- tures. We therefore consider an adaptation of the Lytro op-
tical system. We demonstrate that using the Lytro directly as an oc- ular
replacement, leads to unacceptable spatial vignetting. However, we also found a
setting that allows the use of the Lytro camera in a virtual imaging mode which
prevents the information loss to a large extent. We analyze the new vir- tual
imaging mode and use it in two different setups for im- plementing light-field
microscopy using a Lytro camera. As a practical result, we show that the camera
can be used for low magnification work, as e.g. common in quality control,
surface characterization, etc. We achieve a maximum spa- tial resolution of
about 6.25{\mu}m, albeit at a limited SNR for the side views.
</summary>
    <author>
      <name>Lois Mignard-Debise</name>
    </author>
    <author>
      <name>Ivo Ihrke</name>
    </author>
    <link href="http://arxiv.org/abs/1508.03590v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03590v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06181v1</id>
    <updated>2015-08-25T15:01:47Z</updated>
    <published>2015-08-25T15:01:47Z</published>
    <title>PolyDepth: Real-time Penetration Depth Computation using Iterative
  Contact-Space Projection</title>
    <summary>  We present a real-time algorithm that finds the Penetration Depth (PD)
between general polygonal models based on iterative and local optimization
techniques. Given an in-collision configuration of an object in configuration
space, we find an initial collision-free configuration using several methods
such as centroid difference, maximally clear configuration, motion coherence,
random configuration, and sampling-based search. We project this configuration
on to a local contact space using a variant of continuous collision detection
algorithm and construct a linear convex cone around the projected
configuration. We then formulate a new projection of the in-collision
configuration onto the convex cone as a Linear Complementarity Problem (LCP),
which we solve using a type of Gauss-Seidel iterative algorithm. We repeat this
procedure until a locally optimal PD is obtained. Our algorithm can process
complicated models consisting of tens of thousands triangles at interactive
rates.
</summary>
    <author>
      <name>Changsoo Je</name>
    </author>
    <author>
      <name>Min Tang</name>
    </author>
    <author>
      <name>Youngeun Lee</name>
    </author>
    <author>
      <name>Minkyoung Lee</name>
    </author>
    <author>
      <name>Young J. Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2077341.2077346</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2077341.2077346" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in ACM SIGGRAPH 2012. 15 pages, 23 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics (ToG 2012), Volume 31, Issue 1,
  Article 5, pp. 1-14, January 1, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.06181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; I.3.5; I.3.7; I.6.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01220v1</id>
    <updated>2015-08-23T00:37:47Z</updated>
    <published>2015-08-23T00:37:47Z</published>
    <title>Light Efficient Flutter Shutter</title>
    <summary>  Flutter shutter is a technique in which the exposure is chopped into segments
and light is only integrated part of the time. By carefully selecting the
chopping sequence it is possible to better condition the data for
reconstruction problems such as motion deblurring, focal sweeping, and
compressed sensing. The partial exposure trades better conditioning for less
energy. In problems such as motion deblurring the available energy is what
caused the problem in the first place (as strong illumination allows short
exposure thus eliminates motion blur). It is still beneficial because the
benefit from the better conditioning outweighs the cost in energy.
  This documents is focused on light efficient flutter shutter that provides
better conditioning and better energy utilization than conventional flutter
shutter.
</summary>
    <author>
      <name>Moshe Ben-Ezra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This documnet and the code listing in it are submitted under the
  permissive MIT License in hope it will be useful. In case anyone is
  interesting in 2012 date confirmation - the documnet was notarized at MIT on
  5 Dec 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03335v1</id>
    <updated>2015-09-10T20:58:36Z</updated>
    <published>2015-09-10T20:58:36Z</published>
    <title>Decomposing Digital Paintings into Layers via RGB-space Geometry</title>
    <summary>  In digital painting software, layers organize paintings. However, layers are
not explicitly represented, transmitted, or published with the final digital
painting. We propose a technique to decompose a digital painting into layers.
In our decomposition, each layer represents a coat of paint of a single paint
color applied with varying opacity throughout the image. Our decomposition is
based on the painting's RGB-space geometry. In RGB-space, a geometric structure
is revealed due to the linear nature of the standard Porter-Duff "over" pixel
compositing operation. The vertices of the convex hull of pixels in RGB-space
suggest paint colors. Users choose the degree of simplification to perform on
the convex hull, as well as a layer order for the colors. We solve a
constrained optimization problem to find maximally translucent, spatially
coherent opacity for each layer, such that the composition of the layers
reproduces the original image. We demonstrate the utility of the resulting
decompositions for re-editing.
</summary>
    <author>
      <name>Jianchao Tan</name>
    </author>
    <author>
      <name>Jyh-Ming Lien</name>
    </author>
    <author>
      <name>Yotam Gingold</name>
    </author>
    <link href="http://arxiv.org/abs/1509.03335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03700v1</id>
    <updated>2015-09-12T03:35:20Z</updated>
    <published>2015-09-12T03:35:20Z</published>
    <title>Good Colour Maps: How to Design Them</title>
    <summary>  Many colour maps provided by vendors have highly uneven perceptual contrast
over their range. It is not uncommon for colour maps to have perceptual flat
spots that can hide a feature as large as one tenth of the total data range.
Colour maps may also have perceptual discontinuities that induce the appearance
of false features. Previous work in the design of perceptually uniform colour
maps has mostly failed to recognise that CIELAB space is only designed to be
perceptually uniform at very low spatial frequencies. The most important factor
in designing a colour map is to ensure that the magnitude of the incremental
change in perceptual lightness of the colours is uniform. The specific
requirements for linear, diverging, rainbow and cyclic colour maps are
developed in detail. To support this work two test images for evaluating colour
maps are presented. The use of colour maps in combination with relief shading
is considered and the conditions under which colour can enhance or disrupt
relief shading are identified. Finally, a set of new basis colours for the
construction of ternary images are presented. Unlike the RGB primaries these
basis colours produce images whereby the salience of structures are consistent
irrespective of the assignment of basis colours to data channels.
</summary>
    <author>
      <name>Peter Kovesi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05301v1</id>
    <updated>2015-09-17T15:47:25Z</updated>
    <published>2015-09-17T15:47:25Z</published>
    <title>Humans Are Easily Fooled by Digital Images</title>
    <summary>  Digital images are ubiquitous in our modern lives, with uses ranging from
social media to news, and even scientific papers. For this reason, it is
crucial evaluate how accurate people are when performing the task of identify
doctored images. In this paper, we performed an extensive user study evaluating
subjects capacity to detect fake images. After observing an image, users have
been asked if it had been altered or not. If the user answered the image has
been altered, he had to provide evidence in the form of a click on the image.
We collected 17,208 individual answers from 383 users, using 177 images
selected from public forensic databases. Different from other previously
studies, our method propose different ways to avoid lucky guess when evaluating
users answers. Our results indicate that people show inaccurate skills at
differentiating between altered and non-altered images, with an accuracy of
58%, and only identifying the modified images 46.5% of the time. We also track
user features such as age, answering time, confidence, providing deep analysis
of how such variables influence on the users' performance.
</summary>
    <author>
      <name>Victor Schetinger</name>
    </author>
    <author>
      <name>Manuel M. Oliveira</name>
    </author>
    <author>
      <name>Roberto da Silva</name>
    </author>
    <author>
      <name>Tiago J. Carvalho</name>
    </author>
    <link href="http://arxiv.org/abs/1509.05301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05330v1</id>
    <updated>2015-09-15T09:32:17Z</updated>
    <published>2015-09-15T09:32:17Z</published>
    <title>Elements of Validation of Artificial Lighting through the Software
  CODYRUN: Application to a Test Case of the International Commission on
  Illumination (CIE)</title>
    <summary>  CODYRUN is a software for computational aeraulic and thermal simulation in
buildings developed by the Laboratory of Building Physics and Systems
(L.P.B.S). Numerical simulation codes of artificial lighting have been
introduced to extend the tool capacity. These calculation codes are able to
predict the amount of light received by any point of a given working plane and
from one or more sources installed on the ceiling of the room. The model used
for these calculations is original and semi-detailed (simplified). The test
case references of the task-3 TC-33 International Commission on Illumination
(CIE) were applied to the software to ensure reliability to properly handle
this photometric aspect. This allowed having a precise idea about the
reliability of the results of numerical simulations.
</summary>
    <author>
      <name>Ali Hamada Fakra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Miranville</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <author>
      <name>Dimitri Bigot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <author>
      <name>Harry Boyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IASTED Power and Energy Systems 2010, Sep 2010, Gaborone, Botswana.
  2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.05330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08834v1</id>
    <updated>2015-09-29T16:31:57Z</updated>
    <published>2015-09-29T16:31:57Z</published>
    <title>Visualization techniques for the developing chicken heart</title>
    <summary>  We present a geometric surface parameterization algorithm and several
visualization techniques adapted to the problem of understanding the 4D
peristaltic-like motion of the outflow tract (OFT) in an embryonic chick heart.
We illustrated the techniques using data from hearts under normal conditions
(four embryos), and hearts in which blood flow conditions are altered through
OFT banding (four embryos). The overall goal is to create quantitative measures
of the temporal heart-shape change both within a single subject and between
multiple subjects. These measures will help elucidate how altering hemodynamic
conditions changes the shape and motion of the OFT walls, which in turn
influence the stresses and strains on the developing heart, causing it to
develop differently. We take advantage of the tubular shape and periodic motion
of the OFT to produce successively lower dimensional visualizations of the
cardiac motion (e.g. curvature, volume, and cross-section) over time, and
quantifications of such visualizations.
</summary>
    <author>
      <name>Ly Phan</name>
    </author>
    <author>
      <name>Sandra Rugonyi</name>
    </author>
    <author>
      <name>Cindy Grimm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Longer version of conference paper published in 11th International
  Symposium on Visual Computing (December 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5, J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01113v2</id>
    <updated>2015-10-06T12:54:01Z</updated>
    <published>2015-10-05T11:58:12Z</published>
    <title>RAID: A Relation-Augmented Image Descriptor</title>
    <summary>  As humans, we regularly interpret images based on the relations between image
regions. For example, a person riding object X, or a plank bridging two
objects. Current methods provide limited support to search for images based on
such relations. We present RAID, a relation-augmented image descriptor that
supports queries based on inter-region relations. The key idea of our
descriptor is to capture the spatial distribution of simple point-to-region
relationships to describe more complex relationships between two image regions.
We evaluate the proposed descriptor by querying into a large subset of the
Microsoft COCO database and successfully extract nontrivial images
demonstrating complex inter-region relations, which are easily missed or
erroneously classified by existing methods.
</summary>
    <author>
      <name>Paul Guerrero</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed affiliation and email address of first author</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01113v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01113v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03935v2</id>
    <updated>2018-04-09T05:34:45Z</updated>
    <published>2015-10-14T00:24:22Z</published>
    <title>Surface Approximation via Asymptotic Optimal Geometric Partition</title>
    <summary>  In this paper, we present a surface remeshing method with high approximation
quality based on Principal Component Analysis. Given a triangular mesh and a
user assigned polygon/vertex budget, traditional methods usually require the
extra curvature metric field for the desired anisotropy to best approximate the
surface, even though the estimated curvature metric is known to be imperfect
and already self-contained in the surface. In our approach, this anisotropic
control is achieved through the optimal geometry partition without this
explicit metric field. The minimization of our proposed partition energy has
the following properties: Firstly, on a C2 surface, it is theoretically
guaranteed to have the optimal aspect ratio and cluster size as specified in
approximation theory for L1 piecewise linear approximation. Secondly, it
captures sharp features on practical models without any pre-tagging. We develop
an effective merging-swapping framework to seek the optimal partition and
construct polygonal/triangular mesh afterwards. The effectiveness and
efficiency of our method are demonstrated through the comparison with other
state-of-the-art remeshing methods.
</summary>
    <author>
      <name>Yiqi Cai</name>
    </author>
    <author>
      <name>Xiaohu Guo</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Wenping Wang</name>
    </author>
    <author>
      <name>Weihua Mao</name>
    </author>
    <author>
      <name>Zichun Zhong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.03935v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03935v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04224v2</id>
    <updated>2015-11-17T08:14:34Z</updated>
    <published>2015-11-13T10:19:41Z</published>
    <title>Procedural wood textures</title>
    <summary>  Existing bidirectional reflectance distribution function (BRDF) models are
capable of capturing the distinctive highlights produced by the fibrous nature
of wood. However, capturing parameter textures for even a single specimen
remains a laborious process requiring specialized equipment. In this paper we
take a procedural approach to generating parameters for the wood BSDF. We
characterize the elements of trees that are important for the appearance of
wood, discuss techniques appropriate for representing those features, and
present a complete procedural wood shader capable of reproducing the growth
patterns responsible for the distinctive appearance of highly prized
``figured'' wood. Our procedural wood shader is random-access, 3D, modular, and
is fast enough to generate a preview for design.
</summary>
    <author>
      <name>Albert J. Liu</name>
    </author>
    <author>
      <name>Stephen R. Marschner</name>
    </author>
    <author>
      <name>Victoria E. Dye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version: Increased resolution of images and added YouTube link
  to video</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04224v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04224v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06594v1</id>
    <updated>2015-11-20T13:52:39Z</updated>
    <published>2015-11-20T13:52:39Z</published>
    <title>Bezier curves and surfaces based on modified Bernstein polynomials</title>
    <summary>  In this paper, we use the blending functions of Bernstein polynomials with
shifted knots for construction of Bezier curves and surfaces. We study the
nature of degree elevation and degree reduction for Bezier Bernstein functions
with shifted knots.
  Parametric curves are represented using these modified Bernstein basis and
the concept of total positivity is applied to investigate the shape properties
of the curve. We get Bezier curve defined on [0, 1] when we set the parameter
\alpha=\beta to the value 0. We also present a de Casteljau algorithm to
compute Bernstein Bezier curves and surfaces with shifted knots. The new curves
have some properties similar to Bezier curves. Furthermore, some fundamental
properties for Bernstein Bezier curves and surfaces are discussed.
</summary>
    <author>
      <name>Khalid Khan</name>
    </author>
    <author>
      <name>D. K. Lobiyal</name>
    </author>
    <author>
      <name>Adem Kilicman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1507.04110</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 41A10, 41A25, 41A36" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03012v1</id>
    <updated>2015-12-09T19:42:48Z</updated>
    <published>2015-12-09T19:42:48Z</published>
    <title>ShapeNet: An Information-Rich 3D Model Repository</title>
    <summary>  We present ShapeNet: a richly-annotated, large-scale repository of shapes
represented by 3D CAD models of objects. ShapeNet contains 3D models from a
multitude of semantic categories and organizes them under the WordNet taxonomy.
It is a collection of datasets providing many semantic annotations for each 3D
model such as consistent rigid alignments, parts and bilateral symmetry planes,
physical sizes, keywords, as well as other planned annotations. Annotations are
made available through a public web-based interface to enable data
visualization of object attributes, promote data-driven geometric analysis, and
provide a large-scale quantitative benchmark for research in computer graphics
and vision. At the time of this technical report, ShapeNet has indexed more
than 3,000,000 models, 220,000 models out of which are classified into 3,135
categories (WordNet synsets). In this report we describe the ShapeNet effort as
a whole, provide details for all currently available datasets, and summarize
future plans.
</summary>
    <author>
      <name>Angel X. Chang</name>
    </author>
    <author>
      <name>Thomas Funkhouser</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <author>
      <name>Qixing Huang</name>
    </author>
    <author>
      <name>Zimo Li</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <author>
      <name>Manolis Savva</name>
    </author>
    <author>
      <name>Shuran Song</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Jianxiong Xiao</name>
    </author>
    <author>
      <name>Li Yi</name>
    </author>
    <author>
      <name>Fisher Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1512.03012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08826v1</id>
    <updated>2015-12-30T02:26:46Z</updated>
    <published>2015-12-30T02:26:46Z</published>
    <title>Improving Style Similarity Metrics of 3D Shapes</title>
    <summary>  The idea of style similarity metrics has been recently developed for various
media types such as 2D clip art and 3D shapes. We explore this style metric
problem and improve existing style similarity metrics of 3D shapes in four
novel ways. First, we consider the color and texture of 3D shapes which are
important properties that have not been previously considered. Second, we
explore the effect of clustering a dataset of 3D models by comparing between
style metrics for a single object type and style metrics that combine clusters
of object types. Third, we explore the idea of user-guided learning for this
problem. Fourth, we introduce an iterative approach that can learn a metric
from a general set of 3D models. We demonstrate these contributions with
various classes of 3D shapes and with applications such as style-based
similarity search and scene composition.
</summary>
    <author>
      <name>Kapil Dev</name>
    </author>
    <author>
      <name>Manfred Lau</name>
    </author>
    <link href="http://arxiv.org/abs/1512.08826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03224v1</id>
    <updated>2016-01-13T13:11:42Z</updated>
    <published>2016-01-13T13:11:42Z</published>
    <title>Implicit equations of non-degenerate rational Bezier quadric triangles</title>
    <summary>  In this paper we review the derivation of implicit equations for
non-degenerate quadric patches in rational Bezier triangular form. These are
the case of Steiner surfaces of degree two. We derive the bilinear forms for
such quadrics in a coordinate-free fashion in terms of their control net and
their list of weights in a suitable form. Our construction relies on projective
geometry and is grounded on the pencil of quadrics circumscribed to a
tetrahedron formed by vertices of the control net and an additional point which
is required for the Steiner surface to be a non-degenerate quadric.
</summary>
    <author>
      <name>A. Canton</name>
    </author>
    <author>
      <name>L. Fernandez-Jambrina</name>
    </author>
    <author>
      <name>E. Rosado Maria</name>
    </author>
    <author>
      <name>M. J. Vazquez-Gallo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-22804-4_6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-22804-4_6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8th International Conference Curves and Surfaces, Paris,
  France, June 12-18, 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science 9213, 70-79 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.03224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.05824v1</id>
    <updated>2016-01-21T21:46:22Z</updated>
    <published>2016-01-21T21:46:22Z</published>
    <title>3D digital reassembling of archaeological ceramic pottery fragments
  based on their thickness profile</title>
    <summary>  The reassembly of a broken archaeological ceramic pottery is an open and
complex problem, which remains a scientific process of extreme interest for the
archaeological community. Usually, the solutions suggested by various research
groups and universities depend on various aspects such as the matching process
of the broken surfaces, the outline of sherds or their colors and geometric
characteris-tics, their axis of symmetry, the corners of their contour, the
theme portrayed on the surface, the concentric circular rills that are left
during the base construction in the inner pottery side by the fingers of the
potter artist etc. In this work the reassembly process is based on a different
and more secure idea, since it is based on the thick-ness profile, which is
appropriately identified in every fragment. Specifically, our approach is based
on information encapsulated in the inner part of the sherd (i.e. thickness),
which is not -or at least not heavily- affected by the presence of harsh
environmental conditions, but is safely kept within the sherd itself. Our
method is verified in various use case experiments, using cutting edge
technologies such as 3D representations and precise measurements on surfaces
from the acquired 3D models.
</summary>
    <author>
      <name>Michail I. Stamatopoulos</name>
    </author>
    <author>
      <name>Christos-Nikolaos Anagnostopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01644v1</id>
    <updated>2016-02-04T11:33:22Z</updated>
    <published>2016-02-04T11:33:22Z</published>
    <title>A semi-automatic computer-aided method for surgical template design</title>
    <summary>  This paper presents a generalized integrated framework of semi-automatic
surgical template design. Several algorithms were implemented including the
mesh segmentation, offset surface generation, collision detection, ruled
surface generation, etc., and a special software named TemDesigner was
developed. With a simple user interface, a customized template can be semi-
automatically designed according to the preoperative plan. Firstly, mesh
segmentation with signed scalar of vertex is utilized to partition the inner
surface from the input surface mesh based on the indicated point loop. Then,
the offset surface of the inner surface is obtained through contouring the
distance field of the inner surface, and segmented to generate the outer
surface. Ruled surface is employed to connect inner and outer surfaces.
Finally, drilling tubes are generated according to the preoperative plan
through collision detection and merging. It has been applied to the template
design for various kinds of surgeries, including oral implantology, cervical
pedicle screw insertion, iliosacral screw insertion and osteotomy,
demonstrating the efficiency, functionality and generality of our method.
</summary>
    <author>
      <name>Xiaojun Chen</name>
    </author>
    <author>
      <name>Lu Xu</name>
    </author>
    <author>
      <name>Yue Yang</name>
    </author>
    <author>
      <name>Jan Egger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep20280</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep20280" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 16 figures, 2 tables, 36 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 6, Article number: 20280, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.01644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02139v2</id>
    <updated>2016-08-05T12:27:51Z</updated>
    <published>2016-02-03T23:43:26Z</published>
    <title>A simple method for estimating the fractal dimension from digital
  images: The compression dimension</title>
    <summary>  The fractal structure of real world objects is often analyzed using digital
images. In this context, the compression fractal dimension is put forward. It
provides a simple method for the direct estimation of the dimension of fractals
stored as digital image files. The computational scheme can be implemented
using readily available free software. Its simplicity also makes it very
interesting for introductory elaborations of basic concepts of fractal
geometry, complexity, and information theory. A test of the computational
scheme using limited-quality images of well-defined fractal sets obtained from
the Internet and free software has been performed. Also, a systematic
evaluation of the proposed method using computer generated images of the
Weierstrass cosine function shows an accuracy comparable to those of the
methods most commonly used to estimate the dimension of fractal data sequences
applied to the same test problem.
</summary>
    <author>
      <name>P. Chamorro-Posada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.chaos.2016.08.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.chaos.2016.08.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chaos Solitons Fract 91 (2016) 562-572</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.02139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03206v2</id>
    <updated>2016-03-31T19:22:26Z</updated>
    <published>2016-02-06T17:35:21Z</published>
    <title>Design of false color palettes for grayscale reproduction</title>
    <summary>  Design of false color palette is quite easy but some effort has to be done to
achieve good dynamic range, contrast and overall appearance of the palette.
Such palettes, for instance, are commonly used in scientific papers for
presenting the data. However, to lower the cost of the paper most scientists
decide to let the data to be printed in grayscale. The same applies to e-book
readers based on e-ink where most of them are still grayscale. For majority of
false color palettes reproducing them in grayscale results in ambiguous mapping
of the colors and may be misleading for the reader. In this article design of
false color palettes suitable for grayscale reproduction is described. Due to
the monotonic change of luminance of these palettes grayscale representation is
very similar to the data directly presented with a grayscale palette. Some
suggestions and examples how to design such palettes are provided.
</summary>
    <author>
      <name>Filip A. Sala</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03206v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03206v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06645v1</id>
    <updated>2016-02-22T04:45:43Z</updated>
    <published>2016-02-22T04:45:43Z</published>
    <title>Creating Simplified 3D Models with High Quality Textures</title>
    <summary>  This paper presents an extension to the KinectFusion algorithm which allows
creating simplified 3D models with high quality RGB textures. This is achieved
through (i) creating model textures using images from an HD RGB camera that is
calibrated with Kinect depth camera, (ii) using a modified scheme to update
model textures in an asymmetrical colour volume that contains a higher number
of voxels than that of the geometry volume, (iii) simplifying dense polygon
mesh model using quadric-based mesh decimation algorithm, and (iv) creating and
mapping 2D textures to every polygon in the output 3D model. The proposed
method is implemented in real-time by means of GPU parallel processing.
Visualization via ray casting of both geometry and colour volumes provides
users with a real-time feedback of the currently scanned 3D model. Experimental
results show that the proposed method is capable of keeping the model texture
quality even for a heavily decimated model and that, when reconstructing small
objects, photorealistic RGB textures can still be reconstructed.
</summary>
    <author>
      <name>Song Liu</name>
    </author>
    <author>
      <name>Wanqing Li</name>
    </author>
    <author>
      <name>Philip Ogunbona</name>
    </author>
    <author>
      <name>Yang-Wai Chow</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DICTA.2015.7371249</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DICTA.2015.7371249" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 International Conference on Digital Image Computing: Techniques
  and Applications (DICTA), Page 1 - 8</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07038v2</id>
    <updated>2016-07-06T23:24:19Z</updated>
    <published>2016-02-23T04:47:28Z</published>
    <title>Computer Aided Restoration of Handwritten Character Strokes</title>
    <summary>  This work suggests a new variational approach to the task of computer aided
restoration of incomplete characters, residing in a highly noisy document. We
model character strokes as the movement of a pen with a varying radius.
Following this model, a cubic spline representation is being utilized to
perform gradient descent steps, while maintaining interpolation at some initial
(manually sampled) points. The proposed algorithm was utilized in the process
of restoring approximately 1000 ancient Hebrew characters (dating to ca.
8th-7th century BCE), some of which are presented herein and show that the
algorithm yields plausible results when applied on deteriorated documents.
</summary>
    <author>
      <name>Barak Sober</name>
    </author>
    <author>
      <name>David Levin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U07, 68U10, 65D18, 94A08" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7.5; I.5.4; I.4.5; J.6; I.3.3; I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04060v1</id>
    <updated>2016-03-13T18:38:15Z</updated>
    <published>2016-03-13T18:38:15Z</published>
    <title>Modelling Developable Ribbons Using Ruling Bending Coordinates</title>
    <summary>  This paper presents a new method for modelling the dynamic behaviour of
developable ribbons, two dimensional strips with much smaller width than
length. Instead of approximating such surface with a general triangle mesh, we
characterize it by a set of creases and bending angles across them. This
representation allows the developability to be satisfied everywhere while still
leaves enough degree of freedom to represent salient global deformation. We
show how the potential and kinetic energies can be properly discretized in this
configuration space and time integrated in a fully implicit manner. The result
is a dynamic simulator with several desirable features: We can model
non-trivial deformation using much fewer elements than conventional FEM method.
It is stable under extreme deformation, external force or large timestep size.
And we can readily handle various user constraints in Euclidean space.
</summary>
    <author>
      <name>Zherong Pan</name>
    </author>
    <author>
      <name>Jin Huang</name>
    </author>
    <author>
      <name>Hujun Bao</name>
    </author>
    <link href="http://arxiv.org/abs/1603.04060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05433v2</id>
    <updated>2016-03-18T09:34:34Z</updated>
    <published>2016-03-17T11:30:01Z</published>
    <title>Degree reduction of composite Bézier curves</title>
    <summary>  This paper deals with the problem of multi-degree reduction of a composite
B\'ezier curve with the parametric continuity constraints at the endpoints of
the segments. We present a novel method which is based on the idea of using
constrained dual Bernstein polynomials to compute the control points of the
reduced composite curve. In contrast to other methods, ours minimizes the
$L_2$-error for the whole composite curve instead of minimizing the
$L_2$-errors for each segment separately. As a result, an additional
optimization is possible. Examples show that the new method gives much better
results than multiple application of the degree reduction of a single B\'ezier
curve.
</summary>
    <author>
      <name>Przemysław Gospodarczyk</name>
    </author>
    <author>
      <name>Stanisław Lewanowicz</name>
    </author>
    <author>
      <name>Paweł Woźny</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.amc.2016.08.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.amc.2016.08.004" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Mathematics and Computation 293, p. 40-48 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.05433v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05433v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06078v2</id>
    <updated>2016-08-03T11:11:55Z</updated>
    <published>2016-03-19T10:29:57Z</published>
    <title>Deep Shading: Convolutional Neural Networks for Screen-Space Shading</title>
    <summary>  In computer vision, convolutional neural networks (CNNs) have recently
achieved new levels of performance for several inverse problems where RGB pixel
appearance is mapped to attributes such as positions, normals or reflectance.
In computer graphics, screen-space shading has recently increased the visual
quality in interactive image synthesis, where per-pixel attributes such as
positions, normals or reflectance of a virtual 3D scene are converted into RGB
pixel appearance, enabling effects like ambient occlusion, indirect light,
scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we
consider the diagonal problem: synthesizing appearance from given per-pixel
attributes using a CNN. The resulting Deep Shading simulates various
screen-space effects at competitive quality and speed while not being
programmed by human experts but learned from example images.
</summary>
    <author>
      <name>Oliver Nalbach</name>
    </author>
    <author>
      <name>Elena Arabadzhiyska</name>
    </author>
    <author>
      <name>Dushyant Mehta</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Tobias Ritschel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.13225</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.13225" rel="related"/>
    <link href="http://arxiv.org/abs/1603.06078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06143v2</id>
    <updated>2016-10-13T20:10:09Z</updated>
    <published>2016-03-19T20:58:47Z</published>
    <title>Neurally-Guided Procedural Models: Amortized Inference for Procedural
  Graphics Programs using Neural Networks</title>
    <summary>  Probabilistic inference algorithms such as Sequential Monte Carlo (SMC)
provide powerful tools for constraining procedural models in computer graphics,
but they require many samples to produce desirable results. In this paper, we
show how to create procedural models which learn how to satisfy constraints. We
augment procedural models with neural networks which control how the model
makes random choices based on the output it has generated thus far. We call
such models neurally-guided procedural models. As a pre-computation, we train
these models to maximize the likelihood of example outputs generated via SMC.
They are then used as efficient SMC importance samplers, generating
high-quality results with very few samples. We evaluate our method on
L-system-like models with image-based constraints. Given a desired quality
threshold, neurally-guided models can generate satisfactory results up to 10x
faster than unguided models.
</summary>
    <author>
      <name>Daniel Ritchie</name>
    </author>
    <author>
      <name>Anna Thomas</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <author>
      <name>Noah D. Goodman</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing Systems (NIPS 2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.06143v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06143v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07011v1</id>
    <updated>2016-03-22T22:14:01Z</updated>
    <published>2016-03-22T22:14:01Z</published>
    <title>Graphs Drawing through Fuzzy Clustering</title>
    <summary>  Many problems can be presented in an abstract form through a wide range of
binary objects and relations which are defined over problem domain. In these
problems, graphical demonstration of defined binary objects and solutions is
the most suitable representation approach. In this regard, graph drawing
problem discusses the methods for transforming combinatorial graphs to
geometrical drawings in order to visualize them. This paper studies the
force-directed algorithms and multi-surface techniques for drawing general
undirected graphs. Particularly, this research describes force-directed
approach to model the drawing of a general graph as a numerical optimization
problem. So, it can use rich knowledge which is presented as an established
system by the numerical optimization. Moreover, this research proposes the
multi-surface approach as an efficient tool for overcoming local minimums in
standard force-directed algorithms. Next, we introduce a new method for
multi-surface approach based on fuzzy clustering algorithms.
</summary>
    <author>
      <name>Mohammadreza Ashouri</name>
    </author>
    <author>
      <name>Ali Golshani</name>
    </author>
    <author>
      <name>Dara Moazzmi</name>
    </author>
    <author>
      <name>Mandana Ghasemi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08753v1</id>
    <updated>2016-03-29T13:02:50Z</updated>
    <published>2016-03-29T13:02:50Z</published>
    <title>Curve Networks for Surface Reconstruction</title>
    <summary>  Man-made objects usually exhibit descriptive curved features (i.e., curve
networks). The curve network of an object conveys its high-level geometric and
topological structure. We present a framework for extracting feature curve
networks from unstructured point cloud data. Our framework first generates a
set of initial curved segments fitting highly curved regions. We then optimize
these curved segments to respect both data fitting and structural regularities.
Finally, the optimized curved segments are extended and connected into curve
networks using a clustering method. To facilitate effectiveness in case of
severe missing data and to resolve ambiguities, we develop a user interface for
completing the curve networks. Experiments on various imperfect point cloud
data validate the effectiveness of our curve network extraction framework. We
demonstrate the usefulness of the extracted curve networks for surface
reconstruction from incomplete point clouds.
</summary>
    <author>
      <name>Yuanhao Cao</name>
    </author>
    <author>
      <name>Liangliang Nan</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08984v2</id>
    <updated>2017-04-10T11:09:34Z</updated>
    <published>2016-03-29T22:18:29Z</published>
    <title>SMASH: Physics-guided Reconstruction of Collisions from Videos</title>
    <summary>  Collision sequences are commonly used in games and entertainment to add drama
and excitement. Authoring even two body collisions in the real world can be
difficult, as one has to get timing and the object trajectories to be correctly
synchronized. After tedious trial-and-error iterations, when objects can
actually be made to collide, then they are difficult to capture in 3D. In
contrast, synthetically generating plausible collisions is difficult as it
requires adjusting different collision parameters (e.g., object mass ratio,
coefficient of restitution, etc.) and appropriate initial parameters. We
present SMASH to directly read off appropriate collision parameters directly
from raw input video recordings. Technically we enable this by utilizing laws
of rigid body collision to regularize the problem of lifting 2D trajectories to
a physically valid 3D reconstruction of the collision. The reconstructed
sequences can then be modified and combined to easily author novel and
plausible collisions. We evaluate our system on a range of synthetic scenes and
demonstrate the effectiveness of our method by accurately reconstructing
several complex real world collision events.
</summary>
    <author>
      <name>Aron Monszpart</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2980179.2982421</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2980179.2982421" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH Asia 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 35, 6, Article 199 (November 2016), 14 pages
  (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.08984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00047v1</id>
    <updated>2016-03-31T20:40:14Z</updated>
    <published>2016-03-31T20:40:14Z</published>
    <title>Towards Zero-Waste Furniture Design</title>
    <summary>  In traditional design, shapes are first conceived, and then fabricated. While
this decoupling simplifies the design process, it can result in inefficient
material usage, especially where off-cut pieces are hard to reuse. The
designer, in absence of explicit feedback on material usage remains helpless to
effectively adapt the design -- even though design variabilities exist. In this
paper, we investigate {\em waste minimizing furniture design} wherein based on
the current design, the user is presented with design variations that result in
more effective usage of materials. Technically, we dynamically analyze material
space layout to determine {\em which} parts to change and {\em how}, while
maintaining original design intent specified in the form of design constraints.
We evaluate the approach on simple and complex furniture design scenarios, and
demonstrate effective material usage that is difficult, if not impossible, to
achieve without computational support.
</summary>
    <author>
      <name>Bongjin Koo</name>
    </author>
    <author>
      <name>Jean Hergel</name>
    </author>
    <author>
      <name>Sylvain Lefebvre</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03220v2</id>
    <updated>2020-04-08T14:54:56Z</updated>
    <published>2016-04-09T16:47:06Z</published>
    <title>Algorithms and Identities for B$\acute{e}$zier curves via Post Quantum
  Blossom</title>
    <summary>  In this paper, a new analogue of blossom based on post quantum calculus is
introduced. The post quantum blossom has been adapted for developing identities
and algorithms for Bernstein bases and B$\acute{e}$zier curves. By applying the
post quantum blossom, various new identities and formulae expressing the
monomials in terms of the post quantun Bernstein basis functions and a post
quantun variant of Marsden's identity are investigated. For each post quantum
B$\acute{e}$zier curves of degree $m,$ a collection of $m!$ new, affine
invariant, recursive evaluation algorithms are derived.
</summary>
    <author>
      <name>Alaa Mohammed Obad</name>
    </author>
    <author>
      <name>Khalid Khan</name>
    </author>
    <author>
      <name>D. K. Lobiyal</name>
    </author>
    <author>
      <name>Asif Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, name of two more authors who contributed in
  revised form added, slight change in title of the paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07378v1</id>
    <updated>2016-04-25T19:42:37Z</updated>
    <published>2016-04-25T19:42:37Z</published>
    <title>Towards Real-time Simulation of Hyperelastic Materials</title>
    <summary>  We present a new method for real-time physics-based simulation supporting
many different types of hyperelastic materials. Previous methods such as
Position Based or Projective Dynamics are fast, but support only limited
selection of materials; even classical materials such as the Neo-Hookean
elasticity are not supported. Recently, Xu et al. [2015] introduced new
"spline-based materials" which can be easily controlled by artists to achieve
desired animation effects. Simulation of these types of materials currently
relies on Newton's method, which is slow, even with only one iteration per
timestep. In this paper, we show that Projective Dynamics can be interpreted as
a quasi-Newton method. This insight enables very efficient simulation of a
large class of hyperelastic materials, including the Neo-Hookean, spline-based
materials, and others. The quasi-Newton interpretation also allows us to
leverage ideas from numerical optimization. In particular, we show that our
solver can be further accelerated using L-BFGS updates (Limited-memory
Broyden-Fletcher-Goldfarb-Shanno algorithm). Our final method is typically more
than 10 times faster than one iteration of Newton's method without compromising
quality. In fact, our result is often more accurate than the result obtained
with one iteration of Newton's method. Our method is also easier to implement,
implying reduced software development costs.
</summary>
    <author>
      <name>Tiantian Liu</name>
    </author>
    <author>
      <name>Sofien Bouaziz</name>
    </author>
    <author>
      <name>Ladislav Kavan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01760v1</id>
    <updated>2016-05-05T21:25:54Z</updated>
    <published>2016-05-05T21:25:54Z</published>
    <title>Adaptive Mesh Booleans</title>
    <summary>  We present a new method for performing Boolean operations on volumes
represented as triangle meshes. In contrast to existing methods which treat
meshes as 3D polyhedra and try to partition the faces at their exact
intersection curves, we treat meshes as adaptive surfaces which can be
arbitrarily refined. Rather than depending on computing precise face
intersections, our approach refines the input meshes in the intersection
regions, then discards intersecting triangles and fills the resulting holes
with high-quality triangles. The original intersection curves are approximated
to a user-definable precision, and our method can identify and preserve creases
and sharp features. Advantages of our approach include the ability to trade
speed for accuracy, support for open meshes, and the ability to incorporate
tolerances to handle cases where large numbers of faces are slightly
inter-penetrating or near-coincident.
</summary>
    <author>
      <name>Ryan Schmidt</name>
    </author>
    <author>
      <name>Tyson Brochu</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01816v3</id>
    <updated>2016-05-24T05:44:40Z</updated>
    <published>2016-05-06T04:17:12Z</published>
    <title>Sufficient Conditions for Tuza's Conjecture on Packing and Covering
  Triangles</title>
    <summary>  Given a simple graph $G=(V,E)$, a subset of $E$ is called a triangle cover if
it intersects each triangle of $G$. Let $\nu_t(G)$ and $\tau_t(G)$ denote the
maximum number of pairwise edge-disjoint triangles in $G$ and the minimum
cardinality of a triangle cover of $G$, respectively. Tuza conjectured in 1981
that $\tau_t(G)/\nu_t(G)\le2$ holds for every graph $G$. In this paper, using a
hypergraph approach, we design polynomial-time combinatorial algorithms for
finding small triangle covers. These algorithms imply new sufficient conditions
for Tuza's conjecture on covering and packing triangles. More precisely,
suppose that the set $\mathscr T_G$ of triangles covers all edges in $G$. We
show that a triangle cover of $G$ with cardinality at most $2\nu_t(G)$ can be
found in polynomial time if one of the following conditions is satisfied: (i)
$\nu_t(G)/|\mathscr T_G|\ge\frac13$, (ii) $\nu_t(G)/|E|\ge\frac14$, (iii)
$|E|/|\mathscr T_G|\ge2$.
  Keywords: Triangle cover, Triangle packing, Linear 3-uniform hypergraphs,
Combinatorial algorithms
</summary>
    <author>
      <name>Xujin Chen</name>
    </author>
    <author>
      <name>Zhuo Diao</name>
    </author>
    <author>
      <name>Xiaodong Hu</name>
    </author>
    <author>
      <name>Zhongzheng Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01816v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01816v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04797v2</id>
    <updated>2016-07-02T03:15:10Z</updated>
    <published>2016-05-16T15:09:19Z</published>
    <title>Thingi10K: A Dataset of 10,000 3D-Printing Models</title>
    <summary>  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
</summary>
    <author>
      <name>Qingnan Zhou</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04797v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04797v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09540v1</id>
    <updated>2016-06-30T15:37:53Z</updated>
    <published>2016-06-30T15:37:53Z</published>
    <title>SurfCuit: Surface Mounted Circuits on 3D Prints</title>
    <summary>  We present, SurfCuit, a novel approach to design and construction of electric
circuits on the surface of 3D prints. Our surface mounting technique allows
durable construction of circuits on the surface of 3D prints. SurfCuit does not
require tedious circuit casing design or expensive set-ups, thus we can
expedite the process of circuit construction for 3D models. Our technique
allows the user to construct complex circuits for consumer-level desktop fused
decomposition modeling (FDM) 3D printers. The key idea behind our technique is
that FDM plastic forms a strong bond with metal when it is melted. This
observation enables construction of a robust circuit traces using copper tape
and soldering. We also present an interactive tool to design such circuits on
arbitrary 3D geometry. We demonstrate the effectiveness of our approach through
various actual construction examples.
</summary>
    <author>
      <name>Nobuyuki Umetani</name>
    </author>
    <author>
      <name>Ryan Schmidt</name>
    </author>
    <link href="http://arxiv.org/abs/1606.09540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01102v1</id>
    <updated>2016-07-05T03:22:40Z</updated>
    <published>2016-07-05T03:22:40Z</published>
    <title>A Visualization Method of Four Dimensional Polytopes by Oval Display of
  Parallel Hyperplane Slices</title>
    <summary>  A method to visualize polytopes in a four dimensional euclidian space
$(x,y,z,w)$ is proposed. A polytope is sliced by multiple hyperplanes that are
parallel each other and separated by uniform intervals. Since the hyperplanes
are perpendicular to the $w$ axis, the resulting multiple slices appear in the
three-dimensional $(x,y,z)$ space and they are shown by the standard computer
graphics. The polytope is rotated extrinsically in the four dimensional space
by means of a simple input method based on keyboard typings. The multiple
slices are placed on a parabola curve in the three-dimensional world
coordinates. The slices in a view window form an oval appearance. Both the
simple and the double rotations in the four dimensional space are applied to
the polytope. All slices synchronously change their shapes when a rotation is
applied to the polytope. The compact display in the oval of many slices with
the help of quick rotations facilitate a grasp of the four dimensional
configuration of the polytope.
</summary>
    <author>
      <name>Akira Kageyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05812v1</id>
    <updated>2016-07-20T04:00:44Z</updated>
    <published>2016-07-20T04:00:44Z</published>
    <title>HoloMed: A Low-Cost Gesture-Based Holographic</title>
    <summary>  During medicine studies, visualization of certain elements is common and
indispensable in order to get more information about the way they work.
Currently, we resort to the use of photographs -which are insufficient due to
being static- or tests in patients, which can be invasive or even risky.
Therefore, a low-cost approach is proposed by using a 3D visualization. This
paper presents a holographic system built with low-cost materials for teaching
obstetrics, where student interaction is performed by using voice and gestures.
Our solution, which we called HoloMed, is focused on the projection of a
euthocic normal delivery under a web-based infrastructure which also employs a
Kinect. HoloMed is divided in three (3) essential modules: a gesture analyzer,
a data server, and a holographic projection architecture, which can be executed
in several interconnected computers using different network protocols. Tests
used for determining the user's position, illumination factors, and response
times, demonstrate HoloMed's effectiveness as a low-cost system for teaching,
using a natural user interface and 3D images.
</summary>
    <author>
      <name>Juan Perozo</name>
    </author>
    <author>
      <name>Mimia Lo Leung</name>
    </author>
    <author>
      <name>Esmitt Ramírez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">English version of an accepted paper in the Proceedings of the 4th
  Simposio Cient\'ifico y Tecnol\'ogico en Computaci\'on, 160-168. May 2016.
  Original version in spanish
  http://ccg.ciens.ucv.ve/~esmitt/publications/2016/SCTC2016.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00936v4</id>
    <updated>2016-09-01T14:51:28Z</updated>
    <published>2016-08-02T19:02:40Z</published>
    <title>Multimodal Brain Visualization</title>
    <summary>  Current connectivity diagrams of human brain image data are either overly
complex or overly simplistic. In this work we introduce simple yet accurate
interactive visual representations of multiple brain image structures and the
connectivity among them. We map cortical surfaces extracted from human brain
magnetic resonance imaging (MRI) data onto 2D surfaces that preserve shape
(angle), extent (area), and spatial (neighborhood) information for 2D (circular
disk) and 3D (spherical) mapping, split these surfaces into separate patches,
and cluster functional and diffusion tractography MRI connections between pairs
of these patches. The resulting visualizations are easier to compute on and
more visually intuitive to interact with than the original data, and facilitate
simultaneous exploration of multiple data sets, modalities, and statistical
maps.
</summary>
    <author>
      <name>Saad Nadeem</name>
    </author>
    <author>
      <name>Arie Kaufman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.2217003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.2217003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIE Medical Imaging 2016, Proc. SPIE Medical Imaging: Biomedical
  Applications in Molecular, Structural, and Functional Imaging, 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SPIE Medical Imaging, pp. 97881Y-97881Y. 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.00936v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00936v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01102v1</id>
    <updated>2016-08-03T08:04:24Z</updated>
    <published>2016-08-03T08:04:24Z</published>
    <title>Efficient Optimal Control of Smoke using Spacetime Multigrid</title>
    <summary>  We present a novel algorithm to control the physically-based animation of
smoke. Given a set of keyframe smoke shapes, we compute a dense sequence of
control force fields that can drive the smoke shape to match several keyframes
at certain time instances. Our approach formulates this control problem as a
PDE constrained spacetime optimization and computes locally optimal control
forces as the stationary point of the Karush-Kuhn-Tucker conditions. In order
to reduce the high complexity of multiple passes of fluid resimulation, we
utilize the coherence between consecutive fluid simulation passes and update
our solution using a novel spacetime full approximation scheme (STFAS). We
demonstrate the benefits of our approach by computing accurate solutions on 2D
and 3D benchmarks. In practice, we observe more than an order of magnitude
improvement over prior methods.
</summary>
    <author>
      <name>Zherong Pan</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04082v1</id>
    <updated>2016-08-14T09:55:34Z</updated>
    <published>2016-08-14T09:55:34Z</published>
    <title>A weighted binary average of point-normal pairs with application to
  subdivision schemes</title>
    <summary>  Subdivision is a well-known and established method for generating smooth
curves and surfaces from discrete data by repeated refinements. The typical
input for such a process is a mesh of vertices. In this work we propose to
refine 2D data consisting of vertices of a polygon and a normal at each vertex.
Our core refinement procedure is based on a circle average, which is a new
non-linear weighted average of two points and their corresponding normals. The
ability to locally approximate curves by the circle average is demonstrated.
With this ability, the circle average is a candidate for modifying linear
subdivision schemes refining points, to schemes refining point-normal pairs.
This is done by replacing the weighted binary arithmetic means in a linear
subdivision scheme, expressed in terms of repeated binary averages, by circle
averages with the same weights. Here we investigate the modified
Lane-Riesenfeld algorithm and the 4-point scheme. For the case that the initial
data consists of a control polygon only, a naive method for choosing initial
normals is proposed. An example demonstrates the superiority of the above two
modified schemes, with the naive choice of initial normals over the
corresponding linear schemes, when applied to a control polygon with edges of
significantly different lengths.
</summary>
    <author>
      <name>Evgeny Lipovetsky</name>
    </author>
    <author>
      <name>Nira Dyn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cagd.2016.07.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cagd.2016.07.004" rel="related"/>
    <link href="http://arxiv.org/abs/1608.04082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04366v2</id>
    <updated>2016-11-19T16:24:27Z</updated>
    <published>2016-08-15T19:02:30Z</published>
    <title>Infill Optimization for Additive Manufacturing -- Approaching Bone-like
  Porous Structures</title>
    <summary>  Porous structures such as trabecular bone are widely seen in nature. These
structures exhibit superior mechanical properties whilst being lightweight. In
this paper, we present a method to generate bone-like porous structures as
lightweight infill for additive manufacturing. Our method builds upon and
extends voxel-wise topology optimization. In particular, for the purpose of
generating sparse yet stable structures distributed in the interior of a given
shape, we propose upper bounds on the localized material volume in the
proximity of each voxel in the design domain. We then aggregate the local
per-voxel constraints by their p-norm into an equivalent global constraint, in
order to facilitate an efficient optimization process. Implemented on a
high-resolution topology optimization framework, our results demonstrate
mechanically optimized, detailed porous structures which mimic those found in
nature. We further show variants of the optimized structures subject to
different design specifications, and analyze the optimality and robustness of
the obtained structures.
</summary>
    <author>
      <name>Jun Wu</name>
    </author>
    <author>
      <name>Niels Aage</name>
    </author>
    <author>
      <name>Ruediger Westermann</name>
    </author>
    <author>
      <name>Ole Sigmund</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2017.2655523</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2017.2655523" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TVCG Revision 1</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics, vol. 24,
  no. 2, pp. 1127-1140, Feb. 1 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.04366v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04366v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04721v1</id>
    <updated>2016-08-16T19:38:16Z</updated>
    <published>2016-08-16T19:38:16Z</published>
    <title>Adaptive Position-Based Fluids: Improving Performance of Fluid
  Simulations for Real-Time Applications</title>
    <summary>  The Position Based Fluids (PBF) method is a state-of-the-art approach for
fluid simulations in the context of real-time applications like games. It uses
an iterative solver concept that tries to maintain a constant fluid density
(incompressibility) to realize incompressible fluids like water. However,
larger fluid volumes that consist of several hundred thousand particles (e.g.
for the simulation of oceans) require many iterations and a lot of simulation
power. We present a lightweight and easy-to-integrate extension to PBF that
adaptively adjusts the number of solver iterations on a fine-grained basis.
Using a novel adaptive-simulation approach, we are able to achieve significant
improvements in performance on our evaluation scenarios while maintaining
high-quality results in terms of visualization quality, which makes it a
perfect choice for game developers. Furthermore, our method does not weaken the
advantages of prior work and seamlessly integrates into other position-based
methods for physically-based simulations.
</summary>
    <author>
      <name>Marcel Köster</name>
    </author>
    <author>
      <name>Antonio Krüger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2016.6301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2016.6301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, International Journal of Computer Graphics &amp; Animation
  Vol.6, No.3, July 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05231v2</id>
    <updated>2016-08-27T12:07:01Z</updated>
    <published>2016-08-18T10:34:54Z</published>
    <title>Design and Implementation of a Procedural Content Generation Web
  Application for Vertex Shaders</title>
    <summary>  We present a web application for the procedural generation of transformations
of 3D models. We generate the transformations by algorithmically generating the
vertex shaders of the 3D models. The vertex shaders are created with an
interactive genetic algorithm, which displays to the user the visual effect
caused by each vertex shader, allows the user to select the visual effect the
user likes best, and produces a new generation of vertex shaders using the user
feedback as the fitness measure of the genetic algorithm. We use genetic
programming to represent each vertex shader as a computer program. This paper
presents details of requirements specification, software architecture, high and
low-level design, and prototype user interface. We discuss the project's
current status and development challenges.
</summary>
    <author>
      <name>Juan C. Quiroz</name>
    </author>
    <author>
      <name>Sergiu M. Dascalu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25th International Conference on Software Engineering and Data
  Engineering (SEDE 2016), September 26-28, Denver, CO</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05231v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05231v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05772v1</id>
    <updated>2016-08-20T03:18:57Z</updated>
    <published>2016-08-20T03:18:57Z</published>
    <title>A Data-Driven Approach for Mapping Multivariate Data to Color</title>
    <summary>  A wide variety of color schemes have been devised for mapping scalar data to
color. Some use the data value to index a color scale. Others assign colors to
different, usually blended disjoint materials, to handle areas where materials
overlap. A number of methods can map low-dimensional data to color, however,
these methods do not scale to higher dimensional data. Likewise, schemes that
take a more artistic approach through color mixing and the like also face
limits when it comes to the number of variables they can encode. We address the
challenge of mapping multivariate data to color and avoid these limitations at
the same time. It is a data driven method, which first gauges the similarity of
the attributes and then arranges them according to the periphery of a convex 2D
color space, such as HSL. The color of a multivariate data sample is then
obtained via generalized barycentric coordinate (GBC) interpolation.
</summary>
    <author>
      <name>Shenghui Cheng</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Wen Zhong</name>
    </author>
    <author>
      <name>Klaus Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06368v3</id>
    <updated>2016-09-09T14:10:41Z</updated>
    <published>2016-08-23T03:14:13Z</published>
    <title>Segmenting a Surface Mesh into Pants Using Morse Theory</title>
    <summary>  A pair of pants is a genus zero orientable surface with three boundary
components. A pants decomposition of a surface is a finite collection of
unordered pairwise disjoint simple closed curves embedded in the surface that
decompose the surface into pants. In this paper we present two Morse theory
based algorithms for pants decomposition of a surface mesh. Both algorithms
operates on a choice of an appropriate Morse function on the surface. The first
algorithm uses this Morse function to identify handles that are glued
systematically to obtain a pant decomposition. The second algorithm uses the
Reeb graph of the Morse function to obtain a pant decomposition. Both
algorithms work for surfaces with or without boundaries. Our preliminary
implementation of the two algorithms shows that both algorithms run in much
less time than an existing state-of-the-art method, and the Reeb graph based
algorithm achieves the best time efficiency. Finally, we demonstrate the
robustness of our algorithms against noise.
</summary>
    <author>
      <name>Mustafa Hajij</name>
    </author>
    <author>
      <name>Tamal Dey</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06368v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06368v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08570v1</id>
    <updated>2016-08-30T17:31:36Z</updated>
    <published>2016-08-30T17:31:36Z</published>
    <title>Interpolations of Smoke and Liquid Simulations</title>
    <summary>  We present a novel method to interpolate smoke and liquid simulations in
order to perform data-driven fluid simulations. Our approach calculates a dense
space-time deformation using grid-based signed-distance functions of the
inputs. A key advantage of this implicit Eulerian representation is that it
allows us to use powerful techniques from the optical flow area. We employ a
five-dimensional optical flow solve. In combination with a projection
algorithm, and residual iterations, we achieve a robust matching of the inputs.
Once the match is computed, arbitrary in between variants can be created very
efficiently. To concatenate multiple long-range deformations, we propose a
novel alignment technique. Our approach has numerous advantages, including
automatic matches without user input, volumetric deformations that can be
applied to details around the surface, and the inherent handling of topology
changes. As a result, we can interpolate swirling smoke clouds, and splashing
liquid simulations. We can even match and interpolate phenomena with
fundamentally different physics: a drop of liquid, and a blob of heavy smoke.
</summary>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2956233</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2956233" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics, 36(1), 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.08570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.8; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02072v1</id>
    <updated>2016-09-07T17:03:19Z</updated>
    <published>2016-09-07T17:03:19Z</published>
    <title>Sampling BSSRDFs with non-perpendicular incidence</title>
    <summary>  Sub-surface scattering is key to our perception of translucent materials.
Models based on diffusion theory are used to render such materials in a
realistic manner by evaluating an approximation of the material BSSRDF at any
two points of the surface. Under the assumption of perpendicular incidence,
this BSSRDF approximation can be tabulated over 2 dimensions to provide fast
evaluation and importance sampling. However, accounting for non-perpendicular
incidence with the same approach would require to tabulate over 4 dimensions,
making the model too large for practical applications. In this report, we
present a method to efficiently evaluate and importance sample the
multi-scattering component of diffusion based BSSRDFs for non-perpendicular
incidence. Our approach is based on tabulating a compressed angular model of
Photon Beam Diffusion. We explain how to generate, evaluate and sample our
model. We show that 1 MiB is enough to store a model of the multi-scattering
BSSRDF that is within $0.5\%$ relative error of Photon Beam Diffusion. Finally,
we present a method to use our model in a Monte Carlo particle tracer and show
results of our implementation in PBRT.
</summary>
    <author>
      <name>Etienne Ferrier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supervised by Prof. Wenzel Jakob (EPFL). Contains 9 pages, 13
  figures, 4 tables, 3 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05328v1</id>
    <updated>2016-09-17T12:54:41Z</updated>
    <published>2016-09-17T12:54:41Z</published>
    <title>Hermite interpolation by piecewise polynomial surfaces with polynomial
  area element</title>
    <summary>  This paper is devoted to the construction of polynomial 2-surfaces which
possess a polynomial area element. In particular we study these surfaces in the
Euclidean space $\mathbb R^3$ (where they are equivalent to the PN surfaces)
and in the Minkowski space $\mathbb R^{3,1}$ (where they provide the MOS
surfaces). We show generally in real vector spaces of any dimension and any
metric that the Gram determinant of a parametric set of subspaces is a perfect
square if and only if the Gram determinant of its orthogonal complement is a
perfect square. Consequently the polynomial surfaces of a given degree with
polynomial area element can be constructed from the prescribed normal fields
solving a system of linear equations. The degree of the constructed surface
depending on the degree and the quality of the prescribed normal field is
investigated and discussed. We use the presented approach to interpolate a
network of points and associated normals with piecewise polynomial surfaces
with polynomial area element and demonstrate our method on a number of examples
(constructions of quadrilateral as well as triangular patches
</summary>
    <author>
      <name>Michal Bizzarri</name>
    </author>
    <author>
      <name>Miroslav Lávička</name>
    </author>
    <author>
      <name>Zbyňek Šír</name>
    </author>
    <author>
      <name>Jan Vršek</name>
    </author>
    <link href="http://arxiv.org/abs/1609.05328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07049v1</id>
    <updated>2016-09-22T16:11:57Z</updated>
    <published>2016-09-22T16:11:57Z</published>
    <title>Customized Facial Constant Positive Air Pressure (CPAP) Masks</title>
    <summary>  Sleep apnea is a syndrome that is characterized by sudden breathing halts
while sleeping. One of the common treatments involves wearing a mask that
delivers continuous air flow into the nostrils so as to maintain a steady air
pressure. These masks are designed for an average facial model and are often
difficult to adjust due to poor fit to the actual patient. The incompatibility
is characterized by gaps between the mask and the face, which deteriorates the
impermeability of the mask and leads to air leakage. We suggest a fully
automatic approach for designing a personalized nasal mask interface using a
facial depth scan. The interfaces generated by the proposed method accurately
fit the geometry of the scanned face, and are easy to manufacture. The proposed
method utilizes cheap commodity depth sensors and 3D printing technologies to
efficiently design and manufacture customized masks for patients suffering from
sleep apnea.
</summary>
    <author>
      <name>Matan Sela</name>
    </author>
    <author>
      <name>Nadav Toledo</name>
    </author>
    <author>
      <name>Yaron Honen</name>
    </author>
    <author>
      <name>Ron Kimmel</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07738v1</id>
    <updated>2016-09-25T13:20:58Z</updated>
    <published>2016-09-25T13:20:58Z</published>
    <title>Fast Blended Transformations for Partial Shape Registration</title>
    <summary>  Automatic estimation of skinning transformations is a popular way to deform a
single reference shape into a new pose by providing a small number of control
parameters. We generalize this approach by efficiently enabling the use of
multiple exemplar shapes. Using a small set of representative natural poses, we
propose to express an unseen appearance by a low-dimensional linear subspace,
specified by a redundant dictionary of weighted vertex positions. Minimizing a
nonlinear functional that regulates the example manifold, the suggested
approach supports local-rigid deformations of articulated objects, as well as
nearly isometric embeddings of smooth shapes. A real-time non-rigid deformation
system is demonstrated, and a shape completion and partial registration
framework is introduced. These applications can recover a target pose and
implicit inverse kinematics from a small number of examples and just a few
vertex positions. The result reconstruction is more accurate compared to
state-of-the-art reduced deformable models.
</summary>
    <author>
      <name>Alon Shtern</name>
    </author>
    <author>
      <name>Matan Sela</name>
    </author>
    <author>
      <name>Ron Kimmel</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08313v1</id>
    <updated>2016-09-27T08:35:14Z</updated>
    <published>2016-09-27T08:35:14Z</published>
    <title>Unsupervised Co-segmentation of 3D Shapes via Functional Maps</title>
    <summary>  We present an unsupervised method for co-segmentation of a set of 3D shapes
from the same class with the aim of segmenting the input shapes into consistent
semantic parts and establishing their correspondence across the set. Starting
from meaningful pre-segmentation of all given shapes individually, we construct
the correspondence between same candidate parts and obtain the labels via
functional maps. And then, we use these labels to mark the input shapes and
obtain results of co-segmentation. The core of our algorithm is to seek for an
optimal correspondence between semantically similar parts through functional
maps and mark such shape parts. Experimental results on the benchmark datasets
show the efficiency of this method and comparable accuracy to the
state-of-the-art algorithms.
</summary>
    <author>
      <name>Jun Yang</name>
    </author>
    <author>
      <name>Zhenhua Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08685v2</id>
    <updated>2016-11-08T20:13:56Z</updated>
    <published>2016-09-27T22:00:56Z</published>
    <title>Understanding and Exploiting Object Interaction Landscapes</title>
    <summary>  Interactions play a key role in understanding objects and scenes, for both
virtual and real world agents. We introduce a new general representation for
proximal interactions among physical objects that is agnostic to the type of
objects or interaction involved. The representation is based on tracking
particles on one of the participating objects and then observing them with
sensors appropriately placed in the interaction volume or on the interaction
surfaces. We show how to factorize these interaction descriptors and project
them into a particular participating object so as to obtain a new functional
descriptor for that object, its interaction landscape, capturing its observed
use in a spatio-temporal framework. Interaction landscapes are independent of
the particular interaction and capture subtle dynamic effects in how objects
move and behave when in functional use. Our method relates objects based on
their function, establishes correspondences between shapes based on functional
key points and regions, and retrieves peer and partner objects with respect to
an interaction.
</summary>
    <author>
      <name>Sören Pirk</name>
    </author>
    <author>
      <name>Vojtech Krs</name>
    </author>
    <author>
      <name>Kaimo Hu</name>
    </author>
    <author>
      <name>Suren Deepak Rajasekaran</name>
    </author>
    <author>
      <name>Hao Kang</name>
    </author>
    <author>
      <name>Bedrich Benes</name>
    </author>
    <author>
      <name>Yusuke Yoshiyasu</name>
    </author>
    <author>
      <name>Leonidas J. Guibas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01691v1</id>
    <updated>2016-10-05T23:49:21Z</updated>
    <published>2016-10-05T23:49:21Z</published>
    <title>Towards a Drone Cinematographer: Guiding Quadrotor Cameras using Visual
  Composition Principles</title>
    <summary>  We present a system to capture video footage of human subjects in the real
world. Our system leverages a quadrotor camera to automatically capture
well-composed video of two subjects. Subjects are tracked in a large-scale
outdoor environment using RTK GPS and IMU sensors. Then, given the tracked
state of our subjects, our system automatically computes static shots based on
well-established visual composition principles and canonical shots from
cinematography literature. To transition between these static shots, we
calculate feasible, safe, and visually pleasing transitions using a novel
real-time trajectory planning algorithm. We evaluate the performance of our
tracking system, and experimentally show that RTK GPS significantly outperforms
conventional GPS in capturing a variety of canonical shots. Lastly, we
demonstrate our system guiding a consumer quadrotor camera autonomously
capturing footage of two subjects in a variety of use cases. This is the first
end-to-end system that enables people to leverage the mobility of quadrotors,
as well as the knowledge of expert filmmakers, to autonomously capture
high-quality footage of people in the real world.
</summary>
    <author>
      <name>Niels Joubert</name>
    </author>
    <author>
      <name>Jane L. E</name>
    </author>
    <author>
      <name>Dan B Goldman</name>
    </author>
    <author>
      <name>Floraine Berthouzoz</name>
    </author>
    <author>
      <name>Mike Roberts</name>
    </author>
    <author>
      <name>James A. Landay</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03525v4</id>
    <updated>2018-12-05T16:32:14Z</updated>
    <published>2016-10-11T20:51:48Z</published>
    <title>Polynomial methods for Procedural Terrain Generation</title>
    <summary>  A new method is presented, allowing for the generation of 3D terrain and
texture from coherent noise. The method is significantly faster than prevailing
fractal brownian motion approaches, while producing results of equivalent
quality. The algorithm is derived through a systematic approach that
generalizes to an arbitrary number of spatial dimensions and gradient
smoothness. The results are compared, in terms of performance and quality, to
fundamental and efficient gradient noise methods widely used in the domain of
fast terrain generation: Perlin noise and OpenSimplex noise. Finally, to
objectively quantify the degree of realism of the results, a fractal analysis
of the generated landscapes is performed and compared to real terrain data.
</summary>
    <author>
      <name>Yann Thorimbert</name>
    </author>
    <author>
      <name>Bastien Chopard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03525v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03525v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07368v2</id>
    <updated>2016-10-31T17:36:37Z</updated>
    <published>2016-10-24T11:34:13Z</published>
    <title>Simplification of Multi-Scale Geometry using Adaptive Curvature Fields</title>
    <summary>  We present a novel algorithm to compute multi-scale curvature fields on
triangle meshes. Our algorithm is based on finding robust mean curvatures using
the ball neighborhood, where the radius of a ball corresponds to the scale of
the features. The essential problem is to find a good radius for each ball to
obtain a reliable curvature estimation. We propose an algorithm that finds
suitable radii in an automatic way. In particular, our algorithm is applicable
to meshes produced by image-based reconstruction systems. These meshes often
contain geometric features at various scales, for example if certain regions
have been captured in greater detail. We also show how such a multi-scale
curvature field can be converted to a density field and used to guide
applications like mesh simplification.
</summary>
    <author>
      <name>Patrick Seemann</name>
    </author>
    <author>
      <name>Simon Fuhrmann</name>
    </author>
    <author>
      <name>Stefan Guthe</name>
    </author>
    <author>
      <name>Fabian Langguth</name>
    </author>
    <author>
      <name>Michael Goesele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07368v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07368v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09992v1</id>
    <updated>2016-10-31T16:08:42Z</updated>
    <published>2016-10-31T16:08:42Z</published>
    <title>Deconfliction and Surface Generation from Bathymetry Data Using LR
  B-splines</title>
    <summary>  A set of bathymetry point clouds acquired by different measurement techniques
at different times, having different accuracy and varying patterns of points,
are approximated by an LR B-spline surface. The aim is to represent the sea
bottom with good accuracy and at the same time reduce the data size
considerably. In this process the point clouds must be cleaned by selecting the
"best" points for surface generation. This cleaning process is called
deconfliction, and we use a rough approximation of the combined point clouds as
a reference surface to select a consistent set of points. The reference surface
is updated with the selected points to create an accurate approximation. LR
B-splines is the selected surface format due to its suitability for adaptive
refinement and approximation, and its ability to represent local detail without
a global increase in the data size of the surface
</summary>
    <author>
      <name>Vibeke Skytt</name>
    </author>
    <author>
      <name>Quillon Harpham</name>
    </author>
    <author>
      <name>Tor Dokken</name>
    </author>
    <author>
      <name>Heidi E. I. Dahl</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01765v1</id>
    <updated>2016-11-06T12:26:37Z</updated>
    <published>2016-11-06T12:26:37Z</published>
    <title>A Survey on 3D CAD model quality assurance and testing tools</title>
    <summary>  A new taxonomy of issues related to CAD model quality is presented, which
distinguishes between explicit and procedural models. For each type of model,
morphologic, syntactic, and semantic errors are characterized. The taxonomy was
validated successfully when used to classify quality testing tools, which are
aimed at detecting and repairing data errors that may affect the
simplification, interoperability, and reusability of CAD models. The study
shows that low semantic level errors that hamper simplification are reasonably
covered in explicit representations, although many CAD quality testers are
still unaffordable for Small and Medium Enterprises, both in terms of cost and
training time. Interoperability has been reasonably solved by standards like
STEP AP 203 and AP214, but model reusability is not feasible in explicit
representations. Procedural representations are promising, as interactive
modeling editors automatically prevent most morphologic errors derived from
unsuitable modeling strategies. Interoperability problems between procedural
representations are expected to decrease dramatically with STEP AP242. Higher
semantic aspects of quality such as assurance of design intent, however, are
hardly supported by current CAD quality testers.
</summary>
    <author>
      <name>C. González-Lluch</name>
    </author>
    <author>
      <name>P. Company</name>
    </author>
    <author>
      <name>M. Contero</name>
    </author>
    <author>
      <name>J. D. Camba</name>
    </author>
    <author>
      <name>R. Plumed</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2016.10.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2016.10.003" rel="related"/>
    <link href="http://arxiv.org/abs/1611.01765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01990v2</id>
    <updated>2017-06-25T17:45:00Z</updated>
    <published>2016-11-07T11:11:10Z</published>
    <title>Hamiltonian operator for spectral shape analysis</title>
    <summary>  Many shape analysis methods treat the geometry of an object as a metric space
that can be captured by the Laplace-Beltrami operator. In this paper, we
propose to adapt the classical Hamiltonian operator from quantum mechanics to
the field of shape analysis. To this end we study the addition of a potential
function to the Laplacian as a generator for dual spaces in which shape
processing is performed. We present a general optimization approach for solving
variational problems involving the basis defined by the Hamiltonian using
perturbation theory for its eigenvectors. The suggested operator is shown to
produce better functional spaces to operate with, as demonstrated on different
shape analysis tasks.
</summary>
    <author>
      <name>Yoni Choukroun</name>
    </author>
    <author>
      <name>Alon Shtern</name>
    </author>
    <author>
      <name>Alex Bronstein</name>
    </author>
    <author>
      <name>Ron Kimmel</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01990v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01990v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03079v1</id>
    <updated>2016-11-08T23:22:59Z</updated>
    <published>2016-11-08T23:22:59Z</published>
    <title>Fractal Art Generation using GPUs</title>
    <summary>  Fractal image generation algorithms exhibit extreme parallelizability. Using
general purpose graphics processing unit (GPU) programming to implement
escape-time algorithms for Julia sets of functions,parallel methods generate
visually attractive fractal images much faster than traditional methods. Vastly
improved speeds are achieved using this method of computation, which allow
real-time generation and display of images. A comparison is made between
sequential and parallel implementations of the algorithm. An application
created by the authors demonstrates using the increased speed to create dynamic
imaging of fractals where the user may explore paths of parameter values
corresponding to a given function's Mandelbrot set. Examples are given of
artistic and mathematical insights gained by experiencing fractals
interactively and from the ability to sample the parameter space quickly and
comprehensively.
</summary>
    <author>
      <name>Will. D. Mayfield</name>
    </author>
    <author>
      <name>Justin. C. Eiland</name>
    </author>
    <author>
      <name>Taylor. J. Hutyra</name>
    </author>
    <author>
      <name>Matt. C. Paulsen</name>
    </author>
    <author>
      <name>Bryant. M. Wyatt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03677v2</id>
    <updated>2017-04-07T08:48:44Z</updated>
    <published>2016-11-11T12:20:43Z</published>
    <title>Primal-Dual Optimization for Fluids</title>
    <summary>  We apply a novel optimization scheme from the image processing and machine
learning areas, a fast Primal-Dual method, to achieve controllable and
realistic fluid simulations. While our method is generally applicable to many
problems in fluid simulations, we focus on the two topics of fluid guiding and
separating solid-wall boundary conditions. Each problem is posed as an
optimization problem and solved using our method, which contains acceleration
schemes tailored to each problem. In fluid guiding, we are interested in
partially guiding fluid motion to exert control while preserving fluid
characteristics. With our method, we achieve explicit control over both
large-scale motions and small-scale details which is valuable for many
applications, such as level-of-detail adjustment (after running the coarse
simulation), spatially varying guiding strength, domain modification, and
resimulation with different fluid parameters. For the separating solid-wall
boundary conditions problem, our method effectively eliminates unrealistic
artifacts of fluid crawling up solid walls and sticking to ceilings, requiring
few changes to existing implementations. We demonstrate the fast convergence of
our Primal-Dual method with a variety of test cases for both model problems.
</summary>
    <author>
      <name>Tiffany Inglis</name>
    </author>
    <author>
      <name>Marie-Lena Eckert</name>
    </author>
    <author>
      <name>James Gregson</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.13084</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.13084" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 18 figures, supplemental video
  https://www.youtube.com/watch?v=Pgbat5MXo8Q</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03677v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03677v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.8; I.3.7; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08947v1</id>
    <updated>2016-11-28T01:03:57Z</updated>
    <published>2016-11-28T01:03:57Z</published>
    <title>Navigable videos for presenting scientific data on head-mounted displays</title>
    <summary>  Immersive, stereoscopic viewing enables scientists to better analyze the
spatial structures of visualized physical phenomena. However, their findings
cannot be properly presented in traditional media, which lack these core
attributes. Creating a presentation tool that captures this environment poses
unique challenges, namely related to poor viewing accessibility. Immersive
scientific renderings often require high-end equipment, which can be
impractical to obtain. We address these challenges with our authoring tool and
navigational interface, which is designed for affordable head-mounted displays.
With the authoring tool, scientists can show salient data features as connected
360{\deg} video paths, resulting in a "choose-your-own-adventure" experience.
Our navigational interface features bidirectional video playback for added
viewing control when users traverse the tailor-made content. We evaluate our
system's benefits by authoring case studies on several data sets and conducting
a usability study on the navigational interface's design. In summary, our
approach provides scientists an immersive medium to visually present their
research to the intended audience--spanning from students to colleagues--on
affordable virtual reality headsets.
</summary>
    <author>
      <name>Jacqueline Chu</name>
    </author>
    <author>
      <name>Leonardo Ferrer</name>
    </author>
    <author>
      <name>Min Shih</name>
    </author>
    <author>
      <name>Kwan-Liu Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01944v2</id>
    <updated>2017-01-23T22:00:34Z</updated>
    <published>2016-12-06T18:42:44Z</published>
    <title>Porous Structure Design in Tissue Engineering Using Anisotropic Radial
  Basis Function</title>
    <summary>  Development of additive manufacturing in last decade greatly improves tissue
engineering. During the manufacturing of porous scaffold, simplified but
functionally equivalent models are getting focused for practically reasons.
Scaffolds can be classified into regular porous scaffolds and irregular porous
scaffolds. Several methodologies are developed to design these scaffolds. A
novel method is proposed in this paper using anisotropic radial basis function
(ARBF) interpolation. This is method uses geometric models such as volumetric
meshes as input and proves to be flexible because geometric models are able to
capture the characteristics of complex tissues easily. Moreover, this method is
straightforward and easy to implement.
</summary>
    <author>
      <name>Ke Liu</name>
    </author>
    <author>
      <name>Ye Guo</name>
    </author>
    <author>
      <name>Zeyun Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-03801-4_8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-03801-4_8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Wisconsin Milwaukee</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Visual Computing (ISVC) 2018, Advances
  in Visual Computing, Lecture Notes in Computer Science, vol. 11241, pp. 79-90</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.01944v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01944v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04336v1</id>
    <updated>2016-12-13T20:03:36Z</updated>
    <published>2016-12-13T20:03:36Z</published>
    <title>A Qualitative and Quantitative Evaluation of 8 Clear Sky Models</title>
    <summary>  We provide a qualitative and quantitative evaluation of 8 clear sky models
used in Computer Graphics. We compare the models with each other as well as
with measurements and with a reference model from the physics community. After
a short summary of the physics of the problem, we present the measurements and
the reference model, and how we "invert" it to get the model parameters. We
then give an overview of each CG model, and detail its scope, its algorithmic
complexity, and its results using the same parameters as in the reference
model. We also compare the models with a perceptual study. Our quantitative
results confirm that the less simplifications and approximations are used to
solve the physical equations, the more accurate are the results. We conclude
with a discussion of the advantages and drawbacks of each model, and how to
further improve their accuracy.
</summary>
    <author>
      <name>Eric Bruneton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2016.2622272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2016.2622272" rel="related"/>
    <link href="http://arxiv.org/abs/1612.04336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05395v7</id>
    <updated>2017-04-28T06:43:30Z</updated>
    <published>2016-12-16T08:41:06Z</published>
    <title>Charted Metropolis Light Transport</title>
    <summary>  In this manuscript, inspired by a simpler reformulation of primary sample
space Metropolis light transport, we derive a novel family of general Markov
chain Monte Carlo algorithms called charted Metropolis-Hastings, that
introduces the notion of sampling charts to extend a given sampling domain and
making it easier to sample the desired target distribution and escape from
local maxima through coordinate changes. We further apply the novel algorithms
to light transport simulation, obtaining a new type of algorithm called charted
Metropolis light transport, that can be seen as a bridge between primary sample
space and path space Metropolis light transport. The new algorithms require to
provide only right inverses of the sampling functions, a property that we
believe crucial to make them practical in the context of light transport
simulation. We further propose a method to integrate density estimation into
this framework through a novel scheme that uses it as an independence sampler.
</summary>
    <author>
      <name>Jacopo Pantaleoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3072959.3073677</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3072959.3073677" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures - to be published in the SIGGRAPH 2017
  proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05395v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05395v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.08731v4</id>
    <updated>2017-07-23T08:17:44Z</updated>
    <published>2016-12-20T14:06:20Z</published>
    <title>Quantum Optimal Transport for Tensor Field Processing</title>
    <summary>  This article introduces a new notion of optimal transport (OT) between tensor
fields, which are measures whose values are positive semidefinite (PSD)
matrices. This "quantum" formulation of OT (Q-OT) corresponds to a relaxed
version of the classical Kantorovich transport problem, where the fidelity
between the input PSD-valued measures is captured using the geometry of the
Von-Neumann quantum entropy. We propose a quantum-entropic regularization of
the resulting convex optimization problem, which can be solved efficiently
using an iterative scaling algorithm. This method is a generalization of the
celebrated Sinkhorn algorithm to the quantum setting of PSD matrices. We extend
this formulation and the quantum Sinkhorn algorithm to compute barycenters
within a collection of input tensor fields. We illustrate the usefulness of the
proposed approach on applications to procedural noise generation, anisotropic
meshing, diffusion tensor imaging and spectral texture synthesis.
</summary>
    <author>
      <name>Gabriel Peyré</name>
    </author>
    <author>
      <name>Lenaïc Chizat</name>
    </author>
    <author>
      <name>François-Xavier Vialard</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <link href="http://arxiv.org/abs/1612.08731v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08731v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03754v2</id>
    <updated>2017-01-16T23:13:01Z</updated>
    <published>2017-01-13T17:48:44Z</published>
    <title>LayerBuilder: Layer Decomposition for Interactive Image and Video Color
  Editing</title>
    <summary>  Exploring and editing colors in images is a common task in graphic design and
photography. However, allowing for interactive recoloring while preserving
smooth color blends in the image remains a challenging problem. We present
LayerBuilder, an algorithm that decomposes an image or video into a linear
combination of colored layers to facilitate color-editing applications. These
layers provide an interactive and intuitive means for manipulating individual
colors. Our approach reduces color layer extraction to a fast iterative linear
system. Layer Builder uses locally linear embedding, which represents pixels as
linear combinations of their neighbors, to reduce the number of variables in
the linear solve and extract layers that can better preserve color blending
effects. We demonstrate our algorithm on recoloring a variety of images and
videos, and show its overall effectiveness in recoloring quality and time
complexity compared to previous approaches. We also show how this
representation can benefit other applications, such as automatic recoloring
suggestion, texture synthesis, and color-based filtering.
</summary>
    <author>
      <name>Sharon Lin</name>
    </author>
    <author>
      <name>Matthew Fisher</name>
    </author>
    <author>
      <name>Angela Dai</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; added reference to Tan et al. 2016 and more image
  attribution</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03754v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03754v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06507v2</id>
    <updated>2017-02-02T17:59:10Z</updated>
    <published>2017-01-23T17:05:22Z</published>
    <title>Plausible Shading Decomposition For Layered Photo Retouching</title>
    <summary>  Photographers routinely compose multiple manipulated photos of the same scene
(layers) into a single image, which is better than any individual photo could
be alone. Similarly, 3D artists set up rendering systems to produce layered
images to contain only individual aspects of the light transport, which are
composed into the final result in post-production. Regrettably, both approaches
either take considerable time to capture, or remain limited to synthetic
scenes. In this paper, we suggest a system to allow decomposing a single image
into a plausible shading decomposition (PSD) that approximates effects such as
shadow, diffuse illumination, albedo, and specular shading. This decomposition
can then be manipulated in any off-the-shelf image manipulation software and
recomposited back. We perform such a decomposition by learning a convolutional
neural network trained using synthetic data. We demonstrate the effectiveness
of our decomposition on synthetic (i.e., rendered) and real data (i.e.,
photographs), and use them for common photo manipulation, which are nearly
impossible to perform otherwise from single images.
</summary>
    <author>
      <name>Carlo Innamorati</name>
    </author>
    <author>
      <name>Tobias Ritschel</name>
    </author>
    <author>
      <name>Tim Weyrich</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06507v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06507v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.0; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07110v1</id>
    <updated>2017-01-24T23:35:09Z</updated>
    <published>2017-01-24T23:35:09Z</published>
    <title>By chance is not enough: Preserving relative density through non uniform
  sampling</title>
    <summary>  Dealing with visualizations containing large data set is a challenging issue
and, in the field of Information Visualization, almost every visual technique
reveals its drawback when visualizing large number of items. To deal with this
problem we introduce a formal environment, modeling in a virtual space the
image features we are interested in (e.g, absolute and relative density,
clusters, etc.) and we define some metrics able to characterize the image
decay. Such metrics drive our automatic techniques (i.e., not uniform sampling)
rescuing the image features and making them visible to the user. In this paper
we focus on 2D scatter-plots, devising a novel non uniform data sampling
strategy able to preserve in an effective way relative densities.
</summary>
    <author>
      <name>Enrico Bertini</name>
    </author>
    <author>
      <name>Giuseppe Santucci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: visual clutter, visual quality metrics, non-uniform
  sampling</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings - Eighth International Conference on Information
  Visualisation, IV 2004; London; United Kingdom; 14 July 2004 through 16 July
  2004; Category numberPR02177; Code 63599</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.07110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08893v2</id>
    <updated>2017-02-01T23:30:20Z</updated>
    <published>2017-01-31T02:37:19Z</published>
    <title>Stable and Controllable Neural Texture Synthesis and Style Transfer
  Using Histogram Losses</title>
    <summary>  Recently, methods have been proposed that perform texture synthesis and style
transfer by using convolutional neural networks (e.g. Gatys et al.
[2015,2016]). These methods are exciting because they can in some cases create
results with state-of-the-art quality. However, in this paper, we show these
methods also have limitations in texture quality, stability, requisite
parameter tuning, and lack of user controls. This paper presents a multiscale
synthesis pipeline based on convolutional neural networks that ameliorates
these issues. We first give a mathematical explanation of the source of
instabilities in many previous approaches. We then improve these instabilities
by using histogram losses to synthesize textures that better statistically
match the exemplar. We also show how to integrate localized style losses in our
multiscale framework. These losses can improve the quality of large features,
improve the separation of content and style, and offer artistic controls such
as paint by numbers. We demonstrate that our approach offers improved quality,
convergence in fewer iterations, and more stability over the optimization.
</summary>
    <author>
      <name>Eric Risser</name>
    </author>
    <author>
      <name>Pierre Wilmot</name>
    </author>
    <author>
      <name>Connelly Barnes</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01530v2</id>
    <updated>2018-05-03T18:56:42Z</updated>
    <published>2017-02-06T08:36:10Z</published>
    <title>Ray tracing method for stereo image synthesis using CUDA</title>
    <summary>  This paper presents a realization of the approach to spatial 3D stereo of
visualization of 3D images with use parallel Graphics processing unit (GPU).
The experiments of realization of synthesis of images of a 3D stage by a method
of trace of beams on GPU with Compute Unified Device Architecture (CUDA) have
shown that 60 % of the time is spent for the decision of a computing problem
approximately, the major part of time (40 %) is spent for transfer of data
between the central processing unit and GPU for computations and the
organization process of visualization. The study of the influence of increase
in the size of the GPU network at the speed of computations showed importance
of the correct task of structure of formation of the parallel computer network
and general mechanism of parallelization. Keywords: Volumetric 3D
visualization, stereo 3D visualization, ray tracing, parallel computing on GPU,
CUDA
</summary>
    <author>
      <name>Anas M. Al-Oraiqat</name>
    </author>
    <author>
      <name>S. A. Zori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 Figures, International Journal of Engineering Science and
  Technology 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01540v1</id>
    <updated>2017-02-06T09:38:20Z</updated>
    <published>2017-02-06T09:38:20Z</published>
    <title>Generalized 3D Voxel Image Synthesis Architecture for Volumetric Spatial
  Visualization</title>
    <summary>  A general concept of 3D volumetric visualization systems is described based
on 3D discrete voxel scenes (worlds) representation. Definitions of 3D discrete
voxel scene (world) basic elements and main steps of the image synthesis
algorithm are formulated. An algorithm for solving the problem of the voxelized
world 3D image synthesis, intended for the systems of volumetric spatial
visualization, is proposed. A computer-based architecture for 3D volumetric
visualization of 3D discrete voxel world is presented. On the basis of the
proposed overall concept of discrete voxel representation, the proposed
architecture successfully adapts the ray tracing technique for the synthesis of
3D volumetric images. Since it is algorithmically simple and effectively
supports parallelism, it can efficiently be implemented.
  Key words:Volumetric spatial visualization, 3D volumetric imagesynthesis,
discrete voxel world, ray tracing.
</summary>
    <author>
      <name>Anas M. Al-Oraiqat</name>
    </author>
    <author>
      <name>E. A. Bashkov</name>
    </author>
    <author>
      <name>S. A. Zori</name>
    </author>
    <author>
      <name>Aladdein M. Amro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 Figures, International Publisher for Advanced Scientific
  Journals 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03246v1</id>
    <updated>2017-02-10T16:37:55Z</updated>
    <published>2017-02-10T16:37:55Z</published>
    <title>Towards Developing an Easy-To-Use Scripting Environment for Animating
  Virtual Characters</title>
    <summary>  This paper presents the three scripting commands and main functionalities of
a novel character animation environment called CHASE. CHASE was developed for
enabling inexperienced programmers, animators, artists, and students to animate
in meaningful ways virtual reality characters. This is achieved by scripting
simple commands within CHASE. The commands identified, which are associated
with simple parameters, are responsible for generating a number of predefined
motions and actions of a character. Hence, the virtual character is able to
animate within a virtual environment and to interact with tasks located within
it. An additional functionality of CHASE is supplied. It provides the ability
to generate multiple tasks of a character, such as providing the user the
ability to generate scenario-related animated sequences. However, since
multiple characters may require simultaneous animation, the ability to script
actions of different characters at the same time is also provided.
</summary>
    <author>
      <name>Christos Mousas</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.04852v1</id>
    <updated>2017-02-16T03:49:46Z</updated>
    <published>2017-02-16T03:49:46Z</published>
    <title>Visualization and Analysis of Large-Scale, Tree-Based, Adaptive Mesh
  Refinement Simulations with Arbitrary Rectilinear Geometry</title>
    <summary>  We present here the first systematic treatment of the problems posed by the
visualization and analysis of large-scale, parallel adaptive mesh refinement
(AMR) simulations on an Eulerian grid. When compared to those obtained by
constructing an intermediate unstructured mesh with fully described
connectivity, our primary results indicate a gain of at least 80\% in terms of
memory footprint, with a better rendering while retaining similar execution
speed. In this article, we describe the key concepts that allow us to obtain
these results, together with the methodology that facilitates the design,
implementation, and optimization of algorithms operating directly on such
refined meshes. This native support for AMR meshes has been contributed to the
open source Visualization Toolkit (VTK). This work pertains to a broader
long-term vision, with the dual goal to both improve interactivity when
exploring such data sets in 2 and 3 dimensions, and optimize resource
utilization.
</summary>
    <author>
      <name>Guénolé Harel</name>
    </author>
    <author>
      <name>Jacques-Bernard Lekien</name>
    </author>
    <author>
      <name>Philippe P. Pébaÿ</name>
    </author>
    <link href="http://arxiv.org/abs/1702.04852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.04852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08680v1</id>
    <updated>2017-02-28T07:54:21Z</updated>
    <published>2017-02-28T07:54:21Z</published>
    <title>A Data-driven Approach for Furniture and Indoor Scene Colorization</title>
    <summary>  We present a data-driven approach that colorizes 3D furniture models and
indoor scenes by leveraging indoor images on the internet. Our approach is able
to colorize the furniture automatically according to an example image. The core
is to learn image-guided mesh segmentation to segment the model into different
parts according to the image object. Given an indoor scene, the system supports
colorization-by-example, and has the ability to recommend the colorization
scheme that is consistent with a user-desired color theme. The latter is
realized by formulating the problem as a Markov random field model that imposes
user input as an additional constraint. We contribute to the community a
hierarchically organized image-model database with correspondences between each
image and the corresponding model at the part-level. Our experiments and a user
study show that our system produces perceptually convincing results comparable
to those generated by interior designers.
</summary>
    <author>
      <name>Jie Zhu</name>
    </author>
    <author>
      <name>Yanwen Guo</name>
    </author>
    <author>
      <name>Han Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 16 figures, submission to IEEE TVCG</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.3.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00050v1</id>
    <updated>2017-02-28T20:47:47Z</updated>
    <published>2017-02-28T20:47:47Z</published>
    <title>SceneSeer: 3D Scene Design with Natural Language</title>
    <summary>  Designing 3D scenes is currently a creative task that requires significant
expertise and effort in using complex 3D design interfaces. This effortful
design process starts in stark contrast to the easiness with which people can
use language to describe real and imaginary environments. We present SceneSeer:
an interactive text to 3D scene generation system that allows a user to design
3D scenes using natural language. A user provides input text from which we
extract explicit constraints on the objects that should appear in the scene.
Given these explicit constraints, the system then uses a spatial knowledge base
learned from an existing database of 3D scenes and 3D object models to infer an
arrangement of the objects forming a natural scene matching the input
description. Using textual commands the user can then iteratively refine the
created scene by adding, removing, replacing, and manipulating objects. We
evaluate the quality of 3D scenes generated by SceneSeer in a perceptual
evaluation experiment where we compare against manually designed scenes and
simpler baselines for 3D scene generation. We demonstrate how the generated
scenes can be iteratively refined through simple natural language commands.
</summary>
    <author>
      <name>Angel X. Chang</name>
    </author>
    <author>
      <name>Mihail Eric</name>
    </author>
    <author>
      <name>Manolis Savva</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00061v1</id>
    <updated>2017-02-28T21:21:03Z</updated>
    <published>2017-02-28T21:21:03Z</published>
    <title>SceneSuggest: Context-driven 3D Scene Design</title>
    <summary>  We present SceneSuggest: an interactive 3D scene design system providing
context-driven suggestions for 3D model retrieval and placement. Using a
point-and-click metaphor we specify regions in a scene in which to
automatically place and orient relevant 3D models. Candidate models are ranked
using a set of static support, position, and orientation priors learned from 3D
scenes. We show that our suggestions enable rapid assembly of indoor scenes. We
perform a user study comparing suggestions to manual search and selection, as
well as to suggestions with no automatic orientation. We find that suggestions
reduce total modeling time by 32%, that orientation priors reduce time spent
re-orienting objects by 27%, and that context-driven suggestions reduce the
number of text queries by 50%.
</summary>
    <author>
      <name>Manolis Savva</name>
    </author>
    <author>
      <name>Angel X. Chang</name>
    </author>
    <author>
      <name>Maneesh Agrawala</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00521v1</id>
    <updated>2017-03-01T21:40:16Z</updated>
    <published>2017-03-01T21:40:16Z</published>
    <title>The Signals and Systems Approach to Animation</title>
    <summary>  Animation is ubiquitous in visualization systems, and a common technique for
creating these animations is the transition. In the transition approach,
animations are created by smoothly interpolating a visual attribute between a
start and end value, reaching the end value after a specified duration. This
approach works well when each transition for an attribute is allowed to finish
before the next is triggered, but performs poorly when a new transition is
triggered before the current transition has finished. In particular,
interruptions introduce velocity discontinuities, and frequent interruptions
can slow down the resulting animation. To solve these problems, we model the
problem of animation as a signal processing problem. In our technique,
animations are produced by transformations of signals, or functions over time.
In particular, an animation is produced by transforming an input signal, a
function from time to target attribute value, into an output signal, a function
from time to displayed attribute value. We show that well-known
signal-processing techniques can be applied to produce animations that are free
from velocity discontinuities even when interrupted.
</summary>
    <author>
      <name>Andrew McCaleb Reach</name>
    </author>
    <author>
      <name>Chris North</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07167v1</id>
    <updated>2017-03-17T10:28:00Z</updated>
    <published>2017-03-17T10:28:00Z</published>
    <title>Volumetric parametrization from a level set boundary representation with
  PHT Splines</title>
    <summary>  A challenge in isogeometric analysis is constructing analysis-suitable
volumetric meshes which can accurately represent the geometry of a given
physical domain. In this paper, we propose a method to derive a spline-based
representation of a domain of interest from voxel-based data. We show an
efficient way to obtain a boundary representation of the domain by a level-set
function. Then, we use the geometric information from the boundary (the normal
vectors and curvature) to construct a matching C1 representation with
hierarchical cubic splines. The approximation is done by a single template and
linear transformations (scaling, translations and rotations) without the need
for solving an optimization problem. We illustrate our method with several
examples in two and three dimensions, and show good performance on some
standard benchmark test problems.
</summary>
    <author>
      <name>Chiu Ling Chan</name>
    </author>
    <author>
      <name>Cosmin Anitescu</name>
    </author>
    <author>
      <name>Timon Rabczuk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2016.08.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2016.08.008" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer-Aided Design, Volume 82, January 2017, Pages 29-41</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.07167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10405v1</id>
    <updated>2017-03-30T11:06:02Z</updated>
    <published>2017-03-30T11:06:02Z</published>
    <title>Autocomplete 3D Sculpting</title>
    <summary>  Digital sculpting is a popular means to create 3D models but remains a
challenging task for many users. This can be alleviated by recent advances in
data-driven and procedural modeling, albeit bounded by the underlying data and
procedures. We propose a 3D sculpting system that assists users in freely
creating models without predefined scope. With a brushing interface similar to
common sculpting tools, our system silently records and analyzes users'
workflows, and predicts what they might or should do in the future to reduce
input labor or enhance output quality. Users can accept, ignore, or modify the
suggestions and thus maintain full control and individual style. They can also
explicitly select and clone past workflows over output model regions. Our key
idea is to consider how a model is authored via dynamic workflows in addition
to what it is shaped in static geometry, for more accurate analysis of user
intentions and more general synthesis of shape structures. The workflows
contain potential repetitions for analysis and synthesis, including user inputs
(e.g. pen strokes on a pressure sensing tablet), model outputs (e.g. extrusions
on an object surface), and camera viewpoints. We evaluate our method via user
feedbacks and authored models.
</summary>
    <author>
      <name>Mengqi Peng</name>
    </author>
    <author>
      <name>Jun Xing</name>
    </author>
    <author>
      <name>Li-Yi Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04456v2</id>
    <updated>2018-06-26T17:19:52Z</updated>
    <published>2017-04-14T15:28:37Z</published>
    <title>Liquid Splash Modeling with Neural Networks</title>
    <summary>  This paper proposes a new data-driven approach to model detailed splashes for
liquid simulations with neural networks. Our model learns to generate
small-scale splash detail for the fluid-implicit-particle method using training
data acquired from physically parametrized, high resolution simulations. We use
neural networks to model the regression of splash formation using a classifier
together with a velocity modifier. For the velocity modification, we employ a
heteroscedastic model. We evaluate our method for different spatial scales,
simulation setups, and solvers. Our simulation results demonstrate that our
model significantly improves visual fidelity with a large amount of realistic
droplet formation and yields splash detail much more efficiently than finer
discretizations.
</summary>
    <author>
      <name>Kiwon Um</name>
    </author>
    <author>
      <name>Xiangyu Hu</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Computer Graphics Forum, more information:
  https://ge.in.tum.de/publications/2018-mlflip-um/</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04456v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04456v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04610v1</id>
    <updated>2017-04-15T09:21:57Z</updated>
    <published>2017-04-15T09:21:57Z</published>
    <title>A learning-based approach for automatic image and video colorization</title>
    <summary>  In this paper, we present a color transfer algorithm to colorize a broad
range of gray images without any user intervention. The algorithm uses a
machine learning-based approach to automatically colorize grayscale images. The
algorithm uses the superpixel representation of the reference color images to
learn the relationship between different image features and their corresponding
color values. We use this learned information to predict the color value of
each grayscale image superpixel. As compared to processing individual image
pixels, our use of superpixels helps us to achieve a much higher degree of
spatial consistency as well as speeds up the colorization process. The
predicted color values of the gray-scale image superpixels are used to provide
a 'micro-scribble' at the centroid of the superpixels. These color scribbles
are refined by using a voting based approach. To generate the final
colorization result, we use an optimization-based approach to smoothly spread
the color scribble across all pixels within a superpixel. Experimental results
on a broad range of images and the comparison with existing state-of-the-art
colorization methods demonstrate the greater effectiveness of the proposed
algorithm.
</summary>
    <author>
      <name>Raj Kumar Gupta</name>
    </author>
    <author>
      <name>Alex Yong-Sang Chia</name>
    </author>
    <author>
      <name>Deepu Rajan</name>
    </author>
    <author>
      <name>Huang Zhiyong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics International - 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07854v4</id>
    <updated>2019-02-20T13:27:28Z</updated>
    <published>2017-04-25T18:21:42Z</published>
    <title>Generating Liquid Simulations with Deformation-aware Neural Networks</title>
    <summary>  We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.
</summary>
    <author>
      <name>Lukas Prantl</name>
    </author>
    <author>
      <name>Boris Bonev</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2019, further information and videos at
  https://ge.in.tum.de/publications/2017-prantl-defonn/</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07854v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07854v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00274v2</id>
    <updated>2017-05-12T21:48:29Z</updated>
    <published>2017-04-30T06:40:18Z</published>
    <title>Topologically Robust 3D Shape Matching via Gradual Deflation and
  Inflation</title>
    <summary>  Despite being vastly ignored in the literature, coping with topological noise
is an issue of increasing importance, especially as a consequence of the
increasing number and diversity of 3D polygonal models that are captured by
devices of different qualities or synthesized by algorithms of different
stabilities. One approach for matching 3D shapes under topological noise is to
replace the topology-sensitive geodesic distance with distances that are less
sensitive to topological changes. We propose an alternative approach utilising
gradual deflation (or inflation) of the shape volume, of which purpose is to
bring the pair of shapes to be matched to a \emph{comparable} topology before
the search for correspondences. Illustrative experiments using different
datasets demonstrate that as the level of topological noise increases, our
approach outperforms the other methods in the literature.
</summary>
    <author>
      <name>Asli Genctav</name>
    </author>
    <author>
      <name>Yusuf Sahillioglu</name>
    </author>
    <author>
      <name>Sibel Tari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Section 2 replaced</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01263v1</id>
    <updated>2017-05-03T06:03:08Z</updated>
    <published>2017-05-03T06:03:08Z</published>
    <title>The Iray Light Transport Simulation and Rendering System</title>
    <summary>  While ray tracing has become increasingly common and path tracing is well
understood by now, a major challenge lies in crafting an easy-to-use and
efficient system implementing these technologies. Following a purely
physically-based paradigm while still allowing for artistic workflows, the Iray
light transport simulation and rendering system allows for rendering complex
scenes by the push of a button and thus makes accurate light transport
simulation widely available. In this document we discuss the challenges and
implementation choices that follow from our primary design decisions,
demonstrating that such a rendering system can be made a practical, scalable,
and efficient real-world application that has been adopted by various companies
across many fields and is in use by many industry professionals today.
</summary>
    <author>
      <name>Alexander Keller</name>
    </author>
    <author>
      <name>Carsten Wächter</name>
    </author>
    <author>
      <name>Matthias Raab</name>
    </author>
    <author>
      <name>Daniel Seibert</name>
    </author>
    <author>
      <name>Dietger van Antwerpen</name>
    </author>
    <author>
      <name>Johann Korndörfer</name>
    </author>
    <author>
      <name>Lutz Kettner</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01425v2</id>
    <updated>2017-07-25T14:35:49Z</updated>
    <published>2017-05-03T13:41:49Z</published>
    <title>Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors</title>
    <summary>  We present a novel data-driven algorithm to synthesize high-resolution flow
simulations with reusable repositories of space-time flow data. In our work, we
employ a descriptor learning approach to encode the similarity between fluid
regions with differences in resolution and numerical viscosity. We use
convolutional neural networks to generate the descriptors from fluid data such
as smoke density and flow velocity. At the same time, we present a deformation
limiting patch advection method which allows us to robustly track deformable
fluid regions. With the help of this patch advection, we generate stable
space-time data sets from detailed fluids for our repositories. We can then use
our learned descriptors to quickly localize a suitable data set when running a
new simulation. This makes our approach very efficient, and resolution
independent. We will demonstrate with several examples that our method yields
volumes with very high effective resolutions, and non-dissipative small scale
details that naturally integrate into the motions of the underlying flow.
</summary>
    <author>
      <name>Mengyu Chu</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3072959.3073643</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3072959.3073643" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 17 figures, to appear at SIGGRAPH 2017, v2 only fixes small
  typos</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph.36, 4 (2017), 69:1-69:13</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.01425v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01425v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01661v1</id>
    <updated>2017-05-04T00:11:16Z</updated>
    <published>2017-05-04T00:11:16Z</published>
    <title>Learning Hierarchical Shape Segmentation and Labeling from Online
  Repositories</title>
    <summary>  We propose a method for converting geometric shapes into hierarchically
segmented parts with part labels. Our key idea is to train category-specific
models from the scene graphs and part names that accompany 3D shapes in public
repositories. These freely-available annotations represent an enormous,
untapped source of information on geometry. However, because the models and
corresponding scene graphs are created by a wide range of modelers with
different levels of expertise, modeling tools, and objectives, these models
have very inconsistent segmentations and hierarchies with sparse and noisy
textual tags. Our method involves two analysis steps. First, we perform a joint
optimization to simultaneously cluster and label parts in the database while
also inferring a canonical tag dictionary and part hierarchy. We then use this
labeled data to train a method for hierarchical segmentation and labeling of
new 3D shapes. We demonstrate that our method can mine complex information,
detecting hierarchies in man-made objects and their constituent parts,
obtaining finer scale details than existing alternatives. We also show that, by
performing domain transfer using a few supervised examples, our technique
outperforms fully-supervised techniques that require hundreds of
manually-labeled models.
</summary>
    <author>
      <name>Li Yi</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <author>
      <name>Aaron Hertzmann</name>
    </author>
    <author>
      <name>Vladimir G. Kim</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Ersin Yumer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3072959.3073652</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3072959.3073652" rel="related"/>
    <link href="http://arxiv.org/abs/1705.01661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03811v2</id>
    <updated>2017-09-21T09:35:30Z</updated>
    <published>2017-05-10T15:12:14Z</published>
    <title>From 3D Models to 3D Prints: an Overview of the Processing Pipeline</title>
    <summary>  Due to the wide diffusion of 3D printing technologies, geometric algorithms
for Additive Manufacturing are being invented at an impressive speed. Each
single step, in particular along the Process Planning pipeline, can now count
on dozens of methods that prepare the 3D model for fabrication, while analysing
and optimizing geometry and machine instructions for various objectives. This
report provides a classification of this huge state of the art, and elicits the
relation between each single algorithm and a list of desirable objectives
during Process Planning. The objectives themselves are listed and discussed,
along with possible needs for tradeoffs. Additive Manufacturing technologies
are broadly categorized to explicitly relate classes of devices and supported
features. Finally, this report offers an analysis of the state of the art while
discussing open and challenging problems from both an academic and an
industrial perspective.
</summary>
    <author>
      <name>Marco Livesu</name>
    </author>
    <author>
      <name>Stefano Ellero</name>
    </author>
    <author>
      <name>Jonás Martìnez</name>
    </author>
    <author>
      <name>Sylvain Lefebvre</name>
    </author>
    <author>
      <name>Marco Attene</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.13147</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.13147" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Union (EU); Horizon 2020; H2020-FoF-2015; RIA - Research and
  Innovation action; Grant agreement N. 680448</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06250v1</id>
    <updated>2017-05-12T01:23:55Z</updated>
    <published>2017-05-12T01:23:55Z</published>
    <title>Shape Classification using Spectral Graph Wavelets</title>
    <summary>  Spectral shape descriptors have been used extensively in a broad spectrum of
geometry processing applications ranging from shape retrieval and segmentation
to classification. In this pa- per, we propose a spectral graph wavelet
approach for 3D shape classification using the bag-of-features paradigm. In an
effort to capture both the local and global geometry of a 3D shape, we present
a three-step feature description framework. First, local descriptors are
extracted via the spectral graph wavelet transform having the Mexican hat
wavelet as a generating ker- nel. Second, mid-level features are obtained by
embedding lo- cal descriptors into the visual vocabulary space using the soft-
assignment coding step of the bag-of-features model. Third, a global descriptor
is constructed by aggregating mid-level fea- tures weighted by a geodesic
exponential kernel, resulting in a matrix representation that describes the
frequency of appearance of nearby codewords in the vocabulary. Experimental
results on two standard 3D shape benchmarks demonstrate the effective- ness of
the proposed classification approach in comparison with state-of-the-art
methods.
</summary>
    <author>
      <name>Majid Masoumi</name>
    </author>
    <author>
      <name>A. Ben Hamza</name>
    </author>
    <link href="http://arxiv.org/abs/1705.06250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.11050v2</id>
    <updated>2018-02-01T13:29:19Z</updated>
    <published>2017-05-31T12:10:32Z</published>
    <title>3D Mesh Segmentation via Multi-branch 1D Convolutional Neural Networks</title>
    <summary>  There is an increasing interest in applying deep learning to 3D mesh
segmentation. We observe that 1) existing feature-based techniques are often
slow or sensitive to feature resizing, 2) there are minimal comparative studies
and 3) techniques often suffer from reproducibility issue. This study
contributes in two ways. First, we propose a novel convolutional neural network
(CNN) for mesh segmentation. It uses 1D data, filters and a multi-branch
architecture for separate training of multi-scale features. Together with a
novel way of computing conformal factor (CF), our technique clearly
out-performs existing work. Secondly, we publicly provide implementations of
several deep learning techniques, namely, neural networks (NNs), autoencoders
(AEs) and CNNs, whose architectures are at least two layers deep. The
significance of this study is that it proposes a robust form of CF, offers a
novel and accurate CNN technique, and a comprehensive study of several deep
learning techniques for baseline comparison.
</summary>
    <author>
      <name>David George</name>
    </author>
    <author>
      <name>Xianghua Xie</name>
    </author>
    <author>
      <name>Gary KL Tam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.gmod.2018.01.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.gmod.2018.01.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures, 5 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Graphical Models 96 (2018) 1-10</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.11050v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.11050v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01021v2</id>
    <updated>2017-12-02T05:10:30Z</updated>
    <published>2017-06-04T03:28:48Z</published>
    <title>Where and Who? Automatic Semantic-Aware Person Composition</title>
    <summary>  Image compositing is a method used to generate realistic yet fake imagery by
inserting contents from one image to another. Previous work in compositing has
focused on improving appearance compatibility of a user selected foreground
segment and a background image (i.e. color and illumination consistency). In
this work, we instead develop a fully automated compositing model that
additionally learns to select and transform compatible foreground segments from
a large collection given only an input image background. To simplify the task,
we restrict our problem by focusing on human instance composition, because
human segments exhibit strong correlations with their background and because of
the availability of large annotated data. We develop a novel branching
Convolutional Neural Network (CNN) that jointly predicts candidate person
locations given a background image. We then use pre-trained deep feature
representations to retrieve person instances from a large segment database.
Experimental results show that our model can generate composite images that
look visually convincing. We also develop a user interface to demonstrate the
potential application of our method.
</summary>
    <author>
      <name>Fuwen Tan</name>
    </author>
    <author>
      <name>Crispin Bernier</name>
    </author>
    <author>
      <name>Benjamin Cohen</name>
    </author>
    <author>
      <name>Vicente Ordonez</name>
    </author>
    <author>
      <name>Connelly Barnes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01558v1</id>
    <updated>2017-06-05T23:20:20Z</updated>
    <published>2017-06-05T23:20:20Z</published>
    <title>QuickCSG: Fast Arbitrary Boolean Combinations of N Solids</title>
    <summary>  QuickCSG computes the result for general N-polyhedron boolean expressions
without an intermediate tree of solids. We propose a vertex-centric view of the
problem, which simplifies the identification of final geometric contributions,
and facilitates its spatial decomposition. The problem is then cast in a single
KD-tree exploration, geared toward the result by early pruning of any region of
space not contributing to the final surface. We assume strong regularity
properties on the input meshes and that they are in general position. This
simplifying assumption, in combination with our vertex-centric approach,
improves the speed of the approach. Complemented with a task-stealing
parallelization, the algorithm achieves breakthrough performance, one to two
orders of magnitude speedups with respect to state-of-the-art CPU algorithms,
on boolean operations over two to dozens of polyhedra. The algorithm also
outperforms GPU implementations with approximate discretizations, while
producing an output without redundant facets. Despite the restrictive
assumptions on the input, we show the usefulness of QuickCSG for applications
with large CSG problems and strong temporal constraints, e.g. modeling for 3D
printers, reconstruction from visual hulls and collision detection.
</summary>
    <author>
      <name>Matthijs Douze</name>
    </author>
    <author>
      <name>Jean-Sébastien Franco</name>
    </author>
    <author>
      <name>Bruno Raffin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03024v2</id>
    <updated>2017-06-12T08:18:21Z</updated>
    <published>2017-06-09T16:32:33Z</published>
    <title>A Physically Plausible Model for Rendering Highly Scattering Fluorescent
  Participating Media</title>
    <summary>  We present a novel extension of the path tracing algorithm that is capable of
treating highly scattering participating media in the presence of fluorescent
structures. The extension is based on the formulation of the full radiative
transfer equation when solved on a per-wavelength-basis, resulting in accurate
model and unbiased algorithm for rendering highly scattering fluorescent
participating media. The model accounts for the intrinsic properties of
fluorescent dyes including their absorption and emission spectra, molar
absorptivity and quantum yield and also their concentration. Our algorithm is
applied to render highly scattering isotropic fluorescent solutions under
different illumination conditions. The spectral performance of the model is
validated against emission spectra of different fluorescent dyes that are of
significance in spectroscopy.
</summary>
    <author>
      <name>Marwan Abdellah</name>
    </author>
    <author>
      <name>Ahmet Bilgili</name>
    </author>
    <author>
      <name>Stefan Eilemann</name>
    </author>
    <author>
      <name>Henry Markram</name>
    </author>
    <author>
      <name>Felix Schürmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03950v1</id>
    <updated>2017-06-13T08:21:02Z</updated>
    <published>2017-06-13T08:21:02Z</published>
    <title>Procedural Wang Tile Algorithm for Stochastic Wall Patterns</title>
    <summary>  The game and movie industries always face the challenge of reproducing
materials. This problem is tackled by combining illumination models and various
textures (painted or procedural patterns). Gnerating stochastic wall patterns
is crucial in the creation of a wide range of backgrounds (castles, temples,
ruins...). A specific Wang tile set was introduced previously to tackle this
problem, in a non-procedural fashion. Long lines may appear as visual
artifacts. We use this tile set in a new procedural algorithm to generate
stochastic wall patterns. For this purpose, we introduce specific hash
functions implementing a constrained Wang tiling. This technique makes possible
the generation of boundless textures while giving control over the maximum line
length. The algorithm is simple and easy to implement, and the wall structure
we get from the tiles allows to achieve visuals that reproduce all the small
details of artist painted walls.
</summary>
    <author>
      <name>Alexandre Derouet-Jourdan</name>
    </author>
    <author>
      <name>Marc Salvati</name>
    </author>
    <author>
      <name>Theo Jonchier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Transactions on Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04077v1</id>
    <updated>2017-06-12T05:17:18Z</updated>
    <published>2017-06-12T05:17:18Z</published>
    <title>Interactive Shape Perturbation</title>
    <summary>  We present a web application for the procedural generation of perturbations
of 3D models. We generate the perturbations by generating vertex shaders that
change the positions of vertices that make up the 3D model. The vertex shaders
are created with an interactive genetic algorithm, which displays to the user
the visual effect caused by each vertex shader, allows the user to select the
visual effect the user likes best, and produces a new generation of vertex
shaders using the user feedback as the fitness measure of the genetic
algorithm. We use genetic programming to represent each vertex shader as a
computer program. This paper presents details of requirements specification,
software architecture, high and low-level design, and prototype user interface.
We discuss the project's current status and development challenges.
</summary>
    <author>
      <name>Juan C. Quiroz</name>
    </author>
    <author>
      <name>Sergiu M. Dascalu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. arXiv admin note: substantial text overlap with
  arXiv:1608.05231</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computers and Their Applications, IJCA,
  Vol. 24, No. 1, March 2017, 8 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.04077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06918v1</id>
    <updated>2017-06-21T14:11:32Z</updated>
    <published>2017-06-21T14:11:32Z</published>
    <title>cGAN-based Manga Colorization Using a Single Training Image</title>
    <summary>  The Japanese comic format known as Manga is popular all over the world. It is
traditionally produced in black and white, and colorization is time consuming
and costly. Automatic colorization methods generally rely on greyscale values,
which are not present in manga. Furthermore, due to copyright protection,
colorized manga available for training is scarce. We propose a manga
colorization method based on conditional Generative Adversarial Networks
(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of
training images, our method requires only a single colorized reference image
for training, avoiding the need of a large dataset. Colorizing manga using
cGANs can produce blurry results with artifacts, and the resolution is limited.
We therefore also propose a method of segmentation and color-correction to
mitigate these issues. The final results are sharp, clear, and in high
resolution, and stay true to the character's original color scheme.
</summary>
    <author>
      <name>Paulina Hensman</name>
    </author>
    <author>
      <name>Kiyoharu Aizawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10, 68U05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.10098v1</id>
    <updated>2017-06-30T10:08:11Z</updated>
    <published>2017-06-30T10:08:11Z</published>
    <title>From Big Data to Big Displays: High-Performance Visualization at Blue
  Brain</title>
    <summary>  Blue Brain has pushed high-performance visualization (HPV) to complement its
HPC strategy since its inception in 2007. In 2011, this strategy has been
accelerated to develop innovative visualization solutions through increased
funding and strategic partnerships with other research institutions.
  We present the key elements of this HPV ecosystem, which integrates C++
visualization applications with novel collaborative display systems. We
motivate how our strategy of transforming visualization engines into services
enables a variety of use cases, not only for the integration with high-fidelity
displays, but also to build service oriented architectures, to link into web
applications and to provide remote services to Python applications.
</summary>
    <author>
      <name>Stefan Eilemann</name>
    </author>
    <author>
      <name>Marwan Abdellah</name>
    </author>
    <author>
      <name>Nicolas Antille</name>
    </author>
    <author>
      <name>Ahmet Bilgili</name>
    </author>
    <author>
      <name>Grigory Chevtchenko</name>
    </author>
    <author>
      <name>Raphael Dumusc</name>
    </author>
    <author>
      <name>Cyrille Favreau</name>
    </author>
    <author>
      <name>Juan Hernando</name>
    </author>
    <author>
      <name>Daniel Nachbaur</name>
    </author>
    <author>
      <name>Pawel Podhajski</name>
    </author>
    <author>
      <name>Jafet Villafranca</name>
    </author>
    <author>
      <name>Felix Schürmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISC 2017 Visualization at Scale workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.10098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00648v1</id>
    <updated>2017-06-01T11:51:37Z</updated>
    <published>2017-06-01T11:51:37Z</published>
    <title>Examplar-Based Face Colorization Using Image Morphing</title>
    <summary>  Colorization of gray-scale images relies on prior color information.
Examplar-based methods use a color image as source of such information. Then
the colors of the source image are transferred to the gray-scale image. In the
literature, this transfer is mainly guided by texture descriptors. Face images
usually contain few texture so that the common approaches frequently fail. In
this paper we propose a new method based on image morphing. This technique is
able to compute a correspondence map between images with similar shapes. It is
based on the geometric structure of the images rather than textures which is
more reliable for faces. Our numerical experiments show that our morphing based
approach clearly outperforms state-of-the-art methods.
</summary>
    <author>
      <name>Johannes Persch</name>
    </author>
    <author>
      <name>Fabien Pierre</name>
    </author>
    <author>
      <name>Gabriele Steidl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01170v2</id>
    <updated>2018-03-14T11:33:32Z</updated>
    <published>2017-07-04T22:47:40Z</published>
    <title>Direct interactive visualization of locally refined spline volumes for
  scalar and vector fields</title>
    <summary>  We present a novel approach enabling interactive visualization of volumetric
Locally Refined B-splines (LR-splines). To this end we propose a highly
efficient algorithm for direct visualization of scalar and vector fields given
by an LR-spline. In both cases, our main contribution to achieve interactive
frame rates is an acceleration structure for fast element look-up and a change
of basis for efficient evaluation. To further improve the efficiency, we
present a heuristic for adaptive sampling distance for the numerical
integration. A comparison with existing adaptive approaches is performed. The
algorithms are designed to fully utilize modern graphics processing unit (GPU)
capabilities. Important applications where LR-spline volumes emerge are given
for instance by approximation of large-scale simulation and sensor data, and
Isogeometric Analysis (IGA). We showcase interactive rendering achieved by our
approach on different representative use cases, stemming from simulations of
wind flow around a telescope, Magnetic Resonance (MR) imaging of a human brain,
and simulations of a fluidized bed used for mixing and coating particles in
industrial processes.
</summary>
    <author>
      <name>Franz G. Fuchs</name>
    </author>
    <author>
      <name>Oliver J. D. Barrowclough</name>
    </author>
    <author>
      <name>Jon M. Hjelmervik</name>
    </author>
    <author>
      <name>Heidi E. I. Dahl</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01732v1</id>
    <updated>2017-07-06T11:18:42Z</updated>
    <published>2017-07-06T11:18:42Z</published>
    <title>Nonlinear dance motion analysis and motion editing using Hilbert-Huang
  transform</title>
    <summary>  Human motions (especially dance motions) are very noisy, and it is hard to
analyze and edit the motions. To resolve this problem, we propose a new method
to decompose and modify the motions using the Hilbert-Huang transform (HHT).
First, HHT decomposes a chromatic signal into "monochromatic" signals that are
the so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode
Decomposition (EMD) [6]. After applying the Hilbert Transform to each IMF, the
instantaneous frequencies of the "monochromatic" signals can be obtained. The
HHT has the advantage to analyze non-stationary and nonlinear signals such as
human-joint-motions over FFT or Wavelet transform.
  In the present paper, we propose a new framework to analyze and extract some
new features from a famous Japanese threesome pop singer group called
"Perfume", and compare it with Waltz and Salsa dance. Using the EMD, their
dance motions can be decomposed into motion (choreographic) primitives or IMFs.
Therefore we can scale, combine, subtract, exchange, and modify those IMFs, and
can blend them into new dance motions self-consistently. Our analysis and
framework can lead to a motion editing and blending method to create a new
dance motion from different dance motions.
</summary>
    <author>
      <name>Ran Dong</name>
    </author>
    <author>
      <name>Dongsheng Cai</name>
    </author>
    <author>
      <name>Nobuyoshi Asai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3095140.3095175</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3095140.3095175" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures, Computer Graphics International 2017, Conference
  short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02596v2</id>
    <updated>2017-11-02T09:18:05Z</updated>
    <published>2017-07-09T16:03:30Z</published>
    <title>Localized Manifold Harmonics for Spectral Shape Analysis</title>
    <summary>  The use of Laplacian eigenfunctions is ubiquitous in a wide range of computer
graphics and geometry processing applications. In particular, Laplacian
eigenbases allow generalizing the classical Fourier analysis to manifolds. A
key drawback of such bases is their inherently global nature, as the Laplacian
eigenfunctions carry geometric and topological structure of the entire
manifold. In this paper, we introduce a new framework for local spectral shape
analysis. We show how to efficiently construct localized orthogonal bases by
solving an optimization problem that in turn can be posed as the
eigendecomposition of a new operator obtained by a modification of the standard
Laplacian. We study the theoretical and computational aspects of the proposed
framework and showcase our new construction on the classical problems of shape
approximation and correspondence. We obtain significant improvement compared to
classical Laplacian eigenbases as well as other alternatives for constructing
localized bases.
</summary>
    <author>
      <name>Simone Melzi</name>
    </author>
    <author>
      <name>Emanuele Rodolà</name>
    </author>
    <author>
      <name>Umberto Castellani</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Computer Graphics Forum</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04348v1</id>
    <updated>2017-07-13T22:56:46Z</updated>
    <published>2017-07-13T22:56:46Z</published>
    <title>Natural Boundary Conditions for Smoothing in Geometry Processing</title>
    <summary>  In geometry processing, smoothness energies are commonly used to model
scattered data interpolation, dense data denoising, and regularization during
shape optimization. The squared Laplacian energy is a popular choice of energy
and has a corresponding standard implementation: squaring the discrete
Laplacian matrix. For compact domains, when values along the boundary are not
known in advance, this construction bakes in low-order boundary conditions.
This causes the geometric shape of the boundary to strongly bias the solution.
For many applications, this is undesirable. Instead, we propose using the
squared Frobenious norm of the Hessian as a smoothness energy. Unlike the
squared Laplacian energy, this energy's natural boundary conditions (those that
best minimize the energy) correspond to meaningful high-order boundary
conditions. These boundary conditions model free boundaries where the shape of
the boundary should not bias the solution locally. Our analysis begins in the
smooth setting and concludes with discretizations using finite-differences on
2D grids or mixed finite elements for triangle meshes. We demonstrate the core
behavior of the squared Hessian as a smoothness energy for various tasks.
</summary>
    <author>
      <name>Oded Stein</name>
    </author>
    <author>
      <name>Eitan Grinspun</name>
    </author>
    <author>
      <name>Max Wardetzky</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04805v1</id>
    <updated>2017-07-16T01:42:41Z</updated>
    <published>2017-07-16T01:42:41Z</published>
    <title>A Streamline Selection Technique Overlaying with Isosurfaces</title>
    <summary>  Integration of scalar and vector visualization has been an interesting topic.
This paper presents a technique to appropriately select and display multiple
streamlines while overlaying with isosurfaces, aiming an integrated scalar and
vector field visualization. The technique visualizes a scalar field by multiple
semitransparent isosurfaces, and a vector field by multiple streamlines, while
the technique adequately selects the streamlines considering reduction of
cluttering among the isosurfaces and streamlines. The technique first selects
and renders isosurfaces, and then generates large number of streamlines from
randomly selected seed points. The technique evaluates each of the streamlines
according to their shapes on a 2D display space, distances to critical points
of the given vector fields, and occlusion by isosurfaces. It then selects the
specified number of highly evaluated streamlines. As a result, we can visualize
both scalar and vector fields as a set of view-independently selected
isosurfaces and view-dependently selected streamlines.
</summary>
    <author>
      <name>Shiho Furuya</name>
    </author>
    <author>
      <name>Takayuki Itoh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TopoInVis2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06683v2</id>
    <updated>2017-10-03T03:30:17Z</updated>
    <published>2017-07-20T19:13:55Z</published>
    <title>Visual Detection of Structural Changes in Time-Varying Graphs Using
  Persistent Homology</title>
    <summary>  Topological data analysis is an emerging area in exploratory data analysis
and data mining. Its main tool, persistent homology, has become a popular
technique to study the structure of complex, high-dimensional data. In this
paper, we propose a novel method using persistent homology to quantify
structural changes in time-varying graphs. Specifically, we transform each
instance of the time-varying graph into metric spaces, extract topological
features using persistent homology, and compare those features over time. We
provide a visualization that assists in time-varying graph exploration and
helps to identify patterns of behavior within the data. To validate our
approach, we conduct several case studies on real world data sets and show how
our method can find cyclic patterns, deviations from those patterns, and
one-time events in time-varying graphs. We also examine whether
persistence-based similarity measure as a graph metric satisfies a set of
well-established, desirable properties for graph metrics.
</summary>
    <author>
      <name>Mustafa Hajij</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <author>
      <name>Carlos Scheidegger</name>
    </author>
    <author>
      <name>Paul Rosen</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06683v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06683v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07070v2</id>
    <updated>2018-04-24T20:18:25Z</updated>
    <published>2017-07-21T23:28:53Z</published>
    <title>Steklov Spectral Geometry for Extrinsic Shape Analysis</title>
    <summary>  We propose using the Dirichlet-to-Neumann operator as an extrinsic
alternative to the Laplacian for spectral geometry processing and shape
analysis. Intrinsic approaches, usually based on the Laplace-Beltrami operator,
cannot capture the spatial embedding of a shape up to rigid motion, and many
previous extrinsic methods lack theoretical justification. Instead, we consider
the Steklov eigenvalue problem, computing the spectrum of the
Dirichlet-to-Neumann operator of a surface bounding a volume. A remarkable
property of this operator is that it completely encodes volumetric geometry. We
use the boundary element method (BEM) to discretize the operator, accelerated
by hierarchical numerical schemes and preconditioning; this pipeline allows us
to solve eigenvalue and linear problems on large-scale meshes despite the
density of the Dirichlet-to-Neumann discretization. We further demonstrate that
our operators naturally fit into existing frameworks for geometry processing,
making a shift from intrinsic to extrinsic geometry as simple as substituting
the Laplace-Beltrami operator with the Dirichlet-to-Neumann operator.
</summary>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Mirela Ben-Chen</name>
    </author>
    <author>
      <name>Iosif Polterovich</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Additional experiments added</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07070v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07070v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08323v3</id>
    <updated>2018-07-19T22:42:54Z</updated>
    <published>2017-07-26T08:50:14Z</published>
    <title>Pigmento: Pigment-Based Image Analysis and Editing</title>
    <summary>  The colorful appearance of a physical painting is determined by the
distribution of paint pigments across the canvas, which we model as a per-pixel
mixture of a small number of pigments with multispectral absorption and
scattering coefficients. We present an algorithm to efficiently recover this
structure from an RGB image, yielding a plausible set of pigments and a low RGB
reconstruction error. We show that under certain circumstances we are able to
recover pigments that are close to ground truth, while in all cases our results
are always plausible. Using our decomposition, we repose standard digital image
editing operations as operations in pigment space rather than RGB, with
interestingly novel results. We demonstrate tonal adjustments, selection
masking, cut-copy-paste, recoloring, palette summarization, and edge
enhancement.
</summary>
    <author>
      <name>Jianchao Tan</name>
    </author>
    <author>
      <name>Stephen DiVerdi</name>
    </author>
    <author>
      <name>Jingwan Lu</name>
    </author>
    <author>
      <name>Yotam Gingold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">add copyright to images; add acknowledgements, is accepted by IEEE
  Transactions on Visualization and Computer Graphics (IEEE TVCG)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08323v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08323v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08358v2</id>
    <updated>2017-07-28T10:57:27Z</updated>
    <published>2017-07-26T10:26:08Z</published>
    <title>Notes on optimal approximations for importance sampling</title>
    <summary>  In this manuscript, we derive optimal conditions for building function
approximations that minimize variance when used as importance sampling
estimators for Monte Carlo integration problems. Particularly, we study the
problem of finding the optimal projection $g$ of an integrand $f$ onto certain
classes of piecewise constant functions, in order to minimize the variance of
the unbiased importance sampling estimator $E_g[f/g]$, as well as the related
problem of finding optimal mixture weights to approximate and importance sample
a target mixture distribution $f = \sum_i \alpha_i f_i$ with components $f_i$
in a family $\mathcal{F}$, through a corresponding mixture of importance
sampling densities $g_i$ that are only approximately proportional to $f_i$. We
further show that in both cases the optimal projection is different from the
commonly used $\ell_1$ projection, and provide an intuitive explanation for the
difference.
</summary>
    <author>
      <name>Jacopo Pantaleoni</name>
    </author>
    <author>
      <name>Eric Heitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08360v1</id>
    <updated>2017-07-26T10:30:34Z</updated>
    <published>2017-07-26T10:30:34Z</published>
    <title>Discrete Geodesic Nets for Modeling Developable Surfaces</title>
    <summary>  We present a discrete theory for modeling developable surfaces as
quadrilateral meshes satisfying simple angle constraints. The basis of our
model is a lesser known characterization of developable surfaces as manifolds
that can be parameterized through orthogonal geodesics. Our model is simple,
local, and, unlike previous works, it does not directly encode the surface
rulings. This allows us to model continuous deformations of discrete
developable surfaces independently of their decomposition into torsal and
planar patches or the surface topology. We prove and experimentally demonstrate
strong ties to smooth developable surfaces, including a theorem stating that
every sampling of the smooth counterpart satisfies our constraints up to second
order. We further present an extension of our model that enables a local
definition of discrete isometry. We demonstrate the effectiveness of our
discrete model in a developable surface editing system, as well as computation
of an isometric interpolation between isometric discrete developable shapes.
</summary>
    <author>
      <name>Michael Rabinovich</name>
    </author>
    <author>
      <name>Tim Hoffmann</name>
    </author>
    <author>
      <name>Olga Sorkine-Hornung</name>
    </author>
    <link href="http://arxiv.org/abs/1707.08360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09366v1</id>
    <updated>2017-07-28T08:45:37Z</updated>
    <published>2017-07-28T08:45:37Z</published>
    <title>Continuous Global Optimization in Surface Reconstruction from an
  Oriented Point Cloud</title>
    <summary>  We introduce a continuous global optimization method to the field of surface
reconstruction from discrete noisy cloud of points with weak information on
orientation. The proposed method uses an energy functional combining flux-based
data-fit measures and a regularization term. A continuous convex relaxation
scheme assures the global minima of the geometric surface functional. The
reconstructed surface is implicitly represented by the binary segmentation of
vertices of a 3D uniform grid and a triangulated surface can be obtained by
extracting an appropriate isosurface. Unlike the discrete graph-cut solution,
the continuous global optimization entails advantages like memory requirements,
reduction of metrication errors for geometric quantities, allowing globally
optimal surface reconstruction at higher grid resolutions. We demonstrate the
performance of the proposed method on several oriented point clouds captured by
laser scanners. Experimental results confirm that our approach is robust to
noise, large holes and non-uniform sampling density under the condition of very
coarse orientation information.
</summary>
    <author>
      <name>Rongjiang Pan</name>
    </author>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2011.03.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2011.03.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Aided Design, Vol.43, No.8, pp.896-901, Elsevier, ISSN
  0010-4485, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.09366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09432v1</id>
    <updated>2017-07-28T22:33:01Z</updated>
    <published>2017-07-28T22:33:01Z</published>
    <title>Generation of concept-representative symbols</title>
    <summary>  The visual representation of concepts or ideas through the use of simple
shapes has always been explored in the history of Humanity, and it is believed
to be the origin of writing. We focus on computational generation of visual
symbols to represent concepts. We aim to develop a system that uses background
knowledge about the world to find connections among concepts, with the goal of
generating symbols for a given concept. We are also interested in exploring the
system as an approach to visual dissociation and visual conceptual blending.
This has a great potential in the area of Graphic Design as a tool to both
stimulate creativity and aid in brainstorming in projects such as logo,
pictogram or signage design.
</summary>
    <author>
      <name>João Miguel Cunha</name>
    </author>
    <author>
      <name>Pedro Martins</name>
    </author>
    <author>
      <name>Amílcar Cardoso</name>
    </author>
    <author>
      <name>Penousal Machado</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">cite as "Cunha, J. M., Martins, P., Cardoso, A., &amp; Machado, P.
  (2015). Generation of Concept-Representative Symbols. In ICCBR (Workshops).",
  Computational creativity, Computational generation, Concept representation,
  Visual representation</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03686v1</id>
    <updated>2017-08-11T19:58:46Z</updated>
    <published>2017-08-11T19:58:46Z</published>
    <title>Visualizing Time-Varying Particle Flows with Diffusion Geometry</title>
    <summary>  The tasks of identifying separation structures and clusters in flow data are
fundamental to flow visualization. Significant work has been devoted to these
tasks in flow represented by vector fields, but there are unique challenges in
addressing these tasks for time-varying particle data. The unstructured nature
of particle data, nonuniform and sparse sampling, and the inability to access
arbitrary particles in space-time make it difficult to define separation and
clustering for particle data. We observe that weaker notions of separation and
clustering through continuous measures of these structures are meaningful when
coupled with user exploration. We achieve this goal by defining a measure of
particle similarity between pairs of particles. More specifically, separation
occurs when spatially-localized particles are dissimilar, while clustering is
characterized by sets of particles that are similar to one another. To be
robust to imperfections in sampling we use diffusion geometry to compute
particle similarity. Diffusion geometry is parameterized by a scale that allows
a user to explore separation and clustering in a continuous manner. We
illustrate the benefits of our technique on a variety of 2D and 3D flow
datasets, from particles integrated in fluid simulations based on time-varying
vector fields, to particle-based simulations in astrophysics.
</summary>
    <author>
      <name>Matthew Berger</name>
    </author>
    <author>
      <name>Joshua A. Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 16 figures, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03748v1</id>
    <updated>2017-08-12T07:40:39Z</updated>
    <published>2017-08-12T07:40:39Z</published>
    <title>Calipso: Physics-based Image and Video Editing through CAD Model Proxies</title>
    <summary>  We present Calipso, an interactive method for editing images and videos in a
physically-coherent manner. Our main idea is to realize physics-based
manipulations by running a full physics simulation on proxy geometries given by
non-rigidly aligned CAD models. Running these simulations allows us to apply
new, unseen forces to move or deform selected objects, change physical
parameters such as mass or elasticity, or even add entire new objects that
interact with the rest of the underlying scene. In Calipso, the user makes
edits directly in 3D; these edits are processed by the simulation and then
transfered to the target 2D content using shape-to-image correspondences in a
photo-realistic rendering process. To align the CAD models, we introduce an
efficient CAD-to-image alignment procedure that jointly minimizes for rigid and
non-rigid alignment while preserving the high-level structure of the input
shape. Moreover, the user can choose to exploit image flow to estimate scene
motion, producing coherent physical behavior with ambient dynamics. We
demonstrate Calipso's physics-based editing on a wide range of examples
producing myriad physical behavior while preserving geometric and visual
consistency.
</summary>
    <author>
      <name>Nazim Haouchine</name>
    </author>
    <author>
      <name>Frederick Roy</name>
    </author>
    <author>
      <name>Hadrien Courtecuisse</name>
    </author>
    <author>
      <name>Matthias Nießner</name>
    </author>
    <author>
      <name>Stephane Cotin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03760v2</id>
    <updated>2018-11-05T12:04:56Z</updated>
    <published>2017-08-12T09:52:16Z</published>
    <title>Temporal Upsampling of Depth Maps Using a Hybrid Camera</title>
    <summary>  In recent years, consumer-level depth cameras have been adopted for various
applications. However, they often produce depth maps at only a moderately high
frame rate (approximately 30 frames per second), preventing them from being
used for applications such as digitizing human performance involving fast
motion. On the other hand, low-cost, high-frame-rate video cameras are
available. This motivates us to develop a hybrid camera that consists of a
high-frame-rate video camera and a low-frame-rate depth camera and to allow
temporal interpolation of depth maps with the help of auxiliary color images.
To achieve this, we develop a novel algorithm that reconstructs intermediate
depth maps and estimates scene flow simultaneously. We test our algorithm on
various examples involving fast, non-rigid motions of single or multiple
objects. Our experiments show that our scene flow estimation method is more
precise than a tracking-based method and the state-of-the-art techniques.
</summary>
    <author>
      <name>Ming-Ze Yuan</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <author>
      <name>Shihong Xia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2018.2812879</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2018.2812879" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics, 2018(13
  pages, 16 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03760v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03760v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06034v2</id>
    <updated>2019-11-07T01:37:34Z</updated>
    <published>2017-08-20T23:38:41Z</published>
    <title>Eccentricity Effects on Blur and Depth Perception</title>
    <summary>  Foveation and focus cue are the two most discussed topics on vision in
designing near-eye displays. Foveation reduces rendering load by omitting
spatial details in the content that the peripheral vision cannot appreciate;
Providing richer focal cue can resolve vergence-accommodation conflict thereby
lessening visual discomfort in using near-eye displays. We performed two
psychophysical experiments to investigate the relationship between foveation
and focus cue. The first study measured blur discrimination sensitivity as a
function of visual eccentricity, where we found discrimination thresholds
significantly lower than previously reported. The second study measured depth
discrimination threshold where we found a clear dependency on visual
eccentricity. We discuss the results from the two studies and suggest further
investigation.
</summary>
    <author>
      <name>Qi Sun</name>
    </author>
    <author>
      <name>Fu-Chung Huang</name>
    </author>
    <author>
      <name>Li-Yi Wei</name>
    </author>
    <author>
      <name>David Luebke</name>
    </author>
    <author>
      <name>Arie Kaufman</name>
    </author>
    <author>
      <name>Joohwan Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06695v1</id>
    <updated>2017-07-28T08:33:02Z</updated>
    <published>2017-07-28T08:33:02Z</published>
    <title>A two-level approach to implicit surface modeling with compactly
  supported radial basis functions</title>
    <summary>  We describe a two-level method for computing a function whose zero-level set
is the surface reconstructed from given points scattered over the surface and
associated with surface normal vectors. The function is defined as a linear
combination of compactly supported radial basis functions (CSRBFs). The method
preserves the simplicity and efficiency of implicit surface interpolation with
CSRBFs and the reconstructed implicit surface owns the attributes, which are
previously only associated with globally supported or globally regularized
radial basis functions, such as exhibiting less extra zero-level sets, suitable
for inside and outside tests. First, in the coarse scale approximation, we
choose basis function centers on a grid that covers the enlarged bounding box
of the given point set and compute their signed distances to the underlying
surface using local quadratic approximations of the nearest surface points.
Then a fitting to the residual errors on the surface points and additional
off-surface points is performed with fine scale basis functions. The final
function is the sum of the two intermediate functions and is a good
approximation of the signed distance field to the surface in the bounding box.
Examples of surface reconstruction and set operations between shapes are
provided.
</summary>
    <author>
      <name>Rongjiang Pan</name>
    </author>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Engineering with Computers 2011 27:299-307</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.06695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07391v1</id>
    <updated>2017-08-05T09:43:59Z</updated>
    <published>2017-08-05T09:43:59Z</published>
    <title>A Novel Stretch Energy Minimization Algorithm for Equiareal
  Parameterizations</title>
    <summary>  Surface parameterizations have been widely applied to computer graphics and
digital geometry processing. In this paper, we propose a novel stretch energy
minimization (SEM) algorithm for the computation of equiareal parameterizations
of simply connected open surfaces with a very small area distortion and a
highly improved computational efficiency. In addition, the existence of
nontrivial limit points of the SEM algorithm is guaranteed under some mild
assumptions of the mesh quality. Numerical experiments indicate that the
efficiency, accuracy, and robustness of the proposed SEM algorithm outperform
other state-of-the-art algorithms. Applications of the SEM on surface remeshing
and surface registration for simply connected open surfaces are demonstrated
thereafter. Thanks to the SEM algorithm, the computations for these
applications can be carried out efficiently and robustly.
</summary>
    <author>
      <name>Mei-Heng Yueh</name>
    </author>
    <author>
      <name>Wen-Wei Lin</name>
    </author>
    <author>
      <name>Chin-Tien Wu</name>
    </author>
    <author>
      <name>Shing-Tung Yau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08188v2</id>
    <updated>2017-09-07T23:37:31Z</updated>
    <published>2017-08-28T04:36:03Z</published>
    <title>Active Animations of Reduced Deformable Models with Environment
  Interactions</title>
    <summary>  We present an efficient spacetime optimization method to automatically
generate animations for a general volumetric, elastically deformable body. Our
approach can model the interactions between the body and the environment and
automatically generate active animations. We model the frictional contact
forces using contact invariant optimization and the fluid drag forces using a
simplified model. To handle complex objects, we use a reduced deformable model
and present a novel hybrid optimizer to search for the local minima
efficiently. This allows us to use long-horizon motion planning to
automatically generate animations such as walking, jumping, swimming, and
rolling. We evaluate the approach on different shapes and animations, including
deformable body navigation and combining with an open-loop controller for
realtime forward simulation.
</summary>
    <author>
      <name>Zherong Pan</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.08188v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08188v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01221v2</id>
    <updated>2017-09-07T21:26:02Z</updated>
    <published>2017-09-05T03:08:01Z</published>
    <title>MLSEB: Edge Bundling using Moving Least Squares Approximation</title>
    <summary>  Edge bundling methods can effectively alleviate visual clutter and reveal
high-level graph structures in large graph visualization. Researchers have
devoted significant efforts to improve edge bundling according to different
metrics. As the edge bundling family evolve rapidly, the quality of edge
bundles receives increasing attention in the literature accordingly. In this
paper, we present MLSEB, a novel method to generate edge bundles based on
moving least squares (MLS) approximation. In comparison with previous edge
bundling methods, we argue that our MLSEB approach can generate better results
based on a quantitative metric of quality, and also ensure scalability and the
efficiency for visualizing large graphs.
</summary>
    <author>
      <name>Jieting Wu</name>
    </author>
    <author>
      <name>Jianping Zeng</name>
    </author>
    <author>
      <name>Feiyu Zhu</name>
    </author>
    <author>
      <name>Hongfeng Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the Proceedings of the 25th International Symposium on
  Graph Drawing and Network Visualization (GD 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01250v1</id>
    <updated>2017-09-05T06:24:02Z</updated>
    <published>2017-09-05T06:24:02Z</published>
    <title>Sparse Data Driven Mesh Deformation</title>
    <summary>  Example-based mesh deformation methods are powerful tools for realistic shape
editing. However, existing techniques typically combine all the example
deformation modes, which can lead to overfitting, i.e. using a overly
complicated model to explain the user-specified deformation. This leads to
implausible or unstable deformation results, including unexpected global
changes outside the region of interest. To address this fundamental limitation,
we propose a sparse blending method that automatically selects a smaller number
of deformation modes to compactly describe the desired deformation. This along
with a suitably chosen deformation basis including spatially localized
deformation modes leads to significant advantages, including more meaningful,
reliable, and efficient deformations because fewer and localized deformation
modes are applied. To cope with large rotations, we develop a simple but
effective representation based on polar decomposition of deformation gradients,
which resolves the ambiguity of large global rotations using an
as-consistent-as-possible global optimization. This simple representation has a
closed form solution for derivatives, making it efficient for sparse localized
representation and thus ensuring interactive performance. Experimental results
show that our method outperforms state-of-the-art data-driven mesh deformation
methods, for both quality of results and efficiency.
</summary>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Ling-Xiao Zhang</name>
    </author>
    <author>
      <name>Leif Kobbelt</name>
    </author>
    <author>
      <name>Shihong Xia</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01638v1</id>
    <updated>2017-09-06T00:47:54Z</updated>
    <published>2017-09-06T00:47:54Z</published>
    <title>360 Panorama Cloning on Sphere</title>
    <summary>  In this paper, we address a novel problem of cloning a patch of the source
spherical panoramic image to the target spherical panoramic image, which we
call 360 panorama cloning. Considering the sphere geometry constraint embedded
in spherical panoramic images, we develop a coordinate-based method that
directly clones in the spherical domain. Our method neither differentiates the
polar regions and equatorial regions, nor identifies the boundaries in the
unrolled planar-formatted panorama. We discuss in depth two unique issues in
panorama cloning, i.e. preserving the patch's orientation, and handling the
large-patch cloning (covering over 180 field of view) which may suffer from
discoloration artifacts. As experimental results demonstrate, our method is
able to get visually pleasing cloning results and achieve real time cloning
performance.
</summary>
    <author>
      <name>Qiang Zhao</name>
    </author>
    <author>
      <name>Liang Wan</name>
    </author>
    <author>
      <name>Wei Feng</name>
    </author>
    <author>
      <name>Jiawan Zhang</name>
    </author>
    <author>
      <name>Tien-Tsin Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02782v1</id>
    <updated>2017-09-04T18:30:54Z</updated>
    <published>2017-09-04T18:30:54Z</published>
    <title>Global spectral graph wavelet signature for surface analysis of carpal
  bones</title>
    <summary>  In this paper, we present a spectral graph wavelet approach for shape
analysis of carpal bones of human wrist. We apply a metric called global
spectral graph wavelet signature for representation of cortical surface of the
carpal bone based on eigensystem of Laplace-Beltrami operator. Furthermore, we
propose a heuristic and efficient way of aggregating local descriptors of a
carpal bone surface to global descriptor. The resultant global descriptor is
not only isometric invariant, but also much more efficient and requires less
memory storage. We perform experiments on shape of the carpal bones of ten
women and ten men from a publicly-available database. Experimental results show
the excellency of the proposed GSGW compared to recent proposed GPS embedding
approach for comparing shapes of the carpal bones across populations.
</summary>
    <author>
      <name>Majid Masoumi</name>
    </author>
    <author>
      <name>A. Ben Hamza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1361-6560/aaa71a</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1361-6560/aaa71a" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1705.06250</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04304v2</id>
    <updated>2017-12-16T03:09:15Z</updated>
    <published>2017-09-13T12:59:48Z</published>
    <title>Mesh-based Autoencoders for Localized Deformation Component Analysis</title>
    <summary>  Spatially localized deformation components are very useful for shape analysis
and synthesis in 3D geometry processing. Several methods have recently been
developed, with an aim to extract intuitive and interpretable deformation
components. However, these techniques suffer from fundamental limitations
especially for meshes with noise or large-scale deformations, and may not
always be able to identify important deformation components. In this paper we
propose a novel mesh-based autoencoder architecture that is able to cope with
meshes with irregular topology. We introduce sparse regularization in this
framework, which along with convolutional operations, helps localize
deformations. Our framework is capable of extracting localized deformation
components from mesh data sets with large-scale deformations and is robust to
noise. It also provides a nonlinear approach to reconstruction of meshes using
the extracted basis, which is more effective than the current linear
combination approach. Extensive experiments show that our method outperforms
state-of-the-art methods in both qualitative and quantitative evaluations.
</summary>
    <author>
      <name>Qingyang Tan</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Shihong Xia</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04304v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04304v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04307v3</id>
    <updated>2018-03-29T03:27:39Z</updated>
    <published>2017-09-13T13:09:22Z</published>
    <title>Variational Autoencoders for Deforming 3D Mesh Models</title>
    <summary>  3D geometric contents are becoming increasingly popular. In this paper, we
study the problem of analyzing deforming 3D meshes using deep neural networks.
Deforming 3D meshes are flexible to represent 3D animation sequences as well as
collections of objects of the same category, allowing diverse shapes with
large-scale non-linear deformations. We propose a novel framework which we call
mesh variational autoencoders (mesh VAE), to explore the probabilistic latent
space of 3D surfaces. The framework is easy to train, and requires very few
training examples. We also propose an extended model which allows flexibly
adjusting the significance of different latent variables by altering the prior
distribution. Extensive experiments demonstrate that our general framework is
able to learn a reasonable representation for a collection of deformable
shapes, and produce competitive results for a variety of applications,
including shape generation, shape interpolation, shape space embedding and
shape exploration, outperforming state-of-the-art methods.
</summary>
    <author>
      <name>Qingyang Tan</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Shihong Xia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.04307v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04307v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07581v1</id>
    <updated>2017-09-22T03:17:34Z</updated>
    <published>2017-09-22T03:17:34Z</published>
    <title>Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D
  Generative Adversarial Network</title>
    <summary>  Automatic mesh-based shape generation is of great interest across a wide
range of disciplines, from industrial design to gaming, computer graphics and
various other forms of digital art. While most traditional methods focus on
primitive based model generation, advances in deep learning made it possible to
learn 3-dimensional geometric shape representations in an end-to-end manner.
However, most current deep learning based frameworks focus on the
representation and generation of voxel and point-cloud based shapes, making it
not directly applicable to design and graphics communities. This study
addresses the needs for automatic generation of mesh-based geometries, and
propose a novel framework that utilizes signed distance function representation
that generates detail preserving three-dimensional surface mesh by a deep
learning based approach.
</summary>
    <author>
      <name>Chiyu "Max" Jiang</name>
    </author>
    <author>
      <name>Philip Marcus</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08774v2</id>
    <updated>2023-10-07T03:35:18Z</updated>
    <published>2017-09-26T01:15:26Z</published>
    <title>Exploring the Design Space of Immersive Urban Analytics</title>
    <summary>  Recent years have witnessed the rapid development and wide adoption of
immersive head-mounted devices, such as HTC VIVE, Oculus Rift, and Microsoft
HoloLens. These immersive devices have the potential to significantly extend
the methodology of urban visual analytics by providing critical 3D context
information and creating a sense of presence. In this paper, we propose an
theoretical model to characterize the visualizations in immersive urban
analytics. Further more, based on our comprehensive and concise model, we
contribute a typology of combination methods of 2D and 3D visualizations that
distinguish between linked views, embedded views, and mixed views. We also
propose a supporting guideline to assist users in selecting a proper view under
certain circumstances by considering visual geometry and spatial distribution
of the 2D and 3D visualizations. Finally, based on existing works, possible
future research opportunities are explored and discussed.
</summary>
    <author>
      <name>Chen Zhu-Tian</name>
    </author>
    <author>
      <name>Yifang Wang</name>
    </author>
    <author>
      <name>Tianchen Sun</name>
    </author>
    <author>
      <name>Xiang Gao</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Zhigeng Pan</name>
    </author>
    <author>
      <name>Huamin Qu</name>
    </author>
    <author>
      <name>Yingcai Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages,11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09701v1</id>
    <updated>2017-09-27T18:58:56Z</updated>
    <published>2017-09-27T18:58:56Z</published>
    <title>Functional Characterization of Deformation Fields</title>
    <summary>  In this paper we present a novel representation for deformation fields of 3D
shapes, by considering the induced changes in the underlying metric. In
particular, our approach allows to represent a deformation field in a
coordinate-free way as a linear operator acting on real-valued functions
defined on the shape. Such a representation both provides a way to relate
deformation fields to other classical functional operators and enables analysis
and processing of deformation fields using standard linear-algebraic tools.
This opens the door to a wide variety of applications such as explicitly adding
extrinsic information into the computation of functional maps, intrinsic shape
symmetrization, joint deformation design through precise control of metric
distortion, and coordinate-free deformation transfer without requiring
pointwise correspondences. Our method is applicable to both surface and
volumetric shape representations and we guarantee the equivalence between the
operator-based and standard deformation field representation under mild
genericity conditions in the discrete setting. We demonstrate the utility of
our approach by comparing it with existing techniques and show how our
representation provides a powerful toolbox for a wide variety of challenging
problems.
</summary>
    <author>
      <name>Etienne Corman</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01802v1</id>
    <updated>2017-09-19T22:57:31Z</updated>
    <published>2017-09-19T22:57:31Z</published>
    <title>Automatic Structural Scene Digitalization</title>
    <summary>  In this paper, we present an automatic system for the analysis and labeling
of structural scenes, floor plan drawings in Computer-aided Design (CAD)
format. The proposed system applies a fusion strategy to detect and recognize
various components of CAD floor plans, such as walls, doors, windows and other
ambiguous assets. Technically, a general rule-based filter parsing method is
fist adopted to extract effective information from the original floor plan.
Then, an image-processing based recovery method is employed to correct
information extracted in the first step. Our proposed method is fully automatic
and real-time. Such analysis system provides high accuracy and is also
evaluated on a public website that, on average, archives more than ten
thousands effective uses per day and reaches a relatively high satisfaction
rate.
</summary>
    <author>
      <name>Rui Tang</name>
    </author>
    <author>
      <name>Yuhan Wang</name>
    </author>
    <author>
      <name>Darren Cosker</name>
    </author>
    <author>
      <name>Wenbin Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0187513</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0187513" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper submitted to PloS One</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02862v1</id>
    <updated>2017-10-08T17:43:23Z</updated>
    <published>2017-10-08T17:43:23Z</published>
    <title>Exploration of Heterogeneous Data Using Robust Similarity</title>
    <summary>  Heterogeneous data pose serious challenges to data analysis tasks, including
exploration and visualization. Current techniques often utilize dimensionality
reductions, aggregation, or conversion to numerical values to analyze
heterogeneous data. However, the effectiveness of such techniques to find
subtle structures such as the presence of multiple modes or detection of
outliers is hindered by the challenge to find the proper subspaces or prior
knowledge to reveal the structures. In this paper, we propose a generic
similarity-based exploration technique that is applicable to a wide variety of
datatypes and their combinations, including heterogeneous ensembles. The
proposed concept of similarity has a close connection to statistical analysis
and can be deployed for summarization, revealing fine structures such as the
presence of multiple modes, and detection of anomalies or outliers. We then
propose a visual encoding framework that enables the exploration of a
heterogeneous dataset in different levels of detail and provides insightful
information about both global and local structures. We demonstrate the utility
of the proposed technique using various real datasets, including ensemble data.
</summary>
    <author>
      <name>Mahsa Mirzargar</name>
    </author>
    <author>
      <name>Ross T. Whitaker</name>
    </author>
    <author>
      <name>Robert M. Kirby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Visualization in Data Science (VDS at IEEE VIS 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04867v3</id>
    <updated>2018-11-28T14:59:23Z</updated>
    <published>2017-10-13T10:34:34Z</published>
    <title>Single-image Tomography: 3D Volumes from 2D Cranial X-Rays</title>
    <summary>  As many different 3D volumes could produce the same 2D x-ray image, inverting
this process is challenging. We show that recent deep learning-based
convolutional neural networks can solve this task. As the main challenge in
learning is the sheer amount of data created when extending the 2D image into a
3D volume, we suggest firstly to learn a coarse, fixed-resolution volume which
is then fused in a second step with the input x-ray into a high-resolution
volume. To train and validate our approach we introduce a new dataset that
comprises of close to half a million computer-simulated 2D x-ray images of 3D
volumes scanned from 175 mammalian species. Applications of our approach
include stereoscopic rendering of legacy x-ray images, re-rendering of x-rays
including changes of illumination, view pose or geometry. Our evaluation
includes comparison to previous tomography work, previous learning methods
using our data, a user study and application to a set of real x-rays.
</summary>
    <author>
      <name>Philipp Henzler</name>
    </author>
    <author>
      <name>Volker Rasche</name>
    </author>
    <author>
      <name>Timo Ropinski</name>
    </author>
    <author>
      <name>Tobias Ritschel</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04867v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04867v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05592v2</id>
    <updated>2018-03-04T21:45:06Z</updated>
    <published>2017-10-16T09:39:46Z</published>
    <title>Robust Structure-based Shape Correspondence</title>
    <summary>  We present a robust method to find region-level correspondences between
shapes, which are invariant to changes in geometry and applicable across
multiple shape representations. We generate simplified shape graphs by jointly
decomposing the shapes, and devise an adapted graph-matching technique, from
which we infer correspondences between shape regions. The simplified shape
graphs are designed to primarily capture the overall structure of the shapes,
without reflecting precise information about the geometry of each region, which
enables us to find correspondences between shapes that might have significant
geometric differences. Moreover, due to the special care we take to ensure the
robustness of each part of our pipeline, our method can find correspondences
between shapes with different representations, such as triangular meshes and
point clouds. We demonstrate that the region-wise matching that we obtain can
be used to find correspondences between feature points, reveal the intrinsic
self-similarities of each shape, and even construct point-to-point maps across
shapes. Our method is both time and space efficient, leading to a pipeline that
is significantly faster than comparable approaches. We demonstrate the
performance of our approach through an extensive quantitative and qualitative
evaluation on several benchmarks where we achieve comparable or superior
performance to existing methods.
</summary>
    <author>
      <name>Yanir Kleiman</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Computer Graphics Forum, February 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.05592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06364v1</id>
    <updated>2017-10-17T16:18:04Z</updated>
    <published>2017-10-17T16:18:04Z</published>
    <title>Subtractive Color Mixture Computation</title>
    <summary>  Modeling subtractive color mixture (e.g., the way that paints mix) is
difficult when working with colors described only by three-dimensional color
space values, such as RGB. Although RGB values are sufficient to describe a
specific color sensation, they do not contain enough information to predict the
RGB color that would result from a subtractive mixture of two specified RGB
colors. Methods do exist for accurately modeling subtractive mixture, such as
the Kubelka-Munk equations, but require extensive spectrophotometric
measurements of the mixed components, making them unsuitable for many computer
graphics applications. This paper presents a strategy for modeling subtractive
color mixture given only the RGB information of the colors being mixed, written
for a general audience. The RGB colors are first transformed to generic,
representative spectral distributions, and then this spectral information is
used to perform the subtractive mixture, using the weighted
arithmetic-geometric mean. This strategy provides reasonable, representative
subtractive mixture colors with only modest computational effort and no
experimental measurements. As such, it provides a useful way to model
subtractive color mixture in computer graphics applications.
</summary>
    <author>
      <name>Scott Allen Burns</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06815v1</id>
    <updated>2017-10-18T16:28:08Z</updated>
    <published>2017-10-18T16:28:08Z</published>
    <title>Photo-Guided Exploration of Volume Data Features</title>
    <summary>  In this work, we pose the question of whether, by considering qualitative
information such as a sample target image as input, one can produce a rendered
image of scientific data that is similar to the target. The algorithm resulting
from our research allows one to ask the question of whether features like those
in the target image exists in a given dataset. In that way, our method is one
of imagery query or reverse engineering, as opposed to manual parameter
tweaking of the full visualization pipeline. For target images, we can use
real-world photographs of physical phenomena. Our method leverages deep neural
networks and evolutionary optimization. Using a trained similarity function
that measures the difference between renderings of a phenomenon and real-world
photographs, our method optimizes rendering parameters. We demonstrate the
efficacy of our method using a superstorm simulation dataset and images found
online. We also discuss a parallel implementation of our method, which was run
on NCSA's Blue Waters.
</summary>
    <author>
      <name>Mohammad Raji</name>
    </author>
    <author>
      <name>Alok Hota</name>
    </author>
    <author>
      <name>Robert Sisneros</name>
    </author>
    <author>
      <name>Peter Messmer</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09545v2</id>
    <updated>2019-07-16T22:50:24Z</updated>
    <published>2017-10-26T05:20:05Z</published>
    <title>A Generative Model for Volume Rendering</title>
    <summary>  We present a technique to synthesize and analyze volume-rendered images using
generative models. We use the Generative Adversarial Network (GAN) framework to
compute a model from a large collection of volume renderings, conditioned on
(1) viewpoint and (2) transfer functions for opacity and color. Our approach
facilitates tasks for volume analysis that are challenging to achieve using
existing rendering techniques such as ray casting or texture-based methods. We
show how to guide the user in transfer function editing by quantifying expected
change in the output image. Additionally, the generative model transforms
transfer functions into a view-invariant latent space specifically designed to
synthesize volume-rendered images. We use this space directly for rendering,
enabling the user to explore the space of volume-rendered images. As our model
is independent of the choice of volume rendering process, we show how to
analyze volume-rendered images produced by direct and global illumination
lighting, for a variety of volume datasets.
</summary>
    <author>
      <name>Matthew Berger</name>
    </author>
    <author>
      <name>Jixian Li</name>
    </author>
    <author>
      <name>Joshua A. Levine</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2018.2816059</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2018.2816059" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Vis. Comput. Graph. 25(4) (2019) 1636-1650</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.09545v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09545v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02333v2</id>
    <updated>2021-05-26T23:52:50Z</updated>
    <published>2017-12-06T18:56:47Z</published>
    <title>Bivariate Separable-Dimension Glyphs can Improve Visual Analysis of
  Holistic Features</title>
    <summary>  We introduce the cause of the inefficiency of bivariate glyphs by defining
the corresponding error. To recommend efficient and perceptually accurate
bivariate-glyph design, we present an empirical study of five bivariate glyphs
based on three psychophysics principles: integral-separable dimensions, visual
hierarchy, and pre-attentive pop out, to choose one integral pair
($length_y-length_x$), three separable pairs ($length-color$, $length-texture$,
$length_y-length_y$), and one redundant pair ($length_y-color/length_x$).
Twenty participants performed four tasks requiring: reading numerical values,
estimating ratio, comparing two points, and looking for extreme values among a
subset of points belonging to the same sub-group. The most surprising result
was that $length-texture$ was among the most effective methods, suggesting that
local spatial frequency features can lead to global pattern detection that
facilitate visual search in complex 3D structure. Our results also reveal the
following: $length-color$ bivariate glyphs led to the most accurate answers and
the least task execution time, while $length_y-length_x$ (integral) dimensions
were among the worst and is not recommended; it achieved high performance only
when pop-up color was added.
</summary>
    <author>
      <name>Henan Zhao</name>
    </author>
    <author>
      <name>Jian Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version has not been accepted by a conference or journal. We
  have run new studies thus this work is no longer valid</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.02333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03380v1</id>
    <updated>2017-12-09T12:03:53Z</updated>
    <published>2017-12-09T12:03:53Z</published>
    <title>A Deep Recurrent Framework for Cleaning Motion Capture Data</title>
    <summary>  We present a deep, bidirectional, recurrent framework for cleaning noisy and
incomplete motion capture data. It exploits temporal coherence and joint
correlations to infer adaptive filters for each joint in each frame. A single
model can be trained to denoise a heterogeneous mix of action types, under
substantial amounts of noise. A signal that has both noise and gaps is
preprocessed with a second bidirectional network that synthesizes missing
frames from surrounding context. The approach handles a wide variety of noise
types and long gaps, does not rely on knowledge of the noise distribution, and
operates in a streaming setting. We validate our approach through extensive
evaluations on noise both in joint angles and in joint positions, and show that
it improves upon various alternatives.
</summary>
    <author>
      <name>Utkarsh Mall</name>
    </author>
    <author>
      <name>G. Roshan Lal</name>
    </author>
    <author>
      <name>Siddhartha Chaudhuri</name>
    </author>
    <author>
      <name>Parag Chaudhuri</name>
    </author>
    <link href="http://arxiv.org/abs/1712.03380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05548v4</id>
    <updated>2019-10-04T18:20:35Z</updated>
    <published>2017-12-15T06:06:57Z</published>
    <title>Persistent Homology Guided Force-Directed Graph Layouts</title>
    <summary>  Graphs are commonly used to encode relationships among entities, yet their
abstractness makes them difficult to analyze. Node-link diagrams are popular
for drawing graphs, and force-directed layouts provide a flexible method for
node arrangements that use local relationships in an attempt to reveal the
global shape of the graph. However, clutter and overlap of unrelated structures
can lead to confusing graph visualizations. This paper leverages the persistent
homology features of an undirected graph as derived information for interactive
manipulation of force-directed layouts. We first discuss how to efficiently
extract 0-dimensional persistent homology features from both weighted and
unweighted undirected graphs. We then introduce the interactive persistence
barcode used to manipulate the force-directed graph layout. In particular, the
user adds and removes contracting and repulsing forces generated by the
persistent homology features, eventually selecting the set of persistent
homology features that most improve the layout. Finally, we demonstrate the
utility of our approach across a variety of synthetic and real datasets.
</summary>
    <author>
      <name>Ashley Suh</name>
    </author>
    <author>
      <name>Mustafa Hajij</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <author>
      <name>Carlos Scheidegger</name>
    </author>
    <author>
      <name>Paul Rosen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2019.2934802</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2019.2934802" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics, vol. 26,
  no. 1, pp. 697-707, Jan. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.05548v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05548v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05644v1</id>
    <updated>2017-12-15T12:44:27Z</updated>
    <published>2017-12-15T12:44:27Z</published>
    <title>graphTPP: A multivariate based method for interactive graph layout and
  analysis</title>
    <summary>  Graph layout is the process of creating a visual representation of a graph
through a node-link diagram. Node-attribute graphs have additional data stored
on the nodes which describe certain properties of the nodes called attributes.
Typical force-directed representations often produce hairball-like structures
that neither aid in understanding the graph's topology nor the relationship to
its attributes. The aim of this research was to investigate the use of
node-attributes for graph layout in order to improve the analysis process and
to give further insight into the graph over purely topological layouts. In this
article we present graphTPP, a graph based extension to targeted projection
pursuit (TPP) --- an interactive, linear, dimension reduction technique --- as
a method for graph layout and subsequent further analysis. TPP allows users to
control the projection and is optimised for clustering. Three case studies were
conducted in the areas of influence graphs, network security, and citation
networks. In each case graphTPP was shown to outperform standard force-directed
techniques and even other dimension reduction methods in terms of clarity of
clustered structure in the layout, the association between the structure and
the attributes and the insights elicited in each domain area.
</summary>
    <author>
      <name>Helen Gibson</name>
    </author>
    <author>
      <name>Paul Vickers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06654v1</id>
    <updated>2017-12-18T19:58:06Z</updated>
    <published>2017-12-18T19:58:06Z</published>
    <title>Graphic Narrative with Interactive Stylization Design</title>
    <summary>  We present a system to convert any set of images (e.g., a video clip or a
photo album) into a storyboard. We aim to create multiple pleasing graphic
representations of the content at interactive rates, so the user can explore
and find the storyboard (images, layout, and stylization) that best suits their
needs and taste. The main challenges of this work are: selecting the content
images, placing them into panels, and applying a stylization. For the latter,
we propose an interactive design tool to create new stylizations using a wide
range of filter blocks. This approach unleashes the creativity by allowing the
user to tune, modify, and intuitively design new sequences of filters. In
parallel to this manual design, we propose a novel procedural approach that
automatically assembles sequences of filters for innovative results. We aim to
keep the algorithm complexity as low as possible such that it can run
interactively on a mobile device. Our results include examples of styles
designed using both our interactive and procedural tools, as well as their
final composition into interesting and appealing storyboards.
</summary>
    <author>
      <name>Ignacio Garcia-Dorado</name>
    </author>
    <author>
      <name>Pascal Getreuer</name>
    </author>
    <author>
      <name>Madison Le</name>
    </author>
    <author>
      <name>Robin Debreuil</name>
    </author>
    <author>
      <name>Alex Kauffmann</name>
    </author>
    <author>
      <name>Peyman Milanfar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07485v1</id>
    <updated>2017-12-20T13:59:27Z</updated>
    <published>2017-12-20T13:59:27Z</published>
    <title>On the one method of a third-degree bezier type spline curve
  construction</title>
    <summary>  A method is proposed for constructing a spline curve of the Bezier type,
which is continuous along with its first derivative by a piecewise polynomial
function. Conditions for its existence and uniqueness are given. The
constructed curve lies inside the convex hull of the control points, and the
segments of the broken line connecting the control points are tangent to the
curve. To construct the curve, we use the approach proposed earlier for
constructing a parabolic spline. The idea is to use additional points with
unknown values of some function. Additional points are used as spline nodes,
and the function values are determined from the condition of the first
derivative continuity of a piecewise polynomial curve. In multiple
interpolation nodes, the function takes the given values and the values of the
first derivative, which are determined by the control points. Examples of
constructing a spline curve are given.
</summary>
    <author>
      <name>O. Stelia</name>
    </author>
    <author>
      <name>L. Potapenko</name>
    </author>
    <author>
      <name>I. Sirenko</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D07, 65D07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01155v1</id>
    <updated>2018-01-03T20:28:47Z</updated>
    <published>2018-01-03T20:28:47Z</published>
    <title>A Voxel-based Rendering Pipeline for Large 3D Line Sets</title>
    <summary>  We present a voxel-based rendering pipeline for large 3D line sets that
employs GPU ray-casting to achieve scalable rendering including transparency
and global illumination effects that cannot be achieved with GPU rasterization.
Even for opaque lines we demonstrate superior rendering performance compared to
GPU rasterization of lines, and when transparency is used we can interactively
render large amounts of lines that are infeasible to be rendered via
rasterization. To achieve this, we propose a direction-preserving encoding of
lines into a regular voxel grid, along with the quantization of directions
using face-to-face connectivity in this grid. On the regular grid structure,
parallel GPU ray-casting is used to determine visible fragments in correct
visibility order. To enable interactive rendering of global illumination
effects like low-frequency shadows and ambient occlusions, illumination
simulation is performed during ray-casting on a level-of-detail (LoD) line
representation that considers the number of lines and their lengths per voxel.
In this way we can render effects which are very difficult to render via GPU
rasterization. A detailed performance and quality evaluation compares our
approach to rasterization-based rendering of lines.
</summary>
    <author>
      <name>Mathias Kanzler</name>
    </author>
    <author>
      <name>Marc Rautenhaus</name>
    </author>
    <author>
      <name>Rüdiger Westermann</name>
    </author>
    <link href="http://arxiv.org/abs/1801.01155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02453v1</id>
    <updated>2018-01-08T14:51:57Z</updated>
    <published>2018-01-08T14:51:57Z</published>
    <title>Reversible Harmonic Maps between Discrete Surfaces</title>
    <summary>  Information transfer between triangle meshes is of great importance in
computer graphics and geometry processing. To facilitate this process, a smooth
and accurate map is typically required between the two meshes. While such maps
can sometimes be computed between nearly-isometric meshes, the more general
case of meshes with diverse geometries remains challenging. We propose a novel
approach for direct map computation between triangle meshes without mapping to
an intermediate domain, which optimizes for the harmonicity and reversibility
of the forward and backward maps. Our method is general both in the information
it can receive as input, e.g. point landmarks, a dense map or a functional map,
and in the diversity of the geometries to which it can be applied. We
demonstrate that our maps exhibit lower conformal distortion than the
state-of-the-art, while succeeding in correctly mapping key features of the
input shapes.
</summary>
    <author>
      <name>Danielle Ezuz</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <author>
      <name>Mirela Ben-Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04619v1</id>
    <updated>2018-01-14T22:33:50Z</updated>
    <published>2018-01-14T22:33:50Z</published>
    <title>Innovative Non-parametric Texture Synthesis via Patch Permutations</title>
    <summary>  In this work, we present a non-parametric texture synthesis algorithm capable
of producing plausible images without copying large tiles of the exemplar. We
focus on a simple synthesis algorithm, where we explore two patch match
heuristics; the well known Bidirectional Similarity (BS) measure and a
heuristic that finds near permutations using the solution of an entropy
regularized optimal transport (OT) problem. Innovative synthesis is achieved
with a small patch size, where global plausibility relies on the qualities of
the match. For OT, less entropic regularization also meant near permutations
and more plausible images. We examine the tile maps of the synthesized images,
showing that they are indeed novel superpositions of the input and contain few
or no verbatim copies. Synthesis results are compared to a statistical method,
namely a random convolutional network. We conclude by remarking simple
algorithms using only the input image can synthesize textures decently well and
call for more modest approaches in future algorithm design.
</summary>
    <author>
      <name>Ryan Webster</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06928v1</id>
    <updated>2018-01-22T01:18:06Z</updated>
    <published>2018-01-22T01:18:06Z</published>
    <title>Edge-Preserving Piecewise Linear Image Smoothing Using Piecewise
  Constant Filters</title>
    <summary>  Most image smoothing filters in the literature assume a piecewise constant
model of smoothed output images. However, the piecewise constant model
assumption can cause artifacts such as gradient reversals in applications such
as image detail enhancement, HDR tone mapping, etc. In these applications, a
piecewise linear model assumption is more preferred. In this paper, we propose
a simple yet very effective framework to smooth images of piecewise linear
model assumption using classical filters with the piecewise constant model
assumption. Our method is capable of handling with gradient reversal artifacts
caused by the piecewise constant model assumption. In addition, our method can
further help accelerated methods, which need to quantize image intensity values
into different bins, to achieve similar results that need a large number of
bins using a much smaller number of bins. This can greatly reduce the
computational cost. We apply our method to various classical filters with the
piecewise constant model assumption. Experimental results of several
applications show the effectiveness of the proposed method.
</summary>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Xiaogang Chen</name>
    </author>
    <author>
      <name>Xiaolin Huang</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.10040v1</id>
    <updated>2018-01-30T15:05:15Z</updated>
    <published>2018-01-30T15:05:15Z</published>
    <title>Animation-by-Demonstration Computer Puppetry Authoring Framework</title>
    <summary>  This paper presents Master of Puppets (MOP), an animation-by-demonstration
framework that allows users to control the motion of virtual characters
(puppets) in real time. In the first step, the user is asked to perform the
necessary actions that correspond to the character's motions. The user's
actions are recorded, and a hidden Markov model (HMM) is used to learn the
temporal profile of the actions. During the runtime of the framework, the user
controls the motions of the virtual character based on the specified
activities. The advantage of the MOP framework is that it recognizes and
follows the progress of the user's actions in real time. Based on the forward
algorithm, the method predicts the evolution of the user's actions, which
corresponds to the evolution of the character's motion. This method treats
characters as puppets that can perform only one motion at a time. This means
that combinations of motion segments (motion synthesis), as well as the
interpolation of individual motion sequences, are not provided as
functionalities. By implementing the framework and presenting several computer
puppetry scenarios, its efficiency and flexibility in animating virtual
characters is demonstrated.
</summary>
    <author>
      <name>Yaoyuan Cui</name>
    </author>
    <author>
      <name>Christos Mousas</name>
    </author>
    <link href="http://arxiv.org/abs/1801.10040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.10040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02673v2</id>
    <updated>2018-02-20T01:58:15Z</updated>
    <published>2018-02-07T23:37:20Z</published>
    <title>Position-Based Multi-Agent Dynamics for Real-Time Crowd Simulation (MiG
  paper)</title>
    <summary>  Exploiting the efficiency and stability of Position-Based Dynamics (PBD), we
introduce a novel crowd simulation method that runs at interactive rates for
hundreds of thousands of agents. Our method enables the detailed modeling of
per-agent behavior in a Lagrangian formulation. We model short-range and
long-range collision avoidance to simulate both sparse and dense crowds. On the
particles representing agents, we formulate a set of positional constraints
that can be readily integrated into a standard PBD solver. We augment the
tentative particle motions with planning velocities to determine the preferred
velocities of agents, and project the positions onto the constraint manifold to
eliminate colliding configurations. The local short-range interaction is
represented with collision and frictional contact between agents, as in the
discrete simulation of granular materials. We incorporate a cohesion model for
modeling collective behaviors and propose a new constraint for dealing with
potential future collisions. Our new method is suitable for use in interactive
games.
</summary>
    <author>
      <name>Tomer Weiss</name>
    </author>
    <author>
      <name>Alan Litteneker</name>
    </author>
    <author>
      <name>Chenfanfu Jiang</name>
    </author>
    <author>
      <name>Demetri Terzopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3136457.3136462</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3136457.3136462" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MIG 2017 Proceedings of the Tenth International Conference on
  Motion in Games</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.02673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04706v1</id>
    <updated>2018-02-13T16:21:59Z</updated>
    <published>2018-02-13T16:21:59Z</published>
    <title>Automatic thread painting generation</title>
    <summary>  ThreadTone is an NPR representation of an input image by half-toning using
threads on a circle. Current approaches to create ThreadTone paintings greedily
draw the chords on the circle. We introduce the concept of chord space, and
design a new algorithm to improve the quality of the thread painting. We use an
optimization process that estimates the fitness of every chord in the chord
space, and an error-diffusion based sampling process that selects a moderate
number of chords to produce the output painting. We used an image similarity
measure to evaluate the quality of our thread painting and also conducted a
user study. Our approach can produce high quality results on portraits,
sketches as well as cartoon pictures.
</summary>
    <author>
      <name>Xiao-Nan Fang</name>
    </author>
    <author>
      <name>Bin Liu</name>
    </author>
    <author>
      <name>Ariel Shamir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4310/CIS.2016.v16.n4.a3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4310/CIS.2016.v16.n4.a3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications in Information and Systems, Volume 16 (2016),
  Number 4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.04706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07487v2</id>
    <updated>2018-04-09T07:27:15Z</updated>
    <published>2018-02-21T10:00:09Z</published>
    <title>Sensor-topology based simplicial complex reconstruction</title>
    <summary>  We propose a new method for the reconstruction of simplicial complexes
(combining points, edges and triangles) from 3D point clouds from Mobile Laser
Scanning (MLS). Our main goal is to produce a reconstruction of a scene that is
adapted to the local geometry of objects. Our method uses the inherent topology
of the MLS sensor to define a spatial adjacency relationship between points. We
then investigate each possible connexion between adjacent points and filter
them by searching collinear structures in the scene, or structures
perpendicular to the laser beams. Next, we create triangles for each triplet of
self-connected edges. Last, we improve this method with a regularization based
on the co-planarity of triangles and collinearity of remaining edges. We
compare our results to a naive simplicial complexes reconstruction based on
edge length.
</summary>
    <author>
      <name>Stephane Guinard</name>
    </author>
    <author>
      <name>Bruno Vallet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 14 figures, ISPRS Technical Commission II Symposium 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07591v1</id>
    <updated>2018-01-01T13:55:09Z</updated>
    <published>2018-01-01T13:55:09Z</published>
    <title>Least Square Error Method Robustness of Computation: What is not usually
  considered and taught</title>
    <summary>  There are many practical applications based on the Least Square Error (LSE)
approximation. It is based on a square error minimization 'on a vertical' axis.
The LSE method is simple and easy also for analytical purposes. However, if
data span is large over several magnitudes or non-linear LSE is used, severe
numerical instability can be expected. The presented contribution describes a
simple method for large span of data LSE computation. It is especially
convenient if large span of data are to be processed, when the 'standard'
pseudoinverse matrix is ill conditioned. It is actually based on a LSE solution
using orthogonal basis vectors instead of orthonormal basis vectors. The
presented approach has been used for a linear regression as well as for
approximation using radial basis functions.
</summary>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15439/978-83-946253-7-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15439/978-83-946253-7-5" rel="related"/>
    <link href="http://arxiv.org/abs/1802.07591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07710v1</id>
    <updated>2018-02-21T18:33:04Z</updated>
    <published>2018-02-21T18:33:04Z</published>
    <title>Medical Volume Reconstruction Techniques</title>
    <summary>  Medical visualization is the use of computers to create 3D images from
medical imaging data sets, almost all surgery and cancer treatment in the
developed world relies on it.Volume visualization techniques includes
iso-surface visualization, mesh visualization and point cloud visualization
techniques, these techniques have revolutionized medicine. Much of modern
medicine relies on the 3D imaging that is possible with magnetic resonance
imaging (MRI) scanners, functional magnetic resonance imaging (fMRI)scanners,
positron emission tomography (PET) scanners, ultrasound imaging (US) scanners,
X-Ray scanners, bio-marker microscopy imaging scanners and computed tomography
(CT) scanners, which make 3D images out of 2D slices. The primary goal of this
report is the application-oriented optimization of existing volume rendering
methods providing interactive frame rates. Techniques are presented for
traditional alpha-blending rendering, surface-shaded display, maximum intensity
projection (MIP), and fast previewing with fully interactive parameter control.
Different preprocessing strategies are proposed for interactive iso-surface
rendering and fast previewing, such as the well-known marching cube algorithm.
</summary>
    <author>
      <name>Wenhui Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1206.1148 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08022v1</id>
    <updated>2018-02-22T13:04:42Z</updated>
    <published>2018-02-22T13:04:42Z</published>
    <title>Equalizer 2.0 - Convergence of a Parallel Rendering Framework</title>
    <summary>  Developing complex, real world graphics applications which leverage multiple
GPUs and computers for interactive 3D rendering tasks is a complex task. It
requires expertise in distributed systems and parallel rendering in addition to
the application domain itself. We present a mature parallel rendering framework
which provides a large set of features, algorithms and system integration for a
wide range of real-world research and industry applications. Using the
Equalizer parallel rendering framework, we show how a wide set of generic
algorithms can be integrated in the framework to help application scalability
and development in many different domains, highlighting how concrete
applications benefit from the diverse aspects and use cases of Equalizer. We
present novel parallel rendering algorithms, powerful abstractions for large
visualization setups and virtual reality, as well as new experimental results
for parallel rendering and data distribution.
</summary>
    <author>
      <name>Stefan Eilemann</name>
    </author>
    <author>
      <name>David Steiner</name>
    </author>
    <author>
      <name>Renato Pajarola</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.2.a; I.3.7.g; I.3.8; I.3.6.a; I.6.9.f" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00430v1</id>
    <updated>2018-03-01T15:09:12Z</updated>
    <published>2018-03-01T15:09:12Z</published>
    <title>Interactive Sound Rendering on Mobile Devices using Ray-Parameterized
  Reverberation Filters</title>
    <summary>  We present a new sound rendering pipeline that is able to generate plausible
sound propagation effects for interactive dynamic scenes. Our approach combines
ray-tracing-based sound propagation with reverberation filters using robust
automatic reverb parameter estimation that is driven by impulse responses
computed at a low sampling rate.We propose a unified spherical harmonic
representation of directional sound in both the propagation and auralization
modules and use this formulation to perform a constant number of convolution
operations for any number of sound sources while rendering spatial audio. In
comparison to previous geometric acoustic methods, we achieve a speedup of over
an order of magnitude while delivering similar audio to high-quality
convolution rendering algorithms. As a result, our approach is the first
capable of rendering plausible dynamic sound propagation effects on commodity
smartphones.
</summary>
    <author>
      <name>Carl Schissler</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04305v2</id>
    <updated>2018-10-26T08:28:47Z</updated>
    <published>2018-03-12T15:20:06Z</published>
    <title>Light Transport Simulation via Generalized Multiple Importance Sampling</title>
    <summary>  Multiple importance sampling (MIS) is employed to reduce variance of
estimators, but when sampling and weighting are not suitable to the integrand,
the estimators would have extra variance. Therefore, robust light transport
simulation algorithms based on Monte Carlo sampling for different types of
scenes are still uncompleted. In this paper, we address this problem by present
a general method, named generalized multiple importance sampling (GMIS), to
enhance the robustness of light transport simulation based on MIS. GMIS
combines different sampling techniques and weighting functions, extending MIS
to a more generalized framework. Meanwhile, we implement the GMIS in common
renderers and illustrate how it increase the robustness of light transport
simulation. Experiments show that, by applying GMIS, we obtain better
convergence performance and lower variance, and increase the rendering of
ambient light and specular shadow effects apparently.
</summary>
    <author>
      <name>Qi Liu</name>
    </author>
    <author>
      <name>Yiheng Zhang</name>
    </author>
    <author>
      <name>Lizhuang Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1803.04305v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04305v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04612v1</id>
    <updated>2018-03-13T04:29:54Z</updated>
    <published>2018-03-13T04:29:54Z</published>
    <title>Procedural Planetary Multi-resolution Terrain Generation for Games</title>
    <summary>  Terrains are the main part of an electronic game. To reduce human effort on
game development, procedural techniques are used to generate synthetic
terrains. However rendering a terrain is not a trivial task. Their rendering
techniques must be optimal for gaming. Specially planetary terrains, which must
account for precision and scale conversion. Multi-resolution models are best
fit to planetary terrains. An observer can change his point of view without
noticing any decrease in visual quality. There are several proposals regarding
real-time terrain rendering with multi-resolution models, and there are game
engines capable of generating large scale terrains with fixed resolution.
However for the best of our knowledge, it was noticed that there are no
techniques which combine both aspects. In this paper we present a new technique
capable of generating large-scale multi-resolution terrains, whichcan be
rendered and viewed at different scales. Rendering large scale models with high
definition and low scale areas with finer details added with the aid of
procedural content generation.
</summary>
    <author>
      <name>Ricardo B. D. d'Oliveira</name>
    </author>
    <author>
      <name>Antonio L. Apolinário Jr</name>
    </author>
    <link href="http://arxiv.org/abs/1803.04612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04656v1</id>
    <updated>2018-03-13T06:53:20Z</updated>
    <published>2018-03-13T06:53:20Z</published>
    <title>Effect of Eye Dominance on the Perception of Stereoscopic 3D Video</title>
    <summary>  Asymmetric schemes have widespread applications in the 3D video transmission
pipeline. The significance of eye dominance becomes a concern when designing
such schemes. In this paper, in order to investigate the effect of eye
dominance on the perceptual 3D video quality, a database of representative
asymmetric stereoscopic sequences is prepared and the overall 3D quality of
these sequences is evaluated through subjective experiments. Experiment results
showed that viewers find an asymmetric video more pleasant when the view with
higher quality is projected to their dominant eye. Moreover, the eye dominance
changes the mean opinion quality score by 16 % at most, a result caused by
slight asymmetric video compression. For all other representative types of
asymmetry, the statistical difference is much lower and in some cases even
negligible.
</summary>
    <author>
      <name>Amin Banitalebi-Dehkordi</name>
    </author>
    <author>
      <name>Mahsa T. Pourazad</name>
    </author>
    <author>
      <name>Panos Nasiopoulos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICIP, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.04656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06783v2</id>
    <updated>2018-12-04T06:30:27Z</updated>
    <published>2018-03-19T02:12:05Z</published>
    <title>Low Rank Matrix Approximation for Geometry Filtering</title>
    <summary>  We propose a robust normal estimation method for both point clouds and meshes
using a low rank matrix approximation algorithm. First, we compute a local
feature descriptor for each point and find similar, non-local neighbors that we
organize into a matrix. We then show that a low rank matrix approximation
algorithm can robustly estimate normals for both point clouds and meshes.
Furthermore, we provide a new filtering method for point cloud data to smooth
the position data to fit the estimated normals. We show applications of our
method to point cloud filtering, point set upsampling, surface reconstruction,
mesh denoising, and geometric texture removal. Our experiments show that our
method outperforms current methods in both visual quality and accuracy.
</summary>
    <author>
      <name>Xuequan Lu</name>
    </author>
    <author>
      <name>Scott Schaefer</name>
    </author>
    <author>
      <name>Jun Luo</name>
    </author>
    <author>
      <name>Lizhuang Ma</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated at early August, 2018, Corresponding author: Xuequan Lu
  (xuequanlu@ntu.edu.sg)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE TVCG 2020 (source code https://github.com/xuequanlu/lowrank)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.06783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11385v1</id>
    <updated>2018-03-30T08:34:26Z</updated>
    <published>2018-03-30T08:34:26Z</published>
    <title>H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis</title>
    <summary>  We present a novel spatial hashing based data structure to facilitate 3D
shape analysis using convolutional neural networks (CNNs). Our method well
utilizes the sparse occupancy of 3D shape boundary and builds hierarchical hash
tables for an input model under different resolutions. Based on this data
structure, we design two efficient GPU algorithms namely hash2col and col2hash
so that the CNN operations like convolution and pooling can be efficiently
parallelized. The spatial hashing is nearly minimal, and our data structure is
almost of the same size as the raw input. Compared with state-of-the-art
octree-based methods, our data structure significantly reduces the memory
footprint during the CNN training. As the input geometry features are more
compactly packed, CNN operations also run faster with our data structure. The
experiment shows that, under the same network structure, our method yields
comparable or better benchmarks compared to the state-of-the-art while it has
only one-third memory consumption. Such superior memory performance allows the
CNN to handle high-resolution shape analysis.
</summary>
    <author>
      <name>Tianjia Shao</name>
    </author>
    <author>
      <name>Yin Yang</name>
    </author>
    <author>
      <name>Yanlin Weng</name>
    </author>
    <author>
      <name>Qiming Hou</name>
    </author>
    <author>
      <name>Kun Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2018.2887262</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2018.2887262" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.11385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01225v3</id>
    <updated>2018-06-20T22:34:31Z</updated>
    <published>2018-04-04T03:22:49Z</published>
    <title>Palette-based image decomposition, harmonization, and color transfer</title>
    <summary>  We present a palette-based framework for color composition for visual
applications. Color composition is a critical aspect of visual applications in
art, design, and visualization. The color wheel is often used to explain
pleasing color combinations in geometric terms, and, in digital design, to
provide a user interface to visualize and manipulate colors. We abstract
relationships between palette colors as a compact set of axes describing
harmonic templates over perceptually uniform color wheels. Our framework
provides a basis for a variety of color-aware image operations, such as color
harmonization and color transfer, and can be applied to videos. To enable our
approach, we introduce an extremely scalable and efficient yet simple
palette-based image decomposition algorithm. Our approach is based on the
geometry of images in RGBXY-space. This new geometric approach is orders of
magnitude more efficient than previous work and requires no numerical
optimization. We demonstrate a real-time layer decomposition tool. After
preprocessing, our algorithm can decompose 6 MP images into layers in 20
milliseconds. We also conducted three large-scale, wide-ranging perceptual
studies on the perception of harmonic colors and harmonization algorithms.
</summary>
    <author>
      <name>Jianchao Tan</name>
    </author>
    <author>
      <name>Jose Echevarria</name>
    </author>
    <author>
      <name>Yotam Gingold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01225v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01225v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01253v2</id>
    <updated>2018-04-05T03:13:00Z</updated>
    <published>2018-04-04T06:35:09Z</published>
    <title>How could we ignore the lens and pupils of eyeballs: Metamaterial optics
  for retinal projection</title>
    <summary>  Retinal projection is required for xR applications that can deliver immersive
visual experience throughout the day. If general-purpose retinal projection
methods can be realized at a low cost, not only could the image be displayed on
the retina using less energy, but there is also the possibility of cutting off
the weight of projection unit itself from the AR goggles. Several retinal
projection methods have been previously proposed; however, as the lenses and
iris of the eyeball are in front of the retina, which is a limitation of the
eyeball, the proposal of retinal projection is generally fraught with narrow
viewing angles and small eyebox problems. In this short technical report, we
introduce ideas and samples of an optical system for solving the common
problems of retinal projection by using the metamaterial mirror (plane
symmetric transfer optical system). Using this projection method, the designing
of retinal projection can becomes easy, and if appropriate optics are
available, it would be possible to construct an optical system that allows the
quick follow-up of retinal projection hardware.
</summary>
    <author>
      <name>Yoichi Ochiai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03189v4</id>
    <updated>2018-06-26T20:15:53Z</updated>
    <published>2018-04-09T19:09:27Z</published>
    <title>Deep Painterly Harmonization</title>
    <summary>  Copying an element from a photo and pasting it into a painting is a
challenging task. Applying photo compositing techniques in this context yields
subpar results that look like a collage --- and existing painterly stylization
algorithms, which are global, perform poorly when applied locally. We address
these issues with a dedicated algorithm that carefully determines the local
statistics to be transferred. We ensure both spatial and inter-scale
statistical consistency and demonstrate that both aspects are key to generating
quality results. To cope with the diversity of abstraction levels and types of
paintings, we introduce a technique to adjust the parameters of the transfer
depending on the painting. We show that our algorithm produces significantly
better results than photo compositing or global stylization techniques and that
it enables creative painterly edits that would be otherwise difficult to
achieve.
</summary>
    <author>
      <name>Fujun Luan</name>
    </author>
    <author>
      <name>Sylvain Paris</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Kavita Bala</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03189v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03189v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03977v1</id>
    <updated>2018-04-11T13:34:31Z</updated>
    <published>2018-04-11T13:34:31Z</published>
    <title>Edge-based LBP description of surfaces with colorimetric patterns</title>
    <summary>  In this paper we target the problem of the retrieval of colour patterns over
surfaces. We generalize to surface tessellations the well known Local Binary
Pattern (LBP) descriptor for images. The key concept of the LBP is to code the
variability of the colour values around each pixel. In the case of a surface
tessellation we adopt rings around vertices that are obtained with a
sphere-mesh intersection driven by the edges of the mesh; for this reason, we
name our method edgeLBP. Experimental results are provided to show how this
description performs well for pattern retrieval, also when patterns come from
degraded and corrupted archaeological fragments.
</summary>
    <author>
      <name>Elia Moscoso Thompson</name>
    </author>
    <author>
      <name>Silvia Biasotti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics Workshop on 3D Object Retrieval 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45, 68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04619v1</id>
    <updated>2018-03-22T05:46:27Z</updated>
    <published>2018-03-22T05:46:27Z</published>
    <title>TomoReal: Tomographic Displays</title>
    <summary>  Since the history of display technologies began, people have dreamed an
ultimate 3D display system. In order to get close to the dream, 3D displays
should provide both of psychological and physiological cues for recognition of
depth information. However, it is challenging to satisfy the essential features
without sacrifice in conventional technical values including resolution, frame
rate, and eye-box. Here, we present a new type of 3D displays: tomographic
displays. We claim that tomographic displays may support extremely wide depth
of field, quasi-continuous accommodation, omni-directional motion parallax,
preserved resolution, full frame, and moderate field of view within enough
eye-box. Tomographic displays consist of focus-tunable optics, 2D display
panel, and fast spatially adjustable backlight. The synchronization of the
focus-tunable optics and the backlight enables the 2D display panel to express
the depth information. Tomographic displays have various applications including
tabletop 3D displays, head-up displays, and near-eye stereoscopes. In this
study, we implement a near-eye display named TomoReal, which is one of the most
promising application of tomographic displays. We conclude with the detailed
analysis and thorough discussion for tomographic displays, which would open a
new research field.
</summary>
    <author>
      <name>Seungjae Lee</name>
    </author>
    <author>
      <name>Youngjin Jo</name>
    </author>
    <author>
      <name>Dongheon Yoo</name>
    </author>
    <author>
      <name>Jaebum Cho</name>
    </author>
    <author>
      <name>Dukho Lee</name>
    </author>
    <author>
      <name>Byoungho Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06662v1</id>
    <updated>2018-04-18T11:40:59Z</updated>
    <published>2018-04-18T11:40:59Z</published>
    <title>A New Radial Basis Function Approximation with Reproduction</title>
    <summary>  Approximation of scattered geometric data is often a task in many engineering
problems. The Radial Basis Function (RBF) approximation is appropriate for
large scattered (unordered) datasets in d-dimensional space. This method is
useful for a higher dimension d&gt;=2, because the other methods require a
conversion of a scattered dataset to a semi-regular mesh using some
tessellation techniques, which is computationally expensive. The RBF
approximation is non-separable, as it is based on a distance of two points. It
leads to a solution of overdetermined Linear System of Equations (LSE). In this
paper a new RBF approximation method is derived and presented. The presented
approach is applicable for d dimensional cases in general.
</summary>
    <author>
      <name>Zuzana Majdisova</name>
    </author>
    <author>
      <name>Vaclav Skala</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IHCI 2016; GET 2016; and CGVCVIP 2016, Portugal,
  pp.215-222, ISBN 978-989-8533-52-4, IADIS Press, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.06662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06996v1</id>
    <updated>2018-04-19T05:15:03Z</updated>
    <published>2018-04-19T05:15:03Z</published>
    <title>Metamorphs: Bistable Planar Structures</title>
    <summary>  Extreme deformation can drastically morph a structure from one structural
form into another. Programming such deformation properties into the structure
is often challenging and in many cases an impossible task. The morphed forms do
not hold and usually relapse to the original form, where the structure is in
its lowest energy state. For example, a stick, when bent, resists its bent form
and tends to go back to its initial straight form, where it holds the least
amount of potential energy.
  In this project, we present a computational design method which can create
fabricable planar structure that can morph into two different bistable forms.
Once the user provides the initial desired forms, the method automatically
creates support structures (internal springs), such that, the structure can not
only morph, but also hold the respective forms under external force
application. We achieve this through an iterative nonlinear optimization
strategy for shaping the potential energy of the structure in the two forms
simultaneously. Our approach guarantees first and second-order stability with
respect to the potential energy of the bistable structure.
</summary>
    <author>
      <name>Gaurav Bharaj</name>
    </author>
    <author>
      <name>Danny Kaufman</name>
    </author>
    <author>
      <name>Etienne Vouga</name>
    </author>
    <author>
      <name>Hanspeter Pfister</name>
    </author>
    <link href="http://arxiv.org/abs/1804.06996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08197v4</id>
    <updated>2018-08-22T01:48:32Z</updated>
    <published>2018-04-23T00:04:54Z</published>
    <title>syGlass: Interactive Exploration of Multidimensional Images Using
  Virtual Reality Head-mounted Displays</title>
    <summary>  The quest for deeper understanding of biological systems has driven the
acquisition of increasingly larger multidimensional image datasets. Inspecting
and manipulating data of this complexity is very challenging in traditional
visualization systems. We developed syGlass, a software package capable of
visualizing large scale volumetric data with inexpensive virtual reality
head-mounted display technology. This allows leveraging stereoscopic vision to
significantly improve perception of complex 3D structures, and provides
immersive interaction with data directly in 3D. We accomplished this by
developing highly optimized data flow and volume rendering pipelines, tested on
datasets up to 16TB in size, as well as tools available in a virtual reality
GUI to support advanced data exploration, annotation, and cataloguing.
</summary>
    <author>
      <name>Stanislav Pidhorskyi</name>
    </author>
    <author>
      <name>Michael Morehead</name>
    </author>
    <author>
      <name>Quinn Jones</name>
    </author>
    <author>
      <name>George Spirou</name>
    </author>
    <author>
      <name>Gianfranco Doretto</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08197v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08197v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08968v2</id>
    <updated>2019-04-08T20:53:53Z</updated>
    <published>2018-04-24T11:50:05Z</published>
    <title>Half-Space Power Diagrams and Discrete Surface Offsets</title>
    <summary>  We present an efficient, trivially parallelizable algorithm to compute offset
surfaces of shapes discretized using a dexel data structure. Our algorithm is
based on a two-stage sweeping procedure that is simple to implement and
efficient, entirely avoiding volumetric distance field computations typical of
existing methods. Our construction is based on properties of half-space power
diagrams, where each seed is only visible by a half-space, which were never
used before for the computation of surface offsets. The primary application of
our method is interactive modeling for digital fabrication. Our technique
enables a user to interactively process high-resolution models. It is also
useful in a plethora of other geometry processing tasks requiring fast,
approximate offsets, such as topology optimization, collision detection, and
skeleton extraction. We present experimental timings, comparisons with previous
approaches, and provide a reference implementation in the supplemental
material.
</summary>
    <author>
      <name>Zhen Chen</name>
    </author>
    <author>
      <name>Daniele Panozzo</name>
    </author>
    <author>
      <name>Jeremie Dumas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 20 figures, submitted to IEEE Transactions on Visualization
  and Computer Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08968v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08968v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02918v1</id>
    <updated>2018-06-07T22:42:00Z</updated>
    <published>2018-06-07T22:42:00Z</published>
    <title>Color Sails: Discrete-Continuous Palettes for Deep Color Exploration</title>
    <summary>  We present color sails, a discrete-continuous color gamut representation that
extends the color gradient analogy to three dimensions and allows interactive
control of the color blending behavior. Our representation models a wide
variety of color distributions in a compact manner, and lends itself to
applications such as color exploration for graphic design, illustration and
similar fields. We propose a Neural Network that can fit a color sail to any
image. Then, the user can adjust color sail parameters to change the base
colors, their blending behavior and the number of colors, exploring a wide
range of options for the original design. In addition, we propose a Deep
Learning model that learns to automatically segment an image into
color-compatible alpha masks, each equipped with its own color sail. This
allows targeted color exploration by either editing their corresponding color
sails or using standard software packages. Our model is trained on a custom
diverse dataset of art and design. We provide both quantitative evaluations,
and a user study, demonstrating the effectiveness of color sail interaction.
Interactive demos are available at www.colorsails.com.
</summary>
    <author>
      <name>Maria Shugrina</name>
    </author>
    <author>
      <name>Amlan Kar</name>
    </author>
    <author>
      <name>Karan Singh</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03967v2</id>
    <updated>2018-06-12T02:29:05Z</updated>
    <published>2018-06-11T13:40:35Z</published>
    <title>Latent Space Representation for Shape Analysis and Learning</title>
    <summary>  We propose a novel shape representation useful for analyzing and processing
shape collections, as well for a variety of learning and inference tasks.
Unlike most approaches that capture variability in a collection by using a
template model or a base shape, we show that it is possible to construct a full
shape representation by using the latent space induced by a functional map net-
work, allowing us to represent shapes in the context of a collection without
the bias induced by selecting a template shape. Key to our construction is a
novel analysis of latent functional spaces, which shows that after proper
regularization they can be endowed with a natural geometric structure, giving
rise to a well-defined, stable and fully informative shape representation. We
demonstrate the utility of our representation in shape analysis tasks, such as
highlighting the most distorted shape parts in a collection or separating
variability modes between shape classes. We further exploit our representation
in learning applications by showing how it can naturally be used within deep
learning and convolutional neural networks for shape classi cation or
reconstruction, signi cantly outperforming existing point-based techniques.
</summary>
    <author>
      <name>Ruqi Huang</name>
    </author>
    <author>
      <name>Panos Achlioptas</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03967v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03967v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04455v3</id>
    <updated>2018-10-03T10:18:23Z</updated>
    <published>2018-06-12T12:07:05Z</published>
    <title>Continuous and Orientation-preserving Correspondences via Functional
  Maps</title>
    <summary>  We propose a method for efficiently computing orientation-preserving and
approximately continuous correspondences between non-rigid shapes, using the
functional maps framework. We first show how orientation preservation can be
formulated directly in the functional (spectral) domain without using landmark
or region correspondences and without relying on external symmetry information.
This allows us to obtain functional maps that promote orientation preservation,
even when using descriptors, that are invariant to orientation changes. We then
show how higher quality, approximately continuous and bijective pointwise
correspondences can be obtained from initial functional maps by introducing a
novel refinement technique that aims to simultaneously improve the maps both in
the spectral and spatial domains. This leads to a general pipeline for
computing correspondences between shapes that results in high-quality maps,
while admitting an efficient optimization scheme. We show through extensive
evaluation that our approach improves upon state-of-the-art results on
challenging isometric and non-isometric correspondence benchmarks according to
both measures of continuity and coverage as well as producing semantically
meaningful correspondences as measured by the distance to ground truth maps.
</summary>
    <author>
      <name>Jing Ren</name>
    </author>
    <author>
      <name>Adrien Poulenard</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04455v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04455v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05385v1</id>
    <updated>2018-06-14T06:34:37Z</updated>
    <published>2018-06-14T06:34:37Z</published>
    <title>Perceptual Rasterization for Head-mounted Display Image Synthesis</title>
    <summary>  We suggest a rasterization pipeline tailored towards the need of head-mounted
displays (HMD), where latency and field-of-view requirements pose new
challenges beyond those of traditional desktop displays. Instead of rendering
and warping for low latency, or using multiple passes for foveation, we show
how both can be produced directly in a single perceptual rasterization pass. We
do this with per-fragment ray-casting. This is enabled by derivations of tight
space-time-fovea pixel bounds, introducing just enough flexibility for
requisite geometric tests, but retaining most of the the simplicity and
efficiency of the traditional rasterizaton pipeline. To produce foveated
images, we rasterize to an image with spatially varying pixel density. To
reduce latency, we extend the image formation model to directly produce
"rolling" images where the time at each pixel depends on its display location.
Our approach overcomes limitations of warping with respect to disocclusions,
object motion and view-dependent shading, as well as geometric aliasing
artifacts in other foveated rendering techniques. A set of perceptual user
studies demonstrates the efficacy of our approach.
</summary>
    <author>
      <name>Tobias Ritschel</name>
    </author>
    <author>
      <name>Sebastian Friston</name>
    </author>
    <author>
      <name>Anthony Steed</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05952v2</id>
    <updated>2019-03-06T08:30:25Z</updated>
    <published>2018-06-15T13:35:17Z</published>
    <title>TTHRESH: Tensor Compression for Multidimensional Visual Data</title>
    <summary>  Memory and network bandwidth are decisive bottlenecks when handling
high-resolution multidimensional data sets in visualization applications, and
they increasingly demand suitable data compression strategies. We introduce a
novel lossy compression algorithm for multidimensional data over regular grids.
It leverages the higher-order singular value decomposition (HOSVD), a
generalization of the SVD to three dimensions and higher, together with
bit-plane, run-length and arithmetic coding to compress the HOSVD transform
coefficients. Our scheme degrades the data particularly smoothly and achieves
lower mean squared error than other state-of-the-art algorithms at
low-to-medium bit rates, as it is required in data archiving and management for
visualization purposes. Further advantages of the proposed algorithm include
very fine bit rate selection granularity and the ability to manipulate data at
very small cost in the compression domain, for example to reconstruct filtered
and/or subsampled versions of all (or selected parts) of the data set.
</summary>
    <author>
      <name>Rafael Ballester-Ripoll</name>
    </author>
    <author>
      <name>Peter Lindstrom</name>
    </author>
    <author>
      <name>Renato Pajarola</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2019.2904063</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2019.2904063" rel="related"/>
    <link href="http://arxiv.org/abs/1806.05952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05999v2</id>
    <updated>2018-08-31T21:33:44Z</updated>
    <published>2018-06-15T14:32:32Z</published>
    <title>Mumford-Shah Mesh Processing using the Ambrosio-Tortorelli Functional</title>
    <summary>  The Mumford-Shah functional approximates a function by a piecewise smooth
function. Its versatility makes it ideal for tasks such as image segmentation
or restoration, and it is now a widespread tool of image processing. Recent
work has started to investigate its use for mesh segmentation and feature lines
detection, but we take the stance that the power of this functional could reach
far beyond these tasks and integrate the everyday mesh processing toolbox. In
this paper, we discretize an Ambrosio-Tortorelli approximation via a Discrete
Exterior Calculus formulation. We show that, combined with a new shape
optimization routine, several mesh processing problems can be readily tackled
within the same framework. In particular, we illustrate applications in mesh
denoising, normal map embossing, mesh inpainting and mesh segmentation.
</summary>
    <author>
      <name>Nicolas Bonneel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNRS, Univ. Lyon</arxiv:affiliation>
    </author>
    <author>
      <name>David Coeurjolly</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNRS, Univ. Lyon</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Gueth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Arskan</arxiv:affiliation>
    </author>
    <author>
      <name>Jacques-Olivier Lachaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Université Savoie Mont Blanc</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1806.05999v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05999v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06613v1</id>
    <updated>2018-06-18T12:05:20Z</updated>
    <published>2018-06-18T12:05:20Z</published>
    <title>Coupled Fluid Density and Motion from Single Views</title>
    <summary>  We present a novel method to reconstruct a fluid's 3D density and motion
based on just a single sequence of images. This is rendered possible by using
powerful physical priors for this strongly under-determined problem. More
specifically, we propose a novel strategy to infer density updates strongly
coupled to previous and current estimates of the flow motion. Additionally, we
employ an accurate discretization and depth-based regularizers to compute
stable solutions. Using only one view for the reconstruction reduces the
complexity of the capturing setup drastically and could even allow for online
video databases or smart-phone videos as inputs. The reconstructed 3D velocity
can then be flexibly utilized, e.g., for re-simulation, domain modification or
guiding purposes. We will demonstrate the capacity of our method with a series
of synthetic test cases and the reconstruction of real smoke plumes captured
with a Raspberry Pi camera.
</summary>
    <author>
      <name>Marie-Lena Eckert</name>
    </author>
    <author>
      <name>Wolfgang Heidrich</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum (2018), further information:
  https://ge.in.tum.de/publications/2018-cgf-eckert/, video:
  https://www.youtube.com/watch?v=J2wkPNBJLaI</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.8; I.3.7; G.1.6; I.4.5; G.1.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06639v2</id>
    <updated>2019-03-15T11:04:43Z</updated>
    <published>2018-06-18T12:58:08Z</published>
    <title>HexaLab.net: an online viewer for hexahedral meshes</title>
    <summary>  We introduce HexaLab: a WebGL application for real time visualization,
exploration and assessment of hexahedral meshes. HexaLab can be used by simply
opening www.hexalab.net. Our visualization tool targets both users and
scholars. Practitioners who employ hexmeshes for Finite Element Analysis, can
readily check mesh quality and assess its usability for simulation. Researchers
involved in mesh generation may use HexaLab to perform a detailed analysis of
the mesh structure, isolating weak points and testing new solutions to improve
on the state of the art and generate high quality images. To this end, we
support a wide variety of visualization and volume inspection tools. Our system
offers also immediate access to a repository containing all the publicly
available meshes produced with the most recent techniques for hexmesh
generation. We believe HexaLab, providing a common tool for visualizing,
assessing and distributing results, will push forward the recent strive for
replicability in our scientific community.
</summary>
    <author>
      <name>Matteo Bracci</name>
    </author>
    <author>
      <name>Marco Tarini</name>
    </author>
    <author>
      <name>Nico Pietroni</name>
    </author>
    <author>
      <name>Marco Livesu</name>
    </author>
    <author>
      <name>Paolo Cignoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2018.12.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2018.12.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer-Aided Design, Volume 110, May 2019, Pages 24-36</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.06639v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06639v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07179v2</id>
    <updated>2019-03-06T18:24:01Z</updated>
    <published>2018-06-19T12:29:13Z</published>
    <title>FrankenGAN: Guided Detail Synthesis for Building Mass-Models Using
  Style-Synchonized GANs</title>
    <summary>  Coarse building mass models are now routinely generated at scales ranging
from individual buildings through to whole cities. For example, they can be
abstracted from raw measurements, generated procedurally, or created manually.
However, these models typically lack any meaningful semantic or texture
details, making them unsuitable for direct display. We introduce the problem of
automatically and realistically decorating such models by adding semantically
consistent geometric details and textures. Building on the recent success of
generative adversarial networks (GANs), we propose FrankenGAN, a cascade of
GANs to create plausible details across multiple scales over large
neighborhoods. The various GANs are synchronized to produce consistent style
distributions over buildings and neighborhoods. We provide the user with direct
control over the variability of the output. We allow her to interactively
specify style via images and manipulate style-adapted sliders to control style
variability. We demonstrate our system on several large-scale examples. The
generated outputs are qualitatively evaluated via a set of user studies and are
found to be realistic, semantically-plausible, and style-consistent.
</summary>
    <author>
      <name>Tom Kelly</name>
    </author>
    <author>
      <name>Paul Guerrero</name>
    </author>
    <author>
      <name>Anthony Steed</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3272127.3275065</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3272127.3275065" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">project page: http://geometry.cs.ucl.ac.uk/projects/2018/frankengan/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 2018, (37), 6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.07179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07729v1</id>
    <updated>2018-06-20T13:45:33Z</updated>
    <published>2018-06-20T13:45:33Z</published>
    <title>Void Space Surfaces to Convey Depth in Vessel Visualizations</title>
    <summary>  To enhance depth perception and thus data comprehension, additional depth
cues are often used in 3D visualizations of complex vascular structures.
Accordingly, there is a variety of different approaches described in the
literature, ranging from chromadepth color coding over depth of field to
glyph-based encodings. Unfortunately, the majority of existing approaches
suffers from the same problem. As these cues are directly applied to the
geometry's surface, the display of additional information, such as other
modalities or derived attributes, associated with a vessel is impaired. To
overcome this limitation we propose Void Space Surfaces which utilize the empty
space in between vessel branches to communicate depth and their relative
positioning. This allows us to enhance the depth perception of vascular
structures without interfering with the spatial data and potentially
superimposed parameter information. Within this paper we introduce Void Space
Surfaces, describe their technical realization, and show their application to
various vessel trees. Moreover, we report the outcome of a user study which we
have conducted in order to evaluate the perceptual impact of Void Space
Surfaces as compared to existing vessel visualization techniques.
</summary>
    <author>
      <name>Julian Kreiser</name>
    </author>
    <author>
      <name>Pedro Hermosilla</name>
    </author>
    <author>
      <name>Timo Ropinski</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00410v1</id>
    <updated>2018-07-01T22:41:52Z</updated>
    <published>2018-07-01T22:41:52Z</published>
    <title>$P_N$-Method for Multiple Scattering in Participating Media</title>
    <summary>  Rendering highly scattering participating media using brute force path
tracing is a challenge. The diffusion approximation reduces the problem to
solving a simple linear partial differential equation. Flux-limited diffusion
introduces non-linearities to improve the accuracy of the solution, especially
in low optical depth media, but introduces several ad-hoc assumptions. Both
methods are based on a spherical harmonics expansion of the radiance field that
is truncated after the first order. In this paper, we investigate the open
question of whether going to higher spherical harmonic orders provides a viable
improvement to these two approaches. Increasing the order introduces a set of
complex coupled partial differential equations (the $P_N$-equations), whose
growing number make them difficult to work with at higher orders. We thus use a
computer algebra framework for representing and manipulating the underlying
mathematical equations, and use it to derive the real-valued $P_N$-equations
for arbitrary orders. We further present a staggered-grid $P_N$-solver and
generate its stencil code directly from the expression tree of the
$P_N$-equations. Finally, we discuss how our method compares to prior work for
various standard problems.
</summary>
    <author>
      <name>David Koerner</name>
    </author>
    <author>
      <name>Jamie Portsmouth</name>
    </author>
    <author>
      <name>Wenzel Jakob</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurographics Symposium on Rendering 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00866v1</id>
    <updated>2018-07-02T19:30:44Z</updated>
    <published>2018-07-02T19:30:44Z</published>
    <title>Solid Geometry Processing on Deconstructed Domains</title>
    <summary>  Many tasks in geometry processing are modeled as variational problems solved
numerically using the finite element method. For solid shapes, this requires a
volumetric discretization, such as a boundary conforming tetrahedral mesh.
Unfortunately, tetrahedral meshing remains an open challenge and existing
methods either struggle to conform to complex boundary surfaces or require
manual intervention to prevent failure. Rather than create a single volumetric
mesh for the entire shape, we advocate for solid geometry processing on
deconstructed domains, where a large and complex shape is composed of
overlapping solid subdomains. As each smaller and simpler part is now easier to
tetrahedralize, the question becomes how to account for overlaps during problem
modeling and how to couple solutions on each subdomain together algebraically.
We explore how and why previous coupling methods fail, and propose a method
that couples solid domains only along their boundary surfaces. We demonstrate
the superiority of this method through empirical convergence tests and
qualitative applications to solid geometry processing on a variety of popular
second-order and fourth-order partial differential equations.
</summary>
    <author>
      <name>Silvia Sellán</name>
    </author>
    <author>
      <name>Herng Yi Cheng</name>
    </author>
    <author>
      <name>Yuming Ma</name>
    </author>
    <author>
      <name>Mitchell Dembowski</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Computer Graphics Forum</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01519v1</id>
    <updated>2018-07-04T11:10:38Z</updated>
    <published>2018-07-04T11:10:38Z</published>
    <title>Learning Fuzzy Set Representations of Partial Shapes on Dual Embedding
  Spaces</title>
    <summary>  Modeling relations between components of 3D objects is essential for many
geometry editing tasks. Existing techniques commonly rely on labeled
components, which requires substantial annotation effort and limits components
to a dictionary of predefined semantic parts. We propose a novel framework
based on neural networks that analyzes an uncurated collection of 3D models
from the same category and learns two important types of semantic relations
among full and partial shapes: complementarity and interchangeability. The
former helps to identify which two partial shapes make a complete plausible
object, and the latter indicates that interchanging two partial shapes from
different objects preserves the object plausibility. Our key idea is to jointly
encode both relations by embedding partial shapes as fuzzy sets in dual
embedding spaces. We model these two relations as fuzzy set operations
performed across the dual embedding spaces, and within each space,
respectively. We demonstrate the utility of our method for various retrieval
tasks that are commonly needed in geometric modeling interfaces.
</summary>
    <author>
      <name>Minhyuk Sung</name>
    </author>
    <author>
      <name>Anastasia Dubrovina</name>
    </author>
    <author>
      <name>Vladimir G. Kim</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SGP 2018 (Symposium on Geometry Processing). 11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02284v1</id>
    <updated>2018-07-06T07:00:17Z</updated>
    <published>2018-07-06T07:00:17Z</published>
    <title>Continuous-Scale Kinetic Fluid Simulation</title>
    <summary>  Kinetic approaches, i.e., methods based on the lattice Boltzmann equations,
have long been recognized as an appealing alternative for solving
incompressible Navier-Stokes equations in computational fluid dynamics.
However, such approaches have not been widely adopted in graphics mainly due to
the underlying inaccuracy, instability and inflexibility. In this paper, we try
to tackle these problems in order to make kinetic approaches practical for
graphical applications. To achieve more accurate and stable simulations, we
propose to employ the non-orthogonal central-moment-relaxation model, where we
develop a novel adaptive relaxation method to retain both stability and
accuracy in turbulent flows. To achieve flexibility, we propose a novel
continuous-scale formulation that enables samples at arbitrary resolutions to
easily communicate with each other in a more continuous sense and with loose
geometrical constraints, which allows efficient and adaptive sample
construction to better match the physical scale. Such a capability directly
leads to an automatic sample construction which generates static and dynamic
scales at initialization and during simulation, respectively. This effectively
makes our method suitable for simulating turbulent flows with arbitrary
geometrical boundaries. Our simulation results with applications to smoke
animations show the benefits of our method, with comparisons for justification
and verification.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Kai Bai</name>
    </author>
    <author>
      <name>Xiaopei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 17 figures, accepted by IEEE Transactions on Visualization
  and Computer Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02451v1</id>
    <updated>2018-07-06T15:27:30Z</updated>
    <published>2018-07-06T15:27:30Z</published>
    <title>Evolution of natural patterns from random fields</title>
    <summary>  In the article a transition from pattern evolution equation of
reaction-diffusion type to a cellular automaton (CA) is described. The
applicability of CA is demonstrated by generating patterns of complex irregular
structure on a hexagonal and quadratic lattice. With this aim a random initial
field is transformed by a sequence of CA actions into a new pattern. On the
hexagonal lattice this pattern resembles a lizard skin. The properties of CA
are specified by the most simple majority rule that adapts selected cell state
to the most frequent state of cells in its surrounding. The method could be of
interest for manufacturing of textiles as well as for modeling of patterns on
skin of various animals.
</summary>
    <author>
      <name>Lovrenc Švegl</name>
    </author>
    <author>
      <name>Igor Grabec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8th Conference on Information and Graphic Arts Technology, Ljubljana,
  Slovenia, 7-8 June 2018, Abstracts, R. URBAS, N. PU\v{S}NIK (eds.), Publ.:
  Uni. Ljubljana, Faculty of Nat. Sci. and Eng., Dept., pp. 95-96</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02578v1</id>
    <updated>2018-07-06T22:22:53Z</updated>
    <published>2018-07-06T22:22:53Z</published>
    <title>Guided Proceduralization: Optimizing Geometry Processing and Grammar
  Extraction for Architectural Models</title>
    <summary>  We describe a guided proceduralization framework that optimizes geometry
processing on architectural input models to extract target grammars. We aim to
provide efficient artistic workflows by creating procedural representations
from existing 3D models, where the procedural expressiveness is controlled by
the user. Architectural reconstruction and modeling tasks have been handled as
either time consuming manual processes or procedural generation with difficult
control and artistic influence. We bridge the gap between creation and
generation by converting existing manually modeled architecture to procedurally
editable parametrized models, and carrying the guidance to procedural domain by
letting the user define the target procedural representation. Additionally, we
propose various applications of such procedural representations, including
guided completion of point cloud models, controllable 3D city modeling, and
other benefits of procedural modeling.
</summary>
    <author>
      <name>Ilke Demir</name>
    </author>
    <author>
      <name>Daniel G. Aliaga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cag.2018.05.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cag.2018.05.013" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Graphics, Volume 74, 2018, Pages 257-267, ISSN
  0097-8493</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.02578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 65D18" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03249v1</id>
    <updated>2018-07-09T15:49:49Z</updated>
    <published>2018-07-09T15:49:49Z</published>
    <title>StyleBlit: Fast Example-Based Stylization with Local Guidance</title>
    <summary>  We present StyleBlit---an efficient example-based style transfer algorithm
that can deliver high-quality stylized renderings in real-time on a single-core
CPU. Our technique is especially suitable for style transfer applications that
use local guidance - descriptive guiding channels containing large spatial
variations. Local guidance encourages transfer of content from the source
exemplar to the target image in a semantically meaningful way. Typical local
guidance includes, e.g., normal values, texture coordinates or a displacement
field. Contrary to previous style transfer techniques, our approach does not
involve any computationally expensive optimization. We demonstrate that when
local guidance is used, optimization-based techniques converge to solutions
that can be well approximated by simple pixel-level operations. Inspired by
this observation, we designed an algorithm that produces results visually
similar to, if not better than, the state-of-the-art, and is several orders of
magnitude faster. Our approach is suitable for scenarios with low computational
budget such as games and mobile applications.
</summary>
    <author>
      <name>Daniel Sýkora</name>
    </author>
    <author>
      <name>Ondřej Jamriška</name>
    </author>
    <author>
      <name>Jingwan Lu</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07449v3</id>
    <updated>2019-02-01T15:58:55Z</updated>
    <published>2018-07-19T14:08:45Z</published>
    <title>CNNs based Viewpoint Estimation for Volume Visualization</title>
    <summary>  Viewpoint estimation from 2D rendered images is helpful in understanding how
users select viewpoints for volume visualization and guiding users to select
better viewpoints based on previous visualizations. In this paper, we propose a
viewpoint estimation method based on Convolutional Neural Networks (CNNs) for
volume visualization. We first design an overfit-resistant image rendering
pipeline to generate the training images with accurate viewpoint annotations,
and then train a category-specific viewpoint classification network to estimate
the viewpoint for the given rendered image. Our method can achieve good
performance on images rendered with different transfer functions and rendering
parameters in several categories. We apply our model to recover the viewpoints
of the rendered images in publications, and show how experts look at volumes.
We also introduce a CNN feature-based image similarity measure for similarity
voting based viewpoint selection, which can suggest semantically meaningful
optimal viewpoints for different volumes and transfer functions.
</summary>
    <author>
      <name>Neng Shi</name>
    </author>
    <author>
      <name>Yubo Tao</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Intelligent Systems and Technology (TIST),
  2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.07449v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07449v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08474v1</id>
    <updated>2018-07-23T08:21:36Z</updated>
    <published>2018-07-23T08:21:36Z</published>
    <title>Robust Edge-Preserved Surface Mesh Polycube Deformation</title>
    <summary>  The problem of polycube construction or deformation is an essential problem
in computer graphics. In this paper, we present a robust, simple, efficient and
automatic algorithm to deform the meshes of arbitrary shapes into their
polycube ones. We derive a clear relationship between a mesh and its
corresponding polycube shape. Our algorithm is edge-preserved, and works on
surface meshes with or without boundaries. Our algorithm outperforms previous
ones in speed, robustness, efficiency. Our method is simple to implement. To
demonstrate the robustness and effectiveness of our method, we apply it to
hundreds of models of varying complexity and topology. We demonstrat that our
method compares favorably to other state-of-the-art polycube deformation
methods.
</summary>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Na Lei</name>
    </author>
    <author>
      <name>Xuan Li</name>
    </author>
    <author>
      <name>Peng Zeng</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <author>
      <name>Xianfeng Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9pages,15 figures ,conference or other essential info</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11760v1</id>
    <updated>2018-07-31T11:13:23Z</updated>
    <published>2018-07-31T11:13:23Z</published>
    <title>On-the-Fly Power-Aware Rendering</title>
    <summary>  Power saving is a prevailing concern in desktop computers and, especially, in
battery-powered devices such as mobile phones. This is generating a growing
demand for power-aware graphics applications that can extend battery life,
while preserving good quality. In this paper, we address this issue by
presenting a real-time power-efficient rendering framework, able to dynamically
select the rendering configuration with the best quality within a given power
budget. Different from the current state of the art, our method does not
require precomputation of the whole camera-view space, nor Pareto curves to
explore the vast power-error space; as such, it can also handle dynamic scenes.
Our algorithm is based on two key components: our novel power prediction model,
and our runtime quality error estimation mechanism. These components allow us
to search for the optimal rendering configuration at runtime, being transparent
to the user. We demonstrate the performance of our framework on two different
platforms: a desktop computer, and a mobile device. In both cases, we produce
results close to the maximum quality, while achieving significant power
savings.
</summary>
    <author>
      <name>Yunjin Zhang</name>
    </author>
    <author>
      <name>Marta Ortin</name>
    </author>
    <author>
      <name>Victor Arellano</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Diego Gutierrez</name>
    </author>
    <author>
      <name>Hujun Bao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.13483</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.13483" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.11760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11847v1</id>
    <updated>2018-07-31T14:56:02Z</updated>
    <published>2018-07-31T14:56:02Z</published>
    <title>Fast Sketch Segmentation and Labeling with Deep Learning</title>
    <summary>  We present a simple and efficient method based on deep learning to
automatically decompose sketched objects into semantically valid parts. We
train a deep neural network to transfer existing segmentations and labelings
from 3D models to freehand sketches without requiring numerous well-annotated
sketches as training data. The network takes the binary image of a sketched
object as input and produces a corresponding segmentation map with per-pixel
labelings as output. A subsequent post-process procedure with multi-label graph
cuts further refines the segmentation and labeling result. We validate our
proposed method on two sketch datasets. Experiments show that our method
outperforms the state-of-the-art method in terms of segmentation and labeling
accuracy and is significantly faster, enabling further integration in
interactive drawing systems. We demonstrate the efficiency of our method in a
sketch-based modeling application that automatically transforms input sketches
into 3D models by part assembly.
</summary>
    <author>
      <name>Lei Li</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <author>
      <name>Chiew-Lan Tai</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00328v1</id>
    <updated>2018-08-01T14:10:44Z</updated>
    <published>2018-08-01T14:10:44Z</published>
    <title>There is more to PCG than Meets the Eye: NPC AI, Dynamic Camera, PVS and
  Lightmaps</title>
    <summary>  Procedural content generation (PCG) concerns all sorts of algorithms and
tools which automatically produce game content, without requiring manual
authoring by game artists. Besides generating com-plex static meshes, the PCG
core usually encompasses geometrical information about the game world that can
be useful in supporting other critical subsystems of the game engine. We
discuss our experi-ence from the development of the iOS game title named
"Fallen God: Escape Underworld", and show how our PCG produced extra metadata
regarding the game world, in particular: (i) an annotated dun-geon graph to
support path finding for NPC AI to attack or avoid the player (working for
bipeds, birds, insects and serpents); (ii) a quantized voxel space to allow
discrete A* for the dynamic camera system to work in the continuous 3d space;
(iii) dungeon portals to support a dynamic PVS; and (iv) procedural ambient
occlusion and tessellation of a separate set of simplified meshes to support
very-fast and high-quality light mapping.
</summary>
    <author>
      <name>Anthony Savidis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00607v2</id>
    <updated>2018-08-26T21:46:16Z</updated>
    <published>2018-08-02T00:15:39Z</published>
    <title>Toward A Deep Understanding of What Makes a Scientific Visualization
  Memorable</title>
    <summary>  We report results from a preliminary study exploring the memorability of
spatial scientific visualizations, the goal of which is to understand the
visual features that contribute to memorability. The evaluation metrics include
three objective measures (entropy, feature congestion, the number of edges),
four subjective ratings (clutter, the number of distinct colors, familiarity,
and realism), and two sentiment ratings (interestingness and happiness). We
curate 1142 scientific visualization (SciVis) images from the original 2231
images in published IEEE SciVis papers from 2008 to 2017 and compute
memorability scores of 228 SciVis images from data collected on Amazon
Mechanical Turk (MTurk). Results showed that the memorability of SciVis images
is mostly correlated with clutter and the number of distinct colors. We further
investigate the differences between scientific visualization and infographics
as a means to understand memorability differences by data attributes.
</summary>
    <author>
      <name>Rui Li</name>
    </author>
    <author>
      <name>Jian Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01308v2</id>
    <updated>2020-04-21T17:25:18Z</updated>
    <published>2018-07-14T13:19:31Z</published>
    <title>The Normal Map Based on Area-Preserving Parameterization</title>
    <summary>  In this paper, we present an approach to enhance and improve the current
normal map rendering technique. Our algorithm is based on semi-discrete Optimal
Mass Transportation (OMT) theory and has a solid theoretical base. The key
difference from previous normal map method is that we preserve the local area
when we unwrap a disk-like 3D surface onto 2D plane. Compared to the currently
used techniques which is based on conformal parameterization, our method does
not need to cut a surface into many small pieces to avoid the large area
distortion. The following charts packing step is also unnecessary in our
framework. Our method is practical and makes the normal map technique more
robust and efficient.
</summary>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Kehua Su</name>
    </author>
    <author>
      <name>Ming Ma</name>
    </author>
    <author>
      <name>Na Lei</name>
    </author>
    <author>
      <name>Li Cui</name>
    </author>
    <author>
      <name>Xianfeng Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">we need update it</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02860v2</id>
    <updated>2018-11-01T23:06:13Z</updated>
    <published>2018-08-08T17:00:06Z</published>
    <title>Cinematic Visualization of Multiresolution Data: Ytini for Adaptive Mesh
  Refinement in Houdini</title>
    <summary>  We have entered the era of large multidimensional datasets represented by
increasingly complex data structures. Current tools for scientific
visualization are not optimized to efficiently and intuitively create cinematic
production quality, time-evolving representations of numerical data for broad
impact science communication via film, media, or journalism. To present such
data in a cinematic environment, it is advantageous to develop methods that
integrate these complex data structures into industry standard visual effects
software packages, which provide a myriad of control features otherwise
unavailable in traditional scientific visualization software. In this paper, we
present the general methodology for the import and visualization of nested
multiresolution datasets into commercially available visual effects software.
We further provide a specific example of importing Adaptive Mesh Refinement
data into the software Houdini. This paper builds on our previous work, which
describes a method for using Houdini to visualize uniform Cartesian datasets.
We summarize a tutorial available on the website www.ytini.com, which includes
sample data downloads, Python code, and various other resources to simplify the
process of importing and rendering multiresolution data.
</summary>
    <author>
      <name>Kalina Borkiewicz</name>
    </author>
    <author>
      <name>J. P. Naiman</name>
    </author>
    <author>
      <name>Haoming Lai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3847/1538-3881/ab1f6f</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3847/1538-3881/ab1f6f" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04121v1</id>
    <updated>2018-08-13T09:34:06Z</updated>
    <published>2018-08-13T09:34:06Z</published>
    <title>Image Inpainting Based on a Novel Criminisi Algorithm</title>
    <summary>  In view of the problem of image inpainting error continuation and the
deviation of finding best match block, an improved Criminisi algorithm is
proposed. The improvement was mainly embodied in two aspects. In the repairing
order aspect, we redefine the calculation formula of the priority. In order to
solve the problem of error continuation caused by local confidence item
updating, the mean value of Manhattan distance is used for replace the
confidence item. In the matching strategy aspect, finding the best match block
not only depend on the difference of the two pixels, but also consider the
matching region. Therefore, Euclidean distance is introduced. Experiments
confirm that the improved algorithm can overcome the insufficiencies of the
original algorithm. The repairing effect has been improved, and the results
have a better visual appearance.
</summary>
    <author>
      <name>Song Yuheng</name>
    </author>
    <author>
      <name>Yan Hao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06060v1</id>
    <updated>2018-08-18T09:01:24Z</updated>
    <published>2018-08-18T09:01:24Z</published>
    <title>Universal software platform for visualizing class F curves,
  log-aesthetic curves and development of applied CAD systems</title>
    <summary>  This article describes the capabilities of a universal software platform for
visualizing class F curves and developing specialized applications for CAD
systems based on Microsoft Excel VBA, the software complex FairCurveModeler,
and computer algebra systems. Additionally, it demonstrates the use of a
software platform for visualizing functional and log-aesthetic curves
integrated with CAD Fusion360. The value of the curves is evident in
visualizing the qualitative geometry of the product shape in industrial design.
Moreover, the requirements for the characteristics of class F curves are
emphasized to form a visual purity of shape in industrial design and to provide
a positive emotional perception of the visual image of the product by a person.
</summary>
    <author>
      <name>Rushan Ziatdinov</name>
    </author>
    <author>
      <name>Valerijan G. Muftejev</name>
    </author>
    <author>
      <name>Rustam I. Akhmetshin</name>
    </author>
    <author>
      <name>Alexander P. Zelev</name>
    </author>
    <author>
      <name>Rifkat I. Nabiyev</name>
    </author>
    <author>
      <name>Albert R. Mardanov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06201v1</id>
    <updated>2018-08-19T11:53:50Z</updated>
    <published>2018-08-19T11:53:50Z</published>
    <title>Intelligent Middle-Level Game Control</title>
    <summary>  We propose the concept of intelligent middle-level game control, which lies
on a continuum of control abstraction levels between the following two dual
opposites: 1) high-level control that translates player's simple commands into
complex actions (such as pressing Space key for jumping), and 2) low-level
control which simulates real-life complexities by directly manipulating, e.g.,
joint rotations of the character as it is done in the runner game QWOP. We
posit that various novel control abstractions can be explored using recent
advances in movement intelligence of game characters. We demonstrate this
through design and evaluation of a novel 2-player martial arts game prototype.
In this game, each player guides a simulated humanoid character by clicking and
dragging body parts. This defines the cost function for an online continuous
control algorithm that executes the requested movement. Our control algorithm
uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) in a rolling
horizon manner with custom population seeding techniques. Our playtesting data
indicates that intelligent middle-level control results in producing novel and
innovative gameplay without frustrating interface complexities.
</summary>
    <author>
      <name>Amin Babadi</name>
    </author>
    <author>
      <name>Kourosh Naderi</name>
    </author>
    <author>
      <name>Perttu Hämäläinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2018 IEEE Conference on Computational Intelligence and Games (IEEE
  CIG 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07371v2</id>
    <updated>2019-08-27T21:10:54Z</updated>
    <published>2018-08-22T13:58:36Z</published>
    <title>Everybody Dance Now</title>
    <summary>  This paper presents a simple method for "do as I do" motion transfer: given a
source video of a person dancing, we can transfer that performance to a novel
(amateur) target after only a few minutes of the target subject performing
standard moves. We approach this problem as video-to-video translation using
pose as an intermediate representation. To transfer the motion, we extract
poses from the source subject and apply the learned pose-to-appearance mapping
to generate the target subject. We predict two consecutive frames for
temporally coherent video results and introduce a separate pipeline for
realistic face synthesis. Although our method is quite simple, it produces
surprisingly compelling results (see video). This motivates us to also provide
a forensics tool for reliable synthetic content detection, which is able to
distinguish videos synthesized by our system from real data. In addition, we
release a first-of-its-kind open-source dataset of videos that can be legally
used for training and motion transfer.
</summary>
    <author>
      <name>Caroline Chan</name>
    </author>
    <author>
      <name>Shiry Ginosar</name>
    </author>
    <author>
      <name>Tinghui Zhou</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In ICCV 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07560v3</id>
    <updated>2019-02-26T09:04:34Z</updated>
    <published>2018-08-22T20:53:08Z</published>
    <title>Optimizing B-spline surfaces for developability and paneling
  architectural freeform surfaces</title>
    <summary>  Motivated by applications in architecture and design, we present a novel
method for increasing the developability of a B-spline surface. We use the
property that the Gauss image of a developable surface is 1-dimensional and can
be locally well approximated by circles. This is cast into an algorithm for
thinning the Gauss image by increasing the planarity of the Gauss images of
appropriate neighborhoods. A variation of the main method allows us to tackle
the problem of paneling a freeform architectural surface with developable
panels, in particular enforcing rotational cylindrical, rotational conical and
planar panels, which are the main preferred types of developable panels in
architecture due to the reduced cost of manufacturing.
</summary>
    <author>
      <name>Konstantinos Gavriil</name>
    </author>
    <author>
      <name>Alexander Schiftner</name>
    </author>
    <author>
      <name>Helmut Pottmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2019.01.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2019.01.006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer-Aided Design 111 (2019) 29-43</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.07560v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07560v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10083v1</id>
    <updated>2018-08-30T01:45:57Z</updated>
    <published>2018-08-30T01:45:57Z</published>
    <title>Differential and integral invariants under Mobius transformation</title>
    <summary>  One of the most challenging problems in the domain of 2-D image or 3-D shape
is to handle the non-rigid deformation. From the perspective of transformation
groups, the conformal transformation is a key part of the diffeomorphism.
According to the Liouville Theorem, an important part of the conformal
transformation is the Mobius transformation, so we focus on Mobius
transformation and propose two differential expressions that are invariable
under 2-D and 3-D Mobius transformation respectively. Next, we analyze the
absoluteness and relativity of invariance on them and their components. After
that, we propose integral invariants under Mobius transformation based on the
two differential expressions. Finally, we propose a conjecture about the
structure of differential invariants under conformal transformation according
to our observation on the composition of the above two differential invariants.
</summary>
    <author>
      <name>He Zhang</name>
    </author>
    <author>
      <name>Hanlin Mo</name>
    </author>
    <author>
      <name>You Hao</name>
    </author>
    <author>
      <name>Qi Li</name>
    </author>
    <author>
      <name>Hua Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00028v1</id>
    <updated>2018-09-28T18:23:39Z</updated>
    <published>2018-09-28T18:23:39Z</published>
    <title>Data-Driven Modeling of Group Entitativity in Virtual Environments</title>
    <summary>  We present a data-driven algorithm to model and predict the socio-emotional
impact of groups on observers. Psychological research finds that highly
entitative i.e. cohesive and uniform groups induce threat and unease in
observers. Our algorithm models realistic trajectory-level behaviors to
classify and map the motion-based entitativity of crowds. This mapping is based
on a statistical scheme that dynamically learns pedestrian behavior and
computes the resultant entitativity induced emotion through group motion
characteristics. We also present a novel interactive multi-agent simulation
algorithm to model entitative groups and conduct a VR user study to validate
the socio-emotional predictive power of our algorithm. We further show that
model-generated high-entitativity groups do induce more negative emotions than
low-entitative groups.
</summary>
    <author>
      <name>Aniket Bera</name>
    </author>
    <author>
      <name>Tanmay Randhavane</name>
    </author>
    <author>
      <name>Emily Kubin</name>
    </author>
    <author>
      <name>Husam Shaik</name>
    </author>
    <author>
      <name>Kurt Gray</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at VRST 2018, November 28-December 1, 2018, Tokyo, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.00028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01175v2</id>
    <updated>2019-05-13T10:52:27Z</updated>
    <published>2018-10-02T11:28:10Z</published>
    <title>Line Drawings from 3D Models</title>
    <summary>  This tutorial describes the geometry and algorithms for generating line
drawings from 3D models, focusing on occluding contours.
  The geometry of occluding contours on meshes and on smooth surfaces is
described in detail, together with algorithms for extracting contours,
computing their visibility, and creating stylized renderings and animations.
Exact methods and hardware-accelerated fast methods are both described, and the
trade-offs between different methods are discussed. The tutorial brings
together and organizes material that, at present, is scattered throughout the
literature. It also includes some novel explanations, and implementation tips.
  A thorough survey of the field of non-photorealistic 3D rendering is also
included, covering other kinds of line drawings and artistic shading.
</summary>
    <author>
      <name>Pierre Bénard</name>
    </author>
    <author>
      <name>Aaron Hertzmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1561/0600000075</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1561/0600000075" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Foundations and Trend in Computer Graphics and Vision</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Foundations and Trends in Computer Graphics and Vision (2019).
  Vol. 11: No. 1-2, pp 1-159</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.01175v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01175v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.02303v1</id>
    <updated>2018-10-01T18:03:24Z</updated>
    <published>2018-10-01T18:03:24Z</published>
    <title>Multi-directional Geodesic Neural Networks via Equivariant Convolution</title>
    <summary>  We propose a novel approach for performing convolution of signals on curved
surfaces and show its utility in a variety of geometric deep learning
applications. Key to our construction is the notion of directional functions
defined on the surface, which extend the classic real-valued signals and which
can be naturally convolved with with real-valued template functions. As a
result, rather than trying to fix a canonical orientation or only keeping the
maximal response across all alignments of a 2D template at every point of the
surface, as done in previous works, we show how information across all
rotations can be kept across different layers of the neural network. Our
construction, which we call multi-directional geodesic convolution, or
directional convolution for short, allows, in particular, to propagate and
relate directional information across layers and thus different regions on the
shape. We first define directional convolution in the continuous setting, prove
its key properties and then show how it can be implemented in practice, for
shapes represented as triangle meshes. We evaluate directional convolution in a
wide variety of learning scenarios ranging from classification of signals on
surfaces, to shape segmentation and shape matching, where we show a significant
improvement over several baselines.
</summary>
    <author>
      <name>Adrien Poulenard</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <link href="http://arxiv.org/abs/1810.02303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.07882v1</id>
    <updated>2018-10-18T02:46:20Z</updated>
    <published>2018-10-18T02:46:20Z</published>
    <title>Measuring the Effects of Scalar and Spherical Colormaps on Ensembles of
  DMRI Tubes</title>
    <summary>  We report empirical study results on the color encoding of ensemble scalar
and orientation to visualize diffusion magnetic resonance imaging (DMRI) tubes.
The experiment tested six scalar colormaps for average fractional anisotropy
(FA) tasks (grayscale, blackbody, diverging, isoluminant-rainbow,
extended-blackbody, and coolwarm) and four three-dimensional (3D) directional
encodings for tract tracing tasks (uniform gray, absolute, eigenmap, and Boy's
surface embedding). We found that extended-blackbody, coolwarm, and blackbody
remain the best three approaches for identifying ensemble average in 3D.
Isoluminant-rainbow coloring led to the same ensemble mean accuracy as other
colormaps. However, more than 50% of the answers consistently had higher
estimates of the ensemble average, independent of the mean values. Hue, not
luminance, influences ensemble estimates of mean values. For ensemble
orientation-tracing tasks, we found that the Boy's surface embedding (greatest
spatial resolution and contrast) and absolute color (lowest spatial resolution
and contrast) schemes led to more accurate answers than the eigenmaps scheme
(medium resolution and contrast), acting as the uncanny-valley phenomenon of
visualization design in terms of accuracy.
</summary>
    <author>
      <name>Jian Chen</name>
    </author>
    <author>
      <name>Guohao Zhang</name>
    </author>
    <author>
      <name>Wesley Chiou</name>
    </author>
    <author>
      <name>David H. Laidlaw</name>
    </author>
    <author>
      <name>Alexander P. Auchus</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.08860v1</id>
    <updated>2018-10-20T22:26:27Z</updated>
    <published>2018-10-20T22:26:27Z</published>
    <title>A System for Acquiring, Processing, and Rendering Panoramic Light Field
  Stills for Virtual Reality</title>
    <summary>  We present a system for acquiring, processing, and rendering panoramic light
field still photography for display in Virtual Reality (VR). We acquire
spherical light field datasets with two novel light field camera rigs designed
for portable and efficient light field acquisition. We introduce a novel
real-time light field reconstruction algorithm that uses a per-view geometry
and a disk-based blending field. We also demonstrate how to use a light field
prefiltering operation to project from a high-quality offline reconstruction
model into our real-time model while suppressing artifacts. We introduce a
practical approach for compressing light fields by modifying the VP9 video
codec to provide high quality compression with real-time, random access
decompression.
  We combine these components into a complete light field system offering
convenient acquisition, compact file size, and high-quality rendering while
generating stereo views at 90Hz on commodity VR hardware. Using our system, we
built a freely available light field experience application called Welcome to
Light Fields featuring a library of panoramic light field stills for consumer
VR which has been downloaded over 15,000 times.
</summary>
    <author>
      <name>Ryan S. Overbeck</name>
    </author>
    <author>
      <name>Daniel Erickson</name>
    </author>
    <author>
      <name>Daniel Evangelakos</name>
    </author>
    <author>
      <name>Matt Pharr</name>
    </author>
    <author>
      <name>Paul Debevec</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3272127.3275031</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3272127.3275031" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 14 figures, 2 tables, accepted by SIGGRAPH Asia 2018,
  low-resolution version</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.08860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.09031v1</id>
    <updated>2018-10-21T21:39:32Z</updated>
    <published>2018-10-21T21:39:32Z</published>
    <title>Spherical Parameterization Balancing Angle and Area Distortions</title>
    <summary>  This work presents a novel framework for spherical mesh parameterization. An
efficient angle-preserving spherical parameterization algorithm is introduced,
which is based on dynamic Yamabe flow and the conformal welding method with
solid theoretic foundation. An area-preserving spherical parameterization is
also discussed, which is based on discrete optimal mass transport theory.
Furthermore, a spherical parameterization algorithm, which is based on the
polar decomposition method, balancing angle distortion and area distortion is
presented. The algorithms are tested on 3D geometric data and the experiments
demonstrate the efficiency and efficacy of the proposed methods.
</summary>
    <author>
      <name>Saad Nadeem</name>
    </author>
    <author>
      <name>Zhengyu Su</name>
    </author>
    <author>
      <name>Wei Zeng</name>
    </author>
    <author>
      <name>Arie Kaufman</name>
    </author>
    <author>
      <name>Xianfeng Gu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2016.2542073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2016.2542073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics,
  23(6):1663-1676, 2017 (17 pages, 20 figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Vis. Comput. Graph., 23(6), pp.1663-1676, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.09031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.09031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.10206v1</id>
    <updated>2018-10-24T06:23:46Z</updated>
    <published>2018-10-24T06:23:46Z</published>
    <title>Immercity: a curation content application in Virtual and Augmented
  reality</title>
    <summary>  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
</summary>
    <author>
      <name>Jean-Daniel Taupiac</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICAR</arxiv:affiliation>
    </author>
    <author>
      <name>Nancy Rodriguez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICAR</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Strauss</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICAR</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-91584-5_18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-91584-5_18" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">10th International Conference, VAMR 2018, Held as Part of HCI
  International 2018, II (2), Springer, 2018, Virtual, Augmented and Mixed
  Reality: Applications in Health, Cultural Heritage, and Industry,
  978-3-319-91583-8. https://link.springer.com/book/10.1007/978-3-319-91584-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.10206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.11154v2</id>
    <updated>2018-11-01T17:22:34Z</updated>
    <published>2018-10-26T00:42:34Z</published>
    <title>Lightweight Structure Design Under Force Location Uncertainty</title>
    <summary>  We introduce a lightweight structure optimization approach for problems in
which there is uncertainty in the force locations. Such uncertainty may arise
due to force contact locations that change during use or are simply unknown a
priori. Given an input 3D model, regions on its boundary where arbitrary normal
forces may make contact, and a total force-magnitude budget, our algorithm
generates a minimum weight 3D structure that withstands any force configuration
capped by the budget. Our approach works by repeatedly finding the most
critical force configuration and altering the internal structure accordingly. A
key issue, however, is that the critical force configuration changes as the
structure evolves, resulting in a significant computational challenge. To
address this, we propose an efficient critical instant analysis approach.
Combined with a reduced order formulation, our method provides a practical
solution to the structural optimization problem. We demonstrate our method on a
variety of models and validate it with mechanical tests.
</summary>
    <author>
      <name>Erva Ulu</name>
    </author>
    <author>
      <name>James McCann</name>
    </author>
    <author>
      <name>Levent Burak Kara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3072959.3073626</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3072959.3073626" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics (TOG), 36(4), 158, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.11154v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11154v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03374v1</id>
    <updated>2018-11-08T12:21:43Z</updated>
    <published>2018-11-08T12:21:43Z</published>
    <title>Fast, High Precision Ray/Fiber Intersection using Tight, Disjoint
  Bounding Volumes</title>
    <summary>  Analyzing and identifying the shortcomings of current subdivision methods for
finding intersections of rays with fibers defined by the surface of a circular
contour swept along a B\'ezier curve, we present a new algorithm that improves
precision and performance. Instead of the inefficient pruning using overlapping
axis aligned bounding boxes and determining the closest point of approach of
the ray and the curve, we prune using disjoint bounding volumes defined by
cylinders and calculate the intersections on the limit surface. This in turn
allows for computing accurate parametric position and normal in the point of
intersection. The iteration requires only one bit per subdivision to avoid
costly stack memory operations. At a low number of subdivisions, the
performance of the high precision algorithm is competitive, while for a high
number of subdivisions it dramatically outperforms the state-of-the-art.
Besides an extensive mathematical analysis, source code is provided.
</summary>
    <author>
      <name>Nikolaus Binder</name>
    </author>
    <author>
      <name>Alexander Keller</name>
    </author>
    <link href="http://arxiv.org/abs/1811.03374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04337v2</id>
    <updated>2019-08-19T19:46:28Z</updated>
    <published>2018-11-11T03:21:36Z</published>
    <title>VV-Net: Voxel VAE Net with Group Convolutions for Point Cloud
  Segmentation</title>
    <summary>  We present a novel algorithm for point cloud segmentation. Our approach
transforms unstructured point clouds into regular voxel grids, and further uses
a kernel-based interpolated variational autoencoder (VAE) architecture to
encode the local geometry within each voxel. Traditionally, the voxel
representation only comprises Boolean occupancy information which fails to
capture the sparsely distributed points within voxels in a compact manner. In
order to handle sparse distributions of points, we further employ radial basis
functions (RBF) to compute a local, continuous representation within each
voxel. Our approach results in a good volumetric representation that
effectively tackles noisy point cloud datasets and is more robust for learning.
Moreover, we further introduce group equivariant CNN to 3D, by defining the
convolution operator on a symmetry group acting on $\mathbb{Z}^3$ and its
isomorphic sets. This improves the expressive capacity without increasing
parameters, leading to more robust segmentation results. We highlight the
performance on standard benchmarks and show that our approach outperforms
state-of-the-art segmentation algorithms on the ShapeNet and S3DIS datasets.
</summary>
    <author>
      <name>Hsien-Yu Meng</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>YuKun Lai</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by International Conference on Computer Vision (ICCV) 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04337v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04337v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05046v1</id>
    <updated>2018-11-12T23:37:46Z</updated>
    <published>2018-11-12T23:37:46Z</published>
    <title>Web3D Graphics enabled through Sensor Networks for Cost-Effective
  Assessment and Management of Energy Efficiency in Buildings</title>
    <summary>  The past decade has seen the advent of numerous building energy efficiency
visualization and simulation systems; however, most of them rely on theoretical
thermal models to suggest building structural design for new constructions and
modifications for existing ones. Sustainable methods of construction have made
tremendous progress. The example of the German Energy-Plus- House technology
uses a combination of (almost) zero-carbon passive heating technologies. A
web-enabled X3D visualization and simulation system coupled with a
cost-effective set of temperature/humidity sensors can provide valuable
insights into building design, materials and construction that can lead to
significant energy savings and an improved thermal comfort for residents,
resulting in superior building energy efficiency. A cost-effective
hardware-software prototype system is proposed in this paper that can provide
real-time data driven visualization or offline simulation of 3D thermal maps
for residential and/or commercial buildings on the Web.
</summary>
    <author>
      <name>Felix Hamza-Lup</name>
    </author>
    <author>
      <name>Marcel Maghiar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.gmod.2016.03.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.gmod.2016.03.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Graphical Models Journal, Volume 88, 2016, Pages 66-74, ISSN
  1524-0703</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.05046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06229v1</id>
    <updated>2018-11-15T08:28:00Z</updated>
    <published>2018-11-15T08:28:00Z</published>
    <title>Hair-GANs: Recovering 3D Hair Structure from a Single Image</title>
    <summary>  We introduce Hair-GANs, an architecture of generative adversarial networks,
to recover the 3D hair structure from a single image. The goal of our networks
is to build a parametric transformation from 2D hair maps to 3D hair structure.
The 3D hair structure is represented as a 3D volumetric field which encodes
both the occupancy and the orientation information of the hair strands. Given a
single hair image, we first align it with a bust model and extract a set of 2D
maps encoding the hair orientation information in 2D, along with the bust depth
map to feed into our Hair-GANs. With our generator network, we compute the 3D
volumetric field as the structure guidance for the final hair synthesis. The
modeling results not only resemble the hair in the input image but also
possesses many vivid details in other views. The efficacy of our method is
demonstrated by using a variety of hairstyles and comparing with the prior art.
</summary>
    <author>
      <name>Meng Zhang</name>
    </author>
    <author>
      <name>Youyi Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1811.06229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07023v1</id>
    <updated>2018-11-08T01:28:12Z</updated>
    <published>2018-11-08T01:28:12Z</published>
    <title>An Infinite Parade of Giraffes: Expressive Augmentation and Complexity
  Layers for Cartoon Drawing</title>
    <summary>  In this paper, we explore creative image generation constrained by small
data. To partially automate the creation of cartoon sketches consistent with a
specific designer's style, where acquiring a very large original image set is
impossible or cost prohibitive, we exploit domain specific knowledge for a huge
reduction in original image requirements, creating an effectively infinite
number of cartoon giraffes from just nine original drawings. We introduce
"expressive augmentations" for cartoon sketches, mathematical transformations
that create broad domain appropriate variation, far beyond the usual affine
transformations, and we show that chained GANs models trained on the temporal
stages of drawing or "complexity layers" can effectively add character
appropriate details and finish new drawings in the designer's style.
  We discuss the application of these tools in design processes for textiles,
graphics, architectural elements and interior design.
</summary>
    <author>
      <name>K. G. Greene</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07385v1</id>
    <updated>2018-11-18T19:27:56Z</updated>
    <published>2018-11-18T19:27:56Z</published>
    <title>STOAViz: Visualizing Saturated Thickness of Ogallala Aquifer</title>
    <summary>  In this paper, we introduce STOAViz, a visual analytics tool for analyzing
the saturated thickness of the Ogallala aquifer. The saturated thicknesses are
monitored by sensors integrated on wells distributed on a vast geographic area.
Our analytics application also captures the trends and patterns (such as
average/standard deviation over time, sudden increase/decrease of saturated
thicknesses) of water on an individual well and a group of wells based on their
geographic locations. To highlight the usefulness and effectiveness of STOAViz,
we demonstrate it on the Southern High Plains Aquifer of Texas. The work was
developed using feedback from experts at the water resource center at a
university. Moreover, our technique can be applied on any geographic areas
where wells and their measurements are available.
</summary>
    <author>
      <name>Tommy Dang</name>
    </author>
    <author>
      <name>Long Nguyen</name>
    </author>
    <author>
      <name>Abdullah Karim</name>
    </author>
    <author>
      <name>Venkatesh Uddameri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2312/envirvis.20171102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2312/envirvis.20171102" rel="related"/>
    <link href="http://arxiv.org/abs/1811.07385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07390v1</id>
    <updated>2018-11-18T20:01:04Z</updated>
    <published>2018-11-18T20:01:04Z</published>
    <title>A Study on 3D Surface Graph Representations</title>
    <summary>  Surface graphs have been used in many application domains to represent
three-dimensional (3D) data. Another approach to representing 3D data is making
projections onto two-dimensional (2D) graphs. This approach will result in
multiple displays, which is time-consuming in switching between different
screens for a different perspective. In this work, we study the performance of
3D version of popular 2D visualization techniques for time series: horizon
graph, small multiple, and simple line graph. We explore discrimination tasks
with respect to each visualization technique that requires simultaneous
representations. We demonstrate our study by visualizing saturated thickness of
the Ogallala aquifer - the Southern High Plains Aquifer of Texas in multiple
years. For the evaluation, we design comparison and discrimination tasks and
automatically record result performed by a group of students at a university.
Our results show that 3D small multiples perform well with stable accuracy over
numbers of occurrences. On the other hand, shared-space visualization within a
single 3D coordinate system is more efficient with small number of simultaneous
graphs. 3D horizon graph loses its competence in the 3D coordinate system with
the lowest accuracy comparing to other techniques. Our demonstration of 3D
spatial-temporal is also presented on the Southern High Plains Aquifer of Texas
from 2010 to 2016.
</summary>
    <author>
      <name>Long Nguyen</name>
    </author>
    <author>
      <name>Abdullah Karim</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07580v1</id>
    <updated>2018-11-19T09:49:39Z</updated>
    <published>2018-11-19T09:49:39Z</published>
    <title>Iso-level tool path planning for free-form surfaces</title>
    <summary>  The aim of tool path planning is to maximize the efficiency against some
given precision criteria. In practice, scallop height should be kept constant
to avoid unnecessary cutting, while the tool path should be smooth enough to
maintain a high feed rate. However, iso-scallop and smoothness often conflict
with each other. Existing methods smooth iso-scallop paths one-by-one, which
make the final tool path far from being globally optimal. This paper proposes a
new framework for tool path optimization. It views a family of iso-level curves
of a scalar function defined over the surface as tool path so that desired tool
path can be generated by finding the function that minimizes certain energy
functional and different objectives can be considered simultaneously. We use
the framework to plan globally optimal tool path with respect to iso-scallop
and smoothness. The energy functionals for planning iso-scallop, smoothness,
and optimal tool path are respectively derived, and the path topology is
studied too. Experimental results are given to show the effectiveness of the
proposed methods.
</summary>
    <author>
      <name>Qiang Zou</name>
    </author>
    <author>
      <name>Juyong Zhang</name>
    </author>
    <author>
      <name>Bailin Deng</name>
    </author>
    <author>
      <name>Jibin Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2014.04.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2014.04.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal paper, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer-Aided Design 53 (2014): 117-125</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.07580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.08004v1</id>
    <updated>2018-11-11T01:30:21Z</updated>
    <published>2018-11-11T01:30:21Z</published>
    <title>Photorealistic Facial Synthesis in the Dimensional Affect Space</title>
    <summary>  This paper presents a novel approach for synthesizing facial affect, which is
based on our annotating 600,000 frames of the 4DFAB database in terms of
valence and arousal. The input of this approach is a pair of these emotional
state descriptors and a neutral 2D image of a person to whom the corresponding
affect will be synthesized. Given this target pair, a set of 3D facial meshes
is selected, which is used to build a blendshape model and generate the new
facial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting
is performed and the reconstructed face is deformed to generate the target
facial expressions. Last, the new face is rendered into the original image.
Both qualitative and quantitative experimental studies illustrate the
generation of realistic images, when the neutral image is sampled from a
variety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE,
AFEW-VA, BU-3DFE, Bosphorus.
</summary>
    <author>
      <name>Dimitrios Kollias</name>
    </author>
    <author>
      <name>Shiyang Cheng</name>
    </author>
    <author>
      <name>Maja Pantic</name>
    </author>
    <author>
      <name>Stefanos Zafeiriou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1811.05027</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.08004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10036v1</id>
    <updated>2018-11-25T15:33:34Z</updated>
    <published>2018-11-25T15:33:34Z</published>
    <title>Procedural Crowd Generation for Semantically Augmented Virtual Cities</title>
    <summary>  Authoring realistic behaviors to populate a large virtual city can be a
cumbersome, time-consuming and error-prone task. Believable crowds require the
effort of storytellers and programming experts working together for long
periods of time. In this work, we present a new framework to allow users to
generate populated environments in an easier and faster way, by relying on the
use of procedural techniques. Our framework consists of the procedural
generation of semantically-augmented virtual cities to drive the procedural
generation and simulation of crowds. The main novelty lies in the generation of
agendas for each individual inhabitant (alone or as part of a family) by using
a rule-based grammar that combines city semantics with the autonomous persons'
characteristics. Real-world data can be used to accommodate the generation of a
virtual population, thus enabling the recreation of more realistic scenarios.
Users can author a new population or city by editing rule files with the
flexibility of re-using, combining or extending the rules of previous
populations. The results show how logical and consistent behaviors can be
easily generated for a large crowd providing a good starting point to bring
virtual cities to life.
</summary>
    <author>
      <name>O. Rogla</name>
    </author>
    <author>
      <name>N. Pelechano</name>
    </author>
    <author>
      <name>G. Patow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.10036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10175v1</id>
    <updated>2018-11-26T04:22:54Z</updated>
    <published>2018-11-26T04:22:54Z</published>
    <title>Multilevel active registration for kinect human body scans: from low
  quality to high quality</title>
    <summary>  Registration of 3D human body has been a challenging research topic for over
decades. Most of the traditional human body registration methods require manual
assistance, or other auxiliary information such as texture and markers. The
majority of these methods are tailored for high-quality scans from expensive
scanners. Following the introduction of the low-quality scans from
cost-effective devices such as Kinect, the 3D data capturing of human body
becomes more convenient and easier. However, due to the inevitable holes,
noises and outliers in the low-quality scan, the registration of human body
becomes even more challenging. To address this problem, we propose a fully
automatic active registration method which deforms a high-resolution template
mesh to match the low-quality human body scans. Our registration method
operates on two levels of statistical shape models: (1) the first level is a
holistic body shape model that defines the basic figure of human; (2) the
second level includes a set of shape models for every body part, aiming at
capturing more body details. Our fitting procedure follows a coarse-to-fine
approach that is robust and efficient. Experiments show that our method is
comparable with the state-of-the-art methods.
</summary>
    <author>
      <name>Zongyi Xu</name>
    </author>
    <author>
      <name>Qianni Zhang</name>
    </author>
    <author>
      <name>Shiyang Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, the Journal of Multimedia Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.10175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.11941v1</id>
    <updated>2018-11-29T03:24:03Z</updated>
    <published>2018-11-29T03:24:03Z</published>
    <title>Interactive X-ray and proton therapy training and simulation</title>
    <summary>  External beam X-ray therapy (XRT) and proton therapy (PT) are effective and
widely accepted forms of treatment for many types of cancer. However, the
procedures require extensive computerized planning. Current planning systems
for both XRT and PT have insufficient visual aid to combine real patient data
with the treatment device geometry to account for unforeseen collisions among
system components and the patient. We are proposing a cost-effective method to
extract patient specific S-reps in real time, and combine them with the
treatment system geometry to provide a comprehensive simulation of the XRT/PT
treatment room. The X3D standard is used to implement and deploy the simulator
on the web, enabling its use not only for remote specialists' collaboration,
simulation, and training, but also for patient education.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Shane Farrar</name>
    </author>
    <author>
      <name>Erik Leon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11548-015-1229-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11548-015-1229-7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computer Assisted Radiology and Surgery,
  Vol. 10(10) (2015), pp. 1675-1683</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.11941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.11947v1</id>
    <updated>2018-11-29T03:54:30Z</updated>
    <published>2018-11-29T03:54:30Z</published>
    <title>Online External Beam Radiation Treatment Simulator</title>
    <summary>  Radiation therapy is an effective and widely accepted form of treatment for
many types of cancer that requires extensive computerized planning.
Unfortunately, current treatment planning systems have limited or no visual aid
that combines patient volumetric models extracted from patient-specific CT data
with the treatment device geometry in a 3D interactive simulation. We
illustrate the potential of 3D simulation in radiation therapy with a web-based
interactive system that combines novel standards and technologies. We discuss
related research efforts in this area and present in detail several components
of the simulator. An objective assessment of the accuracy of the simulator and
a usability study prove the potential of such a system for simulation and
training.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Ivan Sopin</name>
    </author>
    <author>
      <name>Omar Zeidan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11548-008-0232-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11548-008-0232-7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Assisted Radiology and Surgery
  (2008), Vol. 3(4), pp. 275-281</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.11947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12597v1</id>
    <updated>2018-11-30T03:21:19Z</updated>
    <published>2018-11-30T03:21:19Z</published>
    <title>Constructing Trivariate B-splines with Positive Jacobian by Pillow
  Operation and Geometric Iterative Fitting</title>
    <summary>  The advent of isogeometric analysis has prompted a need for methods to
generate Trivariate B-spline Solids (TBS) with positive Jacobian. However, it
is difficult to guarantee a positive Jacobian of a TBS since the geometric
pre-condition for ensuring the positive Jacobian is very complicated. In this
paper, we propose a method for generating TBSs with guaranteed positive
Jacobian. For the study, we used a tetrahedral (tet) mesh model and segmented
it into sub-volumes using the pillow operation. Then, to reduce the difficulty
in ensuring a positive Jacobian, we separately fitted the boundary curves and
surfaces and the sub-volumes using a geometric iterative fitting algorithm.
Finally, the smoothness between adjacent TBSs is improved. The experimental
examples presented in this paper demonstrate the effectiveness and efficiency
of the developed algorithm.
</summary>
    <author>
      <name>Hongwei Lin</name>
    </author>
    <author>
      <name>Hao Huang</name>
    </author>
    <author>
      <name>Chuanfeng Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1811.12597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12599v1</id>
    <updated>2018-11-30T03:27:11Z</updated>
    <published>2018-11-30T03:27:11Z</published>
    <title>Gregory Solid Construction for Polyhedral Volume Parameterization by
  Sparse Optimization</title>
    <summary>  In isogeometric analysis, it is frequently required to handle the geometric
models enclosed by four-sided or non-four-sided boundary patches, such as
trimmed surfaces. In this paper, we develop a Gregory solid based method to
parameterize those models. First, we extend the Gregory patch representation to
the trivariate Gregory solid representation. Second, the trivariate Gregory
solid representation is employed to interpolate the boundary patches of a
geometric model, thus generating the polyhedral volume parametrization. To
improve the regularity of the polyhedral volume parametrization, we formulate
the construction of the trivariate Gregory solid as a sparse optimization
problem, where the optimization objective function is a linear combination of
some terms, including a sparse term aiming to reduce the negative Jacobian area
of the Gregory solid. Then, the alternating direction method of multipliers
(ADMM) is used to solve the sparse optimization problem. Lots of experimental
examples illustrated in this paper demonstrate the effectiveness and efficiency
of the developed method.
</summary>
    <author>
      <name>Chuanfeng Hu</name>
    </author>
    <author>
      <name>Hongwei Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1811.12599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.02037v1</id>
    <updated>2019-01-07T19:51:43Z</updated>
    <published>2019-01-07T19:51:43Z</published>
    <title>Modeling Data-Driven Dominance Traits for Virtual Characters using Gait
  Analysis</title>
    <summary>  We present a data-driven algorithm for generating gaits of virtual characters
with varying dominance traits. Our formulation utilizes a user study to
establish a data-driven dominance mapping between gaits and dominance labels.
We use our dominance mapping to generate walking gaits for virtual characters
that exhibit a variety of dominance traits while interacting with the user.
Furthermore, we extract gait features based on known criteria in visual
perception and psychology literature that can be used to identify the dominance
levels of any walking gait. We validate our mapping and the perceived dominance
traits by a second user study in an immersive virtual environment. Our gait
dominance classification algorithm can classify the dominance traits of gaits
with ~73% accuracy. We also present an application of our approach that
simulates interpersonal relationships between virtual characters. To the best
of our knowledge, ours is the first practical approach to classifying gait
dominance and generate dominance traits in virtual characters.
</summary>
    <author>
      <name>Tanmay Randhavane</name>
    </author>
    <author>
      <name>Aniket Bera</name>
    </author>
    <author>
      <name>Emily Kubin</name>
    </author>
    <author>
      <name>Kurt Gray</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <link href="http://arxiv.org/abs/1901.02037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.02037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.03427v2</id>
    <updated>2019-01-19T07:32:09Z</updated>
    <published>2019-01-10T23:04:46Z</published>
    <title>Stroke-based sketched symbol reconstruction and segmentation</title>
    <summary>  Hand-drawn objects usually consist of multiple semantically meaningful parts.
For example, a stick figure consists of a head, a torso, and pairs of legs and
arms. Efficient and accurate identification of these subparts promises to
significantly improve algorithms for stylization, deformation, morphing and
animation of 2D drawings. In this paper, we propose a neural network model that
segments symbols into stroke-level components. Our segmentation framework has
two main elements: a fixed feature extractor and a Multilayer Perceptron (MLP)
network that identifies a component based on the feature. As the feature
extractor we utilize an encoder of a stroke-rnn, which is our newly proposed
generative Variational Auto-Encoder (VAE) model that reconstructs symbols on a
stroke by stroke basis. Experiments show that a single encoder could be reused
for segmenting multiple categories of sketched symbols with negligible effects
on segmentation accuracies. Our segmentation scores surpass existing
methodologies on an available small state of the art dataset. Moreover,
extensive evaluations on our newly annotated big dataset demonstrate that our
framework obtains significantly better accuracies as compared to baseline
models. We release the dataset to the community.
</summary>
    <author>
      <name>Kurmanbek Kaiyrbekov</name>
    </author>
    <author>
      <name>Metin Sezgin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCG.2019.2943333</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCG.2019.2943333" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computer Graphics and Applications, vol. 40, no. 1, pp.
  112-126, 1 Jan.-Feb. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.03427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.03968v1</id>
    <updated>2019-01-13T12:15:11Z</updated>
    <published>2019-01-13T12:15:11Z</published>
    <title>A Fully Bayesian Infinite Generative Model for Dynamic Texture
  Segmentation</title>
    <summary>  Generative dynamic texture models (GDTMs) are widely used for dynamic texture
(DT) segmentation in the video sequences. GDTMs represent DTs as a set of
linear dynamical systems (LDSs). A major limitation of these models concerns
the automatic selection of a proper number of DTs. Dirichlet process mixture
(DPM) models which have appeared recently as the cornerstone of the
non-parametric Bayesian statistics, is an optimistic candidate toward resolving
this issue. Under this motivation to resolve the aforementioned drawback, we
propose a novel non-parametric fully Bayesian approach for DT segmentation,
formulated on the basis of a joint DPM and GDTM construction. This interaction
causes the algorithm to overcome the problem of automatic segmentation
properly. We derive the Variational Bayesian Expectation-Maximization (VBEM)
inference for the proposed model. Moreover, in the E-step of inference, we
apply Rauch-Tung-Striebel smoother (RTSS) algorithm on Variational Bayesian
LDSs. Ultimately, experiments on different video sequences are performed.
Experiment results indicate that the proposed algorithm outperforms the
previous methods in efficiency and accuracy noticeably.
</summary>
    <author>
      <name>Sahar Yousefi</name>
    </author>
    <author>
      <name>M. T. Manzuri Shalmani</name>
    </author>
    <author>
      <name>Antoni B. Chan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages; 15 figures;</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.04686v1</id>
    <updated>2019-01-15T07:21:52Z</updated>
    <published>2019-01-15T07:21:52Z</published>
    <title>Image Synthesis and Style Transfer</title>
    <summary>  Affine transformation, layer blending, and artistic filters are popular
processes that graphic designers employ to transform pixels of an image to
create a desired effect. Here, we examine various approaches that synthesize
new images: pixel-based compositing models and in particular, distributed
representations of deep neural network models. This paper focuses on
synthesizing new images from a learned representation model obtained from the
VGG network. This approach offers an interesting creative process from its
distributed representation of information in hidden layers of a deep VGG
network i.e., information such as contour, shape, etc. are effectively captured
in hidden layers of neural networks. Conceptually, if $\Phi$ is the function
that transforms input pixels into distributed representations of VGG layers
${\bf h}$, a new synthesized image $X$ can be generated from its inverse
function, $X = \Phi^{-1}({\bf h})$. We describe the concept behind the
approach, present some representative synthesized images and style-transferred
image examples.
</summary>
    <author>
      <name>Somnuk Phon-Amnuaisuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.04686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.04944v1</id>
    <updated>2018-12-17T13:14:19Z</updated>
    <published>2018-12-17T13:14:19Z</published>
    <title>Computational Fluid Dynamics on 3D Point Set Surfaces</title>
    <summary>  Computational fluid dynamics (CFD) in many cases requires designing 3D models
manually, which is a tedious task that requires specific skills. In this paper,
we present a novel method for performing CFD directly on scanned 3D point
clouds. The proposed method builds an anisotropic volumetric tetrahedral mesh
adapted around a point-sampled surface, without an explicit surface
reconstruction step. The surface is represented by a new extended implicit
moving least squares (EIMLS) scalar representation that extends the definition
of the function to the entire computational domain, which makes it possible for
use in immersed boundary flow simulations. The workflow we present allows us to
compute flows around point-sampled geometries automatically. It also gives a
better control of the precision around the surface with a limited number of
computational nodes, which is a critical issue in CFD.
</summary>
    <author>
      <name>Hassan Bouchiba</name>
    </author>
    <author>
      <name>Simon Santoso</name>
    </author>
    <author>
      <name>Jean-Emmanuel Deschaud</name>
    </author>
    <author>
      <name>Luisa Rocha-Da-Silva</name>
    </author>
    <author>
      <name>François Goulette</name>
    </author>
    <author>
      <name>Thierry Coupez</name>
    </author>
    <link href="http://arxiv.org/abs/1901.04944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05637v1</id>
    <updated>2019-01-17T06:08:33Z</updated>
    <published>2019-01-17T06:08:33Z</published>
    <title>Computational Design of Lightweight Trusses</title>
    <summary>  Trusses are load-carrying light-weight structures consisting of bars
connected at joints ubiquitously applied in a variety of engineering scenarios.
Designing optimal trusses that satisfy functional specifications with a minimal
amount of material has interested both theoreticians and practitioners for more
than a century. In this paper, we introduce two main ideas to improve upon the
state of the art. First, we formulate an alternating linear programming problem
for geometry optimization. Second, we introduce two sets of complementary
topological operations, including a novel subdivision scheme for global
topology refinement inspired by Michell's famed theoretical study. Based on
these two ideas, we build an efficient computational framework for the design
of lightweight trusses. \AD{We illustrate our framework with a variety of
functional specifications and extensions. We show that our method achieves
trusses with smaller volumes and is over two orders of magnitude faster
compared with recent state-of-the-art approaches.
</summary>
    <author>
      <name>Caigui Jiang</name>
    </author>
    <author>
      <name>Chengcheng Tang</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Renjie Chen</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,34 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.05637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.06408v1</id>
    <updated>2019-01-18T20:02:35Z</updated>
    <published>2019-01-18T20:02:35Z</published>
    <title>Metasurfaces for near-eye augmented reality</title>
    <summary>  Augmented reality (AR) has the potential to revolutionize the way in which
information is presented by overlaying virtual information onto a person's
direct view of their real-time surroundings. By placing the display on the
surface of the eye, a contact lens display (CLD) provides a versatile solution
for compact AR. However, an unaided human eye cannot visualize patterns on the
CLD simply because of the limited accommodation of the eye. Here, we introduce
a holographic display technology that casts virtual information directly to the
retina so that the eye sees it while maintaining the visualization of the
real-world intact. The key to our design is to introduce metasurfaces to create
a phase distribution that projects virtual information in a pixel-by-pixel
manner. Unlike conventional holographic techniques, our metasurface-based
technique is able to display arbitrary patterns using a single passive
hologram. With a small form-factor, the designed metasurface empowers near-eye
AR excluding the need of extra optical elements, such as a spatial light
modulator, for dynamic image control.
</summary>
    <author>
      <name>Shoufeng Lan</name>
    </author>
    <author>
      <name>Xueyue Zhang</name>
    </author>
    <author>
      <name>Mohammad Taghinejad</name>
    </author>
    <author>
      <name>Sean Rodrigues</name>
    </author>
    <author>
      <name>Kyu-Tae Lee</name>
    </author>
    <author>
      <name>Zhaocheng Liu</name>
    </author>
    <author>
      <name>Wenshan Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.06408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.07165v1</id>
    <updated>2019-01-22T03:50:59Z</updated>
    <published>2019-01-22T03:50:59Z</published>
    <title>Generation High resolution 3D model from natural language by Generative
  Adversarial Network</title>
    <summary>  We present a method of generating high resolution 3D shapes from natural
language descriptions. To achieve this goal, we propose two steps that
generating low resolution shapes which roughly reflect texts and generating
high resolution shapes which reflect the detail of texts. In a previous paper,
the authors have shown a method of generating low resolution shapes. We improve
it to generate 3D shapes more faithful to natural language and test the
effectiveness of the method. To generate high resolution 3D shapes, we use the
framework of Conditional Wasserstein GAN. We propose two roles of Critic
separately, which calculate the Wasserstein distance between two probability
distribution, so that we achieve generating high quality shapes or acceleration
of learning speed of model. To evaluate our approach, we performed quantitive
evaluation with several numerical metrics for Critic models. Our method is
first to realize the generation of high quality model by propagating text
embedding information to high resolution task when generating 3D model.
</summary>
    <author>
      <name>Kentaro Fukamizu</name>
    </author>
    <author>
      <name>Masaaki Kondo</name>
    </author>
    <author>
      <name>Ryuichi Sakamoto</name>
    </author>
    <link href="http://arxiv.org/abs/1901.07165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08397v1</id>
    <updated>2019-01-24T13:32:03Z</updated>
    <published>2019-01-24T13:32:03Z</published>
    <title>Periodic-corrected data driven coupling of blood flow and vessel wall
  for virtual surgery</title>
    <summary>  Fast and realistic coupling of blood flow and vessel wall is of great
importance to virtual surgery. In this paper, we propose a novel data-driven
coupling method that formulates physics-based blood flow simulation as a
regression problem, using an improved periodic-corrected neural network
(PcNet), estimating the acceleration of every particle at each frame to obtain
fast, stable and realistic simulation. We design a particle state feature
vector based on smoothed particle hydrodynamics (SPH), modeling the mixed
contribution of neighboring proxy particles on the blood vessel wall and
neighboring blood particles, giving the extrapolation ability to deal with more
complex couplings. We present a semi-supervised training strategy to improve
the traditional BP neural network, which corrects the error periodically to
ensure long term stability. Experimental results demonstrate that our method is
able to implement stable and vivid coupling of blood flow and vessel wall while
greatly improving computational efficiency.
</summary>
    <author>
      <name>Xuejie Mai</name>
    </author>
    <author>
      <name>Zhiyong Yuan</name>
    </author>
    <author>
      <name>Qianqian Tong</name>
    </author>
    <author>
      <name>Tianchen Yuan</name>
    </author>
    <author>
      <name>Jianhui Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1901.08397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.00650v1</id>
    <updated>2019-02-02T06:26:22Z</updated>
    <published>2019-02-02T06:26:22Z</published>
    <title>Volumetric Spline Parameterization for Isogeometric Analysis</title>
    <summary>  Given the spline representation of the boundary of a three dimensional
domain, constructing a volumetric spline parameterization of the domain (i.e.,
a map from a unit cube to the domain) with the given boundary is a fundamental
problem in isogeometric analysis. A good domain parameterization should satisfy
the following criteria: (1) the parameterization is a bijective map; and (2)
the map has lowest possible distortion. However, none of the state-of-the-art
volumetric parameterization methods has fully addressed the above issues. In
this paper, we propose a three-stage approach for constructing volumetric
parameterization satisfying the above criteria. Firstly, a harmonic map is
computed between a unit cube and the computational domain. Then a bijective map
modeled by a max-min optimization problem is computed in a coarse-to-fine way,
and an algorithm based on divide and conquer strategy is proposed to solve the
optimization problem efficiently. Finally, to ensure high quality of the
parameterization, the MIPS (Most Isometric Parameterizations) method is adopted
to reduce the conformal distortion of the bijective map. We provide several
examples to demonstrate the feasibility of our approach and to compare our
approach with some state-of-the-art methods. The results show that our
algorithm produces bijective parameterization with high quality even for
complex domains.
</summary>
    <author>
      <name>Maodong Pan</name>
    </author>
    <author>
      <name>Falai Chen</name>
    </author>
    <author>
      <name>Weihua Tong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cma.2019.112769</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cma.2019.112769" rel="related"/>
    <link href="http://arxiv.org/abs/1902.00650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.00801v1</id>
    <updated>2019-02-02T21:52:51Z</updated>
    <published>2019-02-02T21:52:51Z</published>
    <title>A Robust Volume Conserving Method for Character-Water Interaction</title>
    <summary>  We propose a novel volume conserving framework for character-water
interaction, using a novel volume-of-fluid solver on a skinned tetrahedral
mesh, enabling the high degree of the spatial adaptivity in order to capture
thin films and hair-water interactions. For efficiency, the bulk of the fluid
volume is simulated with a standard Eulerian solver which is two way coupled to
our skinned arbitrary Lagrangian-Eulerian mesh using a fast, robust, and
straightforward to implement partitioned approach. This allows for a
specialized and efficient treatment of the volume-of-fluid solver, since it is
only required in a subset of the domain. The combination of conservation of
fluid volume and a kinematically deforming skinned mesh allows us to robustly
implement interesting effects such as adhesion, and anisotropic porosity. We
illustrate the efficacy of our method by simulating various water effects with
solid objects and animated characters.
</summary>
    <author>
      <name>Minjae Lee</name>
    </author>
    <author>
      <name>David Hyde</name>
    </author>
    <author>
      <name>Kevin Li</name>
    </author>
    <author>
      <name>Ronald Fedkiw</name>
    </author>
    <link href="http://arxiv.org/abs/1902.00801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02371v2</id>
    <updated>2019-02-28T20:02:42Z</updated>
    <published>2019-02-06T19:29:51Z</published>
    <title>Diffeomorphic Medial Modeling</title>
    <summary>  Deformable shape modeling approaches that describe objects in terms of their
medial axis geometry (e.g., m-reps [Pizer et al., 2003]) yield rich geometrical
features that can be useful for analyzing the shape of sheet-like biological
structures, such as the myocardium. We present a novel shape analysis approach
that combines the benefits of medial shape modeling and diffeomorphometry. Our
algorithm is formulated as a problem of matching shapes using diffeomorphic
flows under constraints that approximately preserve medial axis geometry during
deformation. As the result, correspondence between the medial axes of similar
shapes is maintained. The approach is evaluated in the context of modeling the
shape of the left ventricular wall from 3D echocardiography images.
</summary>
    <author>
      <name>Paul A. Yushkevich</name>
    </author>
    <author>
      <name>Ahmed Aly</name>
    </author>
    <author>
      <name>Jiancong Wang</name>
    </author>
    <author>
      <name>Long Xie</name>
    </author>
    <author>
      <name>Robert C. Gorman</name>
    </author>
    <author>
      <name>Laurent Younes</name>
    </author>
    <author>
      <name>Alison Pouch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 26th International Conference on Information
  Processing in Medical Imaging (IPMI 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02806v1</id>
    <updated>2019-02-07T19:11:14Z</updated>
    <published>2019-02-07T19:11:14Z</published>
    <title>Automated pebble mosaic stylization of images</title>
    <summary>  Digital mosaics have usually used regular tiles, simulating the historical
"tessellated" mosaics. In this paper, we present a method for synthesizing
pebble mosaics, a historical mosaic style in which the tiles are rounded
pebbles. We address both the tiling problem, where pebbles are distributed over
the image plane so as to approximate the input image content, and the problem
of geometry, creating a smooth rounded shape for each pebble. We adapt SLIC,
simple linear iterative clustering, to obtain elongated tiles conforming to
image content, and smooth the resulting irregular shapes into shapes resembling
pebble cross-sections. Then, we create an interior and exterior contour for
each pebble and solve a Laplace equation over the region between them to obtain
height-field geometry. The resulting pebble set approximates the input image
while presenting full geometry that can be rendered and textured for a highly
detailed representation of a pebble mosaic.
</summary>
    <author>
      <name>Lars Doyle</name>
    </author>
    <author>
      <name>Forest Anderson</name>
    </author>
    <author>
      <name>Ehren Choy</name>
    </author>
    <author>
      <name>David Mould</name>
    </author>
    <link href="http://arxiv.org/abs/1902.02806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02906v1</id>
    <updated>2019-02-08T01:19:54Z</updated>
    <published>2019-02-08T01:19:54Z</published>
    <title>X3D in Urban Planning - Savannah in 3D</title>
    <summary>  Urban planning often raises complex issues that are difficult to visualize
and challenging to communicate. The increasing availability of 3D modeling
standards has provided the opportunity for many developers, engineers,
designers, planners, investors, and government officials to effectively
collaborate to bring projects to fruition. Because of its real-time
interactivity and widespread web-based content players, X3D proves to be a good
choice for developing and visualizing 3D city content on the Web for planning
purposes.
  Passenger rail is a viable and cost-effective transportation solution in many
areas, especially in view of rising energy costs. The Savannah in 3D (or S3D)
project is a multimedia tool for a feasibility study designed to bring
passenger rail to Savannah; thereby opening up the historic, tourist-friendly
city to a wider audience. The paper outlines the development process of an
interactive 3D train model as it journeys from Atlanta to Savannah, Georgia -
focusing on user interactivity and scene immersion to supplement the city and
transportation planning agenda.
</summary>
    <author>
      <name> Faith-Anne</name>
    </author>
    <author>
      <name>L. Kocadag</name>
    </author>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM-SE 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04285v1</id>
    <updated>2019-02-12T09:09:17Z</updated>
    <published>2019-02-12T09:09:17Z</published>
    <title>Puppet Dubbing</title>
    <summary>  Dubbing puppet videos to make the characters (e.g. Kermit the Frog)
convincingly speak a new speech track is a popular activity with many examples
of well-known puppets speaking lines from films or singing rap songs. But
manually aligning puppet mouth movements to match a new speech track is tedious
as each syllable of the speech must match a closed-open-closed segment of mouth
movement for the dub to be convincing. In this work, we present two methods to
align a new speech track with puppet video, one semi-automatic appearance-based
and the other fully-automatic audio-based. The methods offer complementary
advantages and disadvantages. Our appearance-based approach directly identifies
closed-open-closed segments in the puppet video and is robust to low-quality
audio as well as misalignments between the mouth movements and speech in the
original performance, but requires some manual annotation. Our audio-based
approach assumes the original performance matches a closed-open-closed mouth
segment to each syllable of the original speech. It is fully automatic, robust
to visual occlusions and fast puppet movements, but does not handle
misalignments in the original performance. We compare the methods and show that
both improve the credibility of the resulting video over simple baseline
techniques, via quantitative evaluation and user ratings.
</summary>
    <author>
      <name>Ohad Fried</name>
    </author>
    <author>
      <name>Maneesh Agrawala</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics Symposium on Rendering, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.04285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.05942v2</id>
    <updated>2021-02-03T12:30:49Z</updated>
    <published>2019-02-15T18:50:52Z</published>
    <title>Massively Parallel Path Space Filtering</title>
    <summary>  Restricting path tracing to a small number of paths per pixel for performance
reasons rarely achieves a satisfactory image quality for scenes of interest.
However, path space filtering may dramatically improve the visual quality by
sharing information across vertices of paths classified as proximate. Unlike
screen space-based approaches, these paths neither need to be present on the
screen, nor is filtering restricted to the first intersection with the scene.
While searching proximate vertices had been more expensive than filtering in
screen space, we greatly improve over this performance penalty by storing,
updating, and looking up the required information in a hash table. The keys are
constructed from jittered and quantized information, such that only a single
query very likely replaces costly neighborhood searches. A massively parallel
implementation of the algorithm is demonstrated on a graphics processing unit
(GPU).
</summary>
    <author>
      <name>Nikolaus Binder</name>
    </author>
    <author>
      <name>Sascha Fricke</name>
    </author>
    <author>
      <name>Alexander Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted for the proceedings of MCQMC2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.05942v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05942v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.06082v2</id>
    <updated>2019-10-10T20:22:56Z</updated>
    <published>2019-02-16T10:37:00Z</published>
    <title>Local Fourier Slice Photography</title>
    <summary>  Light field cameras provide intriguing possibilities, such as post-capture
refocus or the ability to synthesize images from novel viewpoints. This comes,
however, at the price of significant storage requirements. Compression
techniques can be used to reduce these but refocusing and reconstruction
require so far again a dense pixel representation. To avoid this, we introduce
local Fourier slice photography that allows for refocused image reconstruction
directly from a sparse wavelet representation of a light field, either to
obtain an image or a compressed representation of it. The result is made
possible by wavelets that respect the "slicing's" intrinsic structure and
enable us to derive exact reconstruction filters for the refocused image in
closed form. Image reconstruction then amounts to applying these filters to the
light field's wavelet coefficients, and hence no reconstruction of a dense
pixel representation is required. We demonstrate that this substantially
reduces storage requirements and also computation times. We furthermore analyze
the computational complexity of our algorithm and show that it scales linearly
with the size of the reconstructed region and the non-negligible wavelet
coefficients, i.e. with the visual complexity.
</summary>
    <author>
      <name>Christian Lessig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">images with reduced quality (please contact the author for a high
  resolution version)</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.06082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.06082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.08228v1</id>
    <updated>2019-02-20T16:03:11Z</updated>
    <published>2019-02-20T16:03:11Z</published>
    <title>A Comprehensive Theory and Variational Framework for Anti-aliasing
  Sampling Patterns</title>
    <summary>  In this paper, we provide a comprehensive theory of anti-aliasing sampling
patterns that explains and revises known results, and show how patterns as
predicted by the theory can be generated via a variational optimization
framework. We start by deriving the exact spectral expression for expected
error in reconstructing an image in terms of power spectra of sampling
patterns, and analyzing how the shape of power spectra is related to
anti-aliasing properties. Based on this analysis, we then formulate the problem
of generating anti-aliasing sampling patterns as constrained variational
optimization on power spectra. This allows us to not rely on any parametric
form, and thus explore the whole space of realizable spectra. We show that the
resulting optimized sampling patterns lead to reconstructions with less visible
aliasing artifacts, while keeping low frequencies as clean as possible.
</summary>
    <author>
      <name>A. Cengiz Öztireli</name>
    </author>
    <link href="http://arxiv.org/abs/1902.08228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.08228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01249v1</id>
    <updated>2019-03-01T18:16:20Z</updated>
    <published>2019-03-01T18:16:20Z</published>
    <title>Liver Pathology Simulation: Algorithm for Haptic Rendering and Force
  Maps for Palpation Assessment</title>
    <summary>  Preoperative gestures include tactile sampling of the mechanical properties
of biological tissue for both histological and pathological considerations.
Tactile properties used in conjunction with visual cues can provide useful
feedback to the surgeon. Development of novel cost effective haptic-based
simulators and their introduction in the minimally invasive surgery learning
cycle can absorb the learning curve for your residents. Receiving pre-training
in a core set of surgical skills can reduce skill acquisition time and risks.
We present the integration of a real-time surface stiffness adjustment
algorithm and a novel paradigm -- force maps -- in a visuo-haptic simulator
module designed to train internal organs disease diagnostics through palpation.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Adrian Seitan</name>
    </author>
    <author>
      <name>Dorin M. Popovici</name>
    </author>
    <author>
      <name>Crenguta M. Bogdan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/978-1-61499-209-7-175</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/978-1-61499-209-7-175" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1812.03325</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Medicine Meets Virtual Reality MMVR, 2013, pp. 175-181</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.01249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.06763v1</id>
    <updated>2019-03-15T19:07:34Z</updated>
    <published>2019-03-15T19:07:34Z</published>
    <title>Smart, Deep Copy-Paste</title>
    <summary>  In this work, we propose a novel system for smart copy-paste, enabling the
synthesis of high-quality results given a masked source image content and a
target image context as input. Our system naturally resolves both shading and
geometric inconsistencies between source and target image, resulting in a
merged result image that features the content from the pasted source image,
seamlessly pasted into the target context. Our framework is based on a novel
training image transformation procedure that allows to train a deep
convolutional neural network end-to-end to automatically learn a representation
that is suitable for copy-pasting. Our training procedure works with any image
dataset without additional information such as labels, and we demonstrate the
effectiveness of our system on two popular datasets, high-resolution face
images and the more complex Cityscapes dataset. Our technique outperforms the
current state of the art on face images, and we show promising results on the
Cityscapes dataset, demonstrating that our system generalizes to much higher
resolution than the training data.
</summary>
    <author>
      <name>Tiziano Portenier</name>
    </author>
    <author>
      <name>Qiyang Hu</name>
    </author>
    <author>
      <name>Paolo Favaro</name>
    </author>
    <author>
      <name>Matthias Zwicker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.06763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02526v1</id>
    <updated>2019-04-03T17:59:41Z</updated>
    <published>2019-04-03T17:59:41Z</published>
    <title>Constrained Generative Adversarial Networks for Interactive Image
  Generation</title>
    <summary>  Generative Adversarial Networks (GANs) have received a great deal of
attention due in part to recent success in generating original, high-quality
samples from visual domains. However, most current methods only allow for users
to guide this image generation process through limited interactions. In this
work we develop a novel GAN framework that allows humans to be "in-the-loop" of
the image generation process. Our technique iteratively accepts relative
constraints of the form "Generate an image more like image A than image B".
After each constraint is given, the user is presented with new outputs from the
GAN, informing the next round of feedback. This feedback is used to constrain
the output of the GAN with respect to an underlying semantic space that can be
designed to model a variety of different notions of similarity (e.g. classes,
attributes, object relationships, color, etc.). In our experiments, we show
that our GAN framework is able to generate images that are of comparable
quality to equivalent unsupervised GANs while satisfying a large number of the
constraints provided by users, effectively changing a GAN into one that allows
users interactive control over image generation without sacrificing image
quality.
</summary>
    <author>
      <name>Eric Heim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear in the Proceedings of the 2019 Conference on Computer
  Vision and Pattern Recognition</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03620v1</id>
    <updated>2019-04-07T10:23:47Z</updated>
    <published>2019-04-07T10:23:47Z</published>
    <title>Teaching GANs to Sketch in Vector Format</title>
    <summary>  Sketching is more fundamental to human cognition than speech. Deep Neural
Networks (DNNs) have achieved the state-of-the-art in speech-related tasks but
have not made significant development in generating stroke-based sketches a.k.a
sketches in vector format. Though there are Variational Auto Encoders (VAEs)
for generating sketches in vector format, there is no Generative Adversarial
Network (GAN) architecture for the same. In this paper, we propose a standalone
GAN architecture SkeGAN and a VAE-GAN architecture VASkeGAN, for sketch
generation in vector format. SkeGAN is a stochastic policy in Reinforcement
Learning (RL), capable of generating both multidimensional continuous and
discrete outputs. VASkeGAN hybridizes a VAE and a GAN, in order to couple the
efficient representation of data by VAE with the powerful generating
capabilities of a GAN, to produce visually appealing sketches. We also propose
a new metric called the Ske-score which quantifies the quality of vector
sketches. We have validated that SkeGAN and VASkeGAN generate visually
appealing sketches by using Human Turing Test and Ske-score.
</summary>
    <author>
      <name>Varshaneya V</name>
    </author>
    <author>
      <name>S Balasubramanian</name>
    </author>
    <author>
      <name>Vineeth N Balasubramanian</name>
    </author>
    <link href="http://arxiv.org/abs/1904.03620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.04954v1</id>
    <updated>2019-04-10T00:28:41Z</updated>
    <published>2019-04-10T00:28:41Z</published>
    <title>Curve and surface construction based on the generalized toric-Bernstein
  basis functions</title>
    <summary>  The construction of parametric curve and surface plays important role in
computer aided geometric design (CAGD), computer aided design (CAD), and
geometric modeling. In this paper, we define a new kind of blending functions
associated with a real points set, called generalized toric-Bernstein
(GT-Bernstein) basis functions. Then the generalized toric-Bezier (GT-B\'ezier)
curves and surfaces are constructed based on the GT-Bernstein basis functions,
which are the projections of the (irrational) toric varieties in fact and the
generalizations of the classical rational B\'ezier curves and surfaces and
toric surface patches. Furthermore, we also study the properties of the
presented curves and surfaces, including the limiting properties of weights and
knots. Some representative examples verify the properties and results.
</summary>
    <author>
      <name>Jing-Gai Li</name>
    </author>
    <author>
      <name>Chun-Gang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, many figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 68U07, 41A20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.07865v4</id>
    <updated>2019-09-12T13:15:05Z</updated>
    <published>2019-04-16T17:04:13Z</published>
    <title>ZoomOut: Spectral Upsampling for Efficient Shape Correspondence</title>
    <summary>  We present a simple and efficient method for refining maps or correspondences
by iterative upsampling in the spectral domain that can be implemented in a few
lines of code. Our main observation is that high quality maps can be obtained
even if the input correspondences are noisy or are encoded by a small number of
coefficients in a spectral basis. We show how this approach can be used in
conjunction with existing initialization techniques across a range of
application scenarios, including symmetry detection, map refinement across
complete shapes, non-rigid partial shape matching and function transfer. In
each application we demonstrate an improvement with respect to both the quality
of the results and the computational speed compared to the best competing
methods, with up to two orders of magnitude speed-up in some applications. We
also demonstrate that our method is both robust to noisy input and is scalable
with respect to shape complexity. Finally, we present a theoretical
justification for our approach, shedding light on structural properties of
functional maps.
</summary>
    <author>
      <name>Simone Melzi</name>
    </author>
    <author>
      <name>Jing Ren</name>
    </author>
    <author>
      <name>Emanuele Rodolà</name>
    </author>
    <author>
      <name>Abhishek Sharma</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 26 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07865v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07865v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08225v1</id>
    <updated>2019-04-17T12:31:58Z</updated>
    <published>2019-04-17T12:31:58Z</published>
    <title>Rendering of Complex Heterogenous Scenes using Progressive Blue Surfels</title>
    <summary>  We present a technique for rendering highly complex 3D scenes in real-time by
generating uniformly distributed points on the scene's visible surfaces. The
technique is applicable to a wide range of scene types, like scenes directly
based on complex and detailed CAD data consisting of billions of polygons (in
contrast to scenes handcrafted solely for visualization). This allows to
visualize such scenes smoothly even in VR on a HMD with good image quality,
while maintaining the necessary frame-rates. In contrast to other point based
rendering methods, we place points in an approximated blue noise distribution
only on visible surfaces and store them in a highly GPU efficient data
structure, allowing to progressively refine the number of rendered points to
maximize the image quality for a given target frame rate. Our evaluation shows
that scenes consisting of a high amount of polygons can be rendered with
interactive frame rates with good visual quality on standard hardware.
</summary>
    <author>
      <name>Sascha Brandt</name>
    </author>
    <author>
      <name>Claudius Jähn</name>
    </author>
    <author>
      <name>Matthias Fischer</name>
    </author>
    <author>
      <name>Friedhelm Meyer auf der Heide</name>
    </author>
    <link href="http://arxiv.org/abs/1904.08225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08921v2</id>
    <updated>2020-03-19T15:07:10Z</updated>
    <published>2019-04-18T17:55:57Z</published>
    <title>Deep Parametric Shape Predictions using Distance Fields</title>
    <summary>  Many tasks in graphics and vision demand machinery for converting shapes into
consistent representations with sparse sets of parameters; these
representations facilitate rendering, editing, and storage. When the source
data is noisy or ambiguous, however, artists and engineers often manually
construct such representations, a tedious and potentially time-consuming
process. While advances in deep learning have been successfully applied to
noisy geometric data, the task of generating parametric shapes has so far been
difficult for these methods. Hence, we propose a new framework for predicting
parametric shape primitives using deep learning. We use distance fields to
transition between shape parameters like control points and input data on a
pixel grid. We demonstrate efficacy on 2D and 3D tasks, including font
vectorization and surface abstraction.
</summary>
    <author>
      <name>Dmitriy Smirnov</name>
    </author>
    <author>
      <name>Matthew Fisher</name>
    </author>
    <author>
      <name>Vladimir G. Kim</name>
    </author>
    <author>
      <name>Richard Zhang</name>
    </author>
    <author>
      <name>Justin Solomon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition. (2020) 561-570</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.08921v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08921v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09530v1</id>
    <updated>2019-04-21T02:36:36Z</updated>
    <published>2019-04-21T02:36:36Z</published>
    <title>Snaxels on a Plane</title>
    <summary>  While many algorithms exist for tracing various contours for illustrating a
meshed object, few algorithms organize these contours into region-bounding
closed loops. Tracing closed-loop boundaries on a mesh can be problematic due
to switchbacks caused by subtle surface variation, and the organization of
these regions into a planar map can lead to many small region components due to
imprecision and noise. This paper adapts "snaxels," an energy minimizing active
contour method designed for robust mesh processing, and repurposes it to
generate visual, shadow and shading contours, and a simplified visual-surface
planar map, useful for stylized vector art illustration of the mesh. The snaxel
active contours can also track contours as the mesh animates, and
frame-to-frame correspondences between snaxels lead to a new method to convert
the moving contours on a 3-D animated mesh into 2-D SVG curve animations for
efficient embedding in Flash, PowerPoint and other dynamic vector art
platforms.
</summary>
    <author>
      <name>Kevin Karsch</name>
    </author>
    <author>
      <name>John C. Hart</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2024676.2024683</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2024676.2024683" rel="related"/>
    <link href="http://arxiv.org/abs/1904.09530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10379v2</id>
    <updated>2019-12-20T12:52:17Z</updated>
    <published>2019-04-23T15:19:39Z</published>
    <title>Multi-modal 3D Shape Reconstruction Under Calibration Uncertainty using
  Parametric Level Set Methods</title>
    <summary>  We consider the problem of 3D shape reconstruction from multi-modal data,
given uncertain calibration parameters. Typically, 3D data modalities can be in
diverse forms such as sparse point sets, volumetric slices, 2D photos and so
on. To jointly process these data modalities, we exploit a parametric level set
method that utilizes ellipsoidal radial basis functions. This method not only
allows us to analytically and compactly represent the object, it also confers
on us the ability to overcome calibration related noise that originates from
inaccurate acquisition parameters. This essentially implicit regularization
leads to a highly robust and scalable reconstruction, surpassing other
traditional methods. In our results we first demonstrate the ability of the
method to compactly represent complex objects. We then show that our
reconstruction method is robust both to a small number of measurements and to
noise in the acquisition parameters. Finally, we demonstrate our reconstruction
abilities from diverse modalities such as volume slices obtained from liquid
displacement (similar to CTscans and XRays), and visual measurements obtained
from shape silhouettes.
</summary>
    <author>
      <name>Moshe Eliasof</name>
    </author>
    <author>
      <name>Andrei Sharf</name>
    </author>
    <author>
      <name>Eran Treister</name>
    </author>
    <link href="http://arxiv.org/abs/1904.10379v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10379v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10754v2</id>
    <updated>2019-08-28T16:27:34Z</updated>
    <published>2019-04-24T11:47:08Z</published>
    <title>OperatorNet: Recovering 3D Shapes From Difference Operators</title>
    <summary>  This paper proposes a learning-based framework for reconstructing 3D shapes
from functional operators, compactly encoded as small-sized matrices. To this
end we introduce a novel neural architecture, called OperatorNet, which takes
as input a set of linear operators representing a shape and produces its 3D
embedding. We demonstrate that this approach significantly outperforms previous
purely geometric methods for the same problem. Furthermore, we introduce a
novel functional operator, which encodes the extrinsic or pose-dependent shape
information, and thus complements purely intrinsic pose-oblivious operators,
such as the classical Laplacian. Coupled with this novel operator, our
reconstruction network achieves very high reconstruction accuracy, even in the
presence of incomplete information about a shape, given a soft or functional
map expressed in a reduced basis. Finally, we demonstrate that the
multiplicative functional algebra enjoyed by these operators can be used to
synthesize entirely new unseen shapes, in the context of shape interpolation
and shape analogy applications.
</summary>
    <author>
      <name>Ruqi Huang</name>
    </author>
    <author>
      <name>Marie-Julie Rakotosaona</name>
    </author>
    <author>
      <name>Panos Achlioptas</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10754v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10754v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10795v2</id>
    <updated>2020-04-06T15:01:51Z</updated>
    <published>2019-04-23T08:18:56Z</published>
    <title>3D Dynamic Point Cloud Inpainting via Temporal Consistency on Graphs</title>
    <summary>  With the development of 3D laser scanning techniques and depth sensors, 3D
dynamic point clouds have attracted increasing attention as a representation of
3D objects in motion, enabling various applications such as 3D immersive
tele-presence, gaming and navigation. However, dynamic point clouds usually
exhibit holes of missing data, mainly due to the fast motion, the limitation of
acquisition and complicated structure. Leveraging on graph signal processing
tools, we represent irregular point clouds on graphs and propose a novel
inpainting method exploiting both intra-frame self-similarity and inter-frame
consistency in 3D dynamic point clouds. Specifically, for each missing region
in every frame of the point cloud sequence, we search for its self-similar
regions in the current frame and corresponding ones in adjacent frames as
references. Then we formulate dynamic point cloud inpainting as an optimization
problem based on the two types of references, which is regularized by a
graph-signal smoothness prior. Experimental results show the proposed approach
outperforms three competing methods significantly, both in objective and
subjective quality.
</summary>
    <author>
      <name>Zeqing Fu</name>
    </author>
    <author>
      <name>Wei Hu</name>
    </author>
    <author>
      <name>Zongming Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, accepted by IEEE ICME 2020 at 2020.04.03. arXiv
  admin note: text overlap with arXiv:1810.03973</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10795v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10795v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.12795v1</id>
    <updated>2019-04-29T16:15:56Z</updated>
    <published>2019-04-29T16:15:56Z</published>
    <title>TileGAN: Synthesis of Large-Scale Non-Homogeneous Textures</title>
    <summary>  We tackle the problem of texture synthesis in the setting where many input
images are given and a large-scale output is required. We build on recent
generative adversarial networks and propose two extensions in this paper.
First, we propose an algorithm to combine outputs of GANs trained on a smaller
resolution to produce a large-scale plausible texture map with virtually no
boundary artifacts. Second, we propose a user interface to enable artistic
control. Our quantitative and qualitative results showcase the generation of
synthesized high-resolution maps consisting of up to hundreds of megapixels as
a case in point.
</summary>
    <author>
      <name>Anna Frühstück</name>
    </author>
    <author>
      <name>Ibraheem Alhashim</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3306346.3322993</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3306346.3322993" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at http://github.com/afruehstueck/tileGAN</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.12795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01562v1</id>
    <updated>2019-05-04T22:48:27Z</updated>
    <published>2019-05-04T22:48:27Z</published>
    <title>A Similarity Measure for Material Appearance</title>
    <summary>  We present a model to measure the similarity in appearance between different
materials, which correlates with human similarity judgments. We first create a
database of 9,000 rendered images depicting objects with varying materials,
shape and illumination. We then gather data on perceived similarity from
crowdsourced experiments; our analysis of over 114,840 answers suggests that
indeed a shared perception of appearance similarity exists. We feed this data
to a deep learning architecture with a novel loss function, which learns a
feature space for materials that correlates with such perceived appearance
similarity. Our evaluation shows that our model outperforms existing metrics.
Last, we demonstrate several applications enabled by our metric, including
appearance-based search for material suggestions, database visualization,
clustering and summarization, and gamut mapping.
</summary>
    <author>
      <name>Manuel Lagunas</name>
    </author>
    <author>
      <name>Sandra Malpica</name>
    </author>
    <author>
      <name>Ana Serrano</name>
    </author>
    <author>
      <name>Elena Garces</name>
    </author>
    <author>
      <name>Diego Gutierrez</name>
    </author>
    <author>
      <name>Belen Masia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3306346.3323036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3306346.3323036" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 17 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics (SIGGRAPH 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.01562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01684v2</id>
    <updated>2020-04-20T22:59:05Z</updated>
    <published>2019-05-05T13:41:05Z</published>
    <title>Unsupervised Detection of Distinctive Regions on 3D Shapes</title>
    <summary>  This paper presents a novel approach to learn and detect distinctive regions
on 3D shapes. Unlike previous works, which require labeled data, our method is
unsupervised. We conduct the analysis on point sets sampled from 3D shapes,
then formulate and train a deep neural network for an unsupervised shape
clustering task to learn local and global features for distinguishing shapes
with respect to a given shape set. To drive the network to learn in an
unsupervised manner, we design a clustering-based nonparametric softmax
classifier with an iterative re-clustering of shapes, and an adapted
contrastive loss for enhancing the feature embedding quality and stabilizing
the learning process. By then, we encourage the network to learn the point
distinctiveness on the input shapes. We extensively evaluate various aspects of
our approach and present its applications for distinctiveness-guided shape
retrieval, sampling, and view selection in 3D scenes.
</summary>
    <author>
      <name>Xianzhi Li</name>
    </author>
    <author>
      <name>Lequan Yu</name>
    </author>
    <author>
      <name>Chi-Wing Fu</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <author>
      <name>Pheng-Ann Heng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM TOG</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01684v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01684v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.02586v1</id>
    <updated>2019-05-06T07:58:41Z</updated>
    <published>2019-05-06T07:58:41Z</published>
    <title>Picturing Bivariate Separable-Features for Univariate Vector Magnitudes
  in Large-Magnitude-Range Quantum Physics Data</title>
    <summary>  We present study results from two experiments to empirically validate that
separable bivariate pairs for univariate representations of
large-magnitude-range vectors are more efficient than integral pairs. The first
experiment with 20 participants compared: one integral pair, three separable
pairs, and one redundant pair, which is a mix of the integral and separable
features. Participants performed three local tasks requiring reading numerical
values, estimating ratio, and comparing two points. The second 18-participant
study compared three separable pairs using three global tasks when participants
must look at the entire field to get an answer: find a specific target in 20
seconds, find the maximum magnitude in 20 seconds, and estimate the total
number of vector exponents within 2 seconds. Our results also reveal the
following: separable pairs led to the most accurate answers and the shortest
task execution time, while integral dimensions were among the least accurate;
it achieved high performance only when a pop-out separable feature (here color)
was added. To reconcile this finding with the existing literature, our second
experiment suggests that the higher the separability, the higher the accuracy;
the reason is probably that the emergent global scene created by the separable
pairs reduces the subsequent search space.
</summary>
    <author>
      <name>Henan Zhao</name>
    </author>
    <author>
      <name>Jian Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1712.02333</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05161v1</id>
    <updated>2019-05-08T17:50:37Z</updated>
    <published>2019-05-08T17:50:37Z</published>
    <title>Spectral Coarsening of Geometric Operators</title>
    <summary>  We introduce a novel approach to measure the behavior of a geometric operator
before and after coarsening. By comparing eigenvectors of the input operator
and its coarsened counterpart, we can quantitatively and visually analyze how
well the spectral properties of the operator are maintained. Using this
measure, we show that standard mesh simplification and algebraic coarsening
techniques fail to maintain spectral properties. In response, we introduce a
novel approach for spectral coarsening. We show that it is possible to
significantly reduce the sampling density of an operator derived from a 3D
shape without affecting the low-frequency eigenvectors. By marrying techniques
developed within the algebraic multigrid and the functional maps literatures,
we successfully coarsen a variety of isotropic and anisotropic operators while
maintaining sparsity and positive semi-definiteness. We demonstrate the utility
of this approach for applications including operator-sensitive sampling, shape
matching, and graph pooling for convolutional neural networks.
</summary>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 30 figures. ACM Transactions on Graphics (SIGGRAPH) 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.05161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06812v2</id>
    <updated>2019-10-15T09:45:42Z</updated>
    <published>2019-05-16T14:53:05Z</published>
    <title>Statistical Analysis and Modeling of the Geometry and Topology of Plant
  Roots</title>
    <summary>  The root is an important organ of a plant since it is responsible for water
and nutrient uptake. Analyzing and modelling variabilities in the geometry and
topology of roots can help in assessing the plant's health, understanding its
growth patterns, and modeling relations between plant species and between
plants and their environment. In this article, we develop a framework for the
statistical analysis and modeling of the geometry and topology of plant roots.
We represent root structures as points in a tree-shape space equipped with a
metric that quantifies geometric and topological differences between pairs of
roots. We then use these building blocks to compute geodesics, i.e., optimal
deformations under the metric between root structures, and to perform
statistical analysis on root populations. We demonstrate the utility of the
proposed framework through an application to a dataset of wheat roots grown in
different environmental conditions. We also show that the framework can be used
in various applications including classification and regression.
</summary>
    <author>
      <name>Guan Wang</name>
    </author>
    <author>
      <name>Hamid Laga</name>
    </author>
    <author>
      <name>Jinyuan Jia</name>
    </author>
    <author>
      <name>Stanley J. Miklavcic</name>
    </author>
    <author>
      <name>Anuj Srivastava</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jtbi.2019.110108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jtbi.2019.110108" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Theoretical Biology, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.06812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07518v1</id>
    <updated>2019-05-18T01:37:38Z</updated>
    <published>2019-05-18T01:37:38Z</published>
    <title>Smooth quasi-developable surfaces bounded by smooth curves</title>
    <summary>  Computing a quasi-developable strip surface bounded by design curves finds
wide industrial applications. Existing methods compute discrete surfaces
composed of developable lines connecting sampling points on input curves which
are not adequate for generating smooth quasi-developable surfaces. We propose
the first method which is capable of exploring the full solution space of
continuous input curves to compute a smooth quasi-developable ruled surface
with as large developability as possible. The resulting surface is exactly
bounded by the input smooth curves and is guaranteed to have no
self-intersections. The main contribution is a variational approach to compute
a continuous mapping of parameters of input curves by minimizing a function
evaluating surface developability. Moreover, we also present an algorithm to
represent a resulting surface as a B-spline surface when input curves are
B-spline curves.
</summary>
    <author>
      <name>Pengbo Bo</name>
    </author>
    <author>
      <name>Yujian Zheng</name>
    </author>
    <author>
      <name>Caiming Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cagd.2020.101863</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cagd.2020.101863" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Aided Geometric Design 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.07518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09661v1</id>
    <updated>2019-05-23T13:53:31Z</updated>
    <published>2019-05-23T13:53:31Z</published>
    <title>Data-Driven Crowd Simulation with Generative Adversarial Networks</title>
    <summary>  This paper presents a novel data-driven crowd simulation method that can
mimic the observed traffic of pedestrians in a given environment. Given a set
of observed trajectories, we use a recent form of neural networks, Generative
Adversarial Networks (GANs), to learn the properties of this set and generate
new trajectories with similar properties. We define a way for simulated
pedestrians (agents) to follow such a trajectory while handling local collision
avoidance. As such, the system can generate a crowd that behaves similarly to
observations, while still enabling real-time interactions between agents. Via
experiments with real-world data, we show that our simulated trajectories
preserve the statistical properties of their input. Our method simulates crowds
in real time that resemble existing crowds, while also allowing insertion of
extra agents, combination with other simulation methods, and user interaction.
</summary>
    <author>
      <name>Javad Amirian</name>
    </author>
    <author>
      <name>Wouter van Toll</name>
    </author>
    <author>
      <name>Jean-Bernard Hayet</name>
    </author>
    <author>
      <name>Julien Pettré</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3328756.3328769</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3328756.3328769" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CASA '19 (Computer Animation and Social Agents)</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.09661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09777v2</id>
    <updated>2020-04-27T23:08:09Z</updated>
    <published>2019-05-23T17:04:31Z</published>
    <title>A Smoothness Energy without Boundary Distortion for Curved Surfaces</title>
    <summary>  Current quadratic smoothness energies for curved surfaces either exhibit
distortions near the boundary due to zero Neumann boundary conditions, or they
do not correctly account for intrinsic curvature, which leads to
unnatural-looking behavior away from the boundary. This leads to an unfortunate
trade-off: one can either have natural behavior in the interior, or a
distortion-free result at the boundary, but not both. We introduce a
generalized Hessian energy for curved surfaces, expressed in terms of the
covariant one-form Dirichlet energy, the Gaussian curvature, and the exterior
derivative. Energy minimizers solve the Laplace-Beltrami biharmonic equation,
correctly accounting for intrinsic curvature, leading to natural-looking
isolines. On the boundary, minimizers are as-linear-as-possible, which reduces
the distortion of isolines at the boundary. We discretize the covariant
one-form Dirichlet energy using Crouzeix-Raviart finite elements, arriving at a
discrete formulation of the Hessian energy for applications on curved surfaces.
We observe convergence of the discretization in our experiments.
</summary>
    <author>
      <name>Oded Stein</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>Max Wardetzky</name>
    </author>
    <author>
      <name>Eitan Grinspun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3377406</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3377406" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.09777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; G.1.8; G.1.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.10763v3</id>
    <updated>2020-08-24T20:44:21Z</updated>
    <published>2019-05-26T08:43:44Z</published>
    <title>ENIGMA: Evolutionary Non-Isometric Geometry Matching</title>
    <summary>  In this paper we propose a fully automatic method for shape correspondence
that is widely applicable, and especially effective for non isometric shapes
and shapes of different topology. We observe that fully-automatic shape
correspondence can be decomposed as a hybrid discrete/continuous optimization
problem, and we find the best sparse landmark correspondence, whose
sparse-to-dense extension minimizes a local metric distortion. To tackle the
combinatorial task of landmark correspondence we use an evolutionary genetic
algorithm, where the local distortion of the sparse-to-dense extension is used
as the objective function. We design novel geometrically guided genetic
operators, which, when combined with our objective, are highly effective for
non isometric shape matching. Our method outperforms state of the art methods
for automatic shape correspondence both quantitatively and qualitatively on
challenging datasets.
</summary>
    <author>
      <name>Michal Edelstein</name>
    </author>
    <author>
      <name>Danielle Ezuz</name>
    </author>
    <author>
      <name>Mirela Ben-Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1905.10763v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10763v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00631v1</id>
    <updated>2019-07-01T09:51:04Z</updated>
    <published>2019-07-01T09:51:04Z</published>
    <title>Automatic reconstruction of fully volumetric 3D building models from
  point clouds</title>
    <summary>  We present a novel method for reconstructing parametric, volumetric,
multi-story building models from unstructured, unfiltered indoor point clouds
by means of solving an integer linear optimization problem. Our approach
overcomes limitations of previous methods in several ways: First, we drop
assumptions about the input data such as the availability of separate scans as
an initial room segmentation. Instead, a fully automatic room segmentation and
outlier removal is performed on the unstructured point clouds. Second,
restricting the solution space of our optimization approach to arrangements of
volumetric wall entities representing the structure of a building enforces a
consistent model of volumetric, interconnected walls fitted to the observed
data instead of unconnected, paper-thin surfaces. Third, we formulate the
optimization as an integer linear programming problem which allows for an exact
solution instead of the approximations achieved with most previous techniques.
Lastly, our optimization approach is designed to incorporate hard constraints
which were difficult or even impossible to integrate before. We evaluate and
demonstrate the capabilities of our proposed approach on a variety of complex
real-world point clouds.
</summary>
    <author>
      <name>Sebastian Ochmann</name>
    </author>
    <author>
      <name>Richard Vock</name>
    </author>
    <author>
      <name>Reinhard Klein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.isprsjprs.2019.03.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.isprsjprs.2019.03.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted to ISPRS Journal of Photogrammetry and Remote
  Sensing. Final version available at
  https://doi.org/10.1016/j.isprsjprs.2019.03.017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ISPRS Journal of Photogrammetry and Remote Sensing, Volume 151,
  May 2019, Pages 251-262</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.00631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00893v1</id>
    <updated>2019-07-01T16:10:13Z</updated>
    <published>2019-07-01T16:10:13Z</published>
    <title>Computational Design of Skinned Quad-Robots</title>
    <summary>  We present a computational design system that assists users to model,
optimize, and fabricate quad-robots with soft skins.Our system addresses the
challenging task of predicting their physical behavior by fully integrating the
multibody dynamics of the mechanical skeleton and the elastic behavior of the
soft skin. The developed motion control strategy uses an alternating
optimization scheme to avoid expensive full space time-optimization,
interleaving space-time optimization for the skeleton and frame-by-frame
optimization for the full dynamics. The output are motor torques to drive the
robot to achieve a user prescribed motion trajectory.We also provide a
collection of convenient engineering tools and empirical manufacturing guidance
to support the fabrication of the designed quad-robot. We validate the
feasibility of designs generated with our system through physics simulations
and with a physically-fabricated prototype.
</summary>
    <author>
      <name>Xudong Feng</name>
    </author>
    <author>
      <name>Jiafeng Liu</name>
    </author>
    <author>
      <name>Huamin Wang</name>
    </author>
    <author>
      <name>Yin Yang</name>
    </author>
    <author>
      <name>Hujun Bao</name>
    </author>
    <author>
      <name>Bernd Bickel</name>
    </author>
    <author>
      <name>Weiwei Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1907.00893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01652v2</id>
    <updated>2021-02-06T06:19:56Z</updated>
    <published>2019-07-02T21:15:02Z</published>
    <title>RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool</title>
    <summary>  This work introduces RadVR, a virtual reality tool for daylighting analysis
that simultaneously combines qualitative assessments through immersive
real-time renderings with quantitative physically correct daylighting
simulations in a 6DOF virtual environment. By taking a 3D building model with
material properties as input, RadVR allows users to (1) perform
physically-based daylighting simulations via Radiance, (2) study sunlight in
different hours-of-the-year, (3) interact with a 9-point-in-time matrix for the
most representative times of the year, and (4) visualize, compare, and analyze
daylighting simulation results. With an end-to-end workflow, RadVR integrates
with 3D modeling software that is commonly used by building designers.
Additionally, by conducting user experiments we compare the proposed system
with DIVA for Rhino, a Radiance-based tool that uses conventional 2D-displays.
The results show that RadVR can provide promising assistance in spatial
understanding tasks, navigation, and sun position analysis in virtual reality.
</summary>
    <author>
      <name>Mohammad Keshavarzi</name>
    </author>
    <author>
      <name>Luisa Caldas</name>
    </author>
    <author>
      <name>Luis Santos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Automation in Construction</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01652v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01652v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04099v2</id>
    <updated>2019-09-09T15:07:02Z</updated>
    <published>2019-07-09T11:49:15Z</published>
    <title>Progressive Refinement Imaging</title>
    <summary>  This paper presents a novel technique for progressive online integration of
uncalibrated image sequences with substantial geometric and/or photometric
discrepancies into a single, geometrically and photometrically consistent
image. Our approach can handle large sets of images, acquired from a nearly
planar or infinitely distant scene at different resolutions in object domain
and under variable local or global illumination conditions. It allows for
efficient user guidance as its progressive nature provides a valid and
consistent reconstruction at any moment during the online refinement process.
Our approach avoids global optimization techniques, as commonly used in the
field of image refinement, and progressively incorporates new imagery into a
dynamically extendable and memory-efficient Laplacian pyramid. Our image
registration process includes a coarse homography and a local refinement stage
using optical flow. Photometric consistency is achieved by retaining the
photometric intensities given in a reference image, while it is being refined.
Globally blurred imagery and local geometric inconsistencies due to, e.g.
motion are detected and removed prior to image fusion. We demonstrate the
quality and robustness of our approach using several image and video sequences,
including handheld acquisition with mobile phones and zooming sequences with
consumer cameras.
</summary>
    <author>
      <name>Markus Kluge</name>
    </author>
    <author>
      <name>Tim Weyrich</name>
    </author>
    <author>
      <name>Andreas Kolb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.13808</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.13808" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article has been published in final form at
  https://doi.org/10.1111/cgf.13808</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04099v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04099v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04353v1</id>
    <updated>2019-07-09T18:21:50Z</updated>
    <published>2019-07-09T18:21:50Z</published>
    <title>Prescription AR: A Fully-Customized Prescription-Embedded Augmented
  Reality Display</title>
    <summary>  In this paper, we present a fully-customized AR display design that considers
the user's prescription, interpupillary distance, and taste of fashion. A
free-form image combiner embedded inside the prescription lens provides
augmented images onto the vision-corrected real world. We establish a
prescription-embedded AR display optical design method as well as the
customization method for individual users. Our design can cover myopia,
hyperopia, astigmatism, and presbyopia, and allows the eye-contact interaction
with privacy protection. A 169$g$ dynamic prototype showed a 40$^\circ$
$\times$ 20 $^\circ$ virtual image with a 23 cpd resolution at center field and
6 mm $\times$ 4 mm eye box, with the vision-correction and varifocal (0.5-3$m$)
capability.
</summary>
    <author>
      <name>Jui-Yi Wu</name>
    </author>
    <author>
      <name>Jonghyun Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/OE.380945</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/OE.380945" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 16 figures, Optica</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04435v1</id>
    <updated>2019-07-09T22:03:40Z</updated>
    <published>2019-07-09T22:03:40Z</published>
    <title>Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows Over
  Time</title>
    <summary>  Large scale shadows from buildings in a city play an important role in
determining the environmental quality of public spaces. They can be both
beneficial, such as for pedestrians during summer, and detrimental, by
impacting vegetation and by blocking direct sunlight. Determining the effects
of shadows requires the accumulation of shadows over time across different
periods in a year. In this paper, we propose a simple yet efficient class of
approach that uses the properties of sun movement to track the changing
position of shadows within a fixed time interval. We use this approach to
extend two commonly used shadowing techniques, shadow maps and ray tracing, and
demonstrate the efficiency of our approach. Our technique is used to develop an
interactive visual analysis system, Shadow Profiler, targeted at city planners
and architects that allows them to test the impact of shadows for different
development scenarios. We validate the usefulness of this system through case
studies set in Manhattan, a dense borough of New York City.
</summary>
    <author>
      <name>Fabio Miranda</name>
    </author>
    <author>
      <name>Harish Doraiswamy</name>
    </author>
    <author>
      <name>Marcos Lage</name>
    </author>
    <author>
      <name>Luc Wilson</name>
    </author>
    <author>
      <name>Mondrian Hsieh</name>
    </author>
    <author>
      <name>Claudio T. Silva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2018.2802945</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2018.2802945" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Video: https://www.youtube.com/watch?v=LsZv23d1LyM, Data:
  https://github.com/ViDA-NYU/shadow-accrual-maps</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics (Volume:
  25, Issue: 3, Mar. 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.04435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05073v2</id>
    <updated>2019-10-07T09:36:52Z</updated>
    <published>2019-07-11T09:29:39Z</published>
    <title>Void-and-Cluster Sampling of Large Scattered Data and Trajectories</title>
    <summary>  We propose a data reduction technique for scattered data based on statistical
sampling. Our void-and-cluster sampling technique finds a representative subset
that is optimally distributed in the spatial domain with respect to the blue
noise property. In addition, it can adapt to a given density function, which we
use to sample regions of high complexity in the multivariate value domain more
densely. Moreover, our sampling technique implicitly defines an ordering on the
samples that enables progressive data loading and a continuous level-of-detail
representation. We extend our technique to sample time-dependent trajectories,
for example pathlines in a time interval, using an efficient and iterative
approach. Furthermore, we introduce a local and continuous error measure to
quantify how well a set of samples represents the original dataset. We apply
this error measure during sampling to guide the number of samples that are
taken. Finally, we use this error measure and other quantities to evaluate the
quality, performance, and scalability of our algorithm.
</summary>
    <author>
      <name>Tobias Rapp</name>
    </author>
    <author>
      <name>Christoph Peters</name>
    </author>
    <author>
      <name>Carsten Dachsbacher</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2019.2934335</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2019.2934335" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Transactions on Visualization and Computer Graphics
  as a special issue from the proceedings of VIS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.05073v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05073v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05783v1</id>
    <updated>2019-07-12T15:11:19Z</updated>
    <published>2019-07-12T15:11:19Z</published>
    <title>Improving the Projection of Global Structures in Data through Spanning
  Trees</title>
    <summary>  The connection of edges in a graph generates a structure that is independent
of a coordinate system. This visual metaphor allows creating a more flexible
representation of data than a two-dimensional scatterplot. In this work, we
present STAD (Spanning Trees as Approximation of Data), a dimensionality
reduction method to approximate the high-dimensional structure into a graph
with or without formulating prior hypotheses. STAD generates an abstract
representation of high-dimensional data by giving each data point a location in
a graph which preserves the distances in the original high-dimensional space.
The STAD graph is built upon the Minimum Spanning Tree (MST) to which new edges
are added until the correlation between the distances from the graph and the
original dataset is maximized. Additionally, STAD supports the inclusion of
additional functions to focus the exploration and allow the analysis of data
from new perspectives, emphasizing traits in data which otherwise would remain
hidden. We demonstrate the effectiveness of our method by applying it to two
real-world datasets: traffic density in Barcelona and temporal measurements of
air quality in Castile and Le\'on in Spain.
</summary>
    <author>
      <name>Daniel Alcaide</name>
    </author>
    <author>
      <name>Jan Aerts</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.08553v2</id>
    <updated>2019-10-07T09:45:36Z</updated>
    <published>2019-07-19T15:49:57Z</published>
    <title>LightGuider: Guiding Interactive Lighting Design using Suggestions,
  Provenance, and Quality Visualization</title>
    <summary>  LightGuider is a novel guidance-based approach to interactive lighting
design, which typically consists of interleaved 3D modeling operations and
light transport simulations. Rather than having designers use a trial-and-error
approach to match their illumination constraints and aesthetic goals,
LightGuider supports the process by simulating potential next modeling steps
that can deliver the most significant improvements. LightGuider takes
predefined quality criteria and the current focus of the designer into account
to visualize suggestions for lighting-design improvements via a specialized
provenance tree. This provenance tree integrates snapshot visualizations of how
well a design meets the given quality criteria weighted by the designer's
preferences. This integration facilitates the analysis of quality improvements
over the course of a modeling workflow as well as the comparison of alternative
design solutions. We evaluate our approach with three lighting designers to
illustrate its usefulness.
</summary>
    <author>
      <name>Andreas Walch</name>
    </author>
    <author>
      <name>Michael Schwärzler</name>
    </author>
    <author>
      <name>Christian Luksch</name>
    </author>
    <author>
      <name>Elmar Eisemann</name>
    </author>
    <author>
      <name>Theresia Gschwandtner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2019.2934658</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2019.2934658" rel="related"/>
    <link href="http://arxiv.org/abs/1907.08553v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08553v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10402v1</id>
    <updated>2019-07-24T12:44:54Z</updated>
    <published>2019-07-24T12:44:54Z</published>
    <title>Data-Driven Physical Face Inversion</title>
    <summary>  Facial animation is one of the most challenging problems in computer
graphics, and it is often solved using linear heuristics like blend-shape
rigging. More expressive approaches like physical simulation have emerged, but
these methods are very difficult to tune, especially when simulating a real
actor's face. We propose to use a simple finite element simulation approach for
face animation, and present a novel method for recovering the required
simulation parameters in order to best match a real actor's face motion. Our
method involves reconstructing a very small number of head poses of the actor
in 3D, where the head poses span different configurations of force directions
due to gravity. Our algorithm can then automatically recover both the
gravity-free rest shape of the face as well as the spatially-varying physical
material stiffness such that a forward simulation will match the captured
targets as closely as possible. As a result, our system can produce
actor-specific, physical parameters that can be immediately used in recent
physical simulation methods for faces. Furthermore, as the simulation results
depend heavily on the chosen spatial layout of material clusters, we analyze
and compare different spatial layouts.
</summary>
    <author>
      <name>Yeara Kozlov</name>
    </author>
    <author>
      <name>Hongyi Xu</name>
    </author>
    <author>
      <name>Moritz Bächer</name>
    </author>
    <author>
      <name>Derek Bradley</name>
    </author>
    <author>
      <name>Markus Gross</name>
    </author>
    <author>
      <name>Thabo Beeler</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11721v3</id>
    <updated>2020-10-07T14:04:14Z</updated>
    <published>2019-07-26T17:37:42Z</published>
    <title>FAKIR: An algorithm for revealing the anatomy and pose of statues from
  raw point sets</title>
    <summary>  3D acquisition of archaeological artefacts has become an essential part of
cultural heritage research for preservation or restoration purpose. Statues, in
particular, have been at the center of many projects. In this paper, we
introduce a way to improve the understanding of acquired statues representing
real or imaginary creatures by registering a simple and pliable articulated
model to the raw point set data. Our approach performs a Forward And bacKward
Iterative Registration (FAKIR) which proceeds joint by joint, needing only a
few iterations to converge. We are thus able to detect the pose and elementary
anatomy of sculptures, with possibly non realistic body proportions. By
adapting our simple skeleton, our method can work on animals and imaginary
creatures.
</summary>
    <author>
      <name>Tong Fu</name>
    </author>
    <author>
      <name>Raphaëlle Chaine</name>
    </author>
    <author>
      <name>Julie Digne</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11721v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11721v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.13534v1</id>
    <updated>2019-07-31T14:44:42Z</updated>
    <published>2019-07-31T14:44:42Z</published>
    <title>A Comparison of Radial and Linear Charts for Visualizing Daily Pattern</title>
    <summary>  Radial charts are generally considered less effective than linear charts.
Perhaps the only exception is in visualizing periodical time-dependent data,
which is believed to be naturally supported by the radial layout. It has been
demonstrated that the drawbacks of radial charts outweigh the benefits of this
natural mapping. Visualization of daily patterns, as a special case, has not
been systematically evaluated using radial charts. In contrast to yearly or
weekly recurrent trends, the analysis of daily patterns on a radial chart may
benefit from our trained skill on reading radial clocks that are ubiquitous in
our culture. In a crowd-sourced experiment with 92 non-expert users, we
evaluated the accuracy, efficiency, and subjective ratings of radial and linear
charts for visualizing daily traffic accident patterns. We systematically
compared juxtaposed 12-hours variants and single 24-hours variants for both
layouts in four low-level tasks and one high-level interpretation task. Our
results show that over all tasks, the most elementary 24-hours linear bar chart
is most accurate and efficient and is also preferred by the users. This
provides strong evidence for the use of linear layouts - even for visualizing
periodical daily patterns.
</summary>
    <author>
      <name>Manuela Waldner</name>
    </author>
    <author>
      <name>Alexandra Diehl</name>
    </author>
    <author>
      <name>Denis Gracanin</name>
    </author>
    <author>
      <name>Rainer Splechtna</name>
    </author>
    <author>
      <name>Claudio Delrieux</name>
    </author>
    <author>
      <name>Kresimir Matkovic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2019.2934784</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2019.2934784" rel="related"/>
    <link href="http://arxiv.org/abs/1907.13534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.13534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00056v1</id>
    <updated>2019-07-31T19:05:39Z</updated>
    <published>2019-07-31T19:05:39Z</published>
    <title>Software-Enhanced Teaching and Visualization Capabilities of an
  Ultra-High-Resolution Video Wall</title>
    <summary>  This paper presents a modular approach to enhance the capabilities and
features of a visualization and teaching room using software. This approach was
applied to a room with a large, high resolution (7680$\times$4320 pixels),
tiled screen of 13 $\times$ 7.5 feet as its main display, and with a variety of
audio and video inputs, connected over a network. Many of the techniques
described are possible because of a software-enhanced setup, utilizing existing
hardware and a collection of mostly open-source tools, allowing to perform
collaborative, high-resolution visualizations as well as broadcasting and
recording workshops and lectures. The software approach is flexible and allows
one to add functionality without changing the hardware.
</summary>
    <author>
      <name>Ramses van Zon</name>
    </author>
    <author>
      <name>Marcelo Ponce</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3332186.3333149</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3332186.3333149" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PEARC'19: "Practice and Experience in Advanced Research Computing",
  July 28-August 1, 2019 - Chicago, IL, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.00056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00576v1</id>
    <updated>2019-08-01T18:45:17Z</updated>
    <published>2019-08-01T18:45:17Z</published>
    <title>Evaluating Ordering Strategies of Star Glyph Axes</title>
    <summary>  Star glyphs are a well-researched visualization technique to represent
multi-dimensional data. They are often used in small multiple settings for a
visual comparison of many data points. However, their overall visual appearance
is strongly influenced by the ordering of dimensions. To this end, two
orthogonal categories of layout strategies are proposed in the literature:
order dimensions by similarity to get homogeneously shaped glyphs vs. order by
dissimilarity to emphasize spikes and salient shapes. While there is evidence
that salient shapes support clustering tasks, evaluation, and direct comparison
of data-driven ordering strategies has not received much research attention. We
contribute an empirical user study to evaluate the efficiency, effectiveness,
and user confidence in visual clustering tasks using star glyphs. In comparison
to similarity-based ordering, our results indicate that dissimilarity-based
star glyph layouts support users better in clustering tasks, especially when
clutter is present.
</summary>
    <author>
      <name>Matthias Miller</name>
    </author>
    <author>
      <name>Xuan Zhang</name>
    </author>
    <author>
      <name>Johannes Fuchs</name>
    </author>
    <author>
      <name>Michael Blumenschein</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01350v1</id>
    <updated>2019-08-04T14:20:13Z</updated>
    <published>2019-08-04T14:20:13Z</published>
    <title>Another Simple but Faster Method for 2D Line Clipping</title>
    <summary>  The majority of methods for line clipping make a rather large number of
comparisons and involve a lot of calculations compared to modern ones. Most of
the times, they are not so efficient as well as not so simple and applicable to
the majority of cases. Besides the most popular ones, namely, Cohen-Sutherland,
Liang-Barsky, Cyrus-Beck and Nicholl-Lee-Nicholl, other line-clipping methods
have been presented over the years, each one having its own advantages and
disadvantages. In this paper a new computation method for 2D line clipping
against a rectangular window is introduced. The proposed method has been
compared with the afore-mentioned ones as well as with two others; namely,
Skala and Kodituwakku-Wijeweera-Chamikara, with respect to the number of
operations performed and the computation time. The performance of the proposed
method has been found to be better than all of the above-mentioned methods and
it is found to be very fast, simple and can be implemented easily in any
programming language or integrated development environment.
</summary>
    <author>
      <name>Dimitrios Matthes</name>
    </author>
    <author>
      <name>Vasileios Drakopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2019.9301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2019.9301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.01350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01938v1</id>
    <updated>2019-08-06T03:02:50Z</updated>
    <published>2019-08-06T03:02:50Z</published>
    <title>Heterogeneous porous scaffold generation in trivariate B-spline solid
  with triply periodic minimal surface in the parametric domain</title>
    <summary>  A porous scaffold is a three-dimensional network structure composed of a
large number of pores, and triply periodic minimal surfaces (TPMSs) are one of
conventional tools for designing porous scaffolds. However, discontinuity,
incompleteness, and high storage space requirements are the three main
shortcomings of TPMSs for porous scaffold design. In this study, we developed
an effective method for heterogeneous porous scaffold generation to overcome
the abovementioned shortcomings of TPMSs. The input of the proposed method is a
trivariate B-spline solid (TBSS) with a cubic parameter domain. The proposed
method first constructs a threshold distribution field (TDF) in the cubic
parameter domain, and then produces a continuous and complete TPMS within it.
Moreover, by mapping the TPMS in the parametric domain to the TBSS, a
continuous and complete porous scaffold is generated in the TBSS. In addition,
if the TBSS does not satisfy engineering requirements, the TDF can be locally
modified in the parameter domain, and the porous scaffold in the TBSS can be
rebuilt. We also defined a new storage space-saving file format based on the
TDF to store porous scaffolds. The experimental results presented in this paper
demonstrate the effectiveness and efficiency of the method using a TBSS as well
as the superior space-saving of the proposed storage format.
</summary>
    <author>
      <name>Chuanfeng Hu</name>
    </author>
    <author>
      <name>Hongwei Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02111v1</id>
    <updated>2019-08-06T12:44:42Z</updated>
    <published>2019-08-06T12:44:42Z</published>
    <title>Point Cloud Super Resolution with Adversarial Residual Graph Networks</title>
    <summary>  Point cloud super-resolution is a fundamental problem for 3D reconstruction
and 3D data understanding. It takes a low-resolution (LR) point cloud as input
and generates a high-resolution (HR) point cloud with rich details. In this
paper, we present a data-driven method for point cloud super-resolution based
on graph networks and adversarial losses. The key idea of the proposed network
is to exploit the local similarity of point cloud and the analogy between LR
input and HR output. For the former, we design a deep network with graph
convolution. For the latter, we propose to add residual connections into graph
convolution and introduce a skip connection between input and output. The
proposed network is trained with a novel loss function, which combines Chamfer
Distance (CD) and graph adversarial loss. Such a loss function captures the
characteristics of HR point cloud automatically without manual design. We
conduct a series of experiments to evaluate our method and validate the
superiority over other methods. Results show that the proposed method achieves
the state-of-the-art performance and have a good generalization ability to
unseen data.
</summary>
    <author>
      <name>Huikai Wu</name>
    </author>
    <author>
      <name>Junge Zhang</name>
    </author>
    <author>
      <name>Kaiqi Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at
  https://github.com/wuhuikai/PointCloudSuperResolution</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02507v1</id>
    <updated>2019-08-07T09:59:08Z</updated>
    <published>2019-08-07T09:59:08Z</published>
    <title>Mesh Variational Autoencoders with Edge Contraction Pooling</title>
    <summary>  3D shape analysis is an important research topic in computer vision and
graphics. While existing methods have generalized image-based deep learning to
meshes using graph-based convolutions, the lack of an effective pooling
operation restricts the learning capability of their networks. In this paper,
we propose a novel pooling operation for mesh datasets with the same
connectivity but different geometry, by building a mesh hierarchy using mesh
simplification. For this purpose, we develop a modified mesh simplification
method to avoid generating highly irregularly sized triangles. Our pooling
operation effectively encodes the correspondence between coarser and finer
meshes in the hierarchy. We then present a variational auto-encoder structure
with the edge contraction pooling and graph-based convolutions, to explore
probability latent spaces of 3D surfaces. Our network requires far fewer
parameters than the original mesh VAE and thus can handle denser models thanks
to our new pooling operation and convolutional kernels. Our evaluation also
shows that our method has better generalization ability and is more reliable in
various applications, including shape generation, shape interpolation and shape
embedding.
</summary>
    <author>
      <name>Yu-Jie Yuan</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1908.02507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02714v1</id>
    <updated>2019-08-07T16:35:27Z</updated>
    <published>2019-08-07T16:35:27Z</published>
    <title>Relighting Humans: Occlusion-Aware Inverse Rendering for Full-Body Human
  Images</title>
    <summary>  Relighting of human images has various applications in image synthesis. For
relighting, we must infer albedo, shape, and illumination from a human
portrait. Previous techniques rely on human faces for this inference, based on
spherical harmonics (SH) lighting. However, because they often ignore light
occlusion, inferred shapes are biased and relit images are unnaturally bright
particularly at hollowed regions such as armpits, crotches, or garment
wrinkles. This paper introduces the first attempt to infer light occlusion in
the SH formulation directly. Based on supervised learning using convolutional
neural networks (CNNs), we infer not only an albedo map, illumination but also
a light transport map that encodes occlusion as nine SH coefficients per pixel.
The main difficulty in this inference is the lack of training datasets compared
to unlimited variations of human portraits. Surprisingly, geometric information
including occlusion can be inferred plausibly even with a small dataset of
synthesized human figures, by carefully preparing the dataset so that the CNNs
can exploit the data coherency. Our method accomplishes more realistic
relighting than the occlusion-ignored formulation.
</summary>
    <author>
      <name>Yoshihiro Kanamori</name>
    </author>
    <author>
      <name>Yuki Endo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3272127.3275104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3272127.3275104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at SIGGRAPH Asia 2018 (ACM Transactions on Graphics).
  Project page with codes, pretrained models, and human model lists is at
  http://kanamori.cs.tsukuba.ac.jp/projects/relighting_human/</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03581v2</id>
    <updated>2020-01-24T22:09:21Z</updated>
    <published>2019-08-09T18:00:15Z</published>
    <title>Fast Tetrahedral Meshing in the Wild</title>
    <summary>  We propose a new tetrahedral meshing method, fTetWild, to convert triangle
soups into high-quality tetrahedral meshes. Our method builds on the TetWild
algorithm, replacing the rational triangle insertion with a new incremental
approach to construct and optimize the output mesh, interleaving triangle
insertion and mesh optimization. Our approach makes it possible to maintain a
valid floating-point tetrahedral mesh at all algorithmic stages, eliminating
the need for costly constructions with rational numbers used by TetWild, while
maintaining full robustness and similar output quality. This allows us to
improve on TetWild in two ways. First, our algorithm is significantly faster,
with running time comparable to less robust Delaunay-based tetrahedralization
algorithms. Second, our algorithm is guaranteed to produce a valid tetrahedral
mesh with floating-point vertex coordinates, while TetWild produces a valid
mesh with rational coordinates which is not guaranteed to be valid after
floating-point conversion. As a trade-off, our algorithm no longer guarantees
that all input triangles are present in the output mesh, but in practice, as
confirmed by our tests on the Thingi10k dataset, the algorithm always succeeds
in inserting all input triangles.
</summary>
    <author>
      <name>Yixin Hu</name>
    </author>
    <author>
      <name>Teseo Schneider</name>
    </author>
    <author>
      <name>Bolun Wang</name>
    </author>
    <author>
      <name>Denis Zorin</name>
    </author>
    <author>
      <name>Daniele Panozzo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386569.3392385</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386569.3392385" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 39, 4, Article 117 (August 2020), 18 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.03581v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03581v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04338v1</id>
    <updated>2019-08-12T19:01:47Z</updated>
    <published>2019-08-12T19:01:47Z</published>
    <title>Convolutional Humanoid Animation via Deformation</title>
    <summary>  In this paper we present a new deep learning-driven approach to image-based
synthesis of animations involving humanoid characters. Unlike previous deep
approaches to image-based animation our method makes no assumptions on the type
of motion to be animated nor does it require dense temporal input to produce
motion. Instead we generate new animations by interpolating between user chosen
keyframes, arranged sparsely in time. Utilizing a novel configuration manifold
learning approach we interpolate suitable motions between these keyframes. In
contrast to previous methods, ours requires less data (animations can be
generated from a single youtube video) and is broadly applicable to a wide
range of motions including facial motion, whole body motion and even scenes
with multiple characters. These improvements serve to significantly reduce the
difficulty in producing image-based animations of humanoid characters, allowing
even broader audiences to express their creativity.
</summary>
    <author>
      <name>John Kanji</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06154v1</id>
    <updated>2019-08-16T20:23:54Z</updated>
    <published>2019-08-16T20:23:54Z</published>
    <title>Extending editing capabilities of subdivision schemes by refinement of
  point-normal pairs</title>
    <summary>  In this paper we extend the 2D circle average of [11] to a 3D binary average
of point-normal pairs, and study its properties. We modify classical
surface-generating linear subdivision schemes with this average obtaining
surface-generating schemes refining point-normal pairs. The modified schemes
give the possibility to generate more geometries by editing the initial
normals. For the case of input data consisting of a mesh only, we present a
method for computing "naive" initial normals from the initial mesh. The
performance of several modified schemes is compared to their linear variants,
when operating on the same initial mesh, and examples of the editing
capabilities of the modified schemes are given. In addition we provide a link
to our repository, where we store the initial and refined mesh files, and the
implementation code. Several videos, demonstrating the editing capabilities of
the initial normals are provided in our Youtube channel.
</summary>
    <author>
      <name>Evgeny Lipovetsky</name>
    </author>
    <author>
      <name>Nira Dyn</name>
    </author>
    <link href="http://arxiv.org/abs/1908.06154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07198v1</id>
    <updated>2019-08-20T07:39:21Z</updated>
    <published>2019-08-20T07:39:21Z</published>
    <title>DeepSketchHair: Deep Sketch-based 3D Hair Modeling</title>
    <summary>  We present sketchhair, a deep learning based tool for interactive modeling of
3D hair from 2D sketches. Given a 3D bust model as reference, our sketching
system takes as input a user-drawn sketch (consisting of hair contour and a few
strokes indicating the hair growing direction within a hair region), and
automatically generates a 3D hair model, which matches the input sketch both
globally and locally. The key enablers of our system are two carefully designed
neural networks, namely, S2ONet, which converts an input sketch to a dense 2D
hair orientation field; and O2VNet, which maps the 2D orientation field to a 3D
vector field. Our system also supports hair editing with additional sketches in
new views. This is enabled by another deep neural network, V2VNet, which
updates the 3D vector field with respect to the new sketches. All the three
networks are trained with synthetic data generated from a 3D hairstyle
database. We demonstrate the effectiveness and expressiveness of our tool using
a variety of hairstyles and also compare our method with prior art.
</summary>
    <author>
      <name>Yuefan Shen</name>
    </author>
    <author>
      <name>Changgeng Zhang</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <author>
      <name>Kun Zhou</name>
    </author>
    <author>
      <name>Youyi Zheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2020.2968433</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2020.2968433" rel="related"/>
    <link href="http://arxiv.org/abs/1908.07198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09530v1</id>
    <updated>2019-08-26T08:52:53Z</updated>
    <published>2019-08-26T08:52:53Z</published>
    <title>A Flexible Neural Renderer for Material Visualization</title>
    <summary>  Photo realism in computer generated imagery is crucially dependent on how
well an artist is able to recreate real-world materials in the scene. The
workflow for material modeling and editing typically involves manual tweaking
of material parameters and uses a standard path tracing engine for visual
feedback. A lot of time may be spent in iterative selection and rendering of
materials at an appropriate quality. In this work, we propose a convolutional
neural network based workflow which quickly generates high-quality ray traced
material visualizations on a shaderball. Our novel architecture allows for
control over environment lighting and assists material selection along with the
ability to render spatially-varying materials. Additionally, our network
enables control over environment lighting which gives an artist more freedom
and provides better visualization of the rendered material. Comparison with
state-of-the-art denoising and neural rendering techniques suggests that our
neural renderer performs faster and better. We provide a interactive
visualization tool and release our training dataset to foster further research
in this area.
</summary>
    <author>
      <name>Aakash KT</name>
    </author>
    <author>
      <name>Parikshit Sakurikar</name>
    </author>
    <author>
      <name>Saurabh Saini</name>
    </author>
    <author>
      <name>P. J. Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09913v3</id>
    <updated>2019-12-04T19:35:26Z</updated>
    <published>2019-08-26T20:40:28Z</published>
    <title>Subdivision of point-normal pairs with application to smoothing feasible
  robot path</title>
    <summary>  In a previous paper [11] we introduced a weighted binary average of two 2D
point-normal pairs, termed circle average, and investigated subdivision schemes
based on it. These schemes refine point-normal pairs in 2D, and converge to
limit curves and limit normals. Such a scheme has the disadvantage that the
limit normals are not the normals of the limit curve. In this paper we solve
this problem by proposing a new averaging method, and obtaining a new family of
algorithms based on it. We demonstrate their new editing capabilities and apply
this subdivision technique to smooth a precomputed feasible polygonal point
robot path.
</summary>
    <author>
      <name>Evgeny Lipovetsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The text shall be reworked and extended significantly</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09913v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09913v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00279v2</id>
    <updated>2019-10-04T11:22:30Z</updated>
    <published>2019-10-01T09:46:46Z</published>
    <title>pylustrator: Code generation for reproducible figures for publication</title>
    <summary>  One major challenge in science is to make all results potentially
reproducible. Thus, along with the raw data, every step from basic processing
of the data, evaluation, to the generation of the figures, has to be documented
as clearly as possible. While there are many programming libraries that cover
the basic processing and plotting steps (e.g. Matplotlib in Python), no library
yet addresses the reproducible composing of single plots into meaningful
figures for publication. Thus, up to now it is still state-of-the-art to
generate publishable figures using image-processing or vector-drawing software
leading to unwanted alterations of the presented data in the worst case and to
figure quality reduction in the best case. Pylustrator a open source library
based on the Matplotlib aims to fill this gap and provides a tool to easily
generate the code necessary to compose publication figures from single plots.
It provides a graphical user interface where the user can interactively compose
the figures. All changes are tracked and converted to code that is
automatically integrated into the calling script file. Thus, this software
provides the missing link from raw data to the complete plot published in
scientific journals and thus contributes to the transparency of the complete
evaluation procedure.
</summary>
    <author>
      <name>Richard Gerum</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21105/joss.01989</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21105/joss.01989" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00279v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00279v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01304v1</id>
    <updated>2019-10-03T05:09:58Z</updated>
    <published>2019-10-03T05:09:58Z</published>
    <title>Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by
  Exploiting Ray Locality</title>
    <summary>  State-of-the-art ray tracing techniques operate on hierarchical acceleration
structures such as BVH trees which wrap objects in a scene into bounding
volumes of decreasing sizes. Acceleration structures reduce the amount of
ray-scene intersections that a ray has to perform to find the intersecting
object. However, we observe a large amount of redundancy when rays are
traversing these acceleration structures. While modern acceleration structures
explore the spatial organization of the scene, they neglect similarities
between rays that traverse the structures and thereby cause redundant
traversals. This paper provides a limit study of a new promising technique,
Hash-Based Ray Path Prediction (HRPP), which exploits the similarity between
rays to predict leaf nodes to avoid redundant acceleration structure
traversals. Our data shows that acceleration structure traversal consumes a
significant proportion of the ray tracing rendering time regardless of the
platform or the target image quality. Our study quantifies unused ray locality
and evaluates the theoretical potential for improved ray traversal performance
for both coherent and seemingly incoherent rays. We show that HRPP is able to
skip, on average, 40% of all hit-all traversal computations.
</summary>
    <author>
      <name>Francois Demoullin</name>
    </author>
    <author>
      <name>Ayub Gubran</name>
    </author>
    <author>
      <name>Tor Aamodt</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01812v1</id>
    <updated>2019-10-04T06:34:17Z</updated>
    <published>2019-10-04T06:34:17Z</published>
    <title>Sparse Surface Constraints for Combining Physics-based Elasticity
  Simulation and Correspondence-Free Object Reconstruction</title>
    <summary>  We address the problem to infer physical material parameters and boundary
conditions from the observed motion of a homogeneous deformable object via the
solution of an inverse problem. Parameters are estimated from potentially
unreliable real-world data sources such as sparse observations without
correspondences. We introduce a novel Lagrangian-Eulerian optimization
formulation, including a cost function that penalizes differences to
observations during an optimization run. This formulation matches
correspondence-free, sparse observations from a single-view depth sequence with
a finite element simulation of deformable bodies. In conjunction with an
efficient hexahedral discretization and a stable, implicit formulation of
collisions, our method can be used in demanding situation to recover a variety
of material parameters, ranging from Young's modulus and Poisson ratio to
gravity and stiffness damping, and even external boundaries. In a number of
tests using synthetic datasets and real-world measurements, we analyse the
robustness of our approach and the convergence behavior of the numerical
optimization scheme.
</summary>
    <author>
      <name>Sebastian Weiss</name>
    </author>
    <author>
      <name>Robert Maier</name>
    </author>
    <author>
      <name>Rüdiger Westermann</name>
    </author>
    <author>
      <name>Daniel Cremers</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPR42600.2020.00474</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPR42600.2020.00474" rel="related"/>
    <link href="http://arxiv.org/abs/1910.01812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02480v2</id>
    <updated>2020-07-30T17:32:00Z</updated>
    <published>2019-10-06T17:14:43Z</published>
    <title>Deep Radiance Caching: Convolutional Autoencoders Deeper in Ray Tracing</title>
    <summary>  Rendering realistic images with global illumination is a computationally
demanding task and often requires dedicated hardware for feasible runtime.
Recent research uses Deep Neural Networks to predict indirect lighting on image
level, but such methods are commonly limited to diffuse materials and require
training on each scene.We present Deep Radiance Caching (DRC), an efficient
variant of Radiance Caching utilizing Convolutional Autoencoders for rendering
global illumination. DRC employs a denoising neural network with Radiance
Caching to support a wide range of material types, without the requirement of
offline pre-computation or training for each scene.This offers high performance
CPU rendering for maximum accessibility. Our method has been evaluated on
interior scenes, and is able to produce high-quality images within 180 seconds
on a single CPU.
</summary>
    <author>
      <name>Giulio Jiang</name>
    </author>
    <author>
      <name>Bernhard Kainz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures, accepted by Computer And Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03310v2</id>
    <updated>2020-04-26T08:30:31Z</updated>
    <published>2019-10-08T10:00:52Z</published>
    <title>Visual Abstraction</title>
    <summary>  In this article we revisit the concept of abstraction as it is used in
visualization and put it on a solid formal footing. While the term
\emph{abstraction} is utilized in many scientific disciplines, arts, as well as
everyday life, visualization inherits the notion of data abstraction or class
abstraction from computer science, topological abstraction from mathematics,
and visual abstraction from arts. All these notions have a lot in common, yet
there is a major discrepancy in the terminology and basic understanding about
visual abstraction in the context of visualization. We thus root the notion of
abstraction in the philosophy of science, clarify the basic terminology, and
provide crisp definitions of visual abstraction as a process. Furthermore, we
clarify how it relates to similar terms often used interchangeably in the field
of visualization. Visual abstraction is characterized by a conceptual space
where this process exists, by the purpose it should serve, and by the
perceptual and cognitive qualities of the beholder. These characteristics can
be used to control the process of visual abstraction to produce effective and
informative visual representations.
</summary>
    <author>
      <name>Ivan Viola</name>
    </author>
    <author>
      <name>Min Chen</name>
    </author>
    <author>
      <name>Tobias Isenberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-34444-3_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-34444-3_2" rel="related"/>
    <link href="http://arxiv.org/abs/1910.03310v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03310v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04942v1</id>
    <updated>2019-10-11T02:21:30Z</updated>
    <published>2019-10-11T02:21:30Z</published>
    <title>Point cloud ridge-valley feature enhancement based on position and
  normal guidance</title>
    <summary>  Ridge-valley features are important elements of point clouds, as they contain
rich surface information. To recognize these features from point clouds, this
paper introduces an extreme point distance (EPD) criterion with scale
independence. Compared with traditional methods, the EPD greatly reduces the
number of potential feature points and improves the robustness of multiscale
feature point recognition. On this basis, a feature enhancement algorithm based
on user priori guidance is proposed that adjusts the coordinates of the feature
area by solving an objective equation containing the expected position and
normal constraints. Since the expected normal can be expressed as a function of
neighborhood point coordinates, the above objective equation can be converted
into linear sparse equations with enhanced feature positions as variables, and
thus, the closed solution can be obtained. In addition, a parameterization
method for scattered point clouds based on feature line guidance is proposed,
which reduces the number of unknowns by 2/3 and eliminates lateral sliding in
the direction perpendicular to feature lines. Finally, the application of the
algorithm in multiscale ridge-valley feature recognition, freeform surface
feature enhancement and computer-aided design (CAD) workpiece sharp feature
restoration verifies its effectiveness.
</summary>
    <author>
      <name>Jianhui Nie</name>
    </author>
    <author>
      <name>Zhaochen Zhang</name>
    </author>
    <author>
      <name>Ye Liu</name>
    </author>
    <author>
      <name>Hao Gao</name>
    </author>
    <author>
      <name>Feng Xu</name>
    </author>
    <author>
      <name>WenKai Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1910.04942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08398v1</id>
    <updated>2019-10-17T07:06:36Z</updated>
    <published>2019-10-17T07:06:36Z</published>
    <title>Statistical Parameter Selection for Clustering Persistence Diagrams</title>
    <summary>  In urgent decision making applications, ensemble simulations are an important
way to determine different outcome scenarios based on currently available data.
In this paper, we will analyze the output of ensemble simulations by
considering so-called persistence diagrams, which are reduced representations
of the original data, motivated by the extraction of topological features.
Based on a recently published progressive algorithm for the clustering of
persistence diagrams, we determine the optimal number of clusters, and
therefore the number of significantly different outcome scenarios, by the
minimization of established statistical score functions. Furthermore, we
present a proof-of-concept prototype implementation of the statistical
selection of the number of clusters and provide the results of an experimental
study, where this implementation has been applied to real-world ensemble data
sets.
</summary>
    <author>
      <name>Max Kontak</name>
    </author>
    <author>
      <name>Jules Vidal</name>
    </author>
    <author>
      <name>Julien Tierny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1907.04565</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.08398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08685v1</id>
    <updated>2019-10-19T03:12:26Z</updated>
    <published>2019-10-19T03:12:26Z</published>
    <title>Real-Time Lip Sync for Live 2D Animation</title>
    <summary>  The emergence of commercial tools for real-time performance-based 2D
animation has enabled 2D characters to appear on live broadcasts and streaming
platforms. A key requirement for live animation is fast and accurate lip sync
that allows characters to respond naturally to other actors or the audience
through the voice of a human performer. In this work, we present a deep
learning based interactive system that automatically generates live lip sync
for layered 2D characters using a Long Short Term Memory (LSTM) model. Our
system takes streaming audio as input and produces viseme sequences with less
than 200ms of latency (including processing time). Our contributions include
specific design decisions for our feature definition and LSTM configuration
that provide a small but useful amount of lookahead to produce accurate lip
sync. We also describe a data augmentation procedure that allows us to achieve
good results with a very small amount of hand-animated training data (13-20
minutes). Extensive human judgement experiments show that our results are
preferred over several competing methods, including those that only support
offline (non-live) processing. Video summary and supplementary results at
GitHub link: https://github.com/deepalianeja/CharacterLipSync2D
</summary>
    <author>
      <name>Deepali Aneja</name>
    </author>
    <author>
      <name>Wilmot Li</name>
    </author>
    <link href="http://arxiv.org/abs/1910.08685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.09166v1</id>
    <updated>2019-10-21T06:37:41Z</updated>
    <published>2019-10-21T06:37:41Z</published>
    <title>Dynamic Upsampling of Smoke through Dictionary-based Learning</title>
    <summary>  Simulating turbulent smoke flows is computationally intensive due to their
intrinsic multiscale behavior, thus requiring relatively high resolution grids
to fully capture their complexity. For iterative editing or simply faster
generation of smoke flows, dynamic upsampling of an input low-resolution
numerical simulation is an attractive, yet currently unattainable goal. In this
paper, we propose a novel dictionary-based learning approach to the dynamic
upsampling of smoke flows. For each frame of an input coarse animation, we seek
a sparse representation of small, local velocity patches of the flow based on
an over-complete dictionary, and use the resulting sparse coefficients to
generate a high-resolution smoke animation sequence. We propose a novel
dictionary-based neural network which learns both a fast evaluation of sparse
patch encoding and a dictionary of corresponding coarse and fine patches from a
sequence of example simulations computed with any numerical solver. Our
upsampling network then injects into coarse input sequences physics-driven fine
details, unlike most previous approaches that only employed fast procedural
models to add high frequency to the input. We present a variety of upsampling
results for smoke flows and offer comparisons to their corresponding
high-resolution simulations to demonstrate the effectiveness of our approach.
</summary>
    <author>
      <name>Kai Bai</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Mathieu Desbrun</name>
    </author>
    <author>
      <name>Xiaopei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.09166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.09166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00193v1</id>
    <updated>2019-11-01T03:31:49Z</updated>
    <published>2019-11-01T03:31:49Z</published>
    <title>Personality-Aware Probabilistic Map for Trajectory Prediction of
  Pedestrians</title>
    <summary>  We present a novel trajectory prediction algorithm for pedestrians based on a
personality-aware probabilistic feature map. This map is computed using a
spatial query structure and each value represents the probability of the
predicted pedestrian passing through various positions in the crowd space. We
update this map dynamically based on the agents in the environment and prior
trajectory of a pedestrian. Furthermore, we estimate the personality
characteristics of each pedestrian and use them to improve the prediction by
estimating the shortest path in this map. Our approach is general and works
well on crowd videos with low and high pedestrian density. We evaluate our
model on standard human-trajectory datasets. In practice, our prediction
algorithm improves the accuracy by 5-9% over prior algorithms.
</summary>
    <author>
      <name>Chaochao Li</name>
    </author>
    <author>
      <name>Pei Lv</name>
    </author>
    <author>
      <name>Mingliang Xu</name>
    </author>
    <author>
      <name>Xinyu Wang</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <author>
      <name>Bing Zhou</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1911.00193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04446v1</id>
    <updated>2019-11-11T18:49:49Z</updated>
    <published>2019-11-11T18:49:49Z</published>
    <title>Enhancing User Experience in Virtual Reality with Radial Basis Function
  Interpolation Based Stereoscopic Camera Control</title>
    <summary>  Providing a depth-rich Virtual Reality (VR) experience to users without
causing discomfort remains to be a challenge with today's commercially
available head-mounted displays (HMDs), which enforce strict measures on
stereoscopic camera parameters for the sake of keeping visual discomfort to a
minimum. However, these measures often lead to an unimpressive VR experience
with shallow depth feeling. We propose the first method ready to be used with
existing consumer HMDs for automated stereoscopic camera control in virtual
environments (VEs). Using radial basis function interpolation and projection
matrix manipulations, our method makes it possible to significantly enhance
user experience in terms of overall perceived depth while maintaining visual
discomfort on a par with the default arrangement. In our implementation, we
also introduce the first immersive interface for authoring a unique 3D
stereoscopic cinematography for any VE to be experienced with consumer HMDs. We
conducted a user study that demonstrates the benefits of our approach in terms
of superior picture quality and perceived depth. We also investigated the
effects of using depth of field (DoF) in combination with our approach and
observed that the addition of our DoF implementation was seen as a degraded
experience, if not similar.
</summary>
    <author>
      <name>Emre Avan</name>
    </author>
    <author>
      <name>Ufuk Celikcan</name>
    </author>
    <author>
      <name>Tolga K. Capin</name>
    </author>
    <author>
      <name>Hasmet Gurcay</name>
    </author>
    <link href="http://arxiv.org/abs/1911.04446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05668v1</id>
    <updated>2019-11-02T20:17:08Z</updated>
    <published>2019-11-02T20:17:08Z</published>
    <title>Point Movement in a DSL for Higher-Order FEM Visualization</title>
    <summary>  Scientific visualization tools tend to be flexible in some ways (e.g., for
exploring isovalues) while restricted in other ways, such as working only on
regular grids, or only on unstructured meshes (as used in the finite element
method, FEM). Our work seeks to expose the common structure of visualization
methods, apart from the specifics of how the fields being visualized are
formed. Recognizing that previous approaches to FEM visualization depend on
efficiently updating computed positions within a mesh, we took an existing
visualization domain-specific language, and added a mesh position type and
associated arithmetic operators. These are orthogonal to the visualization
method itself, so existing programs for visualizing regular grid data work,
with minimal changes, on higher-order FEM data. We reproduce the efficiency
gains of an earlier guided search method of mesh position update for computing
streamlines, and we demonstrate a novel ability to uniformly sample ridge
surfaces of higher-order FEM solutions defined on curved meshes.
</summary>
    <author>
      <name>Teodoro Collin</name>
    </author>
    <author>
      <name>Charisee Chiw</name>
    </author>
    <author>
      <name>L. Ridgway Scott</name>
    </author>
    <author>
      <name>John Reppy</name>
    </author>
    <author>
      <name>Gordon L. Kindlmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VISUAL.2019.8933623</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VISUAL.2019.8933623" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared at IEEE Visualization 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.05668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06001v2</id>
    <updated>2019-12-05T16:58:30Z</updated>
    <published>2019-11-14T08:58:23Z</published>
    <title>Efficient Animation of Sparse Voxel Octrees for Real-Time Ray Tracing</title>
    <summary>  A considerable limitation of employing sparse voxels octrees (SVOs) as a
model format for ray tracing has been that the octree data structure is
inherently static. Due to traversal algorithms' dependence on the strict
hierarchical structure of octrees, it has been challenging to achieve real-time
performance of SVO model animation in ray tracing since the octree data
structure would typically have to be regenerated every frame. Presented in this
article is a novel method for animation of models specified on the SVO format.
The method distinguishes itself by permitting model transformations such as
rotation, translation, and anisotropic scaling, while preserving the
hierarchical structure of SVO models so that they may be efficiently traversed.
Due to its modest memory footprint and straightforward arithmetic operations,
the method is well-suited for implementation in hardware. A software ray
tracing implementation of animated SVO models demonstrates real-time
performance on current-generation desktop GPUs, and shows that the animation
method does not substantially slow down the rendering procedure compared to
rendering static SVOs.
</summary>
    <author>
      <name>Asbjørn Engmark Espe</name>
    </author>
    <author>
      <name>Øystein Gjermundnes</name>
    </author>
    <author>
      <name>Sverre Hendseth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of an article submitted for review to IEEE Transactions on
  Visualization and Computer Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.06001v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06001v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06906v1</id>
    <updated>2019-11-15T23:17:09Z</updated>
    <published>2019-11-15T23:17:09Z</published>
    <title>Applying Rational Envelope curves for skinning purposes</title>
    <summary>  Special curves in the Minkowski space such as Minkowski Pythagorean
hodographs play an important role in Computer Aided Geometric Design, and their
usages have been thoroughly studied in the recent years. Also, several papers
have been published which describe methods for interpolating Hermite data in
R2,1 by MPH curves. Bizzarri et al.introduced the class of RE curves and
presented an interpolation method for G1 Hermite data, where the resulting RE
curve yields a rational boundary for the represented domain. We now propose a
new application area for RE curves: skinning of a discrete set of input
circles. We find the appropriate Hermite data to interpolate so that the
obtained rational envelope curves touch each circle at previously defined
points of contact. This way we overcome the problematic scenarios when the
location of the touching points would not be appropriate for skinning purposes.
</summary>
    <author>
      <name>Kinga Kruppa</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09204v4</id>
    <updated>2020-03-31T18:16:52Z</updated>
    <published>2019-11-20T22:57:51Z</published>
    <title>DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape
  Reconstruction</title>
    <summary>  We introduce a differential visual similarity metric to train deep neural
networks for 3D reconstruction, aimed at improving reconstruction quality. The
metric compares two 3D shapes by measuring distances between multi-view images
differentiably rendered from the shapes. Importantly, the image-space distance
is also differentiable and measures visual similarity, rather than pixel-wise
distortion. Specifically, the similarity is defined by mean-squared errors over
HardNet features computed from probabilistic keypoint maps of the compared
images. Our differential visual shape similarity metric can be easily plugged
into various 3D reconstruction networks, replacing their distortion-based
losses, such as Chamfer or Earth Mover distances, so as to optimize the network
weights to produce reconstructions with better structural fidelity and visual
quality. We demonstrate this both objectively, using well-known shape metrics
for retrieval and classification tasks that are independent from our new
metric, and subjectively through a perceptual study.
</summary>
    <author>
      <name>Jiongchao Jin</name>
    </author>
    <author>
      <name>Akshay Gadi Patil</name>
    </author>
    <author>
      <name>Zhang Xiong</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1911.09204v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09204v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10044v1</id>
    <updated>2019-11-22T13:53:45Z</updated>
    <published>2019-11-22T13:53:45Z</published>
    <title>Virtual Lenses as Embodied Tools for Immersive Analytics</title>
    <summary>  Interactive lenses are useful tools for supporting the analysis of data in
different ways. Most existing lenses are designed for 2D visualization and are
operated using standard mouse and keyboard interaction. On the other hand,
research on virtual lenses for novel 3D immersive visualization environments is
scarce. Our work aims to narrow this gap in the literature. We focus
particularly on the interaction with lenses. Inspired by natural interaction
with magnifying glasses in the real world, our lenses are designed as graspable
tools that can be created and removed as needed, manipulated and parameterized
depending on the task, and even combined to flexibly create new views on the
data. We implemented our ideas in a system for the visual analysis of 3D sonar
data. Informal user feedback from more than a hundred people suggests that the
designed lens interaction is easy to use for the task of finding a hidden wreck
in sonar data.
</summary>
    <author>
      <name>Sven Kluge</name>
    </author>
    <author>
      <name>Stefan Gladisch</name>
    </author>
    <author>
      <name>Uwe Freiherr von Lukas</name>
    </author>
    <author>
      <name>Oliver Staadt</name>
    </author>
    <author>
      <name>Christian Tominski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18420/vrar2020_8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18420/vrar2020_8" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the GI VR/AR Workshop (VAR), 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.10044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12327v1</id>
    <updated>2019-11-27T18:08:21Z</updated>
    <published>2019-11-27T18:08:21Z</published>
    <title>Inattentional Blindness for Redirected Walking Using Dynamic Foveated
  Rendering</title>
    <summary>  Redirected walking is a Virtual Reality(VR) locomotion technique which
enables users to navigate virtual environments (VEs) that are spatially larger
than the available physical tracked space. In this work we present a novel
technique for redirected walking in VR based on the psychological phenomenon of
inattentional blindness. Based on the user's visual fixation points we divide
the user's view into zones. Spatially-varying rotations are applied according
to the zone's importance and are rendered using foveated rendering. Our
technique is real-time and applicable to small and large physical spaces.
Furthermore, the proposed technique does not require the use of stimulated
saccades but rather takes advantage of naturally occurring saccades and blinks
for a complete refresh of the framebuffer. We performed extensive testing and
present the analysis of the results of three user studies conducted for the
evaluation.
</summary>
    <author>
      <name>Yashas Joshi</name>
    </author>
    <author>
      <name>Charalambos Poullis</name>
    </author>
    <link href="http://arxiv.org/abs/1911.12327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01067v5</id>
    <updated>2020-11-04T01:23:13Z</updated>
    <published>2019-12-02T20:19:07Z</published>
    <title>A Bayesian Inference Framework for Procedural Material Parameter
  Estimation</title>
    <summary>  Procedural material models have been gaining traction in many applications
thanks to their flexibility, compactness, and easy editability. We explore the
inverse rendering problem of procedural material parameter estimation from
photographs, presenting a unified view of the problem in a Bayesian framework.
In addition to computing point estimates of the parameters by optimization, our
framework uses a Markov Chain Monte Carlo approach to sample the space of
plausible material parameters, providing a collection of plausible matches that
a user can choose from, and efficiently handling both discrete and continuous
model parameters. To demonstrate the effectiveness of our framework, we fit
procedural models of a range of materials---wall plaster, leather, wood,
anisotropic brushed metals and layered metallic paints---to both synthetic and
real target images.
</summary>
    <author>
      <name>Yu Guo</name>
    </author>
    <author>
      <name>Milos Hasan</name>
    </author>
    <author>
      <name>Lingqi Yan</name>
    </author>
    <author>
      <name>Shuang Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14142</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14142" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 13 figures, Pacific Graphics 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.01067v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01067v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01248v2</id>
    <updated>2019-12-09T09:52:35Z</updated>
    <published>2019-12-03T09:10:24Z</published>
    <title>Multiple Approaches to Frame Field Correction for CAD Models</title>
    <summary>  Three-dimensional frame fields computed on CAD models often contain singular
curves that are not compatible with hexahedral meshing. In this paper, we show
how CAD feature curves can induce non meshable 3-5 singular curves and we study
four different approaches that aims at correcting the frame field topology. All
approaches consist in modifying the frame field computation, the two first ones
consisting in applying internal constraints and the two last ones consisting in
modifying the boundary conditions. Approaches based on internal constraints are
shown not to be very reliable because of their interactions with other
singularities. On the other hand, boundary condition modifications are more
promising as their impact is very localized. We eventually recommend the 3-5
singular curve boundary snapping strategy, which is simple to implement and
allows to generate topologically correct frame fields.
</summary>
    <author>
      <name>Maxence Reberol</name>
    </author>
    <author>
      <name>Alexandre Chemin</name>
    </author>
    <author>
      <name>Jean-Francois Remacle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for 28th International Meshing Roundtable</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.01248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.04000v1</id>
    <updated>2019-12-09T12:36:39Z</updated>
    <published>2019-12-09T12:36:39Z</published>
    <title>Spectral domain decomposition method for physically-based rendering of
  Royaumont abbey</title>
    <summary>  In the context of a virtual reconstitution of the destroyed Royaumont abbey
church, this paper investigates computer sciences issues intrinsic to the
physically-based image rendering. First, a virtual model was designed from
historical sources and archaeological descriptions. Then some materials
physical properties were measured on remains of the church and on pieces from
similar ancient churches. We specify the properties of our lighting source
which is a representation of the sun, and present the rendering algorithm
implemented in our software Virtuelium. In order to accelerate the computation
of the interactions between light-rays and objects, this ray-tracing algorithm
is parallelized by means of domain decomposition techniques. Numerical
experiments show that the computational time saved by a classic parallelization
is much less significant than that gained with our approach.
</summary>
    <author>
      <name>Guillaume Gbikpi-Benissan</name>
    </author>
    <author>
      <name>Patrick Callet</name>
    </author>
    <author>
      <name>Frederic Magoules</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CSE-EUC-DCABES.2016.212</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CSE-EUC-DCABES.2016.212" rel="related"/>
    <link href="http://arxiv.org/abs/1912.04000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06341v1</id>
    <updated>2019-12-13T07:06:52Z</updated>
    <published>2019-12-13T07:06:52Z</published>
    <title>Uncertainty Visualization of 2D Morse Complex Ensembles Using
  Statistical Summary Maps</title>
    <summary>  Morse complexes are gradient-based topological descriptors with close
connections to Morse theory. They are widely applicable in scientific
visualization as they serve as important abstractions for gaining insights into
the topology of scalar fields. Noise inherent to scalar field data due to
acquisitions and processing, however, limits our understanding of the Morse
complexes as structural abstractions. We, therefore, explore uncertainty
visualization of an ensemble of 2D Morse complexes that arise from scalar
fields coupled with data uncertainty. We propose statistical summary maps as
new entities for capturing structural variations and visualizing positional
uncertainties of Morse complexes in ensembles. Specifically, we introduce two
types of statistical summary maps -- the Probabilistic Map and the Survival Map
-- to characterize the uncertain behaviors of local extrema and local gradient
flows, respectively. We demonstrate the utility of our proposed approach using
synthetic and real-world datasets.
</summary>
    <author>
      <name>Tushar Athawale</name>
    </author>
    <author>
      <name>Dan Maljovec</name>
    </author>
    <author>
      <name>Chris R. Johnson</name>
    </author>
    <author>
      <name>Valerio Pascucci</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1912.06341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06395v2</id>
    <updated>2020-03-18T13:33:27Z</updated>
    <published>2019-12-13T10:25:00Z</published>
    <title>Neural Cages for Detail-Preserving 3D Deformations</title>
    <summary>  We propose a novel learnable representation for detail-preserving shape
deformation. The goal of our method is to warp a source shape to match the
general structure of a target shape, while preserving the surface details of
the source. Our method extends a traditional cage-based deformation technique,
where the source shape is enclosed by a coarse control mesh termed \emph{cage},
and translations prescribed on the cage vertices are interpolated to any point
on the source mesh via special weight functions. The use of this sparse cage
scaffolding enables preserving surface details regardless of the shape's
intricacy and topology. Our key contribution is a novel neural network
architecture for predicting deformations by controlling the cage. We
incorporate a differentiable cage-based deformation module in our architecture,
and train our network end-to-end. Our method can be trained with common
collections of 3D models in an unsupervised fashion, without any cage-specific
annotations. We demonstrate the utility of our method for synthesizing shape
variations and deformation transfer.
</summary>
    <author>
      <name>Wang Yifan</name>
    </author>
    <author>
      <name>Noam Aigerman</name>
    </author>
    <author>
      <name>Vladimir G. Kim</name>
    </author>
    <author>
      <name>Siddhartha Chaudhuri</name>
    </author>
    <author>
      <name>Olga Sorkine-Hornung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for oral presentation at CVPR 2020, code available at
  https://github.com/yifita/deep_cage</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.06395v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06395v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06474v1</id>
    <updated>2019-12-09T12:31:50Z</updated>
    <published>2019-12-09T12:31:50Z</published>
    <title>Spectral domain decomposition method for physically-based rendering of
  photochromic/electrochromic glass windows</title>
    <summary>  This paper covers the time consuming issues intrinsic to physically-based
image rendering algorithms. First, glass materials optical properties were
measured on samples of real glasses and other objects materials inside an hotel
room were characterized by deducing spectral data from multiple trichromatic
images. We then present the rendering model and ray-tracing algorithm
implemented in Virtuelium, an open source software. In order to accelerate the
computation of the interactions between light rays and objects, the ray-tracing
algorithm is parallelized by means of domain decomposition method techniques.
Numerical experiments show that the speedups obtained with classical
parallelization techniques are significantly less significant than those
achieved with parallel domain decomposition methods.
</summary>
    <author>
      <name>Guillaume Gbikpi-Benissan</name>
    </author>
    <author>
      <name>Patrick Callet</name>
    </author>
    <author>
      <name>Frederic Magoules</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCABES.2014.27</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCABES.2014.27" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1912.05494</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.06474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08485v2</id>
    <updated>2020-07-31T09:54:48Z</updated>
    <published>2019-12-18T09:44:34Z</published>
    <title>A Comparison of Rendering Techniques for Large 3D Line Sets with
  Transparency</title>
    <summary>  This paper presents a comprehensive study of interactive rendering techniques
for large 3D line sets with transparency. The rendering of transparent lines is
widely used for visualizing trajectories of tracer particles in flow fields.
Transparency is then used to fade out lines deemed unimportant, based on, for
instance, geometric properties or attributes defined along them. Since accurate
blending of transparent lines requires rendering the lines in back-to-front or
front-to-back order, enforcing this order for 3D line sets with tens or even
hundreds of thousands of elements becomes challenging. In this paper, we study
CPU and GPU rendering techniques for large transparent 3D line sets. We compare
accurate and approximate techniques using optimized implementations and a
number of benchmark data sets. We discuss the effects of data size and
transparency on quality, performance and memory consumption. Based on our
study, we propose two improvements to per-pixel fragment lists and multi-layer
alpha blending. The first improves the rendering speed via an improved GPU
sorting operation, and the second improves rendering quality via a
transparency-based bucketing.
</summary>
    <author>
      <name>Michael Kern</name>
    </author>
    <author>
      <name>Christoph Neuhauser</name>
    </author>
    <author>
      <name>Torben Maack</name>
    </author>
    <author>
      <name>Mengjiao Han</name>
    </author>
    <author>
      <name>Will Usher</name>
    </author>
    <author>
      <name>Rüdiger Westermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2020.2975795</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2020.2975795" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 pages appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.08485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08558v1</id>
    <updated>2019-12-18T12:21:01Z</updated>
    <published>2019-12-18T12:21:01Z</published>
    <title>Multi-display Visual Analysis: Model, Interface, and Layout Computation</title>
    <summary>  Modern display environments offer great potential for involving multiple
users in presentations, discussions, and data analysis sessions. By showing
multiple views on multiple displays, information exchange can be improved,
several perspectives on the data can be combined, and different analysis
strategies can be pursued.
  In this report, we describe concepts to support display composition,
information distribution, and analysis coordination for visual data analysis in
multi-display environments. In particular, a basic model for layout modeling is
introduced, a graphical interface for interactive generation of the model is
presented, and a layout mechanism is described that arranges multiple views on
multiple displays automatically. Furthermore, approaches to meta-analysis will
be discussed. The developed approaches are demonstrated in a use case that
focuses on parameter space analysis for the segmentation of time series data.
</summary>
    <author>
      <name>Christian Eichner</name>
    </author>
    <author>
      <name>Heidrun Schumann</name>
    </author>
    <author>
      <name>Christian Tominski</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09580v1</id>
    <updated>2019-12-19T22:39:40Z</updated>
    <published>2019-12-19T22:39:40Z</published>
    <title>MVF Designer: Design and Visualization of Morse Vector Fields</title>
    <summary>  Vector field design on surfaces was originally motivated by applications in
graphics such as texture synthesis and rendering. In this paper, we consider
the idea of vector field design with a new motivation from computational
topology. We are interested in designing and visualizing vector fields to aid
the study of Morse functions, Morse vector fields, and Morse-Smale complexes.
To achieve such a goal, we present MVF Designer, a new interactive design
system that provides fine-grained control over vector field geometry, enables
the editing of vector field topology, and supports a design process in a simple
and efficient way using elementary moves, which are actions that initiate or
advance our design process. Our system allows mathematicians to explore the
complex configuration spaces of Morse functions, their gradients, and their
associated Morse-Smale complexes. Understanding these spaces will help us
expand further their applicability in topological data analysis and
visualization.
</summary>
    <author>
      <name>Youjia Zhou</name>
    </author>
    <author>
      <name>Janis Lazovskis</name>
    </author>
    <author>
      <name>Michael J. Catanzaro</name>
    </author>
    <author>
      <name>Matthew Zabka</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1912.09580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09596v1</id>
    <updated>2019-12-20T00:33:03Z</updated>
    <published>2019-12-20T00:33:03Z</published>
    <title>Comparing Hierarchical Data Structures for Sparse Volume Rendering with
  Empty Space Skipping</title>
    <summary>  Empty space skipping can be efficiently implemented with hierarchical data
structures such as k-d trees and bounding volume hierarchies. This paper
compares several recently published hierarchical data structures with regard to
construction and rendering performance. The papers that form our prior work
have primarily focused on interactively building the data structures and only
showed that rendering performance is superior to using simple acceleration data
structures such as uniform grids with macro cells. In the area of surface ray
tracing, there exists a trade-off between construction and rendering
performance of hierarchical data structures. In this paper we present
performance comparisons for several empty space skipping data structures in
order to determine if such a trade-off also exists for volume rendering with
uniform data topologies.
</summary>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <link href="http://arxiv.org/abs/1912.09596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10637v3</id>
    <updated>2020-06-23T05:37:34Z</updated>
    <published>2019-12-23T05:47:21Z</published>
    <title>GrabAR: Occlusion-aware Grabbing Virtual Objects in AR</title>
    <summary>  Existing augmented reality (AR) applications often ignore occlusion between
real hands and virtual objects when incorporating virtual objects in our views.
The challenges come from the lack of accurate depth and mismatch between real
and virtual depth. This paper presents GrabAR, a new approach that directly
predicts the real-and-virtual occlusion, and bypasses the depth acquisition and
inference. Our goal is to enhance AR applications with interactions between
hand (real) and grabbable objects (virtual). With paired images of hand and
object as inputs, we formulate a neural network that learns to generate the
occlusion mask. To train the network, we compile a synthetic dataset to
pre-train it and a real dataset to fine-tune it, thus reducing the burden of
manual labels and addressing the domain difference. Then, we embed the trained
network in a prototyping AR system that supports hand grabbing of various
virtual objects, demonstrate the system performance, both quantitatively and
qualitatively, and showcase interaction scenarios, in which we can use bare
hand to grab virtual objects and directly manipulate them.
</summary>
    <author>
      <name>Xiao Tang</name>
    </author>
    <author>
      <name>Xiaowei Hu</name>
    </author>
    <author>
      <name>Chi-Wing Fu</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conditionally accepted to UIST 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10637v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10637v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10787v2</id>
    <updated>2020-01-24T20:12:32Z</updated>
    <published>2019-12-08T23:44:33Z</published>
    <title>Learned Interpolation for 3D Generation</title>
    <summary>  In order to generate novel 3D shapes with machine learning, one must allow
for interpolation. The typical approach for incorporating this creative process
is to interpolate in a learned latent space so as to avoid the problem of
generating unrealistic instances by exploiting the model's learned structure.
The process of the interpolation is supposed to form a semantically smooth
morphing. While this approach is sound for synthesizing realistic media such as
lifelike portraits or new designs for everyday objects, it subjectively fails
to directly model the unexpected, unrealistic, or creative. In this work, we
present a method for learning how to interpolate point clouds. By encoding
prior knowledge about real-world objects, the intermediate forms are both
realistic and unlike any existing forms. We show not only how this method can
be used to generate "creative" point clouds, but how the method can also be
leveraged to generate 3D models suitable for sculpture.
</summary>
    <author>
      <name>Austin Dill</name>
    </author>
    <author>
      <name>Songwei Ge</name>
    </author>
    <author>
      <name>Eunsu Kang</name>
    </author>
    <author>
      <name>Chun-Liang Li</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Creativity and Design Workshop at NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10787v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10787v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11565v1</id>
    <updated>2019-12-24T23:32:42Z</updated>
    <published>2019-12-24T23:32:42Z</published>
    <title>Rendering Synthetic Objects into Legacy Photographs</title>
    <summary>  We propose a method to realistically insert synthetic objects into existing
photographs without requiring access to the scene or any additional scene
measurements. With a single image and a small amount of annotation, our method
creates a physical model of the scene that is suitable for realistically
rendering synthetic objects with diffuse, specular, and even glowing materials
while accounting for lighting interactions between the objects and the scene.
We demonstrate in a user study that synthetic images produced by our method are
confusable with real scenes, even for people who believe they are good at
telling the difference. Further, our study shows that our method is competitive
with other insertion methods while requiring less scene information. We also
collected new illumination and reflectance datasets; renderings produced by our
system compare well to ground truth. Our system has applications in the movie
and gaming industry, as well as home decorating and user content creation,
among others.
</summary>
    <author>
      <name>Kevin Karsch</name>
    </author>
    <author>
      <name>Varsha Hedau</name>
    </author>
    <author>
      <name>David Forsyth</name>
    </author>
    <author>
      <name>Derek Hoiem</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11567v1</id>
    <updated>2019-12-24T23:57:22Z</updated>
    <published>2019-12-24T23:57:22Z</published>
    <title>ConstructAide: Analyzing and Visualizing Construction Sites through
  Photographs and Building Models</title>
    <summary>  We describe a set of tools for analyzing, visualizing, and assessing
architectural/construction progress with unordered photo collections and 3D
building models. With our interface, a user guides the registration of the
model in one of the images, and our system automatically computes the alignment
for the rest of the photos using a novel Structure-from-Motion (SfM) technique;
images with nearby viewpoints are also brought into alignment with each other.
After aligning the photo(s) and model(s), our system allows a user, such as a
project manager or facility owner, to explore the construction site seamlessly
in time, monitor the progress of construction, assess errors and deviations,
and create photorealistic architectural visualizations. These interactions are
facilitated by automatic reasoning performed by our system: static and dynamic
occlusions are removed automatically, rendering information is collected, and
semantic selection tools help guide user input. We also demonstrate that our
user-assisted SfM method outperforms existing techniques on both real-world
construction data and established multi-view datasets.
</summary>
    <author>
      <name>Kevin Karsch</name>
    </author>
    <author>
      <name>Mani Golparvar-Fard</name>
    </author>
    <author>
      <name>David Forsyth</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11568v1</id>
    <updated>2019-12-25T00:00:25Z</updated>
    <published>2019-12-25T00:00:25Z</published>
    <title>Blind Recovery of Spatially Varying Reflectance from a Single Image</title>
    <summary>  We propose a new technique for estimating spatially varying parametric
materials from a single image of an object with unknown shape in unknown
illumination. Our method uses a low-order parametric reflectance model, and
incorporates strong assumptions about lighting and shape. We develop new priors
about how materials mix over space, and jointly infer all of these properties
from a single image. This produces a decomposition of an image which
corresponds, in one sense, to microscopic features (material reflectance) and
macroscopic features (weights defining the mixing properties of materials over
space). We have built a large dataset of real objects rendered with different
material models under different illumination fields for training and ground
truth evaluation. Extensive experiments on both our synthetic dataset images as
well as real images show that (a) our method recovers parameters with
reasonable accuracy; (b) material parameters recovered by our method give
accurate predictions of new renderings of the object; and (c) our low-order
reflectance model still provides a good fit to many real-world reflectances.
</summary>
    <author>
      <name>Kevin Karsch</name>
    </author>
    <author>
      <name>David Forsyth</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12297v1</id>
    <updated>2019-12-24T23:43:52Z</updated>
    <published>2019-12-24T23:43:52Z</published>
    <title>Automatic Scene Inference for 3D Object Compositing</title>
    <summary>  We present a user-friendly image editing system that supports a drag-and-drop
object insertion (where the user merely drags objects into the image, and the
system automatically places them in 3D and relights them appropriately),
post-process illumination editing, and depth-of-field manipulation. Underlying
our system is a fully automatic technique for recovering a comprehensive 3D
scene model (geometry, illumination, diffuse albedo and camera parameters) from
a single, low dynamic range photograph. This is made possible by two novel
contributions: an illumination inference algorithm that recovers a full
lighting model of the scene (including light sources that are not directly
visible in the photograph), and a depth estimation algorithm that combines
data-driven depth transfer with geometric reasoning about the scene layout. A
user study shows that our system produces perceptually convincing results, and
achieves the same level of realism as techniques that require significant user
interaction.
</summary>
    <author>
      <name>Kevin Karsch</name>
    </author>
    <author>
      <name>Kalyan Sunkavalli</name>
    </author>
    <author>
      <name>Sunil Hadap</name>
    </author>
    <author>
      <name>Nathan Carr</name>
    </author>
    <author>
      <name>Hailin Jin</name>
    </author>
    <author>
      <name>Rafael Fonte</name>
    </author>
    <author>
      <name>Michael Sittig</name>
    </author>
    <link href="http://arxiv.org/abs/1912.12297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12763v2</id>
    <updated>2020-01-02T18:39:18Z</updated>
    <published>2019-12-30T00:14:57Z</published>
    <title>Animals in Virtual Environments</title>
    <summary>  The core idea in an XR (VR/MR/AR) application is to digitally stimulate one
or more sensory systems (e.g. visual, auditory, olfactory) of the human user in
an interactive way to achieve an immersive experience. Since the early 2000s
biologists have been using Virtual Environments (VE) to investigate the
mechanisms of behavior in non-human animals including insect, fish, and
mammals. VEs have become reliable tools for studying vision, cognition, and
sensory-motor control in animals. In turn, the knowledge gained from studying
such behaviors can be harnessed by researchers designing biologically inspired
robots, smart sensors, and multi-agent artificial intelligence. VE for animals
is becoming a widely used application of XR technology but such applications
have not previously been reported in the technical literature related to XR.
Biologists and computer scientists can benefit greatly from deepening
interdisciplinary research in this emerging field and together we can develop
new methods for conducting fundamental research in behavioral sciences and
engineering. To support our argument we present this review which provides an
overview of animal behavior experiments conducted in virtual environments.
</summary>
    <author>
      <name>Hemal Naik</name>
    </author>
    <author>
      <name>Renaud Bastien</name>
    </author>
    <author>
      <name>Nassir Navab</name>
    </author>
    <author>
      <name>Iain Couzin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2020.2973063</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2020.2973063" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditional acceptance in IEEE TVCG</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics Feb 13
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.12763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00328v2</id>
    <updated>2020-02-05T09:14:29Z</updated>
    <published>2020-02-02T05:09:09Z</published>
    <title>Fast 3D Indoor Scene Synthesis with Discrete and Exact Layout Pattern
  Extraction</title>
    <summary>  We present a fast framework for indoor scene synthesis, given a room geometry
and a list of objects with learnt priors. Unlike existing data-driven
solutions, which often extract priors by co-occurrence analysis and statistical
model fitting, our method measures the strengths of spatial relations by tests
for complete spatial randomness (CSR), and extracts complex priors based on
samples with the ability to accurately represent discrete layout patterns. With
the extracted priors, our method achieves both acceleration and plausibility by
partitioning input objects into disjoint groups, followed by layout
optimization based on the Hausdorff metric. Extensive experiments show that our
framework is capable of measuring more reasonable relations among objects and
simultaneously generating varied arrangements in seconds.
</summary>
    <author>
      <name>Song-Hai Zhang</name>
    </author>
    <author>
      <name>Shao-Kui Zhang</name>
    </author>
    <author>
      <name>Wei-Yu Xie</name>
    </author>
    <author>
      <name>Cheng-Yang Luo</name>
    </author>
    <author>
      <name>Hong-Bo Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We currently received our first valuable comments from reviewers. We
  will continuing modify our paper accordingly, so this paper will be modified
  frequently</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00328v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00328v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03891v1</id>
    <updated>2020-02-10T16:00:01Z</updated>
    <published>2020-02-10T16:00:01Z</published>
    <title>SplitStreams: A Visual Metaphor for Evolving Hierarchies</title>
    <summary>  The visualization of hierarchically structured data over time is an ongoing
challenge and several approaches exist trying to solve it. Techniques such as
animated or juxtaposed tree visualizations are not capable of providing a good
overview of the time series and lack expressiveness in conveying changes over
time. Nested streamgraphs provide a better understanding of the data evolution,
but lack the clear outline of hierarchical structures at a given timestep.
Furthermore, these approaches are often limited to static hierarchies or
exclude complex hierarchical changes in the data, limiting their use cases. We
propose a novel visual metaphor capable of providing a static overview of all
hierarchical changes over time, as well as clearly outlining the hierarchical
structure at each individual time step. Our method allows for smooth
transitions between tree maps and nested streamgraphs, enabling the exploration
of the trade-off between dynamic behavior and hierarchical structure. As our
technique handles topological changes of all types, it is suitable for a wide
range of applications. We demonstrate the utility of our method on several use
cases, evaluate it with a user study, and provide its full source code.
</summary>
    <author>
      <name>Fabian Bolte</name>
    </author>
    <author>
      <name>Mahsan Nourani</name>
    </author>
    <author>
      <name>Eric D. Ragan</name>
    </author>
    <author>
      <name>Stefan Bruckner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will be published at IEEE Transactions on Visualization &amp; Computer
  Graphics 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04509v2</id>
    <updated>2020-08-18T08:35:04Z</updated>
    <published>2020-02-11T16:11:16Z</published>
    <title>Course notes Geometric Algebra for Computer Graphics, SIGGRAPH 2019</title>
    <summary>  What is the best representation for doing euclidean geometry on computers?
These notes from a SIGGRAPH 2019 short course entitled "Geometric algebra for
computer graphics" introduce projective geometric algebra (PGA) as a modern
framework for this task. PGA features: uniform representation of points, lines,
and planes; robust, parallel-safe join and meet operations; compact,
polymorphic syntax for euclidean formulas and constructions; a single intuitive
sandwich form for isometries; native support for automatic differentiation; and
tight integration of kinematics and rigid body mechanics. PGA includes vector,
quaternion, dual quaternion, and exterior algebras as sub-algebras, simplifying
the learning curve and transition path for experienced practitioners. On the
practical side, it can be efficiently implemented, while its rich syntax
enhances programming productivity. The basic ideas are introduced in the 2D
context and developed selectively for 3D. Advantages to traditional approaches
are collected in a table at the end. The article aims to be a self-contained
introduction for practitioners of euclidean geometry and includes numerous
examples, formulas, figures, and tables.
</summary>
    <author>
      <name>Charles G. Gunn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3305366.3328099</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3305366.3328099" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages, 14 figures, 6 tables, Course notes for SIGGRAPH 2019 short
  course July 30, 2019 taught with Steven De Keninck. arXiv admin note:
  substantial text overlap with arXiv:1901.05873</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.04509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05968v2</id>
    <updated>2020-09-27T08:10:50Z</updated>
    <published>2020-02-14T11:06:44Z</published>
    <title>Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling</title>
    <summary>  Point cloud filtering is a fundamental problem in geometry modeling and
processing. Despite of significant advancement in recent years, the existing
methods still suffer from two issues: 1) they are either designed without
preserving sharp features or less robust in feature preservation; and 2) they
usually have many parameters and require tedious parameter tuning. In this
paper, we propose a novel deep learning approach that automatically and
robustly filters point clouds by removing noise and preserving their sharp
features. Our point-wise learning architecture consists of an encoder and a
decoder. The encoder directly takes points (a point and its neighbors) as
input, and learns a latent representation vector which goes through the decoder
to relate the ground-truth position with a displacement vector. The trained
neural network can automatically generate a set of clean points from a noisy
input. Extensive experiments show that our approach outperforms the
state-of-the-art deep learning techniques in terms of both visual quality and
quantitative error metrics. The source code and dataset can be found at
https://github.com/dongbo-BUAA-VR/Pointfilter.
</summary>
    <author>
      <name>Dongbo Zhang</name>
    </author>
    <author>
      <name>Xuequan Lu</name>
    </author>
    <author>
      <name>Hong Qin</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05968v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05968v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06827v3</id>
    <updated>2021-09-17T12:09:26Z</updated>
    <published>2020-02-17T08:25:26Z</published>
    <title>A Large-Scale Evaluation of Shape-Aware Neighborhood Weights and
  Neighborhood Sizes</title>
    <summary>  In this paper, we define and evaluate a weighting scheme for neighborhoods in
point sets. Our weighting takes the shape of the geometry, i.e., the normal
information, into account. This causes the obtained neighborhoods to be more
reliable in the sense that connectivity also depends on the orientation of the
point set. We utilize a sigmoid to define the weights based on the normal
variation. For an evaluation of the weighting scheme, we turn to a Shannon
entropy model for feature classification that can be proven to be
non-degenerate for our family of weights. Based on this model, we evaluate our
weighting terms on a large scale of both clean and real-world models. This
evaluation provides results regarding the choice of optimal parameters within
our weighting scheme. Furthermore, the large-scale evaluation also reveals that
neighborhood sizes should not be fixed globally when processing models.
Finally, we highlight the applicability of our weighting scheme withing the
application context of denoising.
</summary>
    <author>
      <name>Martin Skrodzki</name>
    </author>
    <author>
      <name>Eric Zimmermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cad.2021.103107</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cad.2021.103107" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer-Aided Design (2021): 103107</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.06827v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06827v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 68U07, 65D18, 65D17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07481v1</id>
    <updated>2020-02-18T10:40:02Z</updated>
    <published>2020-02-18T10:40:02Z</published>
    <title>Quantitative Evaluation of Time-Dependent Multidimensional Projection
  Techniques</title>
    <summary>  Dimensionality reduction methods are an essential tool for multidimensional
data analysis, and many interesting processes can be studied as time-dependent
multivariate datasets. There are, however, few studies and proposals that
leverage on the concise power of expression of projections in the context of
dynamic/temporal data. In this paper, we aim at providing an approach to assess
projection techniques for dynamic data and understand the relationship between
visual quality and stability. Our approach relies on an experimental setup that
consists of existing techniques designed for time-dependent data and new
variations of static methods. To support the evaluation of these techniques, we
provide a collection of datasets that has a wide variety of traits that encode
dynamic patterns, as well as a set of spatial and temporal stability metrics
that assess the quality of the layouts. We present an evaluation of 11 methods,
10 datasets, and 12 quality metrics, and elect the best-suited methods for
projecting time-dependent multivariate data, exploring the design choices and
characteristics of each method. All our results are documented and made
available in a public repository to allow reproducibility of results.
</summary>
    <author>
      <name>E. F. Vernier</name>
    </author>
    <author>
      <name>R. Garcia</name>
    </author>
    <author>
      <name>I. P. da Silva</name>
    </author>
    <author>
      <name>J. L. D. Comba</name>
    </author>
    <author>
      <name>A. C. Telea</name>
    </author>
    <link href="http://arxiv.org/abs/2002.07481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07995v2</id>
    <updated>2020-04-15T15:13:52Z</updated>
    <published>2020-02-19T03:59:56Z</published>
    <title>A Survey on Deep Geometry Learning: From a Representation Perspective</title>
    <summary>  Researchers have now achieved great success on dealing with 2D images using
deep learning. In recent years, 3D computer vision and Geometry Deep Learning
gain more and more attention. Many advanced techniques for 3D shapes have been
proposed for different applications. Unlike 2D images, which can be uniformly
represented by regular grids of pixels, 3D shapes have various representations,
such as depth and multi-view images, voxel-based representation, point-based
representation, mesh-based representation, implicit surface representation,
etc. However, the performance for different applications largely depends on the
representation used, and there is no unique representation that works well for
all applications. Therefore, in this survey, we review recent development in
deep learning for 3D geometry from a representation perspective, summarizing
the advantages and disadvantages of different representations in different
applications. We also present existing datasets in these representations and
further discuss future research directions.
</summary>
    <author>
      <name>Yun-Peng Xiao</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Fang-Lue Zhang</name>
    </author>
    <author>
      <name>Chunpeng Li</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/2002.07995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10880v1</id>
    <updated>2020-02-23T17:16:34Z</updated>
    <published>2020-02-23T17:16:34Z</published>
    <title>PolyGen: An Autoregressive Generative Model of 3D Meshes</title>
    <summary>  Polygon meshes are an efficient representation of 3D geometry, and are of
central importance in computer graphics, robotics and games development.
Existing learning-based approaches have avoided the challenges of working with
3D meshes, instead using alternative object representations that are more
compatible with neural architectures and training approaches. We present an
approach which models the mesh directly, predicting mesh vertices and faces
sequentially using a Transformer-based architecture. Our model can condition on
a range of inputs, including object classes, voxels, and images, and because
the model is probabilistic it can produce samples that capture uncertainty in
ambiguous scenarios. We show that the model is capable of producing
high-quality, usable meshes, and establish log-likelihood benchmarks for the
mesh-modelling task. We also evaluate the conditional models on surface
reconstruction metrics against alternative methods, and demonstrate competitive
performance despite not training directly on this task.
</summary>
    <author>
      <name>Charlie Nash</name>
    </author>
    <author>
      <name>Yaroslav Ganin</name>
    </author>
    <author>
      <name>S. M. Ali Eslami</name>
    </author>
    <author>
      <name>Peter W. Battaglia</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10945v1</id>
    <updated>2020-02-22T06:48:28Z</updated>
    <published>2020-02-22T06:48:28Z</published>
    <title>Image Stylization: From Predefined to Personalized</title>
    <summary>  We present a framework for interactive design of new image stylizations using
a wide range of predefined filter blocks. Both novel and off-the-shelf image
filtering and rendering techniques are extended and combined to allow the user
to unleash their creativity to intuitively invent, modify, and tune new styles
from a given set of filters. In parallel to this manual design, we propose a
novel procedural approach that automatically assembles sequences of filters,
leading to unique and novel styles. An important aim of our framework is to
allow for interactive exploration and design, as well as to enable videos and
camera streams to be stylized on the fly. In order to achieve this real-time
performance, we use the \textit{Best Linear Adaptive Enhancement} (BLADE)
framework -- an interpretable shallow machine learning method that simulates
complex filter blocks in real time. Our representative results include over a
dozen styles designed using our interactive tool, a set of styles created
procedurally, and new filters trained with our BLADE approach.
</summary>
    <author>
      <name>Ignacio Garcia-Dorado</name>
    </author>
    <author>
      <name>Pascal Getreuer</name>
    </author>
    <author>
      <name>Bartlomiej Wronski</name>
    </author>
    <author>
      <name>Peyman Milanfar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 22 figures. arXiv admin note: text overlap with
  arXiv:1712.06654</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00792v1</id>
    <updated>2020-03-02T12:09:16Z</updated>
    <published>2020-03-02T12:09:16Z</published>
    <title>Characterisation of rational and NURBS developable surfaces in Computer
  Aided Design</title>
    <summary>  In this paper we provide a characterisation of rational developable surfaces
in terms of the blossoms of the bounding curves and three rational functions
$\Lambda$, $M$, $\nu$. Properties of developable surfaces are revised in this
framework. In particular, a closed algebraic formula for the edge of regression
of the surface is obtained in terms of the functions $\Lambda$, $M$, $\nu$,
which are closely related to the ones that appear in the standard decomposition
of the derivative of the parametrisation of one of the bounding curves in terms
of the director vector of the rulings and its derivative. It is also shown that
all rational developable surfaces can be described as the set of developable
surfaces which can be constructed with a constant $\Lambda$, $M$, $\nu$ . The
results are readily extended to rational spline developable surfaces.
</summary>
    <author>
      <name>Leonardo Fernandez-Jambrina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4208/jcm.2003-m2019-0226</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4208/jcm.2003-m2019-0226" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Journal of Computational Mathematics. 18
  pages, 8 figures, jcmlatex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational Mathematics 39, 550-568 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.00792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D17, 68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01061v1</id>
    <updated>2020-02-27T13:35:07Z</updated>
    <published>2020-02-27T13:35:07Z</published>
    <title>A Feature-aware SPH for Isotropic Unstructured Mesh Generation</title>
    <summary>  In this paper, we present a feature-aware SPH method for the concurrent and
automated isotropic unstructured mesh generation. Two additional objectives are
achieved with the proposed method compared to the original SPH-based mesh
generator (Fu et al., 2019). First, a feature boundary correction term is
introduced to address the issue of incomplete kernel support at the boundary
vicinity. The mesh generation of feature curves, feature surfaces and volumes
can be handled concurrently without explicitly following a dimensional
sequence. Second, a two-phase model is proposed to characterize the
mesh-generation procedure by a feature-size-adaptation phase and a
mesh-quality-optimization phase. By proposing a new error measurement criterion
and an adaptive control system with two sets of simulation parameters, the
objectives of faster feature-size adaptation and local mesh-quality improvement
are merged into a consistent framework. The proposed method is validated with a
set of 2D and 3D numerical tests with different complexities and scales. The
results demonstrate that high-quality meshes are generated with a significant
speedup of convergence.
</summary>
    <author>
      <name>Zhe Ji</name>
    </author>
    <author>
      <name>Lin Fu</name>
    </author>
    <author>
      <name>Xiangyu Hu</name>
    </author>
    <author>
      <name>Nikolaus Adams</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cma.2020.113634</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cma.2020.113634" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages and 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.04187v2</id>
    <updated>2020-03-15T07:47:21Z</updated>
    <published>2020-03-09T15:04:25Z</published>
    <title>Style-compatible Object Recommendation for Multi-room Indoor Scene
  Synthesis</title>
    <summary>  Traditional indoor scene synthesis methods often take a two-step approach:
object selection and object arrangement. Current state-of-the-art object
selection approaches are based on convolutional neural networks (CNNs) and can
produce realistic scenes for a single room. However, they cannot be directly
extended to synthesize style-compatible scenes for multiple rooms with
different functions. To address this issue, we treat the object selection
problem as combinatorial optimization based on a Labeled LDA (L-LDA) model. We
first calculate occurrence probability distribution of object categories
according to a topic model, and then sample objects from each category
considering their function diversity along with style compatibility, while
regarding not only separate rooms, but also associations among rooms. User
study shows that our method outperforms the baselines by incorporating
multi-function and multi-room settings with style constraints, and sometimes
even produces plausible scenes comparable to those produced by professional
designers.
</summary>
    <author>
      <name>Yu He</name>
    </author>
    <author>
      <name>Yun Cai</name>
    </author>
    <author>
      <name>Yuan-Chen Guo</name>
    </author>
    <author>
      <name>Zheng-Ning Liu</name>
    </author>
    <author>
      <name>Shao-Kui Zhang</name>
    </author>
    <author>
      <name>Song-Hai Zhang</name>
    </author>
    <author>
      <name>Hong-Bo Fu</name>
    </author>
    <author>
      <name>Sheng-Yong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.04187v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04187v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05938v1</id>
    <updated>2020-03-12T05:46:41Z</updated>
    <published>2020-03-12T05:46:41Z</published>
    <title>Geodesic Distance Field-based Curved Layer Volume Decomposition for
  Multi-Axis Support-free Printing</title>
    <summary>  This paper presents a new curved layer volume decomposition method for
multi-axis support-free printing of freeform solid parts. Given a solid model
to be printed that is represented as a tetrahedral mesh, we first establish a
geodesic distance field embedded on the mesh, whose value at any vertex is the
geodesic distance to the base of the model. Next, the model is naturally
decomposed into curved layers by interpolating a number of iso-geodesic
distance surfaces (IGDSs). These IGDSs morph from bottom-up in an intrinsic and
smooth way owing to the nature of geodesics, which will be used as the curved
printing layers that are friendly to multi-axis printing. In addition, to cater
to the collision-free requirement and to improve the printing efficiency, we
also propose a printing sequence optimization algorithm for determining the
printing order of the IGDSs, which helps reduce the air-move path length. Ample
experiments in both computer simulation and physical printing are performed,
and the experimental results confirm the advantages of our method.
</summary>
    <author>
      <name>Yamin Li</name>
    </author>
    <author>
      <name>Dong He</name>
    </author>
    <author>
      <name>Xiangyu Wang</name>
    </author>
    <author>
      <name>Kai Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2003.05938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06233v4</id>
    <updated>2022-01-13T13:05:34Z</updated>
    <published>2020-03-13T12:32:24Z</published>
    <title>Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation</title>
    <summary>  Online semantic 3D segmentation in company with real-time RGB-D
reconstruction poses special challenges such as how to perform 3D convolution
directly over the progressively fused 3D geometric data, and how to smartly
fuse information from frame to frame. We propose a novel fusion-aware 3D point
convolution which operates directly on the geometric surface being
reconstructed and exploits effectively the inter-frame correlation for high
quality 3D feature learning. This is enabled by a dedicated dynamic data
structure which organizes the online acquired point cloud with global-local
trees. Globally, we compile the online reconstructed 3D points into an
incrementally growing coordinate interval tree, enabling fast point insertion
and neighborhood query. Locally, we maintain the neighborhood information for
each point using an octree whose construction benefits from the fast query of
the global tree.Both levels of trees update dynamically and help the 3D
convolution effectively exploits the temporal coherence for effective
information fusion across RGB-D frames.
</summary>
    <author>
      <name>Jiazhao Zhang</name>
    </author>
    <author>
      <name>Chenyang Zhu</name>
    </author>
    <author>
      <name>Lintao Zheng</name>
    </author>
    <author>
      <name>Kai Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06233v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06233v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08723v1</id>
    <updated>2020-03-12T12:38:52Z</updated>
    <published>2020-03-12T12:38:52Z</published>
    <title>Latent Space Subdivision: Stable and Controllable Time Predictions for
  Fluid Flow</title>
    <summary>  We propose an end-to-end trained neural networkarchitecture to robustly
predict the complex dynamics of fluid flows with high temporal stability. We
focus on single-phase smoke simulations in 2D and 3D based on the
incompressible Navier-Stokes (NS) equations, which are relevant for a wide
range of practical problems. To achieve stable predictions for long-term flow
sequences, a convolutional neural network (CNN) is trained for spatial
compression in combination with a temporal prediction network that consists of
stacked Long Short-Term Memory (LSTM) layers. Our core contribution is a novel
latent space subdivision (LSS) to separate the respective input quantities into
individual parts of the encoded latent space domain. This allows to
distinctively alter the encoded quantities without interfering with the
remaining latent space values and hence maximizes external control. By
selectively overwriting parts of the predicted latent space points, our
proposed method is capable to robustly predict long-term sequences of complex
physics problems. In addition, we highlight the benefits of a recurrent
training on the latent space creation, which is performed by the spatial
compression network.
</summary>
    <author>
      <name>Steffen Wiewel</name>
    </author>
    <author>
      <name>Byungsoo Kim</name>
    </author>
    <author>
      <name>Vinicius C. Azevedo</name>
    </author>
    <author>
      <name>Barbara Solenthaler</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://ge.in.tum.de/publications/latent-space-subdivision/</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09178v2</id>
    <updated>2021-01-18T08:47:48Z</updated>
    <published>2020-03-20T10:28:24Z</published>
    <title>Gaussian Curvature Filter on 3D Meshes</title>
    <summary>  Minimizing the Gaussian curvature of meshes can play a fundamental role in 3D
mesh processing. However, there is a lack of computationally efficient and
robust Gaussian curvature optimization method. In this paper, we present a
simple yet effective method that can efficiently reduce Gaussian curvature for
3D meshes. We first present the mathematical foundation of our method. Then, we
introduce a simple and robust implicit Gaussian curvature optimization method
named Gaussian Curvature Filter (GCF). GCF implicitly minimizes Gaussian
curvature without the need to explicitly calculate the Gaussian curvature
itself. GCF is highly efficient and this method can be used in a large range of
applications that involve Gaussian curvature. We conduct extensive experiments
to demonstrate that GCF significantly outperforms state-of-the-art methods in
minimizing Gaussian curvature, and geometric feature preserving soothing on 3D
meshes. GCF program is available at https://github.com/tangwenming/GCF-filter.
</summary>
    <author>
      <name>Wenming Tang</name>
    </author>
    <author>
      <name>Yuanhao Gong</name>
    </author>
    <author>
      <name>Kanglin Liu</name>
    </author>
    <author>
      <name>Jun Liu</name>
    </author>
    <author>
      <name>Wei Pan</name>
    </author>
    <author>
      <name>Bozhi Liu</name>
    </author>
    <author>
      <name>Guoping Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.09178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10435v1</id>
    <updated>2020-03-22T21:08:52Z</updated>
    <published>2020-03-22T21:08:52Z</published>
    <title>Pose to Seat: Automated Design of Body-Supporting Surfaces</title>
    <summary>  The design of functional seating furniture is a complicated process which
often requires extensive manual design effort and empirical evaluation. We
propose a computational design framework for pose-driven automated generation
of body-supports which are optimized for comfort of sitting. Given a human body
in a specified pose as input, our method computes an approximate pressure
distribution that also takes frictional forces and body torques into
consideration which serves as an objective measure of comfort. Utilizing this
information to find out where the body needs to be supported in order to
maintain comfort of sitting, our algorithm can create a supporting mesh suited
for a person in that specific pose. This is done in an automated fitting
process, using a template model capable of supporting a large variety of
sitting poses. The results can be used directly or can be considered as a
starting point for further interactive design.
</summary>
    <author>
      <name>Kurt Leimer</name>
    </author>
    <author>
      <name>Andreas Winkler</name>
    </author>
    <author>
      <name>Stefan Ohrhallinger</name>
    </author>
    <author>
      <name>Przemyslaw Musialski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cagd.2020.101855</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cagd.2020.101855" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 15 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Aided Geometric Design Volume 79, May 2020, 101855</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.10435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10558v4</id>
    <updated>2020-10-18T22:17:53Z</updated>
    <published>2020-03-23T21:45:26Z</published>
    <title>Perspective picture from Visual Sphere: a new approach to image
  rasterization</title>
    <summary>  In this paper alternative method for real-time 3D model rasterization is
given. Surfaces are drawn in perspective-map space which acts as a virtual
camera lens. It can render single-pass 360{\deg} angle of view (AOV) image of
unlimited shape, view-directions count and unrestrained projection geometry
(e.g. direct lens distortion, projection mapping, curvilinear perspective),
natively aliasing-free. In conjunction to perspective vector map, visual-sphere
perspective model is proposed. A model capable of combining pictures from
sources previously incompatible, like fish-eye camera and wide-angle lens
picture. More so, method is proposed for measurement and simulation of a real
optical system variable no-parallax point (NPP). This study also explores
philosophical and historical aspects of picture perception and presents a guide
for perspective design.
</summary>
    <author>
      <name>Jakub Maksymilian Fober</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">59 pages, 21 figures, 21 listings, 2 ancillary files, working paper
  published January 7th 2020 at http://maxfober.space/research.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.10558v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10558v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.3.5; I.3.7; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12227v1</id>
    <updated>2020-03-27T04:09:29Z</updated>
    <published>2020-03-27T04:09:29Z</published>
    <title>A Hybrid Lagrangian/Eulerian Collocated Advection and Projection Method
  for Fluid Simulation</title>
    <summary>  We present a hybrid particle/grid approach for simulating incompressible
fluids on collocated velocity grids. We interchangeably use particle and grid
representations of transported quantities to balance efficiency and accuracy. A
novel Backward Semi-Lagrangian method is derived to improve accuracy of grid
based advection. Our approach utilizes the implicit formula associated with
solutions of Burgers' equation. We solve this equation using Newton's method
enabled by $C^1$ continuous grid interpolation. We enforce incompressibility
over collocated, rather than staggered grids. Our projection technique is
variational and designed for B-spline interpolation over regular grids where
multiquadratic interpolation is used for velocity and multilinear interpolation
for pressure. Despite our use of regular grids, we extend the variational
technique to allow for cut-cell definition of irregular flow domains for both
Dirichlet and free surface boundary conditions.
</summary>
    <author>
      <name>Steven W. Gagniere</name>
    </author>
    <author>
      <name>David A. B. Hyde</name>
    </author>
    <author>
      <name>Alan Marquez-Razon</name>
    </author>
    <author>
      <name>Chenfanfu Jiang</name>
    </author>
    <author>
      <name>Ziheng Ge</name>
    </author>
    <author>
      <name>Xuchen Han</name>
    </author>
    <author>
      <name>Qi Guo</name>
    </author>
    <author>
      <name>Joseph Teran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14096</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14096" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 18 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum, 39(8): 1-14 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.12227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13510v4</id>
    <updated>2023-05-08T14:47:44Z</updated>
    <published>2020-03-30T14:33:30Z</published>
    <title>Human Motion Transfer with 3D Constraints and Detail Enhancement</title>
    <summary>  We propose a new method for realistic human motion transfer using a
generative adversarial network (GAN), which generates a motion video of a
target character imitating actions of a source character, while maintaining
high authenticity of the generated results. We tackle the problem by decoupling
and recombining the posture information and appearance information of both the
source and target characters. The innovation of our approach lies in the use of
the projection of a reconstructed 3D human model as the condition of GAN to
better maintain the structural integrity of transfer results in different
poses. We further introduce a detail enhancement net to enhance the details of
transfer results by exploiting the details in real source frames. Extensive
experiments show that our approach yields better results both qualitatively and
quantitatively than the state-of-the-art methods.
</summary>
    <author>
      <name>Yang-Tian Sun</name>
    </author>
    <author>
      <name>Qian-Cheng Fu</name>
    </author>
    <author>
      <name>Yue-Ren Jiang</name>
    </author>
    <author>
      <name>Zitao Liu</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2022.3201904</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2022.3201904" rel="related"/>
    <link href="http://arxiv.org/abs/2003.13510v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13510v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03179v1</id>
    <updated>2020-04-07T08:01:47Z</updated>
    <published>2020-04-07T08:01:47Z</published>
    <title>Iconify: Converting Photographs into Icons</title>
    <summary>  In this paper, we tackle a challenging domain conversion task between photo
and icon images. Although icons often originate from real object images (i.e.,
photographs), severe abstractions and simplifications are applied to generate
icon images by professional graphic designers. Moreover, there is no one-to-one
correspondence between the two domains, for this reason we cannot use it as the
ground-truth for learning a direct conversion function. Since generative
adversarial networks (GAN) can undertake the problem of domain conversion
without any correspondence, we test CycleGAN and UNIT to generate icons from
objects segmented from photo images. Our experiments with several image
datasets prove that CycleGAN learns sufficient abstraction and simplification
ability to generate icon-like images.
</summary>
    <author>
      <name>Takuro Karamatsu</name>
    </author>
    <author>
      <name>Gibran Benitez-Garcia</name>
    </author>
    <author>
      <name>Keiji Yanai</name>
    </author>
    <author>
      <name>Seiichi Uchida</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3379173.3393708</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3379173.3393708" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at 2020 Joint Workshop on Multimedia Artworks Analysis and
  Attractiveness Computing in Multimedia (MMArt-ACM'20)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05980v1</id>
    <updated>2020-04-06T20:46:37Z</updated>
    <published>2020-04-06T20:46:37Z</published>
    <title>NiLBS: Neural Inverse Linear Blend Skinning</title>
    <summary>  In this technical report, we investigate efficient representations of
articulated objects (e.g. human bodies), which is an important problem in
computer vision and graphics. To deform articulated geometry, existing
approaches represent objects as meshes and deform them using "skinning"
techniques. The skinning operation allows a wide range of deformations to be
achieved with a small number of control parameters. This paper introduces a
method to invert the deformations undergone via traditional skinning techniques
via a neural network parameterized by pose. The ability to invert these
deformations allows values (e.g., distance function, signed distance function,
occupancy) to be pre-computed at rest pose, and then efficiently queried when
the character is deformed. We leave empirical evaluation of our approach to
future work.
</summary>
    <author>
      <name>Timothy Jeruzalski</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>Paul Lalonde</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <author>
      <name>Andrea Tagliasacchi</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.07484v3</id>
    <updated>2020-12-22T06:30:56Z</updated>
    <published>2020-04-16T06:57:26Z</published>
    <title>Pulsar: Efficient Sphere-based Neural Rendering</title>
    <summary>  We propose Pulsar, an efficient sphere-based differentiable renderer that is
orders of magnitude faster than competing techniques, modular, and easy-to-use
due to its tight integration with PyTorch. Differentiable rendering is the
foundation for modern neural rendering approaches, since it enables end-to-end
training of 3D scene representations from image observations. However,
gradient-based optimization of neural mesh, voxel, or function representations
suffers from multiple challenges, i.e., topological inconsistencies, high
memory footprints, or slow rendering speeds. To alleviate these problems,
Pulsar employs: 1) a sphere-based scene representation, 2) an efficient
differentiable rendering engine, and 3) neural shading. Pulsar executes orders
of magnitude faster than existing techniques and allows real-time rendering and
optimization of representations with millions of spheres. Using spheres for the
scene representation, unprecedented speed is obtained while avoiding topology
problems. Pulsar is fully differentiable and thus enables a plethora of
applications, ranging from 3D reconstruction to general neural rendering.
</summary>
    <author>
      <name>Christoph Lassner</name>
    </author>
    <author>
      <name>Michael Zollhöfer</name>
    </author>
    <link href="http://arxiv.org/abs/2004.07484v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.07484v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09038v2</id>
    <updated>2020-04-24T04:19:05Z</updated>
    <published>2020-04-20T03:22:23Z</published>
    <title>Developable B-spline surface generation from control rulings</title>
    <summary>  An intuitive design method is proposed for generating developable ruled
B-spline surfaces from a sequence of straight line segments indicating the
surface shape. The first and last line segments are enforced to be the head and
tail ruling lines of the resulting surface while the interior lines are
required to approximate rulings on the resulting surface as much as possible.
This manner of developable surface design is conceptually similar to the
popular way of the freeform curve and surface design in the CAD community,
observing that a developable ruled surface is a single parameter family of
straight lines. This new design mode of the developable surface also provides
more flexibility than the widely employed way of developable surface design
from two boundary curves of the surface. The problem is treated by numerical
optimization methods with which a particular level of distance error is
allowed. We thus provide an effective tool for creating surfaces with a high
degree of developability when the input control rulings do not lie in exact
developable surfaces. We consider this ability as the superiority over
analytical methods in that it can deal with arbitrary design inputs and find
practically useful results.
</summary>
    <author>
      <name>Zixuan Hu</name>
    </author>
    <author>
      <name>Pengbo Bo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11390-022-0680-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11390-022-0680-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 12 figrues</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer Science and Technology, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.09038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09136v1</id>
    <updated>2020-04-20T08:58:50Z</updated>
    <published>2020-04-20T08:58:50Z</published>
    <title>Robust and efficient tool path generation for poor-quality triangular
  mesh surface machining</title>
    <summary>  This paper presents a new method to generate iso-scallop tool paths for
triangular mesh surfaces. With the popularity of 3D scanning techniques,
scanning-derived mesh surfaces have seen a significant increase in their
application to machining. Quite often, such mesh surfaces exhibit defects such
as noises, which differentiate them from the good-quality mesh surfaces
previous research work focuses on. To generate tool paths for such poor-quality
mesh surfaces, the primary challenge lies in robustness against the defects. In
this work, a robust tool path generation method is proposed for poor-quality
mesh surfaces. In addition to robustness, the method is quite efficient,
providing the benefit of faster iterations and improved integration between
scanning and machining. The fundamental principle of the method is to convert
the tool path generation problem to the heat diffusion problem that has robust
and efficient algorithms available. The effectiveness of the method will be
demonstrated by a series of case studies and comparisons.
</summary>
    <author>
      <name>Qiang Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures, Under journal review</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.09136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10354v1</id>
    <updated>2020-04-22T00:59:07Z</updated>
    <published>2020-04-22T00:59:07Z</published>
    <title>A scriptable, generative modelling system for dynamic 3D meshes</title>
    <summary>  We describe a flexible, script-based system for the procedural generation and
animation of 3D geometry. Dynamic triangular meshes are generated through the
real-time execution of scripts written in the Lua programming language. Tight
integration between the programming environment, runtime engine and graphics
visualisation enables a workflow between coding and visual results that
encourages experimentation and rapid prototyping. The system has been used
successfully to generate a variety of complex, dynamic organic forms including
complex branching structures, scalable symmetric manifolds and abstract organic
forms. We use examples in each of these areas to detail the main features of
the system, which include a set of flexible 3D mesh operations integrated with
a Lua-based L-system interpreter that creates geometry using generalised
cylinders.
</summary>
    <author>
      <name>Jon McCormack</name>
    </author>
    <author>
      <name>Ben Porter</name>
    </author>
    <author>
      <name>James Wetter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.7; I.6.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11897v1</id>
    <updated>2020-04-23T18:49:55Z</updated>
    <published>2020-04-23T18:49:55Z</published>
    <title>Tales from the Trenches: Developing sciview, a new 3D viewer for the
  ImageJ community</title>
    <summary>  ImageJ/Fiji is a widely-used tool in the biomedical community for performing
everyday image analysis tasks. However, its 3D viewer component (aptly named 3D
Viewer) has become dated and is no longer actively maintained. We set out to
create an alternative tool that not only brings modern concepts and APIs from
computer graphics to ImageJ, but is designed to be robust to long-term,
open-source development. To achieve this we divided the visualization logic
into two parts: the rendering framework, scenery, and the user-facing
application, sciview. In this paper we describe the development process and
design decisions made, putting an emphasis on sustainable development,
community building, and software engineering best practises. We highlight the
motivation for the Java Virtual Machine (JVM) as a target platform for
visualisation applications. We conclude by discussing the remaining milestones
and strategy for long-term sustainability.
</summary>
    <author>
      <name>Ulrik Günther</name>
    </author>
    <author>
      <name>Kyle I. S. Harrington</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures. As submitted to VisGap workshop at Eurovis 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.11897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12069v1</id>
    <updated>2020-04-25T06:59:10Z</updated>
    <published>2020-04-25T06:59:10Z</published>
    <title>Deep Photon Mapping</title>
    <summary>  Recently, deep learning-based denoising approaches have led to dramatic
improvements in low sample-count Monte Carlo rendering. These approaches are
aimed at path tracing, which is not ideal for simulating challenging light
transport effects like caustics, where photon mapping is the method of choice.
However, photon mapping requires very large numbers of traced photons to
achieve high-quality reconstructions. In this paper, we develop the first deep
learning-based method for particle-based rendering, and specifically focus on
photon density estimation, the core of all particle-based methods. We train a
novel deep neural network to predict a kernel function to aggregate photon
contributions at shading points. Our network encodes individual photons into
per-photon features, aggregates them in the neighborhood of a shading point to
construct a photon local context vector, and infers a kernel function from the
per-photon and photon local context features. This network is easy to
incorporate in many previous photon mapping methods (by simply swapping the
kernel density estimator) and can produce high-quality reconstructions of
complex global illumination effects like caustics with an order of magnitude
fewer photons compared to previous photon mapping methods.
</summary>
    <author>
      <name>Shilin Zhu</name>
    </author>
    <author>
      <name>Zexiang Xu</name>
    </author>
    <author>
      <name>Henrik Wann Jensen</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Ravi Ramamoorthi</name>
    </author>
    <link href="http://arxiv.org/abs/2004.12069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.13896v1</id>
    <updated>2020-04-29T00:08:13Z</updated>
    <published>2020-04-29T00:08:13Z</published>
    <title>Organic Narrative Charts</title>
    <summary>  Storyline visualizations display the interactions of groups and entities and
their development over time. Existing approaches have successfully adopted the
general layout from hand-drawn illustrations to automatically create similar
depictions. Ward Shelley is the author of several diagrammatic paintings that
show the timeline of art-related subjects, such as Downtown Body, a history of
art scenes. His drawings include many stylistic elements that are not covered
by existing storyline visualizations, like links between entities, splits and
merges of streams, and tags or labels to describe the individual elements. We
present a visualization method that provides a visual mapping for the complex
relationships in the data, creates a layout for their display, and adopts a
similar styling of elements to imitate the artistic appeal of such
illustrations. We compare our results to the original drawings and provide an
open-source authoring tool prototype.
</summary>
    <author>
      <name>Fabian Bolte</name>
    </author>
    <author>
      <name>Stefan Bruckner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a short paper at the EuroGraphics2020 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.13896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.14489v1</id>
    <updated>2020-04-29T21:33:28Z</updated>
    <published>2020-04-29T21:33:28Z</published>
    <title>Interactive Video Stylization Using Few-Shot Patch-Based Training</title>
    <summary>  In this paper, we present a learning-based method to the keyframe-based video
stylization that allows an artist to propagate the style from a few selected
keyframes to the rest of the sequence. Its key advantage is that the resulting
stylization is semantically meaningful, i.e., specific parts of moving objects
are stylized according to the artist's intention. In contrast to previous style
transfer techniques, our approach does not require any lengthy pre-training
process nor a large training dataset. We demonstrate how to train an appearance
translation network from scratch using only a few stylized exemplars while
implicitly preserving temporal consistency. This leads to a video stylization
framework that supports real-time inference, parallel processing, and random
access to an arbitrary output frame. It can also merge the content from
multiple keyframes without the need to perform an explicit blending operation.
We demonstrate its practical utility in various interactive scenarios, where
the user paints over a selected keyframe and sees her style transferred to an
existing recorded sequence or a live video stream.
</summary>
    <author>
      <name>Ondřej Texler</name>
    </author>
    <author>
      <name>David Futschik</name>
    </author>
    <author>
      <name>Michal Kučera</name>
    </author>
    <author>
      <name>Ondřej Jamriška</name>
    </author>
    <author>
      <name>Šárka Sochorová</name>
    </author>
    <author>
      <name>Menglei Chai</name>
    </author>
    <author>
      <name>Sergey Tulyakov</name>
    </author>
    <author>
      <name>Daniel Sýkora</name>
    </author>
    <link href="http://arxiv.org/abs/2004.14489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.14489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00074v2</id>
    <updated>2021-02-10T00:25:05Z</updated>
    <published>2020-04-30T19:48:57Z</published>
    <title>Levitating Rigid Objects with Hidden Rods and Wires</title>
    <summary>  We propose a novel algorithm to efficiently generate hidden structures to
support arrangements of floating rigid objects. Our optimization finds a small
set of rods and wires between objects and each other or a supporting surface
(e.g., wall or ceiling) that hold all objects in force and torque equilibrium.
Our objective function includes a sparsity inducing total volume term and a
linear visibility term based on efficiently pre-computed Monte-Carlo
integration, to encourage solutions that are as-hidden-as-possible. The
resulting optimization is convex and the global optimum can be efficiently
recovered via a linear program. Our representation allows for a
user-controllable mixture of tension-, compression-, and shear-resistant rods
or tension-only wires. We explore applications to theatre set design, museum
exhibit curation, and other artistic endeavours.
</summary>
    <author>
      <name>Sarah Kushner</name>
    </author>
    <author>
      <name>Risa Ulinski</name>
    </author>
    <author>
      <name>Karan Singh</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/2005.00074v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00074v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00559v2</id>
    <updated>2020-07-05T19:38:56Z</updated>
    <published>2020-05-01T18:12:44Z</published>
    <title>RigNet: Neural Rigging for Articulated Characters</title>
    <summary>  We present RigNet, an end-to-end automated method for producing animation
rigs from input character models. Given an input 3D model representing an
articulated character, RigNet predicts a skeleton that matches the animator
expectations in joint placement and topology. It also estimates surface skin
weights based on the predicted skeleton. Our method is based on a deep
architecture that directly operates on the mesh representation without making
assumptions on shape class and structure. The architecture is trained on a
large and diverse collection of rigged models, including their mesh, skeletons
and corresponding skin weights. Our evaluation is three-fold: we show better
results than prior art when quantitatively compared to animator rigs;
qualitatively we show that our rigs can be expressively posed and animated at
multiple levels of detail; and finally, we evaluate the impact of various
algorithm choices on our output rigs.
</summary>
    <author>
      <name>Zhan Xu</name>
    </author>
    <author>
      <name>Yang Zhou</name>
    </author>
    <author>
      <name>Evangelos Kalogerakis</name>
    </author>
    <author>
      <name>Chris Landreth</name>
    </author>
    <author>
      <name>Karan Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH 2020. Project page https://zhan-xu.github.io/rig-net/</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.00559v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00559v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00803v1</id>
    <updated>2020-05-02T11:53:05Z</updated>
    <published>2020-05-02T11:53:05Z</published>
    <title>Lagrangian Neural Style Transfer for Fluids</title>
    <summary>  Artistically controlling the shape, motion and appearance of fluid
simulations pose major challenges in visual effects production. In this paper,
we present a neural style transfer approach from images to 3D fluids formulated
in a Lagrangian viewpoint. Using particles for style transfer has unique
benefits compared to grid-based techniques. Attributes are stored on the
particles and hence are trivially transported by the particle motion. This
intrinsically ensures temporal consistency of the optimized stylized structure
and notably improves the resulting quality. Simultaneously, the expensive,
recursive alignment of stylization velocity fields of grid approaches is
unnecessary, reducing the computation time to less than an hour and rendering
neural flow stylization practical in production settings. Moreover, the
Lagrangian representation improves artistic control as it allows for
multi-fluid stylization and consistent color transfer from images, and the
generality of the method enables stylization of smoke and liquids likewise.
</summary>
    <author>
      <name>Byungsoo Kim</name>
    </author>
    <author>
      <name>Vinicius C. Azevedo</name>
    </author>
    <author>
      <name>Markus Gross</name>
    </author>
    <author>
      <name>Barbara Solenthaler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386569.3392473</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386569.3392473" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transaction on Graphics (SIGGRAPH 2020), additional materials:
  http://www.byungsoo.me/project/lnst/index.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 39, 4, Article 1 (July 2020), 10 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.00803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.01003v2</id>
    <updated>2020-11-04T05:58:57Z</updated>
    <published>2020-05-03T06:44:27Z</published>
    <title>Variational Shape Approximation of Point Set Surfaces</title>
    <summary>  In this work, we present a translation of the complete pipeline for
variational shape approximation (VSA) to the setting of point sets. First, we
describe an explicit example for the theoretically known non-convergence of the
currently available VSA approaches. The example motivates us to introduce an
alternate version of VSA based on a switch operation for which we prove
convergence. Second, we discuss how two operations - split and merge - can be
included in a fully automatic pipeline that is in turn independent of the
placement and number of initial seeds. Third and finally, we present two
approaches how to obtain a simplified mesh from the output of the VSA
procedure. This simplification is either based on simple plane intersection or
based on a variational optimization problem. Several qualitative and
quantitative results prove the relevance of our approach.
</summary>
    <author>
      <name>Martin Skrodzki</name>
    </author>
    <author>
      <name>Eric Zimmermann</name>
    </author>
    <author>
      <name>Konrad Polthier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cagd.2020.101875</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cagd.2020.101875" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected two formulae in the "merge" process, fixed dated that the
  preprint was submitted</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Aided Geometric Design Volume 80, June 2020, 101875</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.01003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 68U07, 65D18" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04107v1</id>
    <updated>2020-05-08T15:24:35Z</updated>
    <published>2020-05-08T15:24:35Z</published>
    <title>Sequential Gallery for Interactive Visual Design Optimization</title>
    <summary>  Visual design tasks often involve tuning many design parameters. For example,
color grading of a photograph involves many parameters, some of which
non-expert users might be unfamiliar with. We propose a novel user-in-the-loop
optimization method that allows users to efficiently find an appropriate
parameter set by exploring such a high-dimensional design space through much
easier two-dimensional search subtasks. This method, called sequential plane
search, is based on Bayesian optimization to keep necessary queries to users as
few as possible. To help users respond to plane-search queries, we also propose
using a gallery-based interface that provides options in the two-dimensional
subspace arranged in an adaptive grid view. We call this interactive framework
Sequential Gallery since users sequentially select the best option from the
options provided by the interface. Our experiment with synthetic functions
shows that our sequential plane search can find satisfactory solutions in fewer
iterations than baselines. We also conducted a preliminary user study, results
of which suggest that novices can effectively complete search tasks with
Sequential Gallery in a photo-enhancement scenario.
</summary>
    <author>
      <name>Yuki Koyama</name>
    </author>
    <author>
      <name>Issei Sato</name>
    </author>
    <author>
      <name>Masataka Goto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386569.3392444</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386569.3392444" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published at ACM Trans. Graph. (Proc. SIGGRAPH 2020); Project
  page available at https://koyama.xyz/project/sequential_gallery/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 39, 4 (July 2020), pp.88:1-88:12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.04107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04323v2</id>
    <updated>2020-08-30T00:45:00Z</updated>
    <published>2020-05-09T00:16:38Z</published>
    <title>ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills</title>
    <summary>  Humans are highly adept at walking in environments with foot placement
constraints, including stepping-stone scenarios where the footstep locations
are fully constrained. Finding good solutions to stepping-stone locomotion is a
longstanding and fundamental challenge for animation and robotics. We present
fully learned solutions to this difficult problem using reinforcement learning.
We demonstrate the importance of a curriculum for efficient learning and
evaluate four possible curriculum choices compared to a non-curriculum
baseline. Results are presented for a simulated human character, a realistic
bipedal robot simulation and a monster character, in each case producing
robust, plausible motions for challenging stepping stone sequences and
terrains.
</summary>
    <author>
      <name>Zhaoming Xie</name>
    </author>
    <author>
      <name>Hung Yu Ling</name>
    </author>
    <author>
      <name>Nam Hee Kim</name>
    </author>
    <author>
      <name>Michiel van de Panne</name>
    </author>
    <link href="http://arxiv.org/abs/2005.04323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05406v1</id>
    <updated>2020-04-20T00:57:21Z</updated>
    <published>2020-04-20T00:57:21Z</published>
    <title>SpectralWeight: a spectral graph wavelet framework for weight prediction
  of pork cuts</title>
    <summary>  In this paper, we propose a novel approach for the quality assessment of pork
carcasses using 3D shape analysis. First, we make a 3D model of a pork
half-carcass using a 3D scanner and then we take advantage of spectral graph
wavelet signature (SGWS) to build a local spectral descriptor. Next, we
aggregate the extracted features using the bag-of-geometric-words paradigm to
globally represent the half-carcass shape. We then employ partial least-squares
regression to predict the weight of pork cuts for the quality assessment of
carcasses. Our results demonstrate that SpectralWeight can predict the weight
of different pork cuts and tissues with high accuracy. Although in this study
we evaluate the performance of SGWS for the weight prediction of pork
dissection, our framework is fairly general and enables new ways to estimate
the quality and economical value of carcasses of different animals.
</summary>
    <author>
      <name>Majid Masoumi</name>
    </author>
    <author>
      <name>Marcel Marcoux</name>
    </author>
    <author>
      <name>Laurence Maignel</name>
    </author>
    <author>
      <name>Candido Pomar</name>
    </author>
    <link href="http://arxiv.org/abs/2005.05406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06671v1</id>
    <updated>2020-05-14T00:06:00Z</updated>
    <published>2020-05-14T00:06:00Z</published>
    <title>Optimally Fast Soft Shadows on Curved Terrain with Dynamic Programming
  and Maximum Mipmaps</title>
    <summary>  We present a simple, novel method of efficiently rendering ray cast soft
shadows on curved terrain by using dynamic programming and maximum mipmaps to
rapidly find a global minimum shadow cost in constant runtime complexity.
Additionally, we apply a new method of reducing view ray computation times that
pre-displaces the terrain mesh to bootstrap ray starting positions. Combining
these two methods, our ray casting engine runs in real-time with more than 200%
speed up over uniform ray stepping with comparable image quality and without
hardware ray tracing acceleration. To add support for accurate planetary
ephemerides and interactive features, we integrated the engine into
celestia.Sci, a general space simulation software. We demonstrate the ability
of our engine to accurately handle a large range of distance scales by using it
to generate videos of lunar landing trajectories. The numerical error when
compared with real lunar mission imagery is small, demonstrating the accuracy
and efficiency of our approach.
</summary>
    <author>
      <name>Dawoon Jung</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Korea Aerospace Research Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Fridger Schrempp</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Deutsches Elektronen-Synchrotron</arxiv:affiliation>
    </author>
    <author>
      <name>Seunghee Son</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Korea Aerospace Research Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06998v1</id>
    <updated>2020-05-14T14:20:42Z</updated>
    <published>2020-05-14T14:20:42Z</published>
    <title>Plane-Activated Mapped Microstructure</title>
    <summary>  Querying and interacting with models of massive material micro-structure
requires localized on-demand generation of the micro-structure since the
full-scale storing and retrieving is cost prohibitive. When the micro-structure
is efficiently represented as the image of a canonical structure under a
non-linear space deformation to allow it to conform to curved shape, the
additional challenge is to relate the query of the mapped micro-structure back
to its canonical structure. This paper presents an efficient algorithm to pull
back a mapped micro-structure to a partition of the canonical domain structure
into boxes and only activates boxes whose image is likely intersected by a
plane. The active boxes are organized into a forest whose trees are traversed
depth first to generate mapped micro-structure only of the active boxes. The
traversal supports, for example, 3D print slice generation in additive
manufacturing.
</summary>
    <author>
      <name>Jeremy Youngquist</name>
    </author>
    <author>
      <name>Jörg Peters</name>
    </author>
    <author>
      <name>Meera Sitharam</name>
    </author>
    <link href="http://arxiv.org/abs/2005.06998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07547v2</id>
    <updated>2020-05-23T08:24:14Z</updated>
    <published>2020-05-15T13:52:41Z</published>
    <title>Online path sampling control with progressive spatio-temporal filtering</title>
    <summary>  This work introduces progressive spatio-temporal filtering, an efficient
method to build all-frequency approximations to the light transport
distribution into a scene by filtering individual samples produced by an
underlying path sampler, using online, iterative algorithms and data-structures
that exploit both the spatial and temporal coherence of the approximated light
field. Unlike previous approaches, the proposed method is both more efficient,
due to its use of an iterative temporal feedback loop that massively improves
convergence to a noise-free approximant, and more flexible, due to its
introduction of a spatio-directional hashing representation that allows to
encode directional variations like those due to glossy reflections. We then
introduce four different methods to employ the resulting approximations to
control the underlying path sampler and/or modify its associated estimator,
greatly reducing its variance and enhancing its robustness to complex lighting
scenarios. The core algorithms are highly scalable and low-overhead, requiring
only minor modifications to an existing path tracer.
</summary>
    <author>
      <name>Jacopo Pantaleoni</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07547v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07547v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11269v2</id>
    <updated>2023-05-13T10:28:00Z</updated>
    <published>2020-05-22T16:46:27Z</published>
    <title>Software Implementation of Optimized Bicubic Interpolated Scan
  Conversion in Echocardiography</title>
    <summary>  This paper introduces a novel approach leveraging objective image quality
assessment (IQA) metrics to optimize the outcomes of traditional bicubic (BIC)
image interpolation and interpolated scan conversion algorithms. Specifically,
feature selection through line chart data visualization and computing the IQA
metrics scores are used to estimate the IQA-guided coefficient-k that up-dates
the traditional BIC algorithm weighting function. The resulting optimized
bicubic (OBIC) algorithm was subjectively and objectively evaluated using
natural and ultrasound images. Results showed that the overall performance of
the OBIC algorithm was equivalent to 92.22% of 180 occurrences when compared to
the BIC algorithm, while it was 57.22% of 180 occurrences when compared to
other algorithms. On top of that, the OBIC interpolated scan conversion
algorithm generally produced crisper and better contrast cropped ultrasound
sectored images than the BIC algorithm, as well as other interpolated scan
conversion algorithms mentioned.
</summary>
    <author>
      <name>Olivier Rukundo</name>
    </author>
    <author>
      <name>Samuel E. Schmidt</name>
    </author>
    <author>
      <name>Olaf T von Ramm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.11269v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11269v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11799v2</id>
    <updated>2020-10-10T07:30:59Z</updated>
    <published>2020-05-24T16:43:24Z</published>
    <title>Haptic Rendering of Thin, Deformable Objects with Spatially Varying
  Stiffness</title>
    <summary>  In the real world, we often come across soft objects having spatially varying
stiffness, such as human palm or a wart on the skin. In this paper, we propose
a novel approach to render thin, deformable objects having spatially varying
stiffness (inhomogeneous material). We use the classical Kirchhoff thin plate
theory to compute the deformation. In general, the physics-based rendering of
an arbitrary 3D surface is complex and time-consuming. Therefore, we
approximate the 3D surface locally by a 2D plane using an area-preserving
mapping technique - Gall-Peters mapping. Once the deformation is computed by
solving a fourth-order partial differential equation, we project the points
back onto the original object for proper haptic rendering. The method was
validated through user experiments and was found to be realistic.
</summary>
    <author>
      <name>Priyadarshini Kumari</name>
    </author>
    <author>
      <name>Subhasis Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Eurohaptics 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.11799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12518v1</id>
    <updated>2020-05-26T05:23:32Z</updated>
    <published>2020-05-26T05:23:32Z</published>
    <title>Survey: Machine Learning in Production Rendering</title>
    <summary>  In the past few years, machine learning-based approaches have had some great
success for rendering animated feature films. This survey summarizes several of
the most dramatic improvements in using deep neural networks over traditional
rendering methods, such as better image quality and lower computational
overhead. More specifically, this survey covers the fundamental principles of
machine learning and its applications, such as denoising, path guiding,
rendering participating media, and other notoriously difficult light transport
situations. Some of these techniques have already been used in the latest
released animations while others are still in the continuing development by
researchers in both academia and movie studios. Although learning-based
rendering methods still have some open issues, they have already demonstrated
promising performance in multiple parts of the rendering pipeline, and people
are continuously making new attempts.
</summary>
    <author>
      <name>Shilin Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This was the survey I did for my PhD research exam</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12662v1</id>
    <updated>2020-05-26T12:36:19Z</updated>
    <published>2020-05-26T12:36:19Z</published>
    <title>A Deep Learning based Fast Signed Distance Map Generation</title>
    <summary>  Signed distance map (SDM) is a common representation of surfaces in medical
image analysis and machine learning. The computational complexity of SDM for 3D
parametric shapes is often a bottleneck in many applications, thus limiting
their interest. In this paper, we propose a learning based SDM generation
neural network which is demonstrated on a tridimensional cochlea shape model
parameterized by 4 shape parameters. The proposed SDM Neural Network generates
a cochlea signed distance map depending on four input parameters and we show
that the deep learning approach leads to a 60 fold improvement in the time of
computation compared to more classical SDM generation methods. Therefore, the
proposed approach achieves a good trade-off between accuracy and efficiency.
</summary>
    <author>
      <name>Zihao Wang</name>
    </author>
    <author>
      <name>Clair Vandersteen</name>
    </author>
    <author>
      <name>Thomas Demarcy</name>
    </author>
    <author>
      <name>Dan Gnansia</name>
    </author>
    <author>
      <name>Charles Raffaelli</name>
    </author>
    <author>
      <name>Nicolas Guevara</name>
    </author>
    <author>
      <name>Hervé Delingette</name>
    </author>
    <link href="http://arxiv.org/abs/2005.12662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14370v1</id>
    <updated>2020-05-29T02:45:46Z</updated>
    <published>2020-05-29T02:45:46Z</published>
    <title>Constructing Human Motion Manifold with Sequential Networks</title>
    <summary>  This paper presents a novel recurrent neural network-based method to
construct a latent motion manifold that can represent a wide range of human
motions in a long sequence. We introduce several new components to increase the
spatial and temporal coverage in motion space while retaining the details of
motion capture data. These include new regularization terms for the motion
manifold, combination of two complementary decoders for predicting joint
rotations and joint velocities, and the addition of the forward kinematics
layer to consider both joint rotation and position errors. In addition, we
propose a set of loss terms that improve the overall quality of the motion
manifold from various aspects, such as the capability of reconstructing not
only the motion but also the latent manifold vector, and the naturalness of the
motion through adversarial loss. These components contribute to creating
compact and versatile motion manifold that allows for creating new motions by
performing random sampling and algebraic operations, such as interpolation and
analogy, in the latent motion manifold.
</summary>
    <author>
      <name>Deok-Kyeong Jang</name>
    </author>
    <author>
      <name>Sung-Hee Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14028" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, It will be published at Computer Graphics Forum</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.14370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05 (Primary), 68T07 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.01746v2</id>
    <updated>2020-08-18T03:41:48Z</updated>
    <published>2020-06-02T16:18:55Z</published>
    <title>Accurate Face Rig Approximation with Deep Differential Subspace
  Reconstruction</title>
    <summary>  To be suitable for film-quality animation, rigs for character deformation
must fulfill a broad set of requirements. They must be able to create highly
stylized deformation, allow a wide variety of controls to permit artistic
freedom, and accurately reflect the design intent. Facial deformation is
especially challenging due to its nonlinearity with respect to the animation
controls and its additional precision requirements, which often leads to highly
complex face rigs that are not generalizable to other characters. This lack of
generality creates a need for approximation methods that encode the deformation
in simpler structures. We propose a rig approximation method that addresses
these issues by learning localized shape information in differential
coordinates and, separately, a subspace for mesh reconstruction. The use of
differential coordinates produces a smooth distribution of errors in the
resulting deformed surface, while the learned subspace provides constraints
that reduce the low frequency error in the reconstruction. Our method can
reconstruct both face and body deformations with high fidelity and does not
require a set of well-posed animation examples, as we demonstrate with a
variety of production characters.
</summary>
    <author>
      <name>Steven L. Song</name>
    </author>
    <author>
      <name>Weiqi Shi</name>
    </author>
    <author>
      <name>Michael Reed</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386569.3392491</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386569.3392491" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, ACM Trans. on Graphics (Proceedings of SIGGRAPH 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.01746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02138v3</id>
    <updated>2021-06-13T05:35:04Z</updated>
    <published>2020-05-25T16:48:13Z</published>
    <title>Integrating Deep Learning into CAD/CAE System: Generative Design and
  Evaluation of 3D Conceptual Wheel</title>
    <summary>  Engineering design research integrating artificial intelligence (AI) into
computer-aided design (CAD) and computer-aided engineering (CAE) is actively
being conducted. This study proposes a deep learning-based CAD/CAE framework in
the conceptual design phase that automatically generates 3D CAD designs and
evaluates their engineering performance. The proposed framework comprises seven
stages: (1) 2D generative design, (2) dimensionality reduction, (3) design of
experiment in latent space, (4) CAD automation, (5) CAE automation, (6)
transfer learning, and (7) visualization and analysis. The proposed framework
is demonstrated through a road wheel design case study and indicates that AI
can be practically incorporated into an end-use product design project.
Engineers and industrial designers can jointly review a large number of
generated 3D CAD models by using this framework along with the engineering
performance results estimated by AI and find conceptual design candidates for
the subsequent detailed design stage.
</summary>
    <author>
      <name>Soyoung Yoo</name>
    </author>
    <author>
      <name>Sunghee Lee</name>
    </author>
    <author>
      <name>Seongsin Kim</name>
    </author>
    <author>
      <name>Kwang Hyeon Hwang</name>
    </author>
    <author>
      <name>Jong Ho Park</name>
    </author>
    <author>
      <name>Namwoo Kang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00158-021-02953-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00158-021-02953-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Structural and Multidisciplinary Optimization, 64(4), pp.
  2725-2747 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.02138v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02138v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02532v2</id>
    <updated>2020-09-10T05:07:55Z</updated>
    <published>2020-06-01T12:47:27Z</published>
    <title>MapTree: Recovering Multiple Solutions in the Space of Maps</title>
    <summary>  In this paper we propose an approach for computing multiple high-quality
near-isometric dense correspondences between a pair of 3D shapes. Our method is
fully automatic and does not rely on user-provided landmarks or descriptors.
This allows us to analyze the full space of maps and extract multiple diverse
and accurate solutions, rather than optimizing for a single optimal
correspondence as done in most previous approaches. To achieve this, we propose
a compact tree structure based on the spectral map representation for encoding
and enumerating possible rough initializations, and a novel efficient approach
for refining them to dense pointwise maps. This leads to a new method capable
of both producing multiple high-quality correspondences across shapes and
revealing the symmetry structure of a shape without a priori information. In
addition, we demonstrate through extensive experiments that our method is
robust and results in more accurate correspondences than state-of-the-art for
shape matching and symmetry detection.
</summary>
    <author>
      <name>Jing Ren</name>
    </author>
    <author>
      <name>Simone Melzi</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 26 figures, published in ACM Transactions on Graphics
  (Proc. SIGGRAPH Asia), 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02532v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02532v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05743v1</id>
    <updated>2020-06-10T09:19:48Z</updated>
    <published>2020-06-10T09:19:48Z</published>
    <title>Towards 3D Dance Motion Synthesis and Control</title>
    <summary>  3D human dance motion is a cooperative and elegant social movement. Unlike
regular simple locomotion, it is challenging to synthesize artistic dance
motions due to the irregularity, kinematic complexity and diversity. It
requires the synthesized dance is realistic, diverse and controllable. In this
paper, we propose a novel generative motion model based on temporal convolution
and LSTM,TC-LSTM, to synthesize realistic and diverse dance motion. We
introduce a unique control signal, dance melody line, to heighten
controllability. Hence, our model, and its switch for control signals, promote
a variety of applications: random dance synthesis, music-to-dance, user
control, and more. Our experiments demonstrate that our model can synthesize
artistic dance motion in various dance types. Compared with existing methods,
our method achieved start-of-the-art results.
</summary>
    <author>
      <name>Wenlin Zhuang</name>
    </author>
    <author>
      <name>Yangang Wang</name>
    </author>
    <author>
      <name>Joseph Robinson</name>
    </author>
    <author>
      <name>Congyi Wang</name>
    </author>
    <author>
      <name>Ming Shao</name>
    </author>
    <author>
      <name>Yun Fu</name>
    </author>
    <author>
      <name>Siyu Xia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05921v1</id>
    <updated>2020-06-10T16:15:16Z</updated>
    <published>2020-06-10T16:15:16Z</published>
    <title>Computational Design and Evaluation Methods for Empowering Non-Experts
  in Digital Fabrication</title>
    <summary>  Despite the increasing availability of personal fabrication hardware and
services, the true potential of digital fabrication remains unrealized due to
lack of computational techniques that can support 3D shape design by
non-experts. This work develops computational methods that address two key
aspects of content creation:(1) Function-driven design synthesis, (2) Design
assessment.
  For design synthesis, a generative shape modeling algorithm that facilitates
automatic geometry synthesis and user-driven modification for non-experts is
introduced. A critical observation that arises from this study is that the most
geometrical specifications are dictated by functional requirements. To support
design by high-level functional prescriptions, a physics based shape
optimization method for compliant coupling behavior design has been developed.
In line with this idea, producing complex 3D surfaces from flat 2D sheets by
exploiting the concept of buckling beams has also been explored. Effective
design assessment, the second key aspect, becomes critical for problems in
which computational solutions do not exist. For these problems, this work
proposes crowdsourcing as a way to empower non-experts in esoteric design
domains that traditionally require expertise and specialized knowledge.
</summary>
    <author>
      <name>Nurcan Gecer Ulu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis, Carnegie Mellon University</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07859v1</id>
    <updated>2020-06-14T10:26:20Z</updated>
    <published>2020-06-14T10:26:20Z</published>
    <title>Repulsive Curves</title>
    <summary>  Curves play a fundamental role across computer graphics, physical simulation,
and mathematical visualization, yet most tools for curve design do nothing to
prevent crossings or self-intersections. This paper develops efficient
algorithms for (self-)repulsion of plane and space curves that are well-suited
to problems in computational design. Our starting point is the so-called
tangent-point energy, which provides an infinite barrier to self-intersection.
In contrast to local collision detection strategies used in, e.g., physical
simulation, this energy considers interactions between all pairs of points, and
is hence useful for global shape optimization: local minima tend to be
aesthetically pleasing, physically valid, and nicely distributed in space. A
reformulation of gradient descent, based on a Sobolev-Slobodeckij inner product
enables us to make rapid progress toward local minima---independent of curve
resolution. We also develop a hierarchical multigrid scheme that significantly
reduces the per-step cost of optimization. The energy is easily integrated with
a variety of constraints and penalties (e.g., inextensibility, or obstacle
avoidance), which we use for applications including curve packing, knot
untangling, graph embedding, non-crossing spline interpolation, flow
visualization, and robotic path planning.
</summary>
    <author>
      <name>Christopher Yu</name>
    </author>
    <author>
      <name>Henrik Schumacher</name>
    </author>
    <author>
      <name>Keenan Crane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.07859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08821v4</id>
    <updated>2020-11-19T16:48:05Z</updated>
    <published>2020-06-15T23:30:46Z</published>
    <title>EMU: Efficient Muscle Simulation In Deformation Space</title>
    <summary>  EMU is an efficient and scalable model to simulate bulk musculoskeletal
motion with heterogenous materials. First, EMU requires no model reductions, or
geometric coarsening, thereby producing results visually accurate when compared
to an FEM simulation. Second, EMU is efficient and scales much better than
state-of-the-art FEM with the number of elements in the mesh, and is more
easily parallelizable. Third, EMU can handle heterogeneously stiff meshes with
an arbitrary constitutive model, thus allowing it to simulate soft muscles,
stiff tendons and even stiffer bones all within one unified system. These three
key characteristics of EMU enable us to efficiently orchestrate muscle
activated skeletal movements. We demonstrate the efficacy of our approach via a
number of examples with tendons, muscles, bones and joints.
</summary>
    <author>
      <name>Vismay Modi</name>
    </author>
    <author>
      <name>Lawson Fulton</name>
    </author>
    <author>
      <name>Shinjiro Sueda</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08821v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08821v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10509v1</id>
    <updated>2020-06-18T13:29:46Z</updated>
    <published>2020-06-18T13:29:46Z</published>
    <title>Structure and Design of HoloGen</title>
    <summary>  Increasing popularity of augmented and mixed reality systems has seen a
similar increase of interest in 2D and 3D computer generated holography (CGH).
Unlike stereoscopic approaches, CGH can fully represent a light field including
depth of focus, accommodation and vergence. Along with existing
telecommunications, imaging, projection, lithography, beam shaping and optical
tweezing applications, CGH is an exciting technique applicable to a wide array
of photonic problems including full 3D representation. Traditionally, the
primary roadblock to acceptance has been the significant numerical processing
required to generate holograms requiring both significant expertise and
significant computational power. This article discusses the structure and
design of HoloGen. HoloGen is an MIT licensed application that may be used to
generate holograms using a wide array of algorithms without expert guidance.
HoloGen uses a Cuda C and C++ backend with a C# and Windows Presentation
Framework graphical user interface. The article begins by introducing HoloGen
before providing an in-depth discussion of its design and structure. Particular
focus is given to the communication, data transfer and algorithmic aspects.
</summary>
    <author>
      <name>Peter J. Christopher</name>
    </author>
    <author>
      <name>Timothy D. Wilkinson</name>
    </author>
    <link href="http://arxiv.org/abs/2006.10509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.11348v1</id>
    <updated>2020-06-19T19:54:50Z</updated>
    <published>2020-06-19T19:54:50Z</published>
    <title>Ray-VR: Ray Tracing Virtual Reality in Falcor</title>
    <summary>  NVidia RTX platform has been changing and extending the possibilities for
real time Computer Graphics applications. It is the first time in history that
retail graphics cards have full hardware support for ray tracing primitives. It
still a long way to fully understand and optimize its use and this task itself
is a fertile field for scientific progression. However, another path is to
explore the platform as an expansion of paradigms for other problems. For
example, the integration of real time Ray Tracing and Virtual Reality can
result in interesting applications for visualization of Non-Euclidean Geometry
and 3D Manifolds. In this paper we present Ray-VR, a novel algorithm for real
time stereo ray tracing, constructed on top of Falcor, NVidia's scientific
prototyping framework.
</summary>
    <author>
      <name>Vinicius da Silva</name>
    </author>
    <author>
      <name>Luiz Velho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.11348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.11620v1</id>
    <updated>2020-06-20T17:11:48Z</updated>
    <published>2020-06-20T17:11:48Z</published>
    <title>Technical Note: Generating Realistic Fighting Scenes by Game Tree</title>
    <summary>  Recently, there have been a lot of researches to synthesize / edit the motion
of a single avatar in the virtual environment. However, there has not been so
much work of simulating continuous interactions of multiple avatars such as
fighting. In this paper, we propose a new method to generate a realistic
fighting scene based on motion capture data. We propose a new algorithm called
the temporal expansion approach which maps the continuous time action plan to a
discrete causality space such that turn-based evaluation methods can be used.
As a result, it is possible to use many mature algorithms available in strategy
games such as the Minimax algorithm and $\alpha-\beta$ pruning. We also propose
a method to generate and use an offense/defense table, which illustrates the
spatial-temporal relationship of attacks and dodges, to incorporate tactical
maneuvers of defense into the scene. Using our method, avatars will plan their
strategies taking into account the reaction of the opponent. Fighting scenes
with multiple avatars are generated to demonstrate the effectiveness of our
algorithm. The proposed method can also be applied to other kinds of continuous
activities that require strategy planning such as sport games.
</summary>
    <author>
      <name>Hubert P. H. Shum</name>
    </author>
    <author>
      <name>Taku Komura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.11620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.14726v1</id>
    <updated>2020-06-25T22:56:45Z</updated>
    <published>2020-06-25T22:56:45Z</published>
    <title>Augmenting Image Warping-Based Remote Volume Rendering with Ray Tracing</title>
    <summary>  We propose an image warping-based remote rendering technique for volumes that
decouples the rendering and display phases. Our work builds on prior work that
samples the volume on the client using ray casting and reconstructs a z-value
based on some heuristic. The color and depth buffer are then sent to the client
that reuses this depth image as a stand-in for subsequent frames by warping it
according to the current camera position until new data was received from the
server. We augment that method by implementing the client renderer using ray
tracing. By representing the pixel contributions as spheres, this allows us to
effectively vary their footprint based on the distance to the viewer, which we
find to give better results than point-based rasterization when applied to
volumetric data sets.
</summary>
    <author>
      <name>Stefan Zellmann</name>
    </author>
    <link href="http://arxiv.org/abs/2006.14726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.15059v1</id>
    <updated>2020-06-26T15:38:14Z</updated>
    <published>2020-06-26T15:38:14Z</published>
    <title>Computing Light Transport Gradients using the Adjoint Method</title>
    <summary>  This paper proposes a new equation from continuous adjoint theory to compute
the gradient of quantities governed by the Transport Theory of light. Unlike
discrete gradients ala autograd, which work at the code level, we first
formulate the continuous theory and then discretize it. The key insight of this
paper is that computing gradients in Transport Theory is akin to computing the
importance, a quantity adjoint to radiance that satisfies an adjoint equation.
Importance tells us where to look for light that matters. This is one of the
key insights of this paper. In fact, this mathematical journey started from a
whimsical thought that these adjoints might be related. Computing gradients is
therefore no more complicated than computing the importance field. This insight
and the following paper hopefully will shed some light on this complicated
problem and ease the implementations of gradient computations in existing path
tracers.
</summary>
    <author>
      <name>Jos Stam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures, unpublished manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.15510v1</id>
    <updated>2020-06-28T05:04:01Z</updated>
    <published>2020-06-28T05:04:01Z</published>
    <title>DNF-Net: a Deep Normal Filtering Network for Mesh Denoising</title>
    <summary>  This paper presents a deep normal filtering network, called DNF-Net, for mesh
denoising. To better capture local geometry, our network processes the mesh in
terms of local patches extracted from the mesh. Overall, DNF-Net is an
end-to-end network that takes patches of facet normals as inputs and directly
outputs the corresponding denoised facet normals of the patches. In this way,
we can reconstruct the geometry from the denoised normals with feature
preservation. Besides the overall network architecture, our contributions
include a novel multi-scale feature embedding unit, a residual learning
strategy to remove noise, and a deeply-supervised joint loss function. Compared
with the recent data-driven works on mesh denoising, DNF-Net does not require
manual input to extract features and better utilizes the training data to
enhance its denoising performance. Finally, we present comprehensive
experiments to evaluate our method and demonstrate its superiority over the
state of the art on both synthetic and real-scanned meshes.
</summary>
    <author>
      <name>Xianzhi Li</name>
    </author>
    <author>
      <name>Ruihui Li</name>
    </author>
    <author>
      <name>Lei Zhu</name>
    </author>
    <author>
      <name>Chi-Wing Fu</name>
    </author>
    <author>
      <name>Pheng-Ann Heng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2020.3001681</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2020.3001681" rel="related"/>
    <link href="http://arxiv.org/abs/2006.15510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00074v1</id>
    <updated>2020-06-30T19:36:38Z</updated>
    <published>2020-06-30T19:36:38Z</published>
    <title>Deep Geometric Texture Synthesis</title>
    <summary>  Recently, deep generative adversarial networks for image generation have
advanced rapidly; yet, only a small amount of research has focused on
generative models for irregular structures, particularly meshes. Nonetheless,
mesh generation and synthesis remains a fundamental topic in computer graphics.
In this work, we propose a novel framework for synthesizing geometric textures.
It learns geometric texture statistics from local neighborhoods (i.e., local
triangular patches) of a single reference 3D model. It learns deep features on
the faces of the input triangulation, which is used to subdivide and generate
offsets across multiple scales, without parameterization of the reference or
target mesh. Our network displaces mesh vertices in any direction (i.e., in the
normal and tangential direction), enabling synthesis of geometric textures,
which cannot be expressed by a simple 2D displacement map. Learning and
synthesizing on local geometric patches enables a genus-oblivious framework,
facilitating texture transfer between shapes of different genus.
</summary>
    <author>
      <name>Amir Hertz</name>
    </author>
    <author>
      <name>Rana Hanocka</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386569.3392471</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386569.3392471" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.00074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00308v3</id>
    <updated>2020-10-31T04:12:57Z</updated>
    <published>2020-07-01T08:03:09Z</published>
    <title>Polar Stroking: New Theory and Methods for Stroking Paths</title>
    <summary>  Stroking and filling are the two basic rendering operations on paths in
vector graphics. The theory of filling a path is well-understood in terms of
contour integrals and winding numbers, but when path rendering standards
specify stroking, they resort to the analogy of painting pixels with a brush
that traces the outline of the path. This means important standards such as
PDF, SVG, and PostScript lack a rigorous way to say what samples are inside or
outside a stroked path. Our work fills this gap with a principled theory of
stroking.
  Guided by our theory, we develop a novel polar stroking method to render
stroked paths robustly with an intuitive way to bound the tessellation error
without needing recursion. Because polar stroking guarantees small uniform
steps in tangent angle, it provides an efficient way to accumulate arc length
along a path for texturing or dashing. While this paper focuses on developing
the theory of our polar stroking method, we have successfully implemented our
methods on modern programmable GPUs.
</summary>
    <author>
      <name>Mark J. Kilgard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386569.3392458</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386569.3392458" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 19 figures, ACM Trans. on Graphics (Proceedings of SIGGRAPH
  2020); corrected Fig. 8 and Eq. 6</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics, Vol. 39, No. 4 (2020) 145:1-15</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.00308v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00308v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00324v1</id>
    <updated>2020-07-01T08:43:12Z</updated>
    <published>2020-07-01T08:43:12Z</published>
    <title>On Designing GPU Algorithms with Applications to Mesh Refinement</title>
    <summary>  We present a set of rules to guide the design of GPU algorithms. These rules
are grounded on the principle of reducing waste in GPU utility to achieve good
speed up. In accordance to these rules, we propose GPU algorithms for 2D
constrained, 3D constrained and 3D Restricted Delaunay refinement problems
respectively. Our algorithms take a 2D planar straight line graph (PSLG) or 3D
piecewise linear complex (PLC) $\mathcal{G}$ as input, and generate quality
meshes conforming or approximating to $\mathcal{G}$. The implementation of our
algorithms shows that they are the first to run an order of magnitude faster
than current state-of-the-art counterparts in sequential and parallel manners
while using similar numbers of Steiner points to produce triangulations of
comparable qualities. It thus reduces the computing time of mesh refinement
from possibly hours to a few seconds or minutes for possible use in interactive
graphics applications.
</summary>
    <author>
      <name>Zhenghai Chen</name>
    </author>
    <author>
      <name>Tiow-Seng Tan</name>
    </author>
    <author>
      <name>Hong-Yang Ong</name>
    </author>
    <link href="http://arxiv.org/abs/2007.00324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00987v1</id>
    <updated>2020-07-02T09:51:36Z</updated>
    <published>2020-07-02T09:51:36Z</published>
    <title>ADD: Analytically Differentiable Dynamics for Multi-Body Systems with
  Frictional Contact</title>
    <summary>  We present a differentiable dynamics solver that is able to handle frictional
contact for rigid and deformable objects within a unified framework. Through a
principled mollification of normal and tangential contact forces, our method
circumvents the main difficulties inherent to the non-smooth nature of
frictional contact. We combine this new contact model with fully-implicit time
integration to obtain a robust and efficient dynamics solver that is
analytically differentiable. In conjunction with adjoint sensitivity analysis,
our formulation enables gradient-based optimization with adaptive trade-offs
between simulation accuracy and smoothness of objective function landscapes. We
thoroughly analyse our approach on a set of simulation examples involving rigid
bodies, visco-elastic materials, and coupled multi-body systems. We furthermore
showcase applications of our differentiable simulator to parameter estimation
for deformable objects, motion planning for robotic manipulation, trajectory
optimization for compliant walking robots, as well as efficient self-supervised
learning of control policies.
</summary>
    <author>
      <name>Moritz Geilinger</name>
    </author>
    <author>
      <name>David Hahn</name>
    </author>
    <author>
      <name>Jonas Zehnder</name>
    </author>
    <author>
      <name>Moritz Bächer</name>
    </author>
    <author>
      <name>Bernhard Thomaszewski</name>
    </author>
    <author>
      <name>Stelian Coros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Moritz Geilinger and David Hahn contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.00987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01629v1</id>
    <updated>2020-07-03T11:45:51Z</updated>
    <published>2020-07-03T11:45:51Z</published>
    <title>A Discrete Probabilistic Approach to Dense Flow Visualization</title>
    <summary>  Dense flow visualization is a popular visualization paradigm. Traditionally,
the various models and methods in this area use a continuous formulation,
resting upon the solid foundation of functional analysis. In this work, we
examine a discrete formulation of dense flow visualization. From probability
theory, we derive a similarity matrix that measures the similarity between
different points in the flow domain, leading to the discovery of a whole new
class of visualization models. Using this matrix, we propose a novel
visualization approach consisting of the computation of spectral embeddings,
i.e., characteristic domain maps, defined by particle mixture probabilities.
These embeddings are scalar fields that give insight into the mixing processes
of the flow on different scales. The approach of spectral embeddings is already
well studied in image segmentation, and we see that spectral embeddings are
connected to Fourier expansions and frequencies. We showcase the utility of our
method using different 2D and 3D flows.
</summary>
    <author>
      <name>Daniel Preuß</name>
    </author>
    <author>
      <name>Tino Weinkauf</name>
    </author>
    <author>
      <name>Jens Krüger</name>
    </author>
    <link href="http://arxiv.org/abs/2007.01629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01971v3</id>
    <updated>2020-08-16T20:05:43Z</updated>
    <published>2020-07-04T00:18:27Z</published>
    <title>Structure-Aware Human-Action Generation</title>
    <summary>  Generating long-range skeleton-based human actions has been a challenging
problem since small deviations of one frame can cause a malformed action
sequence. Most existing methods borrow ideas from video generation, which
naively treat skeleton nodes/joints as pixels of images without considering the
rich inter-frame and intra-frame structure information, leading to potential
distorted actions. Graph convolutional networks (GCNs) is a promising way to
leverage structure information to learn structure representations. However,
directly adopting GCNs to tackle such continuous action sequences both in
spatial and temporal spaces is challenging as the action graph could be huge.
To overcome this issue, we propose a variant of GCNs to leverage the powerful
self-attention mechanism to adaptively sparsify a complete action graph in the
temporal space. Our method could dynamically attend to important past frames
and construct a sparse graph to apply in the GCN framework, well-capturing the
structure information in action sequences. Extensive experimental results
demonstrate the superiority of our method on two standard human action datasets
compared with existing methods.
</summary>
    <author>
      <name>Ping Yu</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Chunyuan Li</name>
    </author>
    <author>
      <name>Junsong Yuan</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ECCV 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01971v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01971v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10093v1</id>
    <updated>2020-07-20T13:36:54Z</updated>
    <published>2020-07-20T13:36:54Z</published>
    <title>Learning Adaptive Sampling and Reconstruction for Volume Visualization</title>
    <summary>  A central challenge in data visualization is to understand which data samples
are required to generate an image of a data set in which the relevant
information is encoded. In this work, we make a first step towards answering
the question of whether an artificial neural network can predict where to
sample the data with higher or lower density, by learning of correspondences
between the data, the sampling patterns and the generated images. We introduce
a novel neural rendering pipeline, which is trained end-to-end to generate a
sparse adaptive sampling structure from a given low-resolution input image, and
reconstructs a high-resolution image from the sparse set of samples. For the
first time, to the best of our knowledge, we demonstrate that the selection of
structures that are relevant for the final visual representation can be jointly
learned together with the reconstruction of this representation from these
structures. Therefore, we introduce differentiable sampling and reconstruction
stages, which can leverage back-propagation based on supervised losses solely
on the final image. We shed light on the adaptive sampling patterns generated
by the network pipeline and analyze its use for volume visualization including
isosurface and direct volume rendering.
</summary>
    <author>
      <name>Sebastian Weiss</name>
    </author>
    <author>
      <name>Mustafa Işık</name>
    </author>
    <author>
      <name>Justus Thies</name>
    </author>
    <author>
      <name>Rüdiger Westermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2020.3039340</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2020.3039340" rel="related"/>
    <link href="http://arxiv.org/abs/2007.10093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10430v1</id>
    <updated>2020-07-20T19:41:41Z</updated>
    <published>2020-07-20T19:41:41Z</published>
    <title>A Survey of Algorithms for Geodesic Paths and Distances</title>
    <summary>  Numerical computation of shortest paths or geodesics on curved domains, as
well as the associated geodesic distance, arises in a broad range of
applications across digital geometry processing, scientific computing, computer
graphics, and computer vision. Relative to Euclidean distance computation,
these tasks are complicated by the influence of curvature on the behavior of
shortest paths, as well as the fact that the representation of the domain may
itself be approximate. In spite of the difficulty of this problem, recent
literature has developed a wide variety of sophisticated methods that enable
rapid queries of geodesic information, even on relatively large models. This
survey reviews the major categories of approaches to the computation of
geodesic paths and distances, highlighting common themes and opportunities for
future improvement.
</summary>
    <author>
      <name>Keenan Crane</name>
    </author>
    <author>
      <name>Marco Livesu</name>
    </author>
    <author>
      <name>Enrico Puppo</name>
    </author>
    <author>
      <name>Yipeng Qin</name>
    </author>
    <link href="http://arxiv.org/abs/2007.10430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10918v1</id>
    <updated>2020-07-21T16:23:10Z</updated>
    <published>2020-07-21T16:23:10Z</published>
    <title>DecoSurf: Recursive Geodesic Patterns on Triangle Meshes</title>
    <summary>  In this paper, we show that many complex patterns, which characterize the
decorative style of many artisanal objects, can be generated by the recursive
application of only four operators. Each operator is derived from tracing the
isolines or the integral curves of geodesics fields generated from selected
seeds on the surface. Based on this formulation, we present an interactive
application that lets designers model complex recursive patterns directly on
the object surface, without relying on parametrization. We support interaction
on commodity hardware on meshes of a few million triangles, by combining light
data structures together with an efficient approximate graph-based geodesic
solver. We validate our approach by matching decoration styles from real-world
photos, by analyzing the speed and accuracy of our geodesic solver, and by
validating the interface with a user study.
</summary>
    <author>
      <name>Giacomo Nazzaro</name>
    </author>
    <author>
      <name>Enrico Puppo</name>
    </author>
    <author>
      <name>Fabio Pellacini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11632v3</id>
    <updated>2020-09-14T21:01:21Z</updated>
    <published>2020-07-22T19:06:07Z</published>
    <title>Wavelet-based Heat Kernel Derivatives: Towards Informative Localized
  Shape Analysis</title>
    <summary>  In this paper, we propose a new construction for the Mexican hat wavelets on
shapes with applications to partial shape matching. Our approach takes its main
inspiration from the well-established methodology of diffusion wavelets. This
novel construction allows us to rapidly compute a multiscale family of Mexican
hat wavelet functions, by approximating the derivative of the heat kernel. We
demonstrate that it leads to a family of functions that inherit many attractive
properties of the heat kernel (e.g., a local support, ability to recover
isometries from a single point, efficient computation). Due to its natural
ability to encode high-frequency details on a shape, the proposed method
reconstructs and transfers $\delta$-functions more accurately than the
Laplace-Beltrami eigenfunction basis and other related bases. Finally, we apply
our method to the challenging problems of partial and large-scale shape
matching. An extensive comparison to the state-of-the-art shows that it is
comparable in performance, while both simpler and much faster than competing
approaches.
</summary>
    <author>
      <name>M. Kirgo</name>
    </author>
    <author>
      <name>S. Melzi</name>
    </author>
    <author>
      <name>G. Patanè</name>
    </author>
    <author>
      <name>E. Rodolà</name>
    </author>
    <author>
      <name>M. Ovsjanikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 lages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.11632v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11632v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.14394v1</id>
    <updated>2020-07-28T17:58:53Z</updated>
    <published>2020-07-28T17:58:53Z</published>
    <title>Signed Distance Fields Dynamic Diffuse Global Illumination</title>
    <summary>  Global Illumination (GI) is of utmost importance in the field of
photo-realistic rendering. However, its computation has always been very
complex, especially diffuse GI. State of the art real-time GI methods have
limitations of different nature, such as light leaking, performance issues,
special hardware requirements, noise corruption, bounce number limitations,
among others. To overcome these limitations, we propose a novel approach of
computing dynamic diffuse GI with a signed distance fields approximation of the
scene and discretizing the space domain of the irradiance function. With this
approach, we are able to estimate real-time diffuse GI for dynamic lighting and
geometry, without any precomputations and supporting multi-bounce GI, providing
good quality lighting and high performance at the same time. Our algorithm is
also able to achieve better scalability, and manage both large open scenes and
indoor high-detailed scenes without being corrupted by noise.
</summary>
    <author>
      <name>Jinkai Hu</name>
    </author>
    <author>
      <name>Milo Yip</name>
    </author>
    <author>
      <name>G. Elias Alonso</name>
    </author>
    <author>
      <name>Shihao Gu</name>
    </author>
    <author>
      <name>Xiangjun Tang</name>
    </author>
    <author>
      <name>Xiaogang Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2007.14394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.15227v2</id>
    <updated>2022-02-09T13:25:36Z</updated>
    <published>2020-07-30T04:57:26Z</published>
    <title>Federated Visualization: A Privacy-preserving Strategy for Aggregated
  Visual Query</title>
    <summary>  We present a novel privacy preservation strategy for decentralized
visualization. The key idea is to imitate the flowchart of the federated
learning framework, and reformulate the visualization process within a
federated infrastructure. The federation of visualization is fulfilled by
leveraging a shared global module that composes the encrypted externalizations
of transformed visual features of data pieces in local modules. We design two
implementations of federated visualization: a prediction-based scheme, and a
query-based scheme. We demonstrate the effectiveness of our approach with a set
of visual forms, and verify its robustness with evaluations. We report the
value of federated visualization in real scenarios with an expert review.
</summary>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Yating Wei</name>
    </author>
    <author>
      <name>Zhiyong Wang</name>
    </author>
    <author>
      <name>Shuyue Zhou</name>
    </author>
    <author>
      <name>Bingru Lin</name>
    </author>
    <author>
      <name>Zhiguang Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2007.15227v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.15227v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.15242v1</id>
    <updated>2020-07-30T05:48:58Z</updated>
    <published>2020-07-30T05:48:58Z</published>
    <title>Understanding the Stability of Deep Control Policies for Biped
  Locomotion</title>
    <summary>  Achieving stability and robustness is the primary goal of biped locomotion
control. Recently, deep reinforce learning (DRL) has attracted great attention
as a general methodology for constructing biped control policies and
demonstrated significant improvements over the previous state-of-the-art.
Although deep control policies have advantages over previous controller design
approaches, many questions remain unanswered. Are deep control policies as
robust as human walking? Does simulated walking use similar strategies as human
walking to maintain balance? Does a particular gait pattern similarly affect
human and simulated walking? What do deep policies learn to achieve improved
gait stability? The goal of this study is to answer these questions by
evaluating the push-recovery stability of deep policies compared to human
subjects and a previous feedback controller. We also conducted experiments to
evaluate the effectiveness of variants of DRL algorithms.
</summary>
    <author>
      <name>Hwangpil Park</name>
    </author>
    <author>
      <name>Ri Yu</name>
    </author>
    <author>
      <name>Yoonsang Lee</name>
    </author>
    <author>
      <name>Kyungho Lee</name>
    </author>
    <author>
      <name>Jehee Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.15242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.15242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.15311v3</id>
    <updated>2021-02-25T07:42:00Z</updated>
    <published>2020-07-30T08:47:04Z</published>
    <title>Functionality-Driven Musculature Retargeting</title>
    <summary>  We present a novel retargeting algorithm that transfers the musculature of a
reference anatomical model to new bodies with different sizes, body
proportions, muscle capability, and joint range of motion while preserving the
functionality of the original musculature as closely as possible. The geometric
configuration and physiological parameters of musculotendon units are estimated
and optimized to adapt to new bodies. The range of motion around joints is
estimated from a motion capture dataset and edited further for individual
models. The retargeted model is simulation-ready, so we can physically simulate
muscle-actuated motor skills with the model. Our system is capable of
generating a wide variety of anatomical bodies that can be simulated to walk,
run, jump and dance while maintaining balance under gravity. We will also
demonstrate the construction of individualized musculoskeletal models from
bi-planar X-ray images and medical examinations.
</summary>
    <author>
      <name>Hoseok Ryu</name>
    </author>
    <author>
      <name>Minseok Kim</name>
    </author>
    <author>
      <name>Seunghwan Lee</name>
    </author>
    <author>
      <name>Moon Seok Park</name>
    </author>
    <author>
      <name>Kyoungmin Lee</name>
    </author>
    <author>
      <name>Jehee Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14191</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14191" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 20 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum,40(2021)341-356</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.15311v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.15311v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02344v6</id>
    <updated>2022-04-05T10:43:45Z</updated>
    <published>2020-12-04T00:30:45Z</published>
    <title>Perceptual error optimization for Monte Carlo rendering</title>
    <summary>  Synthesizing realistic images involves computing high-dimensional
light-transport integrals. In practice, these integrals are numerically
estimated via Monte Carlo integration. The error of this estimation manifests
itself as conspicuous aliasing or noise. To ameliorate such artifacts and
improve image fidelity, we propose a perception-oriented framework to optimize
the error of Monte Carlo rendering. We leverage models based on human
perception from the halftoning literature. The result is an optimization
problem whose solution distributes the error as visually pleasing blue noise in
image space. To find solutions, we present a set of algorithms that provide
varying trade-offs between quality and speed, showing substantial improvements
over prior state of the art. We perform evaluations using quantitative and
error metrics, and provide extensive supplemental material to demonstrate the
perceptual improvements achieved by our methods.
</summary>
    <author>
      <name>Vassillen Chizhov</name>
    </author>
    <author>
      <name>Iliyan Georgiev</name>
    </author>
    <author>
      <name>Karol Myszkowski</name>
    </author>
    <author>
      <name>Gurprit Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3504002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3504002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics, Volume 41, Issue 3, June 2022,
  Article No.: 26, pp 1-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.02344v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02344v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02459v1</id>
    <updated>2020-12-04T08:30:57Z</updated>
    <published>2020-12-04T08:30:57Z</published>
    <title>Multiscale Mesh Deformation Component Analysis with Attention-based
  Autoencoders</title>
    <summary>  Deformation component analysis is a fundamental problem in geometry
processing and shape understanding. Existing approaches mainly extract
deformation components in local regions at a similar scale while deformations
of real-world objects are usually distributed in a multi-scale manner. In this
paper, we propose a novel method to exact multiscale deformation components
automatically with a stacked attention-based autoencoder. The attention
mechanism is designed to learn to softly weight multi-scale deformation
components in active deformation regions, and the stacked attention-based
autoencoder is learned to represent the deformation components at different
scales. Quantitative and qualitative evaluations show that our method
outperforms state-of-the-art methods. Furthermore, with the multiscale
deformation components extracted by our method, the user can edit shapes in a
coarse-to-fine fashion which facilitates effective modeling of new shapes.
</summary>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>Qingyang Tan</name>
    </author>
    <author>
      <name>Yihua Huang</name>
    </author>
    <author>
      <name>Shihong Xia</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.02459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.03325v1</id>
    <updated>2020-12-06T17:06:33Z</updated>
    <published>2020-12-06T17:06:33Z</published>
    <title>EasyPBR: A Lightweight Physically-Based Renderer</title>
    <summary>  Modern rendering libraries provide unprecedented realism, producing real-time
photorealistic 3D graphics on commodity hardware. Visual fidelity, however,
comes at the cost of increased complexity and difficulty of usage, with many
rendering parameters requiring a deep understanding of the pipeline. We propose
EasyPBR as an alternative rendering library that strikes a balance between
ease-of-use and visual quality. EasyPBR consists of a deferred renderer that
implements recent state-of-the-art approaches in physically based rendering. It
offers an easy-to-use Python and C++ interface that allows high-quality images
to be created in only a few lines of code or directly through a graphical user
interface. The user can choose between fully controlling the rendering pipeline
or letting EasyPBR automatically infer the best parameters based on the current
scene composition. The EasyPBR library can help the community to more easily
leverage the power of current GPUs to create realistic images. These can then
be used as synthetic data for deep learning or for creating animations for
academic purposes.
</summary>
    <author>
      <name>Radu Alexandru Rosu</name>
    </author>
    <author>
      <name>Sven Behnke</name>
    </author>
    <link href="http://arxiv.org/abs/2012.03325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07959v1</id>
    <updated>2020-12-14T21:51:17Z</updated>
    <published>2020-12-14T21:51:17Z</published>
    <title>Continuous Curve Textures</title>
    <summary>  Repetitive patterns are ubiquitous in natural and human-made objects, and can
be created with a variety of tools and methods. Manual authoring provides
unmatched degree of freedom and control, but can require significant artistic
expertise and manual labor. Computational methods can automate parts of the
manual creation process, but are mainly tailored for discrete pixels or
elements instead of more general continuous structures. We propose an
example-based method to synthesize continuous curve patterns from exemplars.
Our main idea is to extend prior sample-based discrete element synthesis
methods to consider not only sample positions (geometry) but also their
connections (topology). Since continuous structures can exhibit higher
complexity than discrete elements, we also propose robust, hierarchical
synthesis to enhance output quality. Our algorithm can generate a variety of
continuous curve patterns fully automatically. For further quality improvement
and customization, we also present an autocomplete user interface to facilitate
interactive creation and iterative editing. We evaluate our methods and
interface via different patterns, ablation studies, and comparisons with
alternative methods.
</summary>
    <author>
      <name>Peihan Tu</name>
    </author>
    <author>
      <name>Li-Yi Wei</name>
    </author>
    <author>
      <name>Koji Yatani</name>
    </author>
    <author>
      <name>Takeo Igarashi</name>
    </author>
    <author>
      <name>Matthias Zwicker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3414685.3417780</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3414685.3417780" rel="related"/>
    <link href="http://arxiv.org/abs/2012.07959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.10357v2</id>
    <updated>2021-05-21T16:23:17Z</updated>
    <published>2020-12-18T16:59:49Z</published>
    <title>Proceduray -- A light-weight engine for procedural primitive ray tracing</title>
    <summary>  We introduce Proceduray, an engine for real-time ray tracing of procedural
geometry. Its motivation is the current lack of mid-level abstraction tools for
scenes with primitives involving intersection shaders. Those scenes impose
strict engine design choices since they need flexibility in the shader table
setup. Proceduray aims at providing a fair tradeoff between that flexibility
and productivity. It also aims to be didactic. Shader table behavior can be
confusing because parameters for indexing come from different parts of a
system, involving both host and device code. This is different in essence from
ray tracing triangle meshes (which must use a built-in intersection shader for
all objects) or rendering with the traditional graphics or compute pipelines.
Additionals goals of the project include fomenting deeper discussions about
DirectX RayTracing (DXR) host code and providing a good starting point for
developers trying to deal with procedural geometry using DXR.
</summary>
    <author>
      <name>Vinícius da Silva</name>
    </author>
    <author>
      <name>Tiago Novello</name>
    </author>
    <author>
      <name>Hélio Lopes</name>
    </author>
    <author>
      <name>Luiz Velho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.10357v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10357v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05 (Primary) 37-04, 37F10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15176v1</id>
    <updated>2020-12-30T14:39:46Z</updated>
    <published>2020-12-30T14:39:46Z</published>
    <title>Hybrid Function Representation for Heterogeneous Objects</title>
    <summary>  Heterogeneous object modelling is an emerging area where geometric shapes are
considered in concert with their internal physically-based attributes. This
paper describes a novel theoretical and practical framework for modelling
volumetric heterogeneous objects on the basis of a novel unifying
functionally-based hybrid representation called HFRep. This new representation
allows for obtaining a continuous smooth distance field in Euclidean space and
preserves the advantages of the conventional representations based on scalar
fields of different kinds without their drawbacks. We systematically describe
the mathematical and algorithmic basics of HFRep. The steps of the basic
algorithm are presented in detail for both geometry and attributes. To solve
some problematic issues, we have suggested several practical solutions,
including a new algorithm for solving the eikonal equation on hierarchical
grids. Finally, we show the practicality of the approach by modelling several
representative heterogeneous objects, including those of a time-variant nature.
</summary>
    <author>
      <name>A. Tereshin</name>
    </author>
    <author>
      <name>A. Pasko</name>
    </author>
    <author>
      <name>O. Fryazinov</name>
    </author>
    <author>
      <name>V. Adzhiev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 16 figures, submitted to Graphical Models Journal (minor
  revision)</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.15176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.3.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00631v3</id>
    <updated>2021-02-21T04:08:12Z</updated>
    <published>2021-01-03T14:34:21Z</published>
    <title>A Marching Cube Algorithm Based on Edge Growth</title>
    <summary>  Marching Cube algorithm is currently one of the most popular 3D
reconstruction surface rendering algorithms. It forms cube voxels through the
input image, and then uses 15 basic topological configurations to extract the
iso-surfaces in the voxels. It processes each cube voxel in a traversal manner,
but it does not consider the relationship between iso-surfaces in adjacent
cubes. Due to ambiguity, the final reconstructed model may have holes. We
propose a Marching Cube algorithm based on edge growth. The algorithm first
extracts seed triangles, then grows the seed triangles and reconstructs the
entire 3D model. According to the position of the growth edge, we propose 17
topological configurations with iso-surfaces. From the reconstruction results,
the algorithm can reconstruct the 3D model well. When only the main contour of
the 3D model needs to be organized, the algorithm performs well. In addition,
when there are multiple scattered parts in the data, the algorithm can extract
only the 3D contours of the parts connected to the seed by setting the region
selected by the seed.
</summary>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Su Gao</name>
    </author>
    <author>
      <name>Monan Wang</name>
    </author>
    <author>
      <name>Zhenghua Duan</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00631v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00631v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02798v1</id>
    <updated>2021-01-07T23:40:03Z</updated>
    <published>2021-01-07T23:40:03Z</published>
    <title>Enhanced Direct Delta Mush</title>
    <summary>  Direct Delta Mush is a novel skinning deformation technique introduced by Le
and Lewis (2019). It generalizes the iterative Delta Mush algorithm of
Mancewicz et al (2014), providing a direct solution with improved efficiency
and control. Compared to Linear Blend Skinning, Direct Delta Mush offers better
quality of deformations and ease of authoring at comparable performance.
However, Direct Delta Mush does not handle non-rigid joint transformations
correctly which limits its application for most production environments. This
paper presents an extension to Direct Delta Mush that integrates the non-rigid
part of joint transformations into the algorithm. In addition, the paper also
describes practical considerations for computing the orthogonal component of
the transformation and stability issues observed during the implementation and
testing.
</summary>
    <author>
      <name>Serguei Kalentchouk</name>
    </author>
    <author>
      <name>Michael Hutchinson</name>
    </author>
    <author>
      <name>Deepak Tolani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3415264.3425464</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3415264.3425464" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SA '20 Posters: SIGGRAPH Asia 2020 Posters, December 2020, Article
  No. 29</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.02798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02847v1</id>
    <updated>2021-01-08T04:42:39Z</updated>
    <published>2021-01-08T04:42:39Z</published>
    <title>Color Contrast Enhanced Rendering for Optical See-through Head-mounted
  Displays</title>
    <summary>  Most commercially available optical see-through head-mounted displays
(OST-HMDs) utilize optical combiners to simultaneously visualize the physical
background and virtual objects. The displayed images perceived by users are a
blend of rendered pixels and background colors. Enabling high fidelity color
perception in mixed reality (MR) scenarios using OST-HMDs is an important but
challenging task. We propose a real-time rendering scheme to enhance the color
contrast between virtual objects and the surrounding background for OST-HMDs.
Inspired by the discovery of color perception in psychophysics, we first
formulate the color contrast enhancement as a constrained optimization problem.
We then design an end-to-end algorithm to search the optimal complementary
shift in both chromaticity and luminance of the displayed color. This aims at
enhancing the contrast between virtual objects and the real background as well
as keeping the consistency with the original color. We assess the performance
of our approach using a simulated OST-HMD environment and an off-the-shelf
OST-HMD. Experimental results from objective evaluations and subjective user
studies demonstrate that the proposed approach makes rendered virtual objects
more distinguishable from the surrounding background, thereby bringing a better
visual experience.
</summary>
    <author>
      <name>Yunjin Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Evan</arxiv:affiliation>
    </author>
    <author>
      <name>Rui Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Evan</arxiv:affiliation>
    </author>
    <author>
      <name> Yifan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Evan</arxiv:affiliation>
    </author>
    <author>
      <name> Peng</name>
    </author>
    <author>
      <name>Wei Hua</name>
    </author>
    <author>
      <name>Hujun Bao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 22 figures, submitted to TVCG</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02903v1</id>
    <updated>2021-01-08T08:24:08Z</updated>
    <published>2021-01-08T08:24:08Z</published>
    <title>Geometry-Based Layout Generation with Hyper-Relations AMONG Objects</title>
    <summary>  Recent studies show increasing demands and interests in automatically
generating layouts, while there is still much room for improving the
plausibility and robustness. In this paper, we present a data-driven layout
framework without model formulation and loss term optimization. We achieve and
organize priors directly based on samples from datasets instead of sampling
probabilistic models. Therefore, our method enables expressing and generating
mathematically inexpressible relations among three or more objects.
Subsequently, a non-learning geometric algorithm attempts arranging objects
plausibly considering constraints such as walls, windows, etc. Experiments
would show our generated layouts outperform the state-of-art and our framework
is competitive to human designers.
</summary>
    <author>
      <name>Shao-Kui Zhang</name>
    </author>
    <author>
      <name>Wei-Yu Xie</name>
    </author>
    <author>
      <name>Song-Hai Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the proceedings of Computational Visual Media Conference
  2021. 10 Pages. 13 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.04560v1</id>
    <updated>2021-01-12T15:54:03Z</updated>
    <published>2021-01-12T15:54:03Z</published>
    <title>TopoKnit : A Process-Oriented Representation for Modeling the Topology
  of Yarns in Weft-Knitted Textiles</title>
    <summary>  Machine knitted textiles are complex multi-scale material structures
increasingly important in many industries, including consumer products,
architecture, composites, medical, and military. Computational modeling,
simulation, and design of industrial fabrics require efficient representations
of the spatial, material, and physical properties of such structures. We
propose a process-oriented representation, TopoKnit, that defines a
foundational data structure for representing the topology of weft-knitted
textiles at the yarn scale. Process space serves as an intermediary between the
machine and fabric spaces, and supports a concise, computationally efficient
evaluation approach based on on-demand, near constant-time queries. In this
paper, we define the properties of the process space, and design a data
structure to represent it and algorithms to evaluate it. We demonstrate the
effectiveness of the representation scheme by providing results of evaluations
of the data structure in support of common topological operations in the fabric
space.
</summary>
    <author>
      <name>Levi Kapllani</name>
    </author>
    <author>
      <name>Chelsea Amanatides</name>
    </author>
    <author>
      <name>Genevieve Dion</name>
    </author>
    <author>
      <name>Vadim Shapiro</name>
    </author>
    <author>
      <name>David E. Breen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 27 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.04560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.04560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.10503v1</id>
    <updated>2021-01-26T00:59:47Z</updated>
    <published>2021-01-26T00:59:47Z</published>
    <title>Human Centric Accessibility Graph For Environment Analysis</title>
    <summary>  Understanding design decisions in relation to the future occupants of a
building is a crucial part of good design. However, limitations in tools and
expertise hinder meaningful human-centric decisions during the design process.
In this paper, a novel Spatial Human Accessibility graph for Planning and
Environment Analysis (SHAPE) is introduced that brings together the technical
challenges of discrete representations of digital models, with human-based
metrics for evaluating the environment. SHAPE: does not need labeled geometry
as input, works with multi-level buildings, captures surface variations (e.g.,
slopes in a terrain), and can be used with existing graph theory (e.g.,
gravity, centrality) techniques. SHAPE uses ray-casting to perform a search,
generating a dense graph of all accessible locations within the environment and
storing the type of travel required in a graph (e.g., up a slope, down a step).
The ability to simultaneously evaluate and plan paths from multiple human
factors is shown to work on digital models across room, building, and
topography scales. The results enable designers and planners to evaluate
options of the built environment in new ways, and at higher fidelity, that will
lead to more human-friendly and accessible environments.
</summary>
    <author>
      <name>Mathew Schwartz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.autcon.2021.103557</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.autcon.2021.103557" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.10503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.12276v2</id>
    <updated>2023-12-01T07:57:47Z</updated>
    <published>2021-01-28T21:01:43Z</published>
    <title>A causal convolutional neural network for multi-subject motion modeling
  and generation</title>
    <summary>  Inspired by the success of WaveNet in multi-subject speech synthesis, we
propose a novel neural network based on causal convolutions for multi-subject
motion modeling and generation. The network can capture the intrinsic
characteristics of the motion of different subjects, such as the influence of
skeleton scale variation on motion style. Moreover, after fine-tuning the
network using a small motion dataset for a novel skeleton that is not included
in the training dataset, it is able to synthesize high-quality motions with a
personalized style for the novel skeleton. The experimental results demonstrate
that our network can model the intrinsic characteristics of motions well and
can be applied to various motion modeling and synthesis tasks.
</summary>
    <author>
      <name>Shuaiying Hou</name>
    </author>
    <author>
      <name>Congyi Wang</name>
    </author>
    <author>
      <name>Wenlin Zhuang</name>
    </author>
    <author>
      <name>Yu Chen</name>
    </author>
    <author>
      <name>Yangang Wang</name>
    </author>
    <author>
      <name>Hujun Bao</name>
    </author>
    <author>
      <name>Jinxiang Chai</name>
    </author>
    <author>
      <name>Weiwei Xu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s41095-022-0307-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s41095-022-0307-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This preprint has not undergone peer review (when applicable) or any
  post-submission improvements or corrections. The Version of Record of this
  article is published in Computational Visual Media, and is available online
  at https://doi.org/10.1007/s41095-022-0307-3</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Visual Media 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.12276v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12276v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01694v1</id>
    <updated>2021-03-02T12:56:24Z</updated>
    <published>2021-03-02T12:56:24Z</published>
    <title>A Revisit of Shape Editing Techniques: from the Geometric to the Neural
  Viewpoint</title>
    <summary>  3D shape editing is widely used in a range of applications such as movie
production, computer games and computer aided design. It is also a popular
research topic in computer graphics and computer vision. In past decades,
researchers have developed a series of editing methods to make the editing
process faster, more robust, and more reliable. Traditionally, the deformed
shape is determined by the optimal transformation and weights for an energy
term. With increasing availability of 3D shapes on the Internet, data-driven
methods were proposed to improve the editing results. More recently as the deep
neural networks became popular, many deep learning based editing methods have
been developed in this field, which is naturally data-driven. We mainly survey
recent research works from the geometric viewpoint to those emerging neural
deformation techniques and categorize them into organic shape editing methods
and man-made model editing methods. Both traditional methods and recent neural
network based methods are reviewed.
</summary>
    <author>
      <name>Yu-Jie Yuan</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <author>
      <name>Tong Wu</name>
    </author>
    <author>
      <name>Lin Gao</name>
    </author>
    <author>
      <name>Ligang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2103.01694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01891v2</id>
    <updated>2021-08-18T07:26:51Z</updated>
    <published>2021-03-02T17:44:40Z</published>
    <title>Simulating deformable objects for computer animation: a numerical
  perspective</title>
    <summary>  We examine a variety of numerical methods that arise when considering
dynamical systems in the context of physics-based simulations of deformable
objects. Such problems arise in various applications, including animation,
robotics, control and fabrication. The goals and merits of suitable numerical
algorithms for these applications are different from those of typical numerical
analysis research in dynamical systems. Here the mathematical model is not
fixed a priori but must be adjusted as necessary to capture the desired
behaviour, with an emphasis on effectively producing lively animations of
objects with complex geometries. Results are often judged by how realistic they
appear to observers (by the "eye-norm") as well as by the efficacy of the
numerical procedures employed. And yet, we show that with an adjusted view
numerical analysis and applied mathematics can contribute significantly to the
development of appropriate methods and their analysis in a variety of areas
including finite element methods, stiff and highly oscillatory ODEs, model
reduction, and constrained optimization.
</summary>
    <author>
      <name>Uri M. Ascher</name>
    </author>
    <author>
      <name>Egor Larionov</name>
    </author>
    <author>
      <name>Seung Heon Sheen</name>
    </author>
    <author>
      <name>Dinesh K. Pai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01891v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01891v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02114v1</id>
    <updated>2021-03-03T01:15:21Z</updated>
    <published>2021-03-03T01:15:21Z</published>
    <title>A Computational Design and Evaluation Tool for 3D Structures with Planar
  Surfaces</title>
    <summary>  Three dimensional (3D) structures composed of planar surfaces can be build
out of accessible materials using easier fabrication technique with shorter
fabrication time. To better design 3D structures with planar surfaces,
realistic models are required to understand and evaluate mechanical behaviors.
Existing design tools are either effort-consuming (e.g. finite element
analysis) or bounded by assumptions (e.g. numerical solutions). In this
project, We have built a computational design tool that is (1) capable of
rapidly and inexpensively evaluating planar surfaces in 3D structures, with
sufficient computational efficiency and accuracy; (2) applicable to complex
boundary conditions and loading conditions, both isotropic materials and
orthotropic materials; and (3) suitable for rapid accommodation when design
parameters need to be adjusted. We demonstrate the efficiency and necessity of
this design tool by evaluating a glass table as well as a wood bookcase, and
iteratively designing an origami gripper to satisfy performance requirements.
This design tool gives non-expert users as well as engineers a simple and
effective modus operandi in structural design.
</summary>
    <author>
      <name>Chang Liu</name>
    </author>
    <author>
      <name>Wenzhong Yan</name>
    </author>
    <author>
      <name>Pehuen Moure</name>
    </author>
    <author>
      <name>Cody Fan</name>
    </author>
    <author>
      <name>Ankur Mehta</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02533v1</id>
    <updated>2021-03-03T17:17:32Z</updated>
    <published>2021-03-03T17:17:32Z</published>
    <title>Learning to Manipulate Amorphous Materials</title>
    <summary>  We present a method of training character manipulation of amorphous materials
such as those often used in cooking. Common examples of amorphous materials
include granular materials (salt, uncooked rice), fluids (honey), and
visco-plastic materials (sticky rice, softened butter). A typical task is to
spread a given material out across a flat surface using a tool such as a
scraper or knife. We use reinforcement learning to train our controllers to
manipulate materials in various ways. The training is performed in a physics
simulator that uses position-based dynamics of particles to simulate the
materials to be manipulated. The neural network control policy is given
observations of the material (e.g. a low-resolution density map), and the
policy outputs actions such as rotating and translating the knife. We
demonstrate policies that have been successfully trained to carry out the
following tasks: spreading, gathering, and flipping. We produce a final
animation by using inverse kinematics to guide a character's arm and hand to
match the motion of the manipulation tool such as a knife or a frying pan.
</summary>
    <author>
      <name>Yunbo Zhang</name>
    </author>
    <author>
      <name>Wenhao Yu</name>
    </author>
    <author>
      <name>C. Karen Liu</name>
    </author>
    <author>
      <name>Charles C. Kemp</name>
    </author>
    <author>
      <name>Greg Turk</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04183v2</id>
    <updated>2021-03-22T15:19:27Z</updated>
    <published>2021-03-06T19:51:07Z</published>
    <title>HexDom: Polycube-Based Hexahedral-Dominant Mesh Generation</title>
    <summary>  In this paper, we extend our earlier polycube-based all-hexahedral mesh
generation method to hexahedral-dominant mesh generation, and present the
HexDom software package. Given the boundary representation of a solid model,
HexDom creates a hex-dominant mesh by using a semi-automated polycube-based
mesh generation method. The resulting hexahedral dominant mesh includes
hexahedra, tetrahedra, and triangular prisms. By adding non-hexahedral
elements, we are able to generate better quality hexahedral elements than in
all-hexahedral meshes. We explain the underlying algorithms in four modules
including segmentation, polycube construction, hex-dominant mesh generation and
quality improvement, and use a rockerarm model to explain how to run the
software. We also apply our software to a number of other complex models to
test their robustness. The software package and all tested models are availabe
in github (https://github.com/CMU-CBML/HexDom).
</summary>
    <author>
      <name>Yuxuan Yu</name>
    </author>
    <author>
      <name>Jialei Ginny Liu</name>
    </author>
    <author>
      <name>Yongjie Jessica Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2011.14213</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.04183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.05640v1</id>
    <updated>2021-03-09T17:49:45Z</updated>
    <published>2021-03-09T17:49:45Z</published>
    <title>FlowMesher: An automatic unstructured mesh generation algorithm with
  applications from finite element analysis to medical simulations</title>
    <summary>  In this work, we propose an automatic mesh generation algorithm, FlowMesher,
which can be used to generate unstructured meshes for mesh domains in any shape
with minimum (or even no) user intervention. The approach can generate
high-quality simplex meshes directly from scanned images in OBJ format in 2D
and 3D or just from a line drawing in 2-D. Mesh grading can be easily
controlled also. The FlowMesher is robust and easy to be implemented and is
useful for a variety of applications including surgical simulators.
  The core idea of the FlowMesher is that a mesh domain is considered as an
"airtight container" into which fluid particles are "injected" at one or
multiple selected interior points. The particles repel each other and occupy
the whole domain somewhat like blowing up a balloon. When the container is full
of fluid particles and the flow is stopped, a Delaunay triangulation algorithm
is employed to link the fluid particles together to generate an unstructured
mesh (which is then optimized using a combination of automated mesh smoothing
and element removal in 3D). The performance of the FlowMesher is demonstrated
by generating meshes for several 2D and 3D mesh domains including a scanned
image of a bone.
</summary>
    <author>
      <name>Zhujiang Wang</name>
    </author>
    <author>
      <name>Arun R. Srinivasa</name>
    </author>
    <author>
      <name>J. N. Reddy</name>
    </author>
    <author>
      <name>Adam Dubrowski</name>
    </author>
    <link href="http://arxiv.org/abs/2103.05640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.05640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.08141v2</id>
    <updated>2021-03-16T02:34:24Z</updated>
    <published>2021-03-15T05:15:11Z</published>
    <title>Eigen Space of Mesh Distortion Energy Hessian</title>
    <summary>  Mesh distortion optimization is a popular research topic and has wide range
of applications in computer graphics, including geometry modeling, variational
shape interpolation, UV parameterization, elastoplastic simulation, etc. In
recent years, many solvers have been proposed to solve this nonlinear
optimization efficiently, among which projected Newton has been shown to have
best convergence rate and work well in both 2D and 3D applications. Traditional
Newton approach suffers from ill conditioning and indefiniteness of local
energy approximation. A crucial step in projected Newton is to fix this issue
by projecting energy Hessian onto symmetric positive definite (SPD) cone so as
to guarantee the search direction always pointing to decrease the energy
locally. Such step relies on time consuming eigen decomposition of element
Hessian, which has been addressed by several work before on how to obtain a
conjugacy that is as diagonal as possible. In this report, we demonstrate an
analytic form of Hessian eigen system for distortion energy defined using
principal stretches, which is the most general representation. Compared with
existing projected Newton diagonalization approaches, our formulation is more
general as it doesn't require the energy to be representable by tensor
invariants. In this report, we will only show the derivation for 3D and the
extension to 2D case is straightforward.
</summary>
    <author>
      <name>Yufeng Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.08141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.08141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.08275v1</id>
    <updated>2021-03-15T11:02:36Z</updated>
    <published>2021-03-15T11:02:36Z</published>
    <title>Automatic Generation of Large-scale 3D Road Networks based on GIS Data</title>
    <summary>  How to automatically generate a realistic large-scale 3D road network is a
key point for immersive and credible traffic simulations. Existing methods
cannot automatically generate various kinds of intersections in 3D space based
on GIS data. In this paper, we propose a method to generate complex and
large-scale 3D road networks automatically with the open source GIS data,
including satellite imagery, elevation data and two-dimensional(2D) road center
axis data, as input. We first introduce a semantic structure of road network to
obtain high-detailed and well-formed networks in a 3D scene. We then generate
2D shapes and topological data of the road network according to the semantic
structure and 2D road center axis data. At last, we segment the elevation data
and generate the surface of the 3D road network according to the 2D semantic
data and satellite imagery data. Results show that our method does well in the
generation of various types of intersections and the high-detailed features of
roads. The traffic semantic structure, which must be provided in traffic
simulation, can also be generated automatically according to our method.
</summary>
    <author>
      <name>Hua Wang</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Xu Han</name>
    </author>
    <author>
      <name>Mingliang Xu</name>
    </author>
    <author>
      <name>Weizhe Chen</name>
    </author>
    <author>
      <name>Guoliang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2103.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09583v1</id>
    <updated>2021-03-17T11:55:43Z</updated>
    <published>2021-03-17T11:55:43Z</published>
    <title>2D Points Curve Reconstruction Survey and Benchmark</title>
    <summary>  Curve reconstruction from unstructured points in a plane is a fundamental
problem with many applications that has generated research interest for
decades. Involved aspects like handling open, sharp, multiple and non-manifold
outlines, run-time and provability as well as potential extension to 3D for
surface reconstruction have led to many different algorithms. We survey the
literature on 2D curve reconstruction and then present an open-sourced
benchmark for the experimental study. Our unprecedented evaluation on a
selected set of planar curve reconstruction algorithms aims to give an overview
of both quantitative analysis and qualitative aspects for helping users to
select the right algorithm for specific problems in the field. Our benchmark
framework is available online to permit reproducing the results, and easy
integration of new algorithms.
</summary>
    <author>
      <name>Stefan Ohrhallinger</name>
    </author>
    <author>
      <name>Jiju Peethambaran</name>
    </author>
    <author>
      <name>Amal D. Parakkat</name>
    </author>
    <author>
      <name>Tamal K. Dey</name>
    </author>
    <author>
      <name>Ramanathan Muthuganapathy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 22 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.09583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10602v1</id>
    <updated>2021-03-19T02:53:00Z</updated>
    <published>2021-03-19T02:53:00Z</published>
    <title>HeterSkinNet: A Heterogeneous Network for Skin Weights Prediction</title>
    <summary>  Character rigging is universally needed in computer graphics but notoriously
laborious. We present a new method, HeterSkinNet, aiming to fully automate such
processes and significantly boost productivity. Given a character mesh and
skeleton as input, our method builds a heterogeneous graph that treats the mesh
vertices and the skeletal bones as nodes of different types and uses graph
convolutions to learn their relationships. To tackle the graph heterogeneity,
we propose a new graph network convolution operator that transfers information
between heterogeneous nodes. The convolution is based on a new distance
HollowDist that quantifies the relations between mesh vertices and bones. We
show that HeterSkinNet is robust for production characters by providing the
ability to incorporate meshes and skeletons with arbitrary topologies and
morphologies (e.g., out-of-body bones, disconnected mesh components, etc.).
Through exhaustive comparisons, we show that HeterSkinNet outperforms
state-of-the-art methods by large margins in terms of rigging accuracy and
naturalness. HeterSkinNet provides a solution for effective and robust
character rigging.
</summary>
    <author>
      <name>Xiaoyu Pan</name>
    </author>
    <author>
      <name>Jiancong Huang</name>
    </author>
    <author>
      <name>Jiaming Mai</name>
    </author>
    <author>
      <name>He Wang</name>
    </author>
    <author>
      <name>Honglin Li</name>
    </author>
    <author>
      <name>Tongkui Su</name>
    </author>
    <author>
      <name>Wenjun Wang</name>
    </author>
    <author>
      <name>Xiaogang Jin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3451262</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3451262" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I3D 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.11930v1</id>
    <updated>2021-03-22T15:17:33Z</updated>
    <published>2021-03-22T15:17:33Z</published>
    <title>Volumetric Procedural Models for Shape Representation</title>
    <summary>  This article describes a volumetric approach for procedural shape modeling
and a new Procedural Shape Modeling Language (PSML) that facilitates the
specification of these models. PSML provides programmers the ability to
describe shapes in terms of their 3D elements where each element may be a
semantic group of 3D objects, e.g., a brick wall, or an indivisible object,
e.g., an individual brick. Modeling shapes in this manner facilitates the
creation of models that more closely approximate the organization and structure
of their real-world counterparts. As such, users may query these models for
volumetric information such as the number, position, orientation and volume of
3D elements which cannot be provided using surface based model-building
techniques. PSML also provides a number of new language-specific capabilities
that allow for a rich variety of context-sensitive behaviors and
post-processing functions. These capabilities include an object-oriented
approach for model design, methods for querying the model for component-based
information and the ability to access model elements and components to perform
Boolean operations on the model parts. PSML is open-source and includes freely
available tutorial videos, demonstration code and an integrated development
environment to support writing PSML programs.
</summary>
    <author>
      <name>Andrew Willis</name>
    </author>
    <author>
      <name>Prashant Ganesh</name>
    </author>
    <author>
      <name>Kyle Volle</name>
    </author>
    <author>
      <name>Jincheng Zhang</name>
    </author>
    <author>
      <name>Kevin Brink</name>
    </author>
    <link href="http://arxiv.org/abs/2103.11930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.11930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14507v1</id>
    <updated>2021-03-26T15:05:06Z</updated>
    <published>2021-03-26T15:05:06Z</published>
    <title>AVATAR: Blender add-on for fast creation of 3D human models</title>
    <summary>  Create an articulated and realistic human 3D model is a complicated task, not
only get a model with the right body proportions but also to the whole process
of rigging the model with correct articulation points and vertices weights.
Having a tool that can create such a model with just a few clicks will be very
advantageous for amateurs developers to use in their projects, researchers to
easily generate datasets to train neural networks and industry for game
development. We present a software that is integrated in Blender in form of
add-on that allows us to design and animate a dressed 3D human models based on
Makehuman with just a few clicks. Moreover, as it is already integrated in
Blender, python scripts can be created to animate, render and further customize
the current available options.
</summary>
    <author>
      <name>Jordi Sanchez-Riera</name>
    </author>
    <author>
      <name>Aniol Civit</name>
    </author>
    <author>
      <name>Marta Altarriba</name>
    </author>
    <author>
      <name>Francesc Moreno-Noguer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, software description</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14870v2</id>
    <updated>2022-01-08T13:30:42Z</updated>
    <published>2021-03-27T10:04:36Z</published>
    <title>Remeshing-Free Graph-Based Finite Element Method for Ductile and Brittle
  Fracture</title>
    <summary>  Fracture produces new mesh fragments that introduce additional degrees of
freedom in the system dynamics. Existing finite element method (FEM) based
solutions suffer from an explosion in computational cost as the system matrix
size increases. We solve this problem by presenting a graph-based FEM model for
fracture simulation that is remeshing-free and easily scales to high-resolution
meshes. Our algorithm models fracture on the graph induced in a volumetric mesh
with tetrahedral elements. We relabel the edges of the graph using a computed
damage variable to initialize and propagate fracture. We prove that non-linear,
hyper-elastic strain energy is expressible entirely in terms of the edge
lengths of the induced graph. This allows us to reformulate the system dynamics
for the relabeled graph without changing the size of system dynamics matrix and
thus prevents the computational cost from blowing up. The fractured surface has
to be reconstructed explicitly only for visualization purposes. We simulate
standard laboratory experiments from structural mechanics and compare the
results with corresponding real-world experiments. We fracture objects made of
a variety of brittle and ductile materials, and show that our technique offers
stability and speed that is unmatched in current literature.
</summary>
    <author>
      <name>Avirup Mandal</name>
    </author>
    <author>
      <name>Parag Chaudhuri</name>
    </author>
    <author>
      <name>Subhasis Chaudhuri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14725</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14725" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15472v1</id>
    <updated>2021-03-29T10:07:54Z</updated>
    <published>2021-03-29T10:07:54Z</published>
    <title>View-Dependent Formulation of 2.5D Cartoon Models</title>
    <summary>  2.5D cartoon models are methods to simulate three-dimensional (3D)-like
movements, such as out-of-plane rotation, from two-dimensional (2D) shapes in
different views. However, cartoon objects and characters have several distorted
parts which do not correspond to any real 3D positions (e.g., Mickey Mouse's
ears), that implies that existing systems are not suitable for designing such
representations. Hence, we formulate it as a view-dependent deformation (VDD)
problem, which has been proposed in the field of 3D character animation. The
distortions in an arbitrary viewpoint are automatically obtained by blending
the user-specified 2D shapes of key views. This model is simple enough to
easily implement in an existing animation system. Several examples demonstrate
the robustness of our method over previous methods. In addition, we conduct a
user study and confirm that the proposed system is effective for animating
classic cartoon characters.
</summary>
    <author>
      <name>Tsukasa Fukusato</name>
    </author>
    <author>
      <name>Akinobu Maejima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.16807v1</id>
    <updated>2021-03-31T04:48:59Z</updated>
    <published>2021-03-31T04:48:59Z</published>
    <title>Learning and Exploring Motor Skills with Spacetime Bounds</title>
    <summary>  Equipping characters with diverse motor skills is the current bottleneck of
physics-based character animation. We propose a Deep Reinforcement Learning
(DRL) framework that enables physics-based characters to learn and explore
motor skills from reference motions. The key insight is to use loose space-time
constraints, termed spacetime bounds, to limit the search space in an early
termination fashion. As we only rely on the reference to specify loose
spacetime bounds, our learning is more robust with respect to low quality
references. Moreover, spacetime bounds are hard constraints that improve
learning of challenging motion segments, which can be ignored by imitation-only
learning. We compare our method with state-of-the-art tracking-based DRL
methods. We also show how to guide style exploration within the proposed
framework
</summary>
    <author>
      <name>Li-Ke Ma</name>
    </author>
    <author>
      <name>Zeshi Yang</name>
    </author>
    <author>
      <name>Xin Tong</name>
    </author>
    <author>
      <name>Baining Guo</name>
    </author>
    <author>
      <name>KangKang Yin</name>
    </author>
    <link href="http://arxiv.org/abs/2103.16807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.16807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00618v1</id>
    <updated>2021-04-01T16:49:59Z</updated>
    <published>2021-04-01T16:49:59Z</published>
    <title>Real-Time Global Illumination Using OpenGL And Voxel Cone Tracing</title>
    <summary>  Building systems capable of replicating global illumination models with
interactive frame-rates has long been one of the toughest conundrums facing
computer graphics researchers. Voxel Cone Tracing, as proposed by Cyril Crassin
et al. in 2011, makes use of mipmapped 3D textures containing a voxelized
representation of an environments direct light component to trace diffuse,
specular and occlusion cones in linear time to extrapolate a surface fragments
indirect light emitted towards a given photo-receptor. Seemingly providing a
well-disposed balance between performance and physical fidelity, this thesis
examines the algorithms theoretical side on the basis of the rendering equation
as well as its practical side in the context of a self-implemented,
OpenGL-based variant. Whether if it can compete with long standing alternatives
such as radiosity and raytracing will be determined in the subsequent
evaluation.
</summary>
    <author>
      <name>Benjamin Kahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">75 pages, 50 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.00618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00637v1</id>
    <updated>2021-04-01T17:35:40Z</updated>
    <published>2021-04-01T17:35:40Z</published>
    <title>Cortical Morphometry Analysis based on Worst Transportation Theory</title>
    <summary>  Biomarkers play an important role in early detection and intervention in
Alzheimer's disease (AD). However, obtaining effective biomarkers for AD is
still a big challenge. In this work, we propose to use the worst transportation
cost as a univariate biomarker to index cortical morphometry for tracking AD
progression. The worst transportation (WT) aims to find the least economical
way to transport one measure to the other, which contrasts to the optimal
transportation (OT) that finds the most economical way between measures. To
compute the WT cost, we generalize the Brenier theorem for the OT map to the WT
map, and show that the WT map is the gradient of a concave function satisfying
the Monge-Ampere equation. We also develop an efficient algorithm to compute
the WT map based on computational geometry. We apply the algorithm to analyze
cortical shape difference between dementia due to AD and normal aging
individuals. The experimental results reveal the effectiveness of our proposed
method which yields better statistical performance than other competiting
methods including the OT.
</summary>
    <author>
      <name>Min Zhang</name>
    </author>
    <author>
      <name>Dongsheng An</name>
    </author>
    <author>
      <name>Na Lei</name>
    </author>
    <author>
      <name>Jianfeng Wu</name>
    </author>
    <author>
      <name>Tong Zhao</name>
    </author>
    <author>
      <name>Xiaoyin Xu</name>
    </author>
    <author>
      <name>Yalin Wang</name>
    </author>
    <author>
      <name>Xianfeng Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IPMI 2021, 13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.00637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02789v1</id>
    <updated>2021-04-06T21:22:22Z</updated>
    <published>2021-04-06T21:22:22Z</published>
    <title>NeuMIP: Multi-Resolution Neural Materials</title>
    <summary>  We propose NeuMIP, a neural method for representing and rendering a variety
of material appearances at different scales. Classical prefiltering
(mipmapping) methods work well on simple material properties such as diffuse
color, but fail to generalize to normals, self-shadowing, fibers or more
complex microstructures and reflectances. In this work, we generalize
traditional mipmap pyramids to pyramids of neural textures, combined with a
fully connected network. We also introduce neural offsets, a novel method which
allows rendering materials with intricate parallax effects without any
tessellation. This generalizes classical parallax mapping, but is trained
without supervision by any explicit heightfield. Neural materials within our
system support a 7-dimensional query, including position, incoming and outgoing
direction, and the desired filter kernel size. The materials have small storage
(on the order of standard mipmapping except with more texture channels), and
can be integrated within common Monte-Carlo path tracing systems. We
demonstrate our method on a variety of materials, resulting in complex
appearance across levels of detail, with accurate parallax, self-shadowing, and
other effects.
</summary>
    <author>
      <name>Alexandr Kuznetsov</name>
    </author>
    <author>
      <name>Krishna Mullia</name>
    </author>
    <author>
      <name>Zexiang Xu</name>
    </author>
    <author>
      <name>Miloš Hašan</name>
    </author>
    <author>
      <name>Ravi Ramamoorthi</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04387v1</id>
    <updated>2021-04-09T14:21:06Z</updated>
    <published>2021-04-09T14:21:06Z</published>
    <title>Real-time visio-haptic interaction with static soft tissue model shaving
  geometric and material nonlinearity</title>
    <summary>  We propose a new approach allowing visio-haptic interaction with a FE model
of a human liver having both non-linear geometric and material properties. The
material properties used in the model are extracted from the experimental data
of pig liver to make the simulations more realistic. Our computational approach
consists of two main steps: a pre-computation of the configuration space of all
possible deformation states of the model, followed by the interpolation of the
precomputed data for the calculation of the reaction forces displayed to the
user through a haptic device during the real-time interactions. No a priori
assumptions or modeling simplifications about the mathematical complexity of
the underlying soft tissue model, size and irregularity of the FE mesh are
necessary. We show that deformation and force response of the liver in
simulations are heavily influenced by the material model, boundary conditions,
path of the loading and the type of function used for the interpolation of the
pre-computed data.
</summary>
    <author>
      <name>Igor Peterlik</name>
    </author>
    <author>
      <name>Mert Sedef</name>
    </author>
    <author>
      <name>Cagatay Basdogan</name>
    </author>
    <author>
      <name>Ludek Matyska</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cag.2009.10.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cag.2009.10.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Graphics, 2010, Vol. 34, No.1, pp. 43-54</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.04387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04934v1</id>
    <updated>2021-04-11T06:49:29Z</updated>
    <published>2021-04-11T06:49:29Z</published>
    <title>Velocity Skinning for Real-time Stylized Skeletal Animation</title>
    <summary>  Secondary animation effects are essential for liveliness. We propose a
simple, real-time solution for adding them on top of standard skinning,
enabling artist-driven stylization of skeletal motion. Our method takes a
standard skeleton animation as input, along with a skin mesh and rig weights.
It then derives per-vertex deformations from the different linear and angular
velocities along the skeletal hierarchy. We highlight two specific applications
of this general framework, namely the cartoonlike "squashy" and "floppy"
effects, achieved from specific combinations of velocity terms. As our results
show, combining these effects enables to mimic, enhance and stylize
physical-looking behaviours within a standard animation pipeline, for arbitrary
skinned characters. Interactive on CPU, our method allows for GPU
implementation, yielding real-time performances even on large meshes. Animator
control is supported through a simple interface toolkit, enabling to refine the
desired type and magnitude of deformation at relevant vertices by simply
painting weights. The resulting rigged character automatically responds to new
skeletal animation, without further input.
</summary>
    <author>
      <name>Damien Rohmer</name>
    </author>
    <author>
      <name>Marco Tarini</name>
    </author>
    <author>
      <name>Niranjan Kalyanasundaram</name>
    </author>
    <author>
      <name>Faezeh Moshfeghifar</name>
    </author>
    <author>
      <name>Marie-Paule Cani</name>
    </author>
    <author>
      <name>Victor Zordan</name>
    </author>
    <link href="http://arxiv.org/abs/2104.04934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05281v1</id>
    <updated>2021-04-12T08:23:24Z</updated>
    <published>2021-04-12T08:23:24Z</published>
    <title>Shapes In A Box -- Disassembling 3D objects for efficient packing and
  fabrication</title>
    <summary>  Modern 3D printing technologies and the upcoming mass-customization paradigm
call for efficient methods to produce and distribute arbitrarily-shaped 3D
objects. This paper introduces an original algorithm to split a 3D model in
parts that can be efficiently packed within a box, with the objective of
reassembling them after delivery. The first step consists in the creation of a
hierarchy of possible parts that can be tightly packed within their minimum
bounding boxes. In a second step, the hierarchy is exploited to extract the
(single) segmentation whose parts can be most tightly packed. The fact that
shape packing is an NP-complete problem justifies the use of heuristics and
approximated solutions whose efficacy and efficiency must be assessed.
Extensive experimentation demonstrates that our algorithm produces satisfactory
results for arbitrarily-shaped objects while being comparable to ad-hoc methods
when specific shapes are considered.
</summary>
    <author>
      <name>Marco Attene</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.12608</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.12608" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.05281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10622v3</id>
    <updated>2021-04-23T16:55:39Z</updated>
    <published>2021-04-21T16:31:49Z</published>
    <title>Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud</title>
    <summary>  Mesh reconstruction from a 3D point cloud is an important topic in the fields
of computer graphic, computer vision, and multimedia analysis. In this paper,
we propose a voxel structure-based mesh reconstruction framework. It provides
the intrinsic metric to improve the accuracy of local region detection. Based
on the detected local regions, an initial reconstructed mesh can be obtained.
With the mesh optimization in our framework, the initial reconstructed mesh is
optimized into an isotropic one with the important geometric features such as
external and internal edges. The experimental results indicate that our
framework shows great advantages over peer ones in terms of mesh quality,
geometric feature keeping, and processing speed.
</summary>
    <author>
      <name>Chenlei Lv</name>
    </author>
    <author>
      <name>Weisi Lin</name>
    </author>
    <author>
      <name>Baoquan Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2021.3073265</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2021.3073265" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Multimedia</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10622v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10622v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65L50 (Primary) 58E10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10898v2</id>
    <updated>2021-04-26T08:13:48Z</updated>
    <published>2021-04-22T07:07:52Z</published>
    <title>Soft Walks: Real-Time, Two-Ways Interaction between a Character and
  Loose Grounds</title>
    <summary>  When walking on loose terrains, possibly covered with vegetation, the ground
and grass should deform, but the character's gait should also change
accordingly. We propose a method for modeling such two-ways interactions in
real-time. We first complement a layered character model by a high-level
controller, which uses position and angular velocity inputs to improve dynamic
oscillations when walking on various slopes. Secondly, at a refined level, the
feet are set to locally deform the ground and surrounding vegetation using
efficient procedural functions, while the character's response to such
deformations is computed through adapted inverse kinematics. While simple to
set up, our method is generic enough to adapt to any character morphology.
Moreover, its ability to generate in real time, consistent gaits on a variety
of loose grounds of arbitrary slope, possibly covered with grass, makes it an
interesting solution to enhance films and games.
</summary>
    <author>
      <name>Chloé Paliard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Eduardo Alvarado</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX</arxiv:affiliation>
    </author>
    <author>
      <name>Damien Rohmer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX</arxiv:affiliation>
    </author>
    <author>
      <name>Marie-Paule Cani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics (Short), May 2021, Vienna, Austria</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.10898v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10898v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11993v2</id>
    <updated>2021-07-11T00:33:21Z</updated>
    <published>2021-04-24T18:05:11Z</published>
    <title>Normal-Driven Spherical Shape Analogies</title>
    <summary>  This paper introduces a new method to stylize 3D geometry. The key
observation is that the surface normal is an effective instrument to capture
different geometric styles. Centered around this observation, we cast
stylization as a shape analogy problem, where the analogy relationship is
defined on the surface normal. This formulation can deform a 3D shape into
different styles within a single framework. One can plug-and-play different
target styles by providing an exemplar shape or an energy-based style
description (e.g., developable surfaces). Our surface stylization methodology
enables Normal Captures as a geometric counterpart to material captures
(MatCaps) used in rendering, and the prototypical concept of Spherical Shape
Analogies as a geometric counterpart to image analogies in image processing.
</summary>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics Symposium on Geometry Processing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Graphics Forum 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.11993v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11993v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12297v1</id>
    <updated>2021-04-26T00:56:37Z</updated>
    <published>2021-04-26T00:56:37Z</published>
    <title>dualFace:Two-Stage Drawing Guidance for Freehand Portrait Sketching</title>
    <summary>  In this paper, we propose dualFace, a portrait drawing interface to assist
users with different levels of drawing skills to complete recognizable and
authentic face sketches. dualFace consists of two-stage drawing assistance to
provide global and local visual guidance: global guidance, which helps users
draw contour lines of portraits (i.e., geometric structure), and local
guidance, which helps users draws details of facial parts (which conform to
user-drawn contour lines), inspired by traditional artist workflows in portrait
drawing. In the stage of global guidance, the user draws several contour lines,
and dualFace then searches several relevant images from an internal database
and displays the suggested face contour lines over the background of the
canvas. In the stage of local guidance, we synthesize detailed portrait images
with a deep generative model from user-drawn contour lines, but use the
synthesized results as detailed drawing guidance. We conducted a user study to
verify the effectiveness of dualFace, and we confirmed that dualFace
significantly helps achieve a detailed portrait sketch. see
http://www.jaist.ac.jp/~xie/dualface.html
</summary>
    <author>
      <name>Zhengyu Huang</name>
    </author>
    <author>
      <name>Yichen Peng</name>
    </author>
    <author>
      <name>Tomohiro Hibino</name>
    </author>
    <author>
      <name>Chunqi Zhao</name>
    </author>
    <author>
      <name>Haoran Xie</name>
    </author>
    <author>
      <name>Tsukasa Fukusato</name>
    </author>
    <author>
      <name>Kazunori Miyata</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s41095-021-0227-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s41095-021-0227-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the Journal of Computational Visual Media Conference
  2021. 13 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12365v1</id>
    <updated>2021-04-26T06:46:36Z</updated>
    <published>2021-04-26T06:46:36Z</published>
    <title>Efficient Hyperparameter Optimization for Physics-based Character
  Animation</title>
    <summary>  Physics-based character animation has seen significant advances in recent
years with the adoption of Deep Reinforcement Learning (DRL). However,
DRL-based learning methods are usually computationally expensive and their
performance crucially depends on the choice of hyperparameters. Tuning
hyperparameters for these methods often requires repetitive training of control
policies, which is even more computationally prohibitive. In this work, we
propose a novel Curriculum-based Multi-Fidelity Bayesian Optimization framework
(CMFBO) for efficient hyperparameter optimization of DRL-based character
control systems. Using curriculum-based task difficulty as fidelity criterion,
our method improves searching efficiency by gradually pruning search space
through evaluation on easier motor skill tasks. We evaluate our method on two
physics-based character control tasks: character morphology optimization and
hyperparameter tuning of DeepMimic. Our algorithm significantly outperforms
state-of-the-art hyperparameter optimization methods applicable for
physics-based character animation. In particular, we show that hyperparameters
optimized through our algorithm result in at least 5x efficiency gain comparing
to author-released settings in DeepMimic.
</summary>
    <author>
      <name>Zeshi Yang</name>
    </author>
    <author>
      <name>Zhiqi Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in ACM SIGGRAPH Symposium on Interactive 3D Graphics and
  Games 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13755v2</id>
    <updated>2021-05-04T15:24:44Z</updated>
    <published>2021-04-28T13:35:32Z</published>
    <title>Surface Multigrid via Intrinsic Prolongation</title>
    <summary>  This paper introduces a novel geometric multigrid solver for unstructured
curved surfaces. Multigrid methods are highly efficient iterative methods for
solving systems of linear equations. Despite the success in solving problems
defined on structured domains, generalizing multigrid to unstructured curved
domains remains a challenging problem. The critical missing ingredient is a
prolongation operator to transfer functions across different multigrid levels.
We propose a novel method for computing the prolongation for triangulated
surfaces based on intrinsic geometry, enabling an efficient geometric multigrid
solver for curved surfaces. Our surface multigrid solver achieves better
convergence than existing multigrid methods. Compared to direct solvers, our
solver is orders of magnitude faster. We evaluate our method on many geometry
processing applications and a wide variety of complex shapes with and without
boundaries. By simply replacing the direct solver, we upgrade existing
algorithms to interactive frame rates, and shift the computational bottleneck
away from solving linear systems.
</summary>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Jiayi Eris Zhang</name>
    </author>
    <author>
      <name>Mirela Ben-Chen</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450626.3459768</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450626.3459768" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 27 figures, SIGGRAPH 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph., Vol. 40, No. 4, Article 80. Publication date:
  August 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.13755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14667v1</id>
    <updated>2021-04-29T21:28:40Z</updated>
    <published>2021-04-29T21:28:40Z</published>
    <title>Efficacy of Images Versus Data Buffers: Optimizing Interactive
  Applications Utilizing OpenCL for Scientific Visualization</title>
    <summary>  This paper examines an algorithm using dual OpenCL image buffers to optimize
data streaming for ensemble processing and visualization. Image buffers were
utilized because they allow cached memory access, unlike simple data buffers,
which are more commonly used. OpenCL image object performance was improved by
allowing upload and mapping into one buffer to occur concurrently with mapping
and/or processing of data in another buffer. This technique was applied in an
interactive application allowing multiple flood extent maps to be combined into
a single image, and allowing users to vary input image sets in real time. The
efficiency of this technique was tested by varying both dimensions of input
images and number of iterations; computation scaled linearly with number of
input images, with best results achieved using ~4k images. Tests were performed
to determine the rate at which data could be moved from data buffers to image
buffers, examining a large range of possible image buffer dimensions.
Additional tests examined kernel runtimes with different image and buffer
variants. Limitations of the algorithm and possible applications are discussed.
</summary>
    <author>
      <name>Donald W. Johnson</name>
    </author>
    <author>
      <name>T. J. Jankun-Kelly</name>
    </author>
    <link href="http://arxiv.org/abs/2104.14667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01604v1</id>
    <updated>2021-05-04T16:25:36Z</updated>
    <published>2021-05-04T16:25:36Z</published>
    <title>Orienting Point Clouds with Dipole Propagation</title>
    <summary>  Establishing a consistent normal orientation for point clouds is a
notoriously difficult problem in geometry processing, requiring attention to
both local and global shape characteristics. The normal direction of a point is
a function of the local surface neighborhood; yet, point clouds do not disclose
the full underlying surface structure. Even assuming known geodesic proximity,
calculating a consistent normal orientation requires the global context. In
this work, we introduce a novel approach for establishing a globally consistent
normal orientation for point clouds. Our solution separates the local and
global components into two different sub-problems. In the local phase, we train
a neural network to learn a coherent normal direction per patch (i.e.,
consistently oriented normals within a single patch). In the global phase, we
propagate the orientation across all coherent patches using a dipole
propagation. Our dipole propagation decides to orient each patch using the
electric field defined by all previously orientated patches. This gives rise to
a global propagation that is stable, as well as being robust to nearby
surfaces, holes, sharp features and noise.
</summary>
    <author>
      <name>Gal Metzer</name>
    </author>
    <author>
      <name>Rana Hanocka</name>
    </author>
    <author>
      <name>Denis Zorin</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Daniele Panozzo</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450626.3459835</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450626.3459835" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.01604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02475v1</id>
    <updated>2021-05-06T07:13:59Z</updated>
    <published>2021-05-06T07:13:59Z</published>
    <title>A Practical Ply-Based Appearance Modeling for Knitted Fabrics</title>
    <summary>  Modeling the geometry and the appearance of knitted fabrics has been
challenging due to their complex geometries and interactions with light.
  Previous surface-based models have difficulties capturing fine-grained knit
geometries; Micro-appearance models, on the other hands, typically store
individual cloth fibers explicitly and are expensive to be generated and
rendered.
  Further, neither of the models have been matched the photographs to capture
both the reflection and the transmission of light simultaneously.
  In this paper, we introduce an efficient technique to generate knit models
with user-specified knitting patterns.
  Our model stores individual knit plies with fiber-level detailed depicted
using normal and tangent mapping.
  We evaluate our generated models using a wide array of knitting patterns.
Further, we compare qualitatively renderings to our models to photos of real
samples.
</summary>
    <author>
      <name>Zahra Montazeri</name>
    </author>
    <author>
      <name>Soren Gammelmark</name>
    </author>
    <author>
      <name>Henrik W. Jensen</name>
    </author>
    <author>
      <name>Shuang Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2105.02475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02507v1</id>
    <updated>2021-05-06T08:16:43Z</updated>
    <published>2021-05-06T08:16:43Z</published>
    <title>PH-CPF: Planar Hexagonal Meshing using Coordinate Power Fields</title>
    <summary>  We present a new approach for computing planar hexagonal meshes that
approximate a given surface, represented as a triangle mesh. Our method is
based on two novel technical contributions. First, we introduce Coordinate
Power Fields, which are a pair of tangent vector fields on the surface that
fulfill a certain continuity constraint. We prove that the fulfillment of this
constraint guarantees the existence of a seamless parameterization with
quantized rotational jumps, which we then use to regularly remesh the surface.
We additionally propose an optimization framework for finding Coordinate Power
Fields, which also fulfill additional constraints, such as alignment, sizing
and bijectivity. Second, we build upon this framework to address a challenging
meshing problem: planar hexagonal meshing. To this end, we suggest a
combination of conjugacy, scaling and alignment constraints, which together
lead to planarizable hexagons. We demonstrate our approach on a variety of
surfaces, automatically generating planar hexagonal meshes on complicated
meshes, which were not achievable with existing methods.
</summary>
    <author>
      <name>Kacper Pluta</name>
    </author>
    <author>
      <name>Michal Edelstein</name>
    </author>
    <author>
      <name>Amir Vaxman</name>
    </author>
    <author>
      <name>Mirela Ben-Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450626.3459770</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450626.3459770" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages (excluding supplementary material), 25 figures, SIGGRAPH
  2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph., Vol. 40, No. 4, Article 156. Publication date:
  August 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.02507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06858v3</id>
    <updated>2021-10-05T09:00:56Z</updated>
    <published>2021-05-14T14:32:08Z</published>
    <title>Fit4CAD: A point cloud benchmark for fitting simple geometric primitives
  in CAD objects</title>
    <summary>  We propose Fit4CAD, a benchmark for the evaluation and comparison of methods
for fitting simple geometric primitives in point clouds representing CAD
objects. This benchmark is meant to help both method developers and those who
want to identify the best performing tools. The Fit4CAD dataset is composed by
225 high quality point clouds, each of which has been obtained by sampling a
CAD object. The way these elements were created by using existing platforms and
datasets makes the benchmark easily expandable. The dataset is already split
into a training set and a test set. To assess performance and accuracy of the
different primitive fitting methods, various measures are defined. To
demonstrate the effective use of Fit4CAD, we have tested it on two methods
belonging to two different categories of approaches to the primitive fitting
problem: a clustering method based on a primitive growing framework and a
parametric method based on the Hough transform.
</summary>
    <author>
      <name>Chiara Romanengo</name>
    </author>
    <author>
      <name>Andrea Raffo</name>
    </author>
    <author>
      <name>Yifan Qie</name>
    </author>
    <author>
      <name>Nabil Anwer</name>
    </author>
    <author>
      <name>Bianca Falcidieno</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cag.2021.09.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cag.2021.09.013" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Graphics 102 (2022) 133-143</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.06858v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06858v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.6; I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09476v1</id>
    <updated>2021-05-19T16:53:42Z</updated>
    <published>2021-05-19T16:53:42Z</published>
    <title>Projection matrices and related viewing frustums: new ways to create and
  apply</title>
    <summary>  In computer graphics, the field of view of a camera is represented by a
viewing frustum and a corresponding projection matrix, the properties of which,
in the absence of restrictions on rectangular shape of the near plane and its
parallelism to the far plane are currently not fully explored and structured.
This study aims to consider the properties of arbitrary affine frustums, as
well as various techniques for their transformation for practical use in
devices with limited resources. Additionally, this article explores the methods
of working with the visible volume as an arbitrary frustum that is not
associated with the projection matrix. To study the properties of affine
frustums, the dependencies between its planes and formulas for obtaining key
points from the inverse projection matrix were derived. Methods of constructing
frustum by key points and given planes were also considered. Moreover, frustum
transformation formulas were obtained to simulate the effects of reflection,
refraction and cropping in devices with limited resources. In conclusion, a
method was proposed for applying an arbitrary frustum, which does not have a
corresponding projection matrix, to limit the visible volume and then transform
the points into NDC space.
</summary>
    <author>
      <name>Nikita Glushkov</name>
    </author>
    <author>
      <name>Tyuleneva Emiliya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14N05 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12319v2</id>
    <updated>2021-10-10T00:20:01Z</updated>
    <published>2021-05-26T04:10:00Z</published>
    <title>Neural Radiosity</title>
    <summary>  We introduce Neural Radiosity, an algorithm to solve the rendering equation
by minimizing the norm of its residual similar as in traditional radiosity
techniques. Traditional basis functions used in radiosity techniques, such as
piecewise polynomials or meshless basis functions are typically limited to
representing isotropic scattering from diffuse surfaces. Instead, we propose to
leverage neural networks to represent the full four-dimensional radiance
distribution, directly optimizing network parameters to minimize the norm of
the residual. Our approach decouples solving the rendering equation from
rendering (perspective) images similar as in traditional radiosity techniques,
and allows us to efficiently synthesize arbitrary views of a scene. In
addition, we propose a network architecture using geometric learnable features
that improves convergence of our solver compared to previous techniques. Our
approach leads to an algorithm that is simple to implement, and we demonstrate
its effectiveness on a variety of scenes with non-diffuse surfaces.
</summary>
    <author>
      <name>Saeed Hadadan</name>
    </author>
    <author>
      <name>Shuhong Chen</name>
    </author>
    <author>
      <name>Matthias Zwicker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3478513.3480569</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3478513.3480569" rel="related"/>
    <link href="http://arxiv.org/abs/2105.12319v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12319v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12620v2</id>
    <updated>2021-05-28T13:28:48Z</updated>
    <published>2021-05-26T15:19:07Z</published>
    <title>Lessons Learned and Improvements when Building Screen-Space Samplers
  with Blue-Noise Error Distribution</title>
    <summary>  Recent work has shown that the error of Monte-Carlo rendering is visually
more acceptable when distributed as blue-noise in screen-space. Despite recent
efforts, building a screen-space sampler is still an open problem. In this
talk, we present the lessons we learned while improving our previous
screen-space sampler. Specifically: we advocate for a new criterion to assess
the quality of such samplers; we introduce a new screen-space sampler based on
rank-1 lattices; we provide a parallel optimization method that is compatible
with a GPU implementation and that achieves better quality; we detail the
pitfalls of using such samplers in renderers and how to cope with many
dimensions; and we provide empirical proofs of the versatility of the
optimization process.
</summary>
    <author>
      <name>Laurent Belcour</name>
    </author>
    <author>
      <name>Eric Heitz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450623.3464645</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450623.3464645" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.12620v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12620v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13240v2</id>
    <updated>2022-03-10T19:57:06Z</updated>
    <published>2021-05-27T15:38:14Z</published>
    <title>Local Latent Representation based on Geometric Convolution for Particle
  Data Feature Exploration</title>
    <summary>  Feature related particle data analysis plays an important role in many
scientific applications such as fluid simulations, cosmology simulations and
molecular dynamics. Compared to conventional methods that use hand-crafted
feature descriptors, some recent studies focus on transforming the data into a
new latent space, where features are easier to be identified, compared and
extracted. However, it is challenging to transform particle data into latent
representations, since the convolution neural networks used in prior studies
require the data presented in regular grids. In this paper, we adopt Geometric
Convolution, a neural network building block designed for 3D point clouds, to
create latent representations for scientific particle data. These latent
representations capture both the particle positions and their physical
attributes in the local neighborhood so that features can be extracted by
clustering in the latent space, and tracked by applying tracking algorithms
such as mean-shift. We validate the extracted features and tracking results
from our approach using datasets from three applications and show that they are
comparable to the methods that define hand-crafted features for each specific
dataset.
</summary>
    <author>
      <name>Haoyu Li</name>
    </author>
    <author>
      <name>Han-Wei Shen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2022.3159114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2022.3159114" rel="related"/>
    <link href="http://arxiv.org/abs/2105.13240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13277v1</id>
    <updated>2021-05-27T16:22:44Z</updated>
    <published>2021-05-27T16:22:44Z</published>
    <title>MeshCNN Fundamentals: Geometric Learning through a Reconstructable
  Representation</title>
    <summary>  Mesh-based learning is one of the popular approaches nowadays to learn
shapes. The most established backbone in this field is MeshCNN. In this paper,
we propose infusing MeshCNN with geometric reasoning to achieve higher quality
learning. Through careful analysis of the way geometry is represented
through-out the network, we submit that this representation should be rigid
motion invariant, and should allow reconstructing the original geometry.
Accordingly, we introduce the first and second fundamental forms as an
edge-centric, rotation and translation invariant, reconstructable
representation. In addition, we update the originally proposed pooling scheme
to be more geometrically driven. We validate our analysis through
experimentation, and present consistent improvement upon the MeshCNN baseline,
as well as other more elaborate state-of-the-art architectures. Furthermore, we
demonstrate this fundamental forms-based representation opens the door to
accessible generative machine learning over meshes.
</summary>
    <author>
      <name>Amir Barda</name>
    </author>
    <author>
      <name>Yotam Erel</name>
    </author>
    <author>
      <name>Amit H. Bermano</name>
    </author>
    <link href="http://arxiv.org/abs/2105.13277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.14548v2</id>
    <updated>2022-02-21T10:59:02Z</updated>
    <published>2021-05-30T13:58:24Z</published>
    <title>Z2P: Instant Visualization of Point Clouds</title>
    <summary>  We present a technique for visualizing point clouds using a neural network.
Our technique allows for an instant preview of any point cloud, and bypasses
the notoriously difficult surface reconstruction problem or the need to
estimate oriented normals for splat-based rendering. We cast the preview
problem as a conditional image-to-image translation task, and design a neural
network that translates point depth-map directly into an image, where the point
cloud is visualized as though a surface was reconstructed from it. Furthermore,
the resulting appearance of the visualized point cloud can be, optionally,
conditioned on simple control variables (e.g., color and light). We demonstrate
that our technique instantly produces plausible images, and can, on-the-fly
effectively handle noise, non-uniform sampling, and thin surfaces sheets.
</summary>
    <author>
      <name>Gal Metzer</name>
    </author>
    <author>
      <name>Rana Hanocka</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eurographics 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.14548v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14548v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.01326v2</id>
    <updated>2021-07-21T08:35:30Z</updated>
    <published>2021-06-02T17:38:22Z</published>
    <title>Robust Voxelization and Visualization by Improved Tetrahedral Mesh
  Generation</title>
    <summary>  When obtaining interior 3D voxel data from triangular meshes, most existing
methods fail to handle low quality meshes which happens to take up a big
portion on the internet. In this work we present a robust voxelization method
that is based on tetrahedral mesh generation within a user defined error bound.
Comparing to other tetrahedral mesh generation methods, our method produces
much higher quality tetrahedral meshes as the intermediate outcome, which
allows us to utilize a faster voxelization algorithm that is based on a
stronger assumption. We show the results comparing to various methods including
the state-of-the-art. Our contribution includes a framework which takes
triangular mesh as an input and produces voxelized data, a proof to an unproved
algorithm that performs better than the state-of-the-art, and various
experiments including parallelization built on the GPU and CPU. We further
tested our method on various dataset including Princeton ModelNet and Thingi10k
to show the robustness of the framework, where near 100% availability is
achieved, while others can only achieve around 50%.
</summary>
    <author>
      <name>Joseph Chen</name>
    </author>
    <author>
      <name>Ko-Wei Tai</name>
    </author>
    <author>
      <name>Wen-Chin Chen</name>
    </author>
    <author>
      <name>Ming Ouhyoung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.01326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.03804v1</id>
    <updated>2021-06-07T17:15:38Z</updated>
    <published>2021-06-07T17:15:38Z</published>
    <title>Deep Medial Fields</title>
    <summary>  Implicit representations of geometry, such as occupancy fields or signed
distance fields (SDF), have recently re-gained popularity in encoding 3D solid
shape in a functional form. In this work, we introduce medial fields: a field
function derived from the medial axis transform (MAT) that makes available
information about the underlying 3D geometry that is immediately useful for a
number of downstream tasks. In particular, the medial field encodes the local
thickness of a 3D shape, and enables O(1) projection of a query point onto the
medial axis. To construct the medial field we require nothing but the SDF of
the shape itself, thus allowing its straightforward incorporation in any
application that relies on signed distance fields. Working in unison with the
O(1) surface projection supported by the SDF, the medial field opens the door
for an entirely new set of efficient, shape-aware operations on implicit
representations. We present three such applications, including a modification
to sphere tracing that renders implicit representations with better convergence
properties, a fast construction method for memory-efficient rigid-body
collision proxies, and an efficient approximation of ambient occlusion that
remains stable with respect to viewpoint variations.
</summary>
    <author>
      <name>Daniel Rebain</name>
    </author>
    <author>
      <name>Ke Li</name>
    </author>
    <author>
      <name>Vincent Sitzmann</name>
    </author>
    <author>
      <name>Soroosh Yazdani</name>
    </author>
    <author>
      <name>Kwang Moo Yi</name>
    </author>
    <author>
      <name>Andrea Tagliasacchi</name>
    </author>
    <link href="http://arxiv.org/abs/2106.03804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.03804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04521v9</id>
    <updated>2024-08-10T19:56:27Z</updated>
    <published>2021-06-08T16:59:19Z</published>
    <title>An App for the Discovery of Properties of Poncelet Triangles</title>
    <summary>  We describe a newly-developed, free, browser-based application, for the
interactive exploration of the dynamic geometry of Poncelet families of
triangles. The main focus is on responsive display of the beauteous loci of
centers of such families, refreshing them smoothly upon any changes in
simulation parameters. The app informs the user when curves swept are conics
and reports if certain metric quantities are conserved. Live simulations can be
easily shared via a URL. A list of more than 400 pre-made experiments is
included which can be regarded as conjectures and/or exercises. Millions of
experiment combinations are possible.
</summary>
    <author>
      <name>Iverton Darlan</name>
    </author>
    <author>
      <name>Dan Reznik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 30 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.04521v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04521v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A72, 00A08, 37-04, 37M05, 51M04, 51N20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.05143v1</id>
    <updated>2021-06-09T15:36:23Z</updated>
    <published>2021-06-09T15:36:23Z</published>
    <title>Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent
  Resolution of Particle-Based Liquids</title>
    <summary>  We present a novel up-resing technique for generating high-resolution liquids
based on scene flow estimation using deep neural networks. Our approach infers
and synthesizes small- and large-scale details solely from a low-resolution
particle-based liquid simulation. The proposed network leverages neighborhood
contributions to encode inherent liquid properties throughout convolutions. We
also propose a particle-based approach to interpolate between liquids generated
from varying simulation discretizations using a state-of-the-art bidirectional
optical flow solver method for fluids in addition to a novel key-event
topological alignment constraint. In conjunction with the neighborhood
contributions, our loss formulation allows the inference model throughout
epochs to reward important differences in regard to significant gaps in
simulation discretizations. Even when applied in an untested simulation setup,
our approach is able to generate plausible high-resolution details. Using this
interpolation approach and the predicted displacements, our approach combines
the input liquid properties with the predicted motion to infer semi-Lagrangian
advection. We furthermore showcase how the proposed interpolation approach can
facilitate generating large simulation datasets with a subset of initial
condition parameters.
</summary>
    <author>
      <name>Bruno Roy</name>
    </author>
    <author>
      <name>Pierre Poulin</name>
    </author>
    <author>
      <name>Eric Paquette</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3480147</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3480147" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 18 figures, and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.05143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.05161v1</id>
    <updated>2021-06-09T16:08:56Z</updated>
    <published>2021-06-09T16:08:56Z</published>
    <title>Interactive Modelling of Volumetric Musculoskeletal Anatomy</title>
    <summary>  We present a new approach for modelling musculoskeletal anatomy. Unlike
previous methods, we do not model individual muscle shapes as geometric
primitives (polygonal meshes, NURBS etc.). Instead, we adopt a volumetric
segmentation approach where every point in our volume is assigned to a muscle,
fat, or bone tissue. We provide an interactive modelling tool where the user
controls the segmentation via muscle curves and we visualize the muscle shapes
using volumetric rendering. Muscle curves enable intuitive yet powerful control
over the muscle shapes. This representation allows us to automatically handle
intersections between different tissues (musclemuscle, muscle-bone, and
muscle-skin) during the modelling and automates computation of muscle fiber
fields. We further introduce a novel algorithm for converting the volumetric
muscle representation into tetrahedral or surface geometry for use in
downstream tasks. Additionally, we introduce an interactive skeleton authoring
tool that allows the users to create skeletal anatomy starting from only a skin
mesh using a library of bone parts.
</summary>
    <author>
      <name>Rinat Abdrashitov</name>
    </author>
    <author>
      <name>Seungbae Bang</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <author>
      <name>Karan Singh</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450626.3459769</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450626.3459769" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 20 figures, SIGGRAPH 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph., Vol. 40, No. 4, Article 122. Publication date:
  August 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.05161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.05306v3</id>
    <updated>2022-07-17T16:07:04Z</updated>
    <published>2021-06-09T18:02:10Z</published>
    <title>DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact</title>
    <summary>  Cloth simulation has wide applications in computer animation, garment design,
and robot-assisted dressing. This work presents a differentiable cloth
simulator whose additional gradient information facilitates cloth-related
applications. Our differentiable simulator extends a state-of-the-art cloth
simulator based on Projective Dynamics (PD) and with dry frictional contact. We
draw inspiration from previous work to propose a fast and novel method for
deriving gradients in PD-based cloth simulation with dry frictional contact.
Furthermore, we conduct a comprehensive analysis and evaluation of the
usefulness of gradients in contact-rich cloth simulation. Finally, we
demonstrate the efficacy of our simulator in a number of downstream
applications, including system identification, trajectory optimization for
assisted dressing, closed-loop control, inverse design, and real-to-sim
transfer. We observe a substantial speedup obtained from using our gradient
information in solving most of these applications.
</summary>
    <author>
      <name>Yifei Li</name>
    </author>
    <author>
      <name>Tao Du</name>
    </author>
    <author>
      <name>Kui Wu</name>
    </author>
    <author>
      <name>Jie Xu</name>
    </author>
    <author>
      <name>Wojciech Matusik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3527660</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3527660" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Graphics (TOG), 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.05306v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05306v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09198v1</id>
    <updated>2021-06-17T01:22:52Z</updated>
    <published>2021-06-17T01:22:52Z</published>
    <title>Learning Perceptual Manifold of Fonts</title>
    <summary>  Along the rapid development of deep learning techniques in generative models,
it is becoming an urgent issue to combine machine intelligence with human
intelligence to solve the practical applications. Motivated by this
methodology, this work aims to adjust the machine generated character fonts
with the effort of human workers in the perception study. Although numerous
fonts are available online for public usage, it is difficult and challenging to
generate and explore a font to meet the preferences for common users. To solve
the specific issue, we propose the perceptual manifold of fonts to visualize
the perceptual adjustment in the latent space of a generative model of fonts.
In our framework, we adopt the variational autoencoder network for the font
generation. Then, we conduct a perceptual study on the generated fonts from the
multi-dimensional latent space of the generative model. After we obtained the
distribution data of specific preferences, we utilize manifold learning
approach to visualize the font distribution. In contrast to the conventional
user interface in our user study, the proposed font-exploring user interface is
efficient and helpful in the designated user preference.
</summary>
    <author>
      <name>Haoran Xie</name>
    </author>
    <author>
      <name>Yuki Fujita</name>
    </author>
    <author>
      <name>Kazunori Miyata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.09198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.10363v1</id>
    <updated>2021-06-18T21:35:09Z</updated>
    <published>2021-06-18T21:35:09Z</published>
    <title>Construction of Planar and Symmetric Truss Structures with Interlocking
  Edge Elements</title>
    <summary>  In this paper, we present an algorithmic approach to design and construct
planar truss structures based on symmetric lattices using modular elements. The
method of assembly is similar to Leonardo grids as they both rely on the
property of interlocking. In theory, our modular elements can be assembled by
the same type of binary operations. Our modular elements embody the principle
of geometric interlocking, a principle recently introduced in literature that
allows for pieces of an assembly to be interlocked in a way that they can
neither be assembled nor disassembled unless the pieces are subjected to
deformation or breakage. We demonstrate that breaking the pieces can indeed
facilitate the effective assembly of these pieces through the use of a simple
key-in-hole concept. As a result, these modular elements can be assembled
together to form an interlocking structure, in which the locking pieces apply
the force necessary to hold the entire assembly together.
</summary>
    <author>
      <name>Anantha Natarajan</name>
    </author>
    <author>
      <name>Jiaqi Cui</name>
    </author>
    <author>
      <name>Ergun Akleman</name>
    </author>
    <author>
      <name>Vinayak Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/2106.10363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.10592v2</id>
    <updated>2021-06-22T16:05:07Z</updated>
    <published>2021-06-20T01:03:59Z</published>
    <title>ExplorerTree: a focus+context exploration approach for 2D embeddings</title>
    <summary>  In exploratory tasks involving high-dimensional datasets, dimensionality
reduction (DR) techniques help analysts to discover patterns and other useful
information. Although scatter plot representations of DR results allow for
cluster identification and similarity analysis, such a visual metaphor presents
problems when the number of instances of the dataset increases, resulting in
cluttered visualizations. In this work, we propose a scatter plot-based
multilevel approach to display DR results and address clutter-related problems
when visualizing large datasets, together with the definition of a methodology
to use focus+context interaction on non-hierarchical embeddings. The proposed
technique, called ExplorerTree, uses a sampling selection technique on scatter
plots to reduce visual clutter and guide users through exploratory tasks. We
demonstrate ExplorerTree's effectiveness through a use case, where we visually
explore activation images of the convolutional layers of a neural network.
Finally, we also conducted a user experiment to evaluate ExplorerTree's ability
to convey embedding structures using different sampling strategies.
</summary>
    <author>
      <name>Wilson E. Marcílio-Jr</name>
    </author>
    <author>
      <name>Danilo M. Eler</name>
    </author>
    <author>
      <name>Fernando V. Paulovich</name>
    </author>
    <author>
      <name>José F. Rodrigues-Jr</name>
    </author>
    <author>
      <name>Almir O. Artero</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.bdr.2021.100239</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.bdr.2021.100239" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Big Data Research 25 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.10592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12138v1</id>
    <updated>2021-06-23T03:09:10Z</updated>
    <published>2021-06-23T03:09:10Z</published>
    <title>Statistical Rendering for Visualization of Red Sea Eddy Simulation Data</title>
    <summary>  Analyzing the effects of ocean eddies is important in oceanology for gaining
insights into transport of energy and biogeochemical particles. We present an
application of statistical visualization algorithms for the analysis of the Red
Sea eddy simulation ensemble. Specifically, we demonstrate the applications of
statistical volume rendering and statistical Morse complex summary maps to a
velocity magnitude field for studying the eddy positions in the flow dataset.
In statistical volume rendering, we model per-voxel data uncertainty using
noise models, such as parametric and nonparametric, and study the propagation
of uncertainty into the volume rendering pipeline. In the statistical Morse
complex summary maps, we derive histograms charactering uncertainty of gradient
flow destinations to understand Morse complex topological variations across the
ensemble. We demonstrate the utility of our statistical visualizations for an
effective analysis of the potential eddy positions and their spatial
uncertainty.
</summary>
    <author>
      <name>Tushar M. Athawale</name>
    </author>
    <author>
      <name>Alireza Entezari</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <author>
      <name>Chris R. Johnson</name>
    </author>
    <link href="http://arxiv.org/abs/2106.12138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12372v2</id>
    <updated>2021-06-25T08:09:48Z</updated>
    <published>2021-06-23T13:09:58Z</published>
    <title>Real-time Neural Radiance Caching for Path Tracing</title>
    <summary>  We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.
</summary>
    <author>
      <name>Thomas Müller</name>
    </author>
    <author>
      <name>Fabrice Rousselle</name>
    </author>
    <author>
      <name>Jan Novák</name>
    </author>
    <author>
      <name>Alexander Keller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3450626.3459812</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3450626.3459812" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at SIGGRAPH 2021. 16 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.12372v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12372v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13425v1</id>
    <updated>2021-06-25T04:45:23Z</updated>
    <published>2021-06-25T04:45:23Z</published>
    <title>Half-body Portrait Relighting with Overcomplete Lighting Representation</title>
    <summary>  We present a neural-based model for relighting a half-body portrait image by
simply referring to another portrait image with the desired lighting condition.
Rather than following classical inverse rendering methodology that involves
estimating normals, albedo and environment maps, we implicitly encode the
subject and lighting in a latent space, and use these latent codes to generate
relighted images by neural rendering. A key technical innovation is the use of
a novel overcomplete lighting representation, which facilitates lighting
interpolation in the latent space, as well as helping regularize the
self-organization of the lighting latent space during training. In addition, we
propose a novel multiplicative neural render that more effectively combines the
subject and lighting latent codes for rendering. We also created a large-scale
photorealistic rendered relighting dataset for training, which allows our model
to generalize well to real images. Extensive experiments demonstrate that our
system not only outperforms existing methods for referral-based portrait
relighting, but also has the capability generate sequences of relighted images
via lighting rotations.
</summary>
    <author>
      <name>Guoxian Song</name>
    </author>
    <author>
      <name>Tat-Jen Cham</name>
    </author>
    <author>
      <name>Jianfei Cai</name>
    </author>
    <author>
      <name>Jianmin Zheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14384</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14384" rel="related"/>
    <link href="http://arxiv.org/abs/2106.13425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14360v1</id>
    <updated>2021-06-28T01:12:20Z</updated>
    <published>2021-06-28T01:12:20Z</published>
    <title>Frame Field Operators</title>
    <summary>  Differential operators are widely used in geometry processing for problem
domains like spectral shape analysis, data interpolation, parametrization and
mapping, and meshing. In addition to the ubiquitous cotangent Laplacian,
anisotropic second-order operators, as well as higher-order operators such as
the Bilaplacian, have been discretized for specialized applications. In this
paper, we study a class of operators that generalizes the fourth-order
Bilaplacian to support anisotropic behavior. The anisotropy is parametrized by
a symmetric frame field, first studied in connection with quadrilateral and
hexahedral meshing, which allows for fine-grained control of local directions
of variation. We discretize these operators using a mixed finite element
scheme, verify convergence of the discretization, study the behavior of the
operator under pullback, and present potential applications.
</summary>
    <author>
      <name>David R. Palmer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Massachusetts Institute of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Oded Stein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Massachusetts Institute of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Justin Solomon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Massachusetts Institute of Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 15 figures. To be published in proceedings of the 2021
  Symposium on Geometry Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.14360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.00671v1</id>
    <updated>2021-08-02T07:16:23Z</updated>
    <published>2021-08-02T07:16:23Z</published>
    <title>Automatic Polygon Layout for Primal-Dual Visualization of Hypergraphs</title>
    <summary>  N-ary relationships, which relate N entities where N is not necessarily two,
can be visually represented as polygons whose vertices are the entities of the
relationships. Manually generating a high-quality layout using this
representation is labor-intensive. In this paper, we provide an automatic
polygon layout generation algorithm for the visualization of N-ary
relationships. At the core of our algorithm is a set of objective functions
motivated by a number of design principles that we have identified. These
objective functions are then used in an optimization framework that we develop
to achieve high-quality layouts. Recognizing the duality between entities and
relationships in the data, we provide a second visualization in which the roles
of entities and relationships in the original data are reversed. This can lead
to additional insight about the data. Furthermore, we enhance our framework for
a joint optimization on the primal layout (original data) and the dual layout
(where the roles of entities and relationships are reversed). This allows users
to inspect their data using two complementary views. We apply our visualization
approach to a number of datasets that include co-authorship data and social
contact pattern data.
</summary>
    <author>
      <name>Botong Qu</name>
    </author>
    <author>
      <name>Eugene Zhang</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures, to be published on VIS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.00671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03066v1</id>
    <updated>2021-08-06T11:33:07Z</updated>
    <published>2021-08-06T11:33:07Z</published>
    <title>Uncertainty Visualization of the Marching Squares and Marching Cubes
  Topology Cases</title>
    <summary>  Marching squares (MS) and marching cubes (MC) are widely used algorithms for
level-set visualization of scientific data. In this paper, we address the
challenge of uncertainty visualization of the topology cases of the MS and MC
algorithms for uncertain scalar field data sampled on a uniform grid. The
visualization of the MS and MC topology cases for uncertain data is challenging
due to their exponential nature and the possibility of multiple topology cases
per cell of a grid. We propose the topology case count and entropy-based
techniques for quantifying uncertainty in the topology cases of the MS and MC
algorithms when noise in data is modeled with probability distributions. We
demonstrate the applicability of our techniques for independent and correlated
uncertainty assumptions. We visualize the quantified topological uncertainty
via color mapping proportional to uncertainty, as well as with interactive
probability queries in the MS case and entropy isosurfaces in the MC case. We
demonstrate the utility of our uncertainty quantification framework in
identifying the isovalues exhibiting relatively high topological uncertainty.
We illustrate the effectiveness of our techniques via results on synthetic,
simulation, and hixel datasets.
</summary>
    <author>
      <name>Tushar M. Athawale</name>
    </author>
    <author>
      <name>Sudhanshu Sane</name>
    </author>
    <author>
      <name>Chris R. Johnson</name>
    </author>
    <link href="http://arxiv.org/abs/2108.03066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04136v6</id>
    <updated>2024-07-01T15:23:34Z</updated>
    <published>2021-08-09T16:06:44Z</published>
    <title>A computational medical XR discipline</title>
    <summary>  Computational Medical Extended Reality (CMXR), brings together life sciences
and neuroscience with mathematics, engineering and computer science. It unifies
computational science (scientific computing) with intelligent extended reality
and spatial computing for the medical field. It significantly differs from
previous "Clinical XR" or "Medical XR" terms, as it is focusing on how to
integrate computational methods from neural simulation to computational
geometry, computational vision and computer graphics with deep learning models
to solve specific hard problems in medicine and neuroscience: from
low/no-code/genAI authoring platforms to deep learning XR systems for training,
planning, operative navigation, therapy and rehabilitation.
</summary>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <author>
      <name>Walter Greenleaf</name>
    </author>
    <author>
      <name>Michael Cole</name>
    </author>
    <author>
      <name>Mark Zhang</name>
    </author>
    <author>
      <name>Rabi Datta</name>
    </author>
    <author>
      <name>Mathias Delahaye</name>
    </author>
    <author>
      <name>Eleni Grigoriou</name>
    </author>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Antonis Protopsaltis</name>
    </author>
    <author>
      <name>Philippe Bijlenga</name>
    </author>
    <author>
      <name>Nadia Magnenat-Thalmann</name>
    </author>
    <author>
      <name>Eleftherios Tsiridis</name>
    </author>
    <author>
      <name>Eustathios Kenanidis</name>
    </author>
    <author>
      <name>Kyriakos Vamvakidis</name>
    </author>
    <author>
      <name>Ioannis Koutelidakis</name>
    </author>
    <author>
      <name>Oliver A Kannape</name>
    </author>
    <link href="http://arxiv.org/abs/2108.04136v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04136v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04393v1</id>
    <updated>2021-08-10T00:56:19Z</updated>
    <published>2021-08-10T00:56:19Z</published>
    <title>Stroke Correspondence by Labeling Closed Areas</title>
    <summary>  Constructing stroke correspondences between keyframes is one of the most
important processes in the production pipeline of hand-drawn inbetweening
frames. This process requires time-consuming manual work imposing a tremendous
burden on the animators. We propose a method to estimate stroke correspondences
between raster character images (keyframes) without vectorization processes.
First, the proposed system separates the closed areas in each keyframe and
estimates the correspondences between closed areas by using the characteristics
of shape, depth, and closed area connection. Second, the proposed system
estimates stroke correspondences from the estimated closed area
correspondences. We demonstrate the effectiveness of our method by performing a
user study and comparing the proposed system with conventional approaches.
</summary>
    <author>
      <name>Ryoma Miyauchi</name>
    </author>
    <author>
      <name>Tsukasa Fukusato</name>
    </author>
    <author>
      <name>Haoran Xie</name>
    </author>
    <author>
      <name>Kazunori Miyata</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/NICOINT52941.2021.00014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/NICOINT52941.2021.00014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NICOGRAPH International 2021. 9 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04886v1</id>
    <updated>2021-08-10T19:25:06Z</updated>
    <published>2021-08-10T19:25:06Z</published>
    <title>Differentiable Surface Rendering via Non-Differentiable Sampling</title>
    <summary>  We present a method for differentiable rendering of 3D surfaces that supports
both explicit and implicit representations, provides derivatives at occlusion
boundaries, and is fast and simple to implement. The method first samples the
surface using non-differentiable rasterization, then applies differentiable,
depth-aware point splatting to produce the final image. Our approach requires
no differentiable meshing or rasterization steps, making it efficient for large
3D models and applicable to isosurfaces extracted from implicit surface
definitions. We demonstrate the effectiveness of our method for implicit-,
mesh-, and parametric-surface-based inverse rendering and neural-network
training applications. In particular, we show for the first time efficient,
differentiable rendering of an isosurface extracted from a neural radiance
field (NeRF), and demonstrate surface-based, rather than volume-based,
rendering of a NeRF.
</summary>
    <author>
      <name>Forrester Cole</name>
    </author>
    <author>
      <name>Kyle Genova</name>
    </author>
    <author>
      <name>Avneesh Sud</name>
    </author>
    <author>
      <name>Daniel Vlasic</name>
    </author>
    <author>
      <name>Zhoutong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05182v2</id>
    <updated>2021-08-31T19:10:49Z</updated>
    <published>2021-08-09T15:09:57Z</published>
    <title>Jurassic Mark: Inattentional Blindness for a Datasaurus Reveals that
  Visualizations are Explored, not Seen</title>
    <summary>  Graphs effectively communicate data because they capitalize on the visual
system's ability to rapidly extract patterns. Yet, this pattern extraction does
not occur in a single glance. Instead, research on visual attention suggests
that the visual system iteratively applies a sequence of filtering operations
on an image, extracting patterns from subsets of visual information over time,
while selectively inhibiting other information at each of these moments. To
demonstrate that this powerful series of filtering operations also occurs
during the perception of visualized data, we designed a task where participants
made judgments from one class of marks on a scatterplot, presumably
incentivizing them to relatively ignore other classes of marks. Participants
consistently missed a conspicuous dinosaur in the ignored collection of marks
(93% for a 1s presentation, and 61% for 2.5s), but not in a control condition
where the incentive to ignore that collection was removed (25% for a 1s
presentation, and 11% for 2.5s), revealing that data visualizations are not
"seen" in a single glance, and instead require an active process of
exploration.
</summary>
    <author>
      <name>Tal Boger</name>
    </author>
    <author>
      <name>Steven B. Most</name>
    </author>
    <author>
      <name>Steven L. Franconeri</name>
    </author>
    <link href="http://arxiv.org/abs/2108.05182v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05182v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05263v1</id>
    <updated>2021-08-11T15:00:56Z</updated>
    <published>2021-08-11T15:00:56Z</published>
    <title>Dynamic Diffuse Global Illumination Resampling</title>
    <summary>  Interactive global illumination remains a challenge in radiometrically- and
geometrically-complex scenes. Specialized sampling strategies are effective for
specular and near-specular transport because the scattering has relatively low
directional variance per scattering event. In contrast, the high variance from
transport paths comprising multiple rough glossy or diffuse scattering events
remains notoriously difficult to resolve with a small number of samples. We
extend unidirectional path tracing to address this by combining screen-space
reservoir resampling and sparse world-space probes, significantly improving
sample efficiency for transport contributions that terminate on diffuse
scattering events. Our experiments demonstrate a clear improvement -- at equal
time and equal quality -- over purely path traced and purely probe-based
baselines. Moreover, when combined with commodity denoisers, we are able to
interactively render global illumination in complex scenes.
</summary>
    <author>
      <name>Zander Majercik</name>
    </author>
    <author>
      <name>Thomas Müller</name>
    </author>
    <author>
      <name>Alexander Keller</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <author>
      <name>Morgan McGuire</name>
    </author>
    <link href="http://arxiv.org/abs/2108.05263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05281v1</id>
    <updated>2021-08-11T15:29:13Z</updated>
    <published>2021-08-11T15:29:13Z</published>
    <title>"Deep Cut": An all-in-one Geometric Algorithm for Unconstrained Cut,
  Tear and Drill of Soft-bodies in Mobile VR</title>
    <summary>  In this work, we present an integrated geometric framework: "deep- cut" that
enables for the first time a user to geometrically and algorithmically cut,
tear and drill the surface of a skinned model without prior constraints,
layered on top of a custom soft body mesh deformation algorithm. Both layered
algorithms in this frame- work yield real-time results and are amenable for
mobile Virtual Reality, in order to be utilized in a variety of interactive
application scenarios. Our framework dramatically improves real-time user
experience and task performance in VR, without pre-calculated or artificially
designed cuts, tears, drills or surface deformations via predefined rigged
animations, which is the current state-of-the-art in mobile VR. Thus our
framework improves user experience on one hand, on the other hand saves both
time and costs from expensive, manual, labour-intensive design pre-calculation
stages.
</summary>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Nick Lydatakis</name>
    </author>
    <author>
      <name>Antonis Protopsaltis</name>
    </author>
    <author>
      <name>John Petropoulos</name>
    </author>
    <author>
      <name>Michail Tamiolakis</name>
    </author>
    <author>
      <name>Paul Zikas</name>
    </author>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <link href="http://arxiv.org/abs/2108.05281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05467v1</id>
    <updated>2021-08-11T22:39:45Z</updated>
    <published>2021-08-11T22:39:45Z</published>
    <title>Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach</title>
    <summary>  Edge bundling techniques cluster edges with similar attributes (i.e.
similarity in direction and proximity) together to reduce the visual clutter.
All edge bundling techniques to date implicitly or explicitly cluster groups of
individual edges, or parts of them, together based on these attributes. These
clusters can result in ambiguous connections that do not exist in the data.
Confluent drawings of networks do not have these ambiguities, but require the
layout to be computed as part of the bundling process. We devise a new bundling
method, Edge-Path bundling, to simplify edge clutter while greatly reducing
ambiguities compared to previous bundling techniques. Edge-Path bundling takes
a layout as input and clusters each edge along a weighted, shortest path to
limit its deviation from a straight line. Edge-Path bundling does not incur
independent edge ambiguities typically seen in all edge bundling methods, and
the level of bundling can be tuned through shortest path distances, Euclidean
distances, and combinations of the two. Also, directed edge bundling naturally
emerges from the model. Through metric evaluations, we demonstrate the
advantages of Edge-Path bundling over other techniques.
</summary>
    <author>
      <name>Markus Wallinger</name>
    </author>
    <author>
      <name>Daniel Archambault</name>
    </author>
    <author>
      <name>David Auber</name>
    </author>
    <author>
      <name>Martin Nöllenburg</name>
    </author>
    <author>
      <name>Jaakko Peltonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2021.3114795</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2021.3114795" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VIS 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Visualization and Computer Graphics (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.05467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05481v3</id>
    <updated>2021-09-18T19:33:56Z</updated>
    <published>2021-08-12T00:40:05Z</published>
    <title>Ships, Splashes, and Waves on a Vast Ocean</title>
    <summary>  The simulation of large open water surface is challenging using a uniform
volumetric discretization of the Navier-Stokes equations. Simulating water
splashes near moving objects, which height field methods for water waves cannot
capture, necessitates high resolutions. Such simulations can be carried out
using the Fluid-Implicit-Particle (FLIP) method. However, the FLIP method is
not efficient for the long-lasting water waves that propagate to long
distances, which require sufficient depth for a correct dispersion
relationship. This paper presents a new method to tackle this dilemma through
an efficient hybridization of volumetric and surface-based advection-projection
discretizations. We design a hybrid time-stepping algorithm that combines a
FLIP domain and an adaptively remeshed Boundary Element Method (BEM) domain for
the incompressible Euler equations. The resulting framework captures the
detailed water splashes near moving objects with the FLIP method, and produces
convincing water waves with correct dispersion relationships at modest
additional costs.
</summary>
    <author>
      <name>Libo Huang</name>
    </author>
    <author>
      <name>Ziyin Qu</name>
    </author>
    <author>
      <name>Xun Tan</name>
    </author>
    <author>
      <name>Xinxin Zhang</name>
    </author>
    <author>
      <name>Dominik L. Michels</name>
    </author>
    <author>
      <name>Chenfanfu Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2108.05481v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05481v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.07115v1</id>
    <updated>2021-08-16T14:37:34Z</updated>
    <published>2021-08-16T14:37:34Z</published>
    <title>Autocomplete Repetitive Stroking with Image Guidance</title>
    <summary>  Image-guided drawing can compensate for the lack of skills but often requires
a significant number of repetitive strokes to create textures. Existing
automatic stroke synthesis methods are usually limited to predefined styles or
require indirect manipulation that may break the spontaneous flow of drawing.
We present a method to autocomplete repetitive short strokes during users'
normal drawing process. Users can draw over a reference image as usual. At the
same time, our system silently analyzes the input strokes and the reference to
infer strokes that follow users' input style when certain repetition is
detected. Users can accept, modify, or ignore the system predictions and
continue drawing, thus maintaining the fluid control of drawing. Our key idea
is to jointly analyze image regions and operation history for detecting and
predicting repetitions. The proposed system can effectively reduce users'
workload in drawing repetitive short strokes and facilitates users in creating
results with rich patterns.
</summary>
    <author>
      <name>Yilan Chen</name>
    </author>
    <author>
      <name>Kin Chung Kwan</name>
    </author>
    <author>
      <name>Li-Yi Wei</name>
    </author>
    <author>
      <name>Hongbo Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2108.07115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.10480v2</id>
    <updated>2022-05-18T20:25:22Z</updated>
    <published>2021-08-24T02:02:31Z</published>
    <title>Fast Evaluation of Smooth Distance Constraints on Co-Dimensional
  Geometry</title>
    <summary>  We present a new method for computing a smooth minimum distance function
based on the LogSumExp function for point clouds, edge meshes, triangle meshes,
and combinations of all three. We derive blending weights and a modified
Barnes-Hut acceleration approach that ensure our method approximates the true
distance, and is conservative (points outside the zero isosurface are
guaranteed to be outside the surface) and efficient to evaluate for all the
above data types. This, in combination with its ability to smooth sparsely
sampled and noisy data, like point clouds, shortens the gap between data
acquisition and simulation, and thereby enables new applications such as
direct, co-dimensional rigid body simulation using unprocessed lidar data.
</summary>
    <author>
      <name>Abhishek Madan</name>
    </author>
    <author>
      <name>David I. W. Levin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3528223.3530093</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3528223.3530093" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 23 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Graph. 41, 4, Article 68 (July 2022), 17 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.10480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12536v1</id>
    <updated>2021-08-28T00:22:30Z</updated>
    <published>2021-08-28T00:22:30Z</published>
    <title>DASH: Modularized Human Manipulation Simulation with Vision and Language
  for Embodied AI</title>
    <summary>  Creating virtual humans with embodied, human-like perceptual and actuation
constraints has the promise to provide an integrated simulation platform for
many scientific and engineering applications. We present Dynamic and Autonomous
Simulated Human (DASH), an embodied virtual human that, given natural language
commands, performs grasp-and-stack tasks in a physically-simulated cluttered
environment solely using its own visual perception, proprioception, and touch,
without requiring human motion data. By factoring the DASH system into a vision
module, a language module, and manipulation modules of two skill categories, we
can mix and match analytical and machine learning techniques for different
modules so that DASH is able to not only perform randomly arranged tasks with a
high success rate, but also do so under anthropomorphic constraints and with
fluid and diverse motions. The modular design also favors analysis and
extensibility to more complex manipulation skills.
</summary>
    <author>
      <name>Yifeng Jiang</name>
    </author>
    <author>
      <name>Michelle Guo</name>
    </author>
    <author>
      <name>Jiangshan Li</name>
    </author>
    <author>
      <name>Ioannis Exarchos</name>
    </author>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <author>
      <name>C. Karen Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3475946.3480950</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3475946.3480950" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SCA'2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In The ACM SIGGRAPH / Eurographics Symposium on Computer Animation
  (SCA 21), September 6~9, 2021, Virtual Event, USA. ACM, New York, NY, USA, 12
  pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.12536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.00780v1</id>
    <updated>2021-09-02T08:40:05Z</updated>
    <published>2021-09-02T08:40:05Z</published>
    <title>Non-Photorealistic Rendering of Layered Materials: A Multispectral
  Approach</title>
    <summary>  We present multispectral rendering techniques for visualizing layered
materials found in biological specimens. We are the first to use acquired data
from the near-infrared and ultraviolet spectra for non-photorealistic rendering
(NPR). Several plant and animal species are more comprehensively understood by
multispectral analysis. However, traditional NPR techniques ignore unique
information outside the visible spectrum. We introduce algorithms and
principles for processing wavelength dependent surface normals and reflectance.
Our registration and feature detection methods are used to formulate
stylization effects not considered by current NPR methods including: Spectral
Band Shading which isolates and emphasizes shape features at specific
wavelengths at multiple scales. Experts in our user study demonstrate the
effectiveness of our system for applications in the biological sciences.
</summary>
    <author>
      <name>Corey Toler-Franklin</name>
    </author>
    <author>
      <name>Shashank Ranjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 35 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.00780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.00780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.3.8; I.4.0; I.4.1; I.4.3; I.4.8; I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01018v1</id>
    <updated>2021-09-02T15:29:45Z</updated>
    <published>2021-09-02T15:29:45Z</published>
    <title>Dynamic Scene Novel View Synthesis via Deferred Spatio-temporal
  Consistency</title>
    <summary>  Structure from motion (SfM) enables us to reconstruct a scene via casual
capture from cameras at different viewpoints, and novel view synthesis (NVS)
allows us to render a captured scene from a new viewpoint. Both are hard with
casual capture and dynamic scenes: SfM produces noisy and spatio-temporally
sparse reconstructed point clouds, resulting in NVS with spatio-temporally
inconsistent effects. We consider SfM and NVS parts together to ease the
challenge. First, for SfM, we recover stable camera poses, then we defer the
requirement for temporally-consistent points across the scene and reconstruct
only a sparse point cloud per timestep that is noisy in space-time. Second, for
NVS, we present a variational diffusion formulation on depths and colors that
lets us robustly cope with the noise by enforcing spatio-temporal consistency
via per-pixel reprojection weights derived from the input views. Together, this
deferred approach generates novel views for dynamic scenes without requiring
challenging spatio-temporally consistent reconstructions nor training complex
models on large datasets. We demonstrate our algorithm on real-world dynamic
scenes against classic and more recent learning-based baseline approaches.
</summary>
    <author>
      <name>Beatrix-Emőke Fülöp-Balogh</name>
    </author>
    <author>
      <name>Eleanor Tursman</name>
    </author>
    <author>
      <name>James Tompkin</name>
    </author>
    <author>
      <name>Julie Digne</name>
    </author>
    <author>
      <name>Nicolas Bonneel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accompanying video: https://youtu.be/RXK2iv980nU</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01170v1</id>
    <updated>2021-09-02T18:32:08Z</updated>
    <published>2021-09-02T18:32:08Z</published>
    <title>Volume Preserving Simulation of Soft Tissue with Skin</title>
    <summary>  Simulation of human soft tissues in contact with their environment is
essential in many fields, including visual effects and apparel design.
Biological tissues are nearly incompressible. However, standard methods employ
compressible elasticity models and achieve incompressibility indirectly by
setting Poisson's ratio to be close to 0.5. This approach can produce results
that are plausible qualitatively but inaccurate quantatively. This approach
also causes numerical instabilities and locking in coarse discretizations or
otherwise poses a prohibitive restriction on the size of the time step. We
propose a novel approach to alleviate these issues by replacing indirect volume
preservation using Poisson's ratios with direct enforcement of zonal volume
constraints, while controlling fine-scale volumetric deformation through a
cell-wise compression penalty. To increase realism, we propose an epidermis
model to mimic the dramatically higher surface stiffness on real skinned
bodies. We demonstrate that our method produces stable realistic deformations
with precise volume preservation but without locking artifacts. Due to the
volume preservation not being tied to mesh discretization, our method also
allows a resolution consistent simulation of incompressible materials. Our
method improves the stability of the standard neo-Hookean model and the general
compression recovery in the Stable neo-Hookean model.
</summary>
    <author>
      <name>Seung Heon Sheen</name>
    </author>
    <author>
      <name>Egor Larionov</name>
    </author>
    <author>
      <name>Dinesh K. Pai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3480143</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3480143" rel="related"/>
    <link href="http://arxiv.org/abs/2109.01170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01750v1</id>
    <updated>2021-09-03T23:02:45Z</updated>
    <published>2021-09-03T23:02:45Z</published>
    <title>CodeNeRF: Disentangled Neural Radiance Fields for Object Categories</title>
    <summary>  CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}
</summary>
    <author>
      <name>Wonbong Jang</name>
    </author>
    <author>
      <name>Lourdes Agapito</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 15 figures, ICCV 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01769v2</id>
    <updated>2022-06-11T15:53:53Z</updated>
    <published>2021-09-04T02:44:48Z</published>
    <title>SFCDecomp: Multicriteria Optimized Tool Path Planning in 3D Printing
  using Space-Filling Curve Based Domain Decomposition</title>
    <summary>  We explore efficient optimization of toolpaths based on multiple criteria for
large instances of 3D printing problems. We first show that the minimum turn
cost 3D printing problem is NP-hard, even when the region is a simple polygon.
We develop SFCDecomp, a space filling curve based decomposition framework to
solve large instances of 3D printing problems efficiently by solving these
optimization subproblems independently. For the Buddha model, our framework
builds toolpaths over a total of 799,716 nodes across 169 layers, and for the
Bunny model it builds toolpaths over 812,733 nodes across 360 layers. Building
on SFCDecomp, we develop a multicriteria optimization approach for toolpath
planning. We demonstrate the utility of our framework by maximizing or
minimizing tool path edge overlap between adjacent layers, while jointly
minimizing turn costs. Strength testing of a tensile test specimen printed with
tool paths that maximize or minimize adjacent layer edge overlaps reveal
significant differences in tensile strength between the two classes of prints.
</summary>
    <author>
      <name>Prashant Gupta</name>
    </author>
    <author>
      <name>Yiran Guo</name>
    </author>
    <author>
      <name>Narasimha Boddeti</name>
    </author>
    <author>
      <name>Bala Krishnamoorthy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0218195921500096</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0218195921500096" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor edits to incorporate reviewers' comments. Published in IJCGA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computational Geometry &amp; Applications
  (IJCGA), 31, 04, pp 193-220 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.01769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 68Q17, 90C11" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07202v1</id>
    <updated>2021-09-15T10:41:51Z</updated>
    <published>2021-09-15T10:41:51Z</published>
    <title>Deep 3D Mesh Watermarking with Self-Adaptive Robustness</title>
    <summary>  Robust 3D mesh watermarking is a traditional research topic in computer
graphics, which provides an efficient solution to the copyright protection for
3D meshes. Traditionally, researchers need manually design watermarking
algorithms to achieve sufficient robustness for the actual application
scenarios. In this paper, we propose the first deep learning-based 3D mesh
watermarking framework, which can solve this problem once for all. In detail,
we propose an end-to-end network, consisting of a watermark embedding
sub-network, a watermark extracting sub-network and attack layers. We adopt the
topology-agnostic graph convolutional network (GCN) as the basic convolution
operation for 3D meshes, so our network is not limited by registered meshes
(which share a fixed topology). For the specific application scenario, we can
integrate the corresponding attack layers to guarantee adaptive robustness
against possible attacks. To ensure the visual quality of watermarked 3D
meshes, we design a curvature-based loss function to constrain the local
geometry smoothness of watermarked meshes. Experimental results show that the
proposed method can achieve more universal robustness and faster watermark
embedding than baseline methods while guaranteeing comparable visual quality.
</summary>
    <author>
      <name>Feng Wang</name>
    </author>
    <author>
      <name>Hang Zhou</name>
    </author>
    <author>
      <name>Han Fang</name>
    </author>
    <author>
      <name>Xiaojuan Dong</name>
    </author>
    <author>
      <name>Weiming Zhang</name>
    </author>
    <author>
      <name>Xi Yang</name>
    </author>
    <author>
      <name>Nenghai Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2109.07202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07249v1</id>
    <updated>2021-09-15T12:38:44Z</updated>
    <published>2021-09-15T12:38:44Z</published>
    <title>Temporal Parameter-free Deep Skinning of Animated Meshes</title>
    <summary>  In computer graphics, animation compression is essential for efficient
storage, streaming and reproduction of animated meshes. Previous work has
presented efficient techniques for compression by deriving skinning
transformations and weights using clustering of vertices based on geometric
features of vertices over time. In this work we present a novel approach that
assigns vertices to bone-influenced clusters and derives weights using deep
learning through a training set that consists of pairs of vertex trajectories
(temporal vertex sequences) and the corresponding weights drawn from fully
rigged animated characters. The approximation error of the resulting linear
blend skinning scheme is significantly lower than the error of competent
previous methods by producing at the same time a minimal number of bones.
Furthermore, the optimal set of transformation and vertices is derived in fewer
iterations due to the better initial positioning in the multidimensional
variable space. Our method requires no parameters to be determined or tuned by
the user during the entire process of compressing a mesh animation sequence.
</summary>
    <author>
      <name>Anastasia Moutafidou</name>
    </author>
    <author>
      <name>Vasileios Toulatzis</name>
    </author>
    <author>
      <name>Ioannis Fudos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-89029-2_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-89029-2_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CGI 2021, LNCS Proceedings, to appear. For video and presentation and
  other info please see http://www.cgrg.cs.uoi.gr/single-publication?ID=48</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Computer Graphics. CGI 2021. Lecture Notes in Computer
  Science, vol 13002. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.07249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.6; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07683v1</id>
    <updated>2021-09-16T03:05:15Z</updated>
    <published>2021-09-16T03:05:15Z</published>
    <title>Intuitive and Efficient Roof Modeling for Reconstruction and Synthesis</title>
    <summary>  We propose a novel and flexible roof modeling approach that can be used for
constructing planar 3D polygon roof meshes. Our method uses a graph structure
to encode roof topology and enforces the roof validity by optimizing a simple
but effective planarity metric we propose. This approach is significantly more
efficient than using general purpose 3D modeling tools such as 3ds Max or
SketchUp, and more powerful and expressive than specialized tools such as the
straight skeleton. Our optimization-based formulation is also flexible and can
accommodate different styles and user preferences for roof modeling. We
showcase two applications. The first application is an interactive roof editing
framework that can be used for roof design or roof reconstruction from aerial
images. We highlight the efficiency and generality of our approach by
constructing a mesh-image paired dataset consisting of 2539 roofs. Our second
application is a generative model to synthesize new roof meshes from scratch.
We use our novel dataset to combine machine learning and our roof optimization
techniques, by using transformers and graph convolutional networks to model
roof topology, and our roof optimization methods to enforce the planarity
constraint.
</summary>
    <author>
      <name>Jing Ren</name>
    </author>
    <author>
      <name>Biao Zhang</name>
    </author>
    <author>
      <name>Bojian Wu</name>
    </author>
    <author>
      <name>Jianqiang Huang</name>
    </author>
    <author>
      <name>Lubin Fan</name>
    </author>
    <author>
      <name>Maks Ovsjanikov</name>
    </author>
    <author>
      <name>Peter Wonka</name>
    </author>
    <link href="http://arxiv.org/abs/2109.07683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09547v1</id>
    <updated>2021-09-20T13:56:50Z</updated>
    <published>2021-09-20T13:56:50Z</published>
    <title>Egocentric Network Exploration for Immersive Analytics</title>
    <summary>  To exploit the potential of immersive network analytics for engaging and
effective exploration, we promote the metaphor of "egocentrism", where data
depiction and interaction are adapted to the perspective of the user within a
3D network. Egocentrism has the potential to overcome some of the inherent
downsides of virtual environments, e.g., visual clutter and cyber-sickness. To
investigate the effect of this metaphor on immersive network exploration, we
designed and evaluated interfaces of varying degrees of egocentrism. In a user
study, we evaluated the effect of these interfaces on visual search tasks,
efficiency of network traversal, spatial orientation, as well as
cyber-sickness. Results show that a simple egocentric interface considerably
improves visual search efficiency and navigation performance, yet does not
decrease spatial orientation or increase cyber-sickness. An occlusion-free
Ego-Bubble view of the neighborhood only marginally improves the user's
performance. We tie our findings together in an open online tool for egocentric
network exploration, providing actionable insights on the benefits of the
egocentric network exploration metaphor.
</summary>
    <author>
      <name>Johannes Sorger</name>
    </author>
    <author>
      <name>Alessio Arleo</name>
    </author>
    <author>
      <name>Peter Kán</name>
    </author>
    <author>
      <name>Wolfgang Knecht</name>
    </author>
    <author>
      <name>Manuela Waldner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pacific Graphics 2021, Volume 40 (2021), Number 7</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10595v2</id>
    <updated>2021-09-24T06:26:47Z</updated>
    <published>2021-09-22T08:47:43Z</published>
    <title>Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation</title>
    <summary>  To the best of our knowledge, we first present a live system that generates
personalized photorealistic talking-head animation only driven by audio signals
at over 30 fps. Our system contains three stages. The first stage is a deep
neural network that extracts deep audio features along with a manifold
projection to project the features to the target person's speech space. In the
second stage, we learn facial dynamics and motions from the projected audio
features. The predicted motions include head poses and upper body motions,
where the former is generated by an autoregressive probabilistic model which
models the head pose distribution of the target person. Upper body motions are
deduced from head poses. In the final stage, we generate conditional feature
maps from previous predictions and send them with a candidate image set to an
image-to-image translation network to synthesize photorealistic renderings. Our
method generalizes well to wild audio and successfully synthesizes
high-fidelity personalized facial details, e.g., wrinkles, teeth. Our method
also allows explicit control of head poses. Extensive qualitative and
quantitative evaluations, along with user studies, demonstrate the superiority
of our method over state-of-the-art techniques.
</summary>
    <author>
      <name>Yuanxun Lu</name>
    </author>
    <author>
      <name>Jinxiang Chai</name>
    </author>
    <author>
      <name>Xun Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH Asia 2021, 17 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.10595v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10595v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12888v2</id>
    <updated>2022-01-31T11:11:41Z</updated>
    <published>2021-09-27T09:19:41Z</published>
    <title>Mixed Integer Neural Inverse Design</title>
    <summary>  In computational design and fabrication, neural networks are becoming
important surrogates for bulky forward simulations. A long-standing,
intertwined question is that of inverse design: how to compute a design that
satisfies a desired target performance? Here, we show that the piecewise linear
property, very common in everyday neural networks, allows for an inverse design
formulation based on mixed-integer linear programming. Our mixed-integer
inverse design uncovers globally optimal or near optimal solutions in a
principled manner. Furthermore, our method significantly facilitates emerging,
but challenging, combinatorial inverse design tasks, such as material
selection. For problems where finding the optimal solution is not desirable or
tractable, we develop an efficient yet near-optimal hybrid optimization.
Eventually, our method is able to find solutions provably robust to possible
fabrication perturbations among multiple designs with similar performances.
</summary>
    <author>
      <name>Navid Ansari</name>
    </author>
    <author>
      <name>Hans-Peter Seidel</name>
    </author>
    <author>
      <name>Vahid Babaei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3528223.3530083</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3528223.3530083" rel="related"/>
    <link href="http://arxiv.org/abs/2109.12888v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12888v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.14398v1</id>
    <updated>2021-09-29T13:01:46Z</updated>
    <published>2021-09-29T13:01:46Z</published>
    <title>Position-free Multiple-bounce Computations for Smith Microfacet BSDFs</title>
    <summary>  Bidirectional Scattering Distribution Functions (BSDFs) encode how a material
reflects or transmits the incoming light. The most commonly used model is the
Microfacet BSDF. It computes material response from the micro-geometry of the
surface assuming a single bounce on specular microfacets. The original model
ignores multiple bounces on the micro-geometry, resulting in energy loss,
especially with large roughness. In this paper, we present a position-free
formulation of multiple bounces inside the micro-geometry, which eliminates
this energy loss. We use an explicit mathematical definition of path space that
describes single and multiple bounces in a uniform way. We then study the
behavior of light on the different vertices and segments in path space, leading
to an accurate and reciprocal multiple-bounce description of BSDFs. We also
present practical, unbiased Monte-Carlo estimators to compute multiple
scattering. Our method is less noisy than existing algorithms for computing
multiple scattering. It is almost noise-free with a very-low sampling rate,
from 2 to 4 samples per pixel.
</summary>
    <author>
      <name>Beibei Wang</name>
    </author>
    <author>
      <name>Wenhua Jin</name>
    </author>
    <author>
      <name>Jiahui Fan</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Nicolas Holzschuch</name>
    </author>
    <author>
      <name>Ling-Qi Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2109.14398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.14434v1</id>
    <updated>2021-09-29T14:14:24Z</updated>
    <published>2021-09-29T14:14:24Z</published>
    <title>Convex polyhedral meshing for robust solid modeling</title>
    <summary>  We introduce a new technique to create a mesh of convex polyhedra
representing the interior volume of a triangulated input surface. Our approach
is particularly tolerant to defects in the input, which is allowed to
self-intersect, to be non-manifold, disconnected, and to contain surface holes
and gaps. We guarantee that the input surface is exactly represented as the
union of polygonal facets of the output volume mesh. Thanks to our algorithm,
traditionally difficult solid modeling operations such as mesh booleans and
Minkowski sums become surprisingly robust and easy to implement, even if the
input has defects. Our technique leverages on the recent concept of indirect
geometric predicate to provide an unprecedented combination of guaranteed
robustness and speed, thus enabling the practical implementation of robust
though flexible solid modeling systems. We have extensively tested our method
on all the 10000 models of the Thingi10k dataset, and concluded that no
existing method provides comparable robustness, precision and performances.
</summary>
    <author>
      <name>Lorenzo Diazzi</name>
    </author>
    <author>
      <name>Marco Attene</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Siggraph Asia 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.14434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.14722v2</id>
    <updated>2021-10-04T21:27:10Z</updated>
    <published>2021-09-29T21:04:07Z</published>
    <title>SliceHub: Augmenting Shared 3D Model Repositories with Slicing Results
  for 3D Printing</title>
    <summary>  In this paper, we explore how to augment shared 3D model repositories, such
as Thingiverse, with slicing results that are readily available to all users.
By having print time and material consumption for different print resolution
profiles and model scales available in real-time, users are able to explore
different slicing configurations efficiently to find the one that best fits
their time and material constraints. To prototype this idea, we build a system
called SliceHub, which consists of three components: (1) a repository with an
evolving database of 3D models, for which we store the print time and material
consumption for various print resolution profiles and model scales, (2) a user
interface integrated into an existing slicer that allows users to explore the
slicing information from the 3D models, and (3) a computational infrastructure
to quickly generate new slicing results, either through parallel slicing of
multiple print resolution profiles and model scales or through interpolation.
We motivate our work with a formative study of the challenges faced by users of
existing slicers and provide a technical evaluation of the SliceHub system.
</summary>
    <author>
      <name>Faraz Faruqi</name>
    </author>
    <author>
      <name>Kenneth Friedman</name>
    </author>
    <author>
      <name>Leon Cheng</name>
    </author>
    <author>
      <name>Michael Wessely</name>
    </author>
    <author>
      <name>Sriram Subramanian</name>
    </author>
    <author>
      <name>Stefanie Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/2109.14722v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14722v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00965v3</id>
    <updated>2022-01-26T05:42:37Z</updated>
    <published>2021-10-03T09:46:24Z</published>
    <title>Coverage Axis: Inner Point Selection for 3D Shape Skeletonization</title>
    <summary>  In this paper, we present a simple yet effective formulation called Coverage
Axis for 3D shape skeletonization. Inspired by the set cover problem, our key
idea is to cover all the surface points using as few inside medial balls as
possible. This formulation inherently induces a compact and expressive
approximation of the Medial Axis Transform (MAT) of a given shape. Different
from previous methods that rely on local approximation error, our method allows
a global consideration of the overall shape structure, leading to an efficient
high-level abstraction and superior robustness to noise. Another appealing
aspect of our method is its capability to handle more generalized input such as
point clouds and poor-quality meshes. Extensive comparisons and evaluations
demonstrate the remarkable effectiveness of our method for generating compact
and expressive skeletal representation to approximate the MAT.
</summary>
    <author>
      <name>Zhiyang Dou</name>
    </author>
    <author>
      <name>Cheng Lin</name>
    </author>
    <author>
      <name>Rui Xu</name>
    </author>
    <author>
      <name>Lei Yang</name>
    </author>
    <author>
      <name>Shiqing Xin</name>
    </author>
    <author>
      <name>Taku Komura</name>
    </author>
    <author>
      <name>Wenping Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2110.00965v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00965v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01614v1</id>
    <updated>2021-10-03T09:43:01Z</updated>
    <published>2021-10-03T09:43:01Z</published>
    <title>Neural Implicit Surfaces for Efficient and Accurate Collisions in
  Physically Based Simulations</title>
    <summary>  Current trends in the computer graphics community propose leveraging the
massive parallel computational power of GPUs to accelerate physically based
simulations. Collision detection and solving is a fundamental part of this
process. It is also the most significant bottleneck on physically based
simulations and it easily becomes intractable as the number of vertices in the
scene increases. Brute force approaches carry a quadratic growth in both
computational time and memory footprint. While their parallelization is trivial
in GPUs, their complexity discourages from using such approaches. Acceleration
structures -- such as BVH -- are often applied to increase performance,
achieving logarithmic computational times for individual point queries.
Nonetheless, their memory footprint also grows rapidly and their
parallelization in a GPU is problematic due to their branching nature. We
propose using implicit surface representations learnt through deep learning for
collision handling in physically based simulations. Our proposed architecture
has a complexity of O(n) -- or O(1) for a single point query -- and has no
parallelization issues. We will show how this permits accurate and efficient
collision handling in physically based simulations, more specifically, for
cloth. In our experiments, we query up to 1M points in 300 milliseconds.
</summary>
    <author>
      <name>Hugo Bertiche</name>
    </author>
    <author>
      <name>Meysam Madadi</name>
    </author>
    <author>
      <name>Sergio Escalera</name>
    </author>
    <link href="http://arxiv.org/abs/2110.01614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05433v1</id>
    <updated>2021-10-11T17:24:52Z</updated>
    <published>2021-10-11T17:24:52Z</published>
    <title>Mesh Draping: Parametrization-Free Neural Mesh Transfer</title>
    <summary>  Despite recent advances in geometric modeling, 3D mesh modeling still
involves a considerable amount of manual labor by experts. In this paper, we
introduce Mesh Draping: a neural method for transferring existing mesh
structure from one shape to another. The method drapes the source mesh over the
target geometry and at the same time seeks to preserve the carefully designed
characteristics of the source mesh. At its core, our method deforms the source
mesh using progressive positional encoding. We show that by leveraging
gradually increasing frequencies to guide the neural optimization, we are able
to achieve stable and high quality mesh transfer. Our approach is simple and
requires little user guidance, compared to contemporary surface mapping
techniques which rely on parametrization or careful manual tuning. Most
importantly, Mesh Draping is a parameterization-free method, and thus
applicable to a variety of target shape representations, including point
clouds, polygon soups, and non-manifold meshes. We demonstrate that the
transferred meshing remains faithful to the source mesh design characteristics,
and at the same time fits the target geometry well.
</summary>
    <author>
      <name>Amir Hertz</name>
    </author>
    <author>
      <name>Or Perel</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Olga Sorkine-Hornung</name>
    </author>
    <author>
      <name>Daniel Cohen-Or</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. Portions of this work previously appeared as
  arXiv:2104.09125v1 which has been split into two works: arXiv:2104.09125v2+
  and this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.05433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05805v1</id>
    <updated>2021-10-12T08:02:20Z</updated>
    <published>2021-10-12T08:02:20Z</published>
    <title>Real-time Skeletonization for Sketch-based Modeling</title>
    <summary>  Skeleton creation is an important phase in the character animation pipeline.
However, handcrafting skeleton takes extensive labor time and domain knowledge.
Automatic skeletonization provides a solution. However, most of the current
approaches are far from real-time and lack the flexibility to control the
skeleton complexity. In this paper, we present an efficient skeletonization
method, which can be seamlessly integrated into the sketch-based modeling
process in real-time. The method contains three steps: local sub-skeleton
extraction; sub-skeleton connection; and global skeleton refinement. Firstly,
the local skeleton is extracted from the processed polygon stroke and forms a
subpart along with the sub-mesh. Then, local sub-skeletons are connected
according to the intersecting relationships and the modeling sequence of
subparts. Lastly, a global refinement method is proposed to give users
coarse-to-fine control on the connected skeleton. We demonstrate the
effectiveness of our method on a variety of examples created by both novices
and professionals.
</summary>
    <author>
      <name>Jing Ma</name>
    </author>
    <author>
      <name>Jin Wang</name>
    </author>
    <author>
      <name>Jituo Li</name>
    </author>
    <author>
      <name>Dongliang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Shape Modeling International 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.05805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06901v2</id>
    <updated>2021-11-23T08:44:41Z</updated>
    <published>2021-10-13T17:29:50Z</published>
    <title>A Survey on Deep Learning for Skeleton-Based Human Animation</title>
    <summary>  Human character animation is often critical in entertainment content
production, including video games, virtual reality or fiction films. To this
end, deep neural networks drive most recent advances through deep learning and
deep reinforcement learning. In this article, we propose a comprehensive survey
on the state-of-the-art approaches based on either deep learning or deep
reinforcement learning in skeleton-based human character animation. First, we
introduce motion data representations, most common human motion datasets and
how basic deep models can be enhanced to foster learning of spatial and
temporal patterns in motion data. Second, we cover state-of-the-art approaches
divided into three large families of applications in human animation pipelines:
motion synthesis, character control and motion editing. Finally, we discuss the
limitations of the current state-of-the-art methods based on deep learning
and/or deep reinforcement learning in skeletal human character animation and
possible directions of future research to alleviate current limitations and
meet animators' needs.
</summary>
    <author>
      <name>L. Mourot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InterDigital, Inc</arxiv:affiliation>
    </author>
    <author>
      <name>L. Hoyet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InterDigital, Inc</arxiv:affiliation>
    </author>
    <author>
      <name>F. Le Clerc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InterDigital, Inc</arxiv:affiliation>
    </author>
    <author>
      <name>François Schnitzler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InterDigital, Inc</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Hellier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InterDigital, Inc</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14426</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14426" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 10 figures, 4 tables, published in Computer Graphics Forum
  (CGF)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06901v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06901v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.07145v2</id>
    <updated>2023-03-02T01:16:45Z</updated>
    <published>2021-10-14T04:21:43Z</published>
    <title>SpongeCake: A Layered Microflake Surface Appearance Model</title>
    <summary>  In this paper, we propose SpongeCake: a layered BSDF model where each layer
is a volumetric scattering medium, defined using microflake or other phase
functions. We omit any reflecting and refracting interfaces between the layers.
The first advantage of this formulation is that an exact and analytic solution
for single scattering, regardless of the number of volumetric layers, can be
derived. We propose to approximate multiple scattering by an additional
single-scattering lobe with modified parameters and a Lambertian lobe. We use a
parameter mapping neural network to find the parameters of the newly added
lobes to closely approximate the multiple scattering effect. Despite the
absence of layer interfaces, we demonstrate that many common material effects
can be achieved with layers of SGGX microflake and other volumes with
appropriate parameters. A normal mapping effect can also be achieved through
mapping of microflake orientations, which avoids artifacts common in standard
normal maps. Thanks to the analytical formulation, our model is very fast to
evaluate and sample. Through various parameter settings, our model is able to
handle many types of materials, like plastics, wood, cloth, etc., opening a
number of practical applications.
</summary>
    <author>
      <name>Beibei Wang</name>
    </author>
    <author>
      <name>Wenhua Jin</name>
    </author>
    <author>
      <name>Miloš Hašan</name>
    </author>
    <author>
      <name>Ling-Qi Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2110.07145v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.07145v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
