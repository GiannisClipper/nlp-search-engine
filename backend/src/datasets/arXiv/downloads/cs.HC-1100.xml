<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.HC%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.HC&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/YTV0JOZKONPGRZ3YGkpvaEpEEBQ</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">23414</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0708.0598v2</id>
    <updated>2008-02-03T19:41:44Z</updated>
    <published>2007-08-04T02:38:19Z</published>
    <title>An Application of Chromatic Prototypes</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Matthew McCool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.0598v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0598v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.0461v1</id>
    <updated>2008-09-02T17:25:25Z</updated>
    <published>2008-09-02T17:25:25Z</published>
    <title>The Semiotic Machine</title>
    <summary>  A semiotic model of the user interface in human-computer interaction.
Algorithmic sign, semotics, algorithmic art.
</summary>
    <author>
      <name>Eric Engle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.0461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.0461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11765v1</id>
    <updated>2022-09-22T06:30:31Z</updated>
    <published>2022-09-22T06:30:31Z</published>
    <title>Process Diagrams</title>
    <summary>  This paper is simply a collection of process diagrams for further use and
reference. These are diagrams about different approaches to research.
</summary>
    <author>
      <name>Sheelagh Carpendale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Computing Science, Simon Fraser University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.11765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.11765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.03699v1</id>
    <updated>2024-04-30T19:46:23Z</updated>
    <published>2024-04-30T19:46:23Z</published>
    <title>HCC Is All You Need: Alignment-The Sensible Kind Anyway-Is Just
  Human-Centered Computing</title>
    <summary>  This article argues that AI Alignment is a type of Human-Centered Computing.
</summary>
    <author>
      <name>Eric Gilbert</name>
    </author>
    <link href="http://arxiv.org/abs/2405.03699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.03699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02263v1</id>
    <updated>2024-11-04T16:54:48Z</updated>
    <published>2024-11-04T16:54:48Z</published>
    <title>AI Should Challenge, Not Obey</title>
    <summary>  Let's transform our robot secretaries into Socratic gadflies.
</summary>
    <author>
      <name>Advait Sarkar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3649404</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3649404" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Advait Sarkar. 2024. AI Should Challenge, Not Obey. Commun. ACM 67,
  10 (October 2024), 18-21. https://doi.org/10.1145/3649404</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Commun. ACM 67, 10 (October 2024), 18-21</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2411.02263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508042v2</id>
    <updated>2006-06-29T06:46:34Z</updated>
    <published>2005-08-04T22:49:38Z</published>
    <title>OpenVanilla - A Non-Intrusive Plug-In Framework of Text Services</title>
    <summary>  This paper has been withdrawn by the author, because it was merged into
cs.HC/0508041
</summary>
    <author>
      <name>Tien-chien Chiang</name>
    </author>
    <author>
      <name> Deng-Liu</name>
    </author>
    <author>
      <name>Kang-min Liu</name>
    </author>
    <author>
      <name>Weizhong Yang</name>
    </author>
    <author>
      <name>Pek-tiong Tan</name>
    </author>
    <author>
      <name>Mengjuei Hsieh</name>
    </author>
    <author>
      <name>Tsung-hsiang Chang</name>
    </author>
    <author>
      <name>Wen-Lien Hsu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0508042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409041v1</id>
    <updated>2004-09-23T19:08:17Z</updated>
    <published>2004-09-23T19:08:17Z</published>
    <title>Four Principles Fundamental to Design Practice for Human Centred Systems</title>
    <summary>  A Survey of the principal literature on Human Centred Design reveals the four
most referenced principles. These are discussed with reference to the
application of a particular website, and a user survey is constructed based
upon the four principles.
</summary>
    <author>
      <name>Vita Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0409041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611042v1</id>
    <updated>2006-11-09T21:10:32Z</updated>
    <published>2006-11-09T21:10:32Z</published>
    <title>CSCR:Computer Supported Collaborative Research</title>
    <summary>  It is suggested that a new area of CSCR (Computer Supported Collaborative
Research) is distinguished from CSCW and CSCL and that the demarcation between
the three areas could do with greater clarification and prescription.
</summary>
    <author>
      <name>Vita Hinze-Hoare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 Pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1191v1</id>
    <updated>2009-08-08T21:13:27Z</updated>
    <published>2009-08-08T21:13:27Z</published>
    <title>Embedded Spreadsheet Modelling</title>
    <summary>  In larger accounting firms, specialist modellers typically sit in separate
teams. This paper will look at the advantages of embedding a specialist
modeller within a Corporate Finance Team.
</summary>
    <author>
      <name>Angela Collins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2009 95-99
  ISBN 978-1-905617-89-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.1191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.0256v1</id>
    <updated>2010-04-01T23:42:11Z</updated>
    <published>2010-04-01T23:42:11Z</published>
    <title>From Playability to a Hierarchical Game Usability Model</title>
    <summary>  This paper presents a brief review of current game usability models. This
leads to the conception of a high-level game development-centered usability
model that integrates current usability approaches in game industry and game
research.
</summary>
    <author>
      <name>Lennart E. Nacke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1639601.1639609</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1639601.1639609" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97Rxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.8.0; H.5.1; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07310v1</id>
    <updated>2015-05-26T15:45:00Z</updated>
    <published>2015-05-26T15:45:00Z</published>
    <title>Use of Laplacian Projection Technique for Summarizing Likert Scale
  Annotations</title>
    <summary>  Summarizing Likert scale ratings from human annotators is an important step
for collecting human judgments. In this project we study a novel, graph
theoretic method for this purpose. We also analyze a few interesting properties
for this approach using real annotation datasets.
</summary>
    <author>
      <name>M. Iftekhar Tanveer</name>
    </author>
    <link href="http://arxiv.org/abs/1505.07310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05737v2</id>
    <updated>2015-11-24T01:42:21Z</updated>
    <published>2015-11-18T11:25:50Z</published>
    <title>Designing for Collaborative Sensemaking: Leveraging Human Cognition For
  Complex Tasks</title>
    <summary>  My research aims to design systems for complex sensemaking by remotely
located non-expert collaborators (crowds), to solve computationally hard
problems like crimes.
</summary>
    <author>
      <name>Nitesh Goyal</name>
    </author>
    <author>
      <name>Susan R. Fussell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference. Companion of 15th IFIP TC 13 Human-Computer Interaction
  INTERACT 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05737v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05737v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05922v1</id>
    <updated>2016-12-18T14:58:37Z</updated>
    <published>2016-12-18T14:58:37Z</published>
    <title>Super Event Driven System OOP GUI Design</title>
    <summary>  This article presents a new proposal design of GUI and new technology in
programming Namely "Super Technology" which can be applied for supporting the
proposal design of GUI
</summary>
    <author>
      <name>Mahmoud Samir Fayed</name>
    </author>
    <author>
      <name>Ehab Aziz Khalil</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01796v1</id>
    <updated>2017-09-06T12:10:11Z</updated>
    <published>2017-09-06T12:10:11Z</published>
    <title>Interakt---A Multimodal Multisensory Interactive Cognitive Assessment
  Tool</title>
    <summary>  Cognitive assistance may be valuable in applications for doctors and
therapists that reduce costs and improve quality in healthcare systems. Use
cases and scenarios include the assessment of dementia. In this paper, we
present our approach to the (semi-)automatic assessment of dementia.
</summary>
    <author>
      <name>Daniel Sonntag</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09733v1</id>
    <updated>2017-09-07T19:25:48Z</updated>
    <published>2017-09-07T19:25:48Z</published>
    <title>NIME: A Community of Communities</title>
    <summary>  Commentary on the article Fourteen Years of NIME: The Value and Meaning of
Community in Interactive Music Research by A. Marquez-Borbon and P. Stapleton.
</summary>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6084/m9.figshare.5386786</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6084/m9.figshare.5386786" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A NIME Reader: Fifteen Years of New Interfaces for Musical
  Expression, A.R. Jensenius and M.J. Lyons (eds.), Springer, pp.477-478, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.09733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01145v1</id>
    <updated>2018-03-03T11:13:53Z</updated>
    <published>2018-03-03T11:13:53Z</published>
    <title>Designing Interactions with Furniture: Towards Multi-Sensorial
  Interaction Design Processes for Interactive Furniture</title>
    <summary>  In this paper, we argue for novel user experience design methods, in the
context of reimagining ergonomics of interactive furniture.
</summary>
    <author>
      <name>Pedro Campos</name>
    </author>
    <author>
      <name>Nils Ehrenberg</name>
    </author>
    <author>
      <name>Miguel Campos</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10685v1</id>
    <updated>2018-04-23T19:42:55Z</updated>
    <published>2018-04-23T19:42:55Z</published>
    <title>Yes and...? Using Improv to Design for Narrative in Lights Out</title>
    <summary>  Mixed reality experiences often require detailed narrative that can be used
to craft physical and virtual design components. This work elaborates on a
mentoring experience at the Carnegie Mellon's ETC to consider how improv games
may be used ideate and iterate on storytelling experiences.
</summary>
    <author>
      <name>Alina Striner</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06467v1</id>
    <updated>2022-09-30T17:45:16Z</updated>
    <published>2022-09-30T17:45:16Z</published>
    <title>Against Interaction Design</title>
    <summary>  Against Interaction Design is a short manifesto that distils a position
that's emerged through a decade of creating interactive art.
  I intend it here as a provocation and a speculation on an alternative future
relationship between people and machines.
</summary>
    <author>
      <name>Tim Murray-Browne</name>
    </author>
    <link href="http://arxiv.org/abs/2210.06467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09862v1</id>
    <updated>2023-04-13T01:11:15Z</updated>
    <published>2023-04-13T01:11:15Z</published>
    <title>Accurate and Fast VR Eye-Tracking using Deflectometric Information</title>
    <summary>  We present two methods for fast and precise eye-tracking in VR headsets. Both
methods exploit deflectometric information, i.e., the specular reflection of an
extended screen over the eye surface.
</summary>
    <author>
      <name>Jiazhang Wang</name>
    </author>
    <author>
      <name>Tianfu Wang</name>
    </author>
    <author>
      <name>Bingjie Xu</name>
    </author>
    <author>
      <name>Oliver Cossairt</name>
    </author>
    <author>
      <name>Florian Willomitzer</name>
    </author>
    <link href="http://arxiv.org/abs/2304.09862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.13340v1</id>
    <updated>2023-05-21T14:48:25Z</updated>
    <published>2023-05-21T14:48:25Z</published>
    <title>El Sonido como Elemento Clave en Prácticas de Realidad Virtual</title>
    <summary>  This article discusses the importance of sound for virtual reality systems.
For this, the emotional effects generated by sound are analyzed, and its
contribution to the effect of immersion.
</summary>
    <author>
      <name>Yesid Ospitia Medina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Spanish language</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.13340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.13340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.13513v1</id>
    <updated>2023-09-24T00:20:16Z</updated>
    <published>2023-09-24T00:20:16Z</published>
    <title>The Study of Perceptual Training of Chinese Mandarin Tones for
  Monolingual Speakers of English Using Adaptive Computer Based Training
  Software</title>
    <summary>  The study explored a new technique of phonetic tone training, which may have
a positive impact on second language learning and tone training.
</summary>
    <author>
      <name>Yuke Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">53 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.13513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.13513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.11519v1</id>
    <updated>2023-12-12T22:02:32Z</updated>
    <published>2023-12-12T22:02:32Z</published>
    <title>Analysing user sentiment data for architectural interior spaces</title>
    <summary>  This study aims to develop a data driven system to enhance the analysis and
improvement of user experiences in interior spaces, acknowledging the
significant impact of design on individuals health, productivity, and quality
of life.
</summary>
    <author>
      <name>Mi Kyoung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.11519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.11519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.02971v1</id>
    <updated>2024-05-05T15:31:08Z</updated>
    <published>2024-05-05T15:31:08Z</published>
    <title>Achieving Narrative Change Through AR: Displacing the Single Story to
  Create Spatial Justice</title>
    <summary>  The ability of Augmented Reality to overcome the bias of single stories
through multidimensionality is explored in the artifacts of a youth gun
violence prevention project and its goal of narrative change.
</summary>
    <author>
      <name>Janice Tisha Samuels</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CHI 2024 (arXiv:2404.05889)</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.02971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.02971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.01056v1</id>
    <updated>2024-09-02T08:30:45Z</updated>
    <published>2024-09-02T08:30:45Z</published>
    <title>Review for future research in digital leadership</title>
    <summary>  Information Technology (IT) enables challenges and opportunities for how
enterprises organize themselves and how work unfolds in digital settings.
</summary>
    <author>
      <name>Raluca A. Stana</name>
    </author>
    <author>
      <name>Louise Harder Fischer</name>
    </author>
    <author>
      <name>Hanne Westh Nicolajsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Event41st Information Systems Research in Scandinavia Conference -
  Odder, Odder, Denmark</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.01056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.01056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601021v1</id>
    <updated>2006-01-07T11:06:23Z</updated>
    <published>2006-01-07T11:06:23Z</published>
    <title>Lighting Control using Pressure-Sensitive Touchpads</title>
    <summary>  We introduce a novel approach to control physical lighting parameters by
means of a pressure-sensitive touchpad. The two-dimensional area of the
touchpad is subdivided into 5 virtual sliders, each controlling the intensity
of a color (red, green, blue, yellow, and white). The physical interaction
methodology is modeled directly after ubiquitous mechanical sliders and dimmers
which tend to be used for intensity/volume control. Our abstraction to a
pressure-sensitive touchpad provides advantages and introduces additional
benefits over such existing devices.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0601021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.4.2; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604102v1</id>
    <updated>2006-04-25T19:32:03Z</updated>
    <published>2006-04-25T19:32:03Z</published>
    <title>HCI and Educational Metrics as Tools for VLE Evaluation</title>
    <summary>  The general set of HCI and Educational principles are considered and a
classification system constructed. A frequency analysis of principles is used
to obtain the most significant set. Metrics are devised to provide objective
measures of these principles and a consistent testing regime devised. These
principles are used to analyse Blackboard and Moodle.
</summary>
    <author>
      <name>Vita Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0604102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611158v2</id>
    <updated>2007-02-02T08:10:20Z</updated>
    <published>2006-11-30T13:28:29Z</published>
    <title>Articulation entre élaboration de solutions et argumentation
  polyphonique</title>
    <summary>  In this paper, we propose an analytical framework that aims to bring out the
nature of participants' contributions to co-design meetings, in a way that
synthesises content and function dimensions, together with the dimension of
dialogicality. We term the resulting global vision of contribution, the
"interactive profile".
</summary>
    <author>
      <name>Michael Baker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <author>
      <name>Kristine Lundt</name>
    </author>
    <author>
      <name>Arnauld Séjourné</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans EPIQUE'2003 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611158v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611158v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702003v1</id>
    <updated>2007-02-01T09:02:38Z</updated>
    <published>2007-02-01T09:02:38Z</published>
    <title>Expert Programming Knowledge: a Schema-Based Approach</title>
    <summary>  The topic of this chapter is the role of expert programming knowledge in the
understanding activity. In the "schema-based approach", the role of semantic
structures is emphasized whereas, in the "control-flow approach", the role of
syntactic structures is emphasized. Data which support schema-based models of
understanding are presented. Data which are more consistent with the
"control-flow approach" allow to discuss the limits of the former kind of
models.
</summary>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Psychology of ProgrammingAcademic Press (Ed.) (1990) 205-222</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3638v1</id>
    <updated>2007-07-24T20:19:47Z</updated>
    <published>2007-07-24T20:19:47Z</published>
    <title>The Review and Analysis of Human Computer Interaction (HCI) Principles</title>
    <summary>  The History of HCI is briefly reviewed together with three HCI models and
structure including CSCW, CSCL and CSCR. It is shown that a number of
authorities consider HCI to be a fragmented discipline with no agreed set of
unifying design principles. An analysis of usability criteria based upon
citation frequency of authors is performed in order to discover the eight most
recognised HCI principles.
</summary>
    <author>
      <name>V. Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/0707.3638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.0877v1</id>
    <updated>2007-08-07T19:53:24Z</updated>
    <published>2007-08-07T19:53:24Z</published>
    <title>A Portal Analysis for the Design of a Collaborative Research Environment
  for Students and Supervisors (CRESS) within the CSCR Domain</title>
    <summary>  In a previous paper the CSCR domain was defined. Here this is taken to the
next stage where we consider the design of a particular Collaborative Research
Environment to support Students and Supervisors CRESS. Following the CSCR
structure a preliminary design for CRESS has been established and a portal
framework analysis is undertaken in order to determine the most appropriate set
of tools for its implementation.
</summary>
    <author>
      <name>V. Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/0708.0877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1624v1</id>
    <updated>2007-08-12T19:58:37Z</updated>
    <published>2007-08-12T19:58:37Z</published>
    <title>Designing a Collaborative Research Environment for Students and their
  Supervisors (CRESS)</title>
    <summary>  In a previous paper the CSCR domain was defined. Here this is taken to the
next stage where the design of a particular Collaborative Research Environment
to support Students and Supervisors (CRESS) is considered. Following the CSCR
structure this paper deals with an analysis of 13 collaborative working
environments to determine a preliminary design for CRESS in order to discover
the most appropriate set of tools for its implementation.
</summary>
    <author>
      <name>V. Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/0708.1624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0178v2</id>
    <updated>2008-10-05T10:19:14Z</updated>
    <published>2007-09-03T09:32:28Z</published>
    <title>Effective Generation of Subjectively Random Binary Sequences</title>
    <summary>  We present an algorithm for effectively generating binary sequences which
would be rated by people as highly likely to have been generated by a random
process, such as flipping a fair coin.
</summary>
    <author>
      <name>Yasmine B. Sanderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Introduction and Section 6 revised</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.0178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.6.8; J.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3476v1</id>
    <updated>2008-02-24T01:34:36Z</updated>
    <published>2008-02-24T01:34:36Z</published>
    <title>Fun Boy Three Were Wrong: it is what you do, not the way that you do it</title>
    <summary>  I revisit some classic publications on modularity, to show what problems its
pioneers wanted to solve. These problems occur with spreadsheets too: to
recognise them may help us avoid them.
</summary>
    <author>
      <name>Jocelyn Paine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 105-116
  ISBN 978-905617-58-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.1974v1</id>
    <updated>2008-11-12T20:31:34Z</updated>
    <published>2008-11-12T20:31:34Z</published>
    <title>Magic Fairy Tales as Source for Interface Metaphors</title>
    <summary>  The work is devoted to a problem of search of metaphors for interactive
systems and systems based on Virtual Reality (VR) environments. The analysis of
magic fairy tales as a source of metaphors for interface and virtual reality is
offered. Some results of design process based on magic metaphors are
considered.
</summary>
    <author>
      <name>Vladimir L. Averbukh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.1974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.1974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0932v1</id>
    <updated>2009-08-06T18:52:41Z</updated>
    <published>2009-08-06T18:52:41Z</published>
    <title>The Medical Algorithms Project</title>
    <summary>  The Medical Algorithms Project, a web-based resource located at
www.medal.org, is the world's largest collection of medical-related
spreadsheets, consisting of over 13,500 Excel spreadsheets each encoding a
medical algorithm from 45 different areas of medical practice. This free
resource is in use worldwide with over 106,000 registered users as of March 1,
2009.
</summary>
    <author>
      <name>M. Sriram Iyengar</name>
    </author>
    <author>
      <name>John R. Svirbely</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 2 Colour Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2009 113-118
  ISBN 978-1-905617-89-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.0932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1186v1</id>
    <updated>2009-08-08T20:38:12Z</updated>
    <published>2009-08-08T20:38:12Z</published>
    <title>Checks and Controls in Spreadsheets</title>
    <summary>  Spreadsheets that are informally created are harder to test than they should
be. Simple cross-foot checks or being easily readable are modest but attainable
goals for every spreadsheet developer. This paper lists some tips on building
self-checking into a spreadsheet in order to provide more confidence to the
reader that a spreadsheet is robust.
</summary>
    <author>
      <name>Patrick O'Beirne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2009 1-7 ISBN
  978-1-905617-89-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.1186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2479v2</id>
    <updated>2010-01-12T15:20:14Z</updated>
    <published>2009-12-13T06:10:42Z</published>
    <title>Pervasive Emotions in Pervasive Computing Environments</title>
    <summary>  This submission has been withdrawn by arXiv admin. It is a verbatim copy of
arXiv:0912.1810 with only the author name and title changed.
</summary>
    <author>
      <name>Vishal Goyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This submission has been withdrawn by arXiv admin. It is a verbatim
  copy of arXiv:0912.1810 with only the author name and title changed</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.2479v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2479v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0420v1</id>
    <updated>2010-01-03T23:42:53Z</updated>
    <published>2010-01-03T23:42:53Z</published>
    <title>The Role of Head-Up Display in Computer- Assisted Instruction</title>
    <summary>  We investigated the role of HUDs in CAI. HUDs have been used in various
situations in daily lives by recent downsizing and cost down of the display
devices. CAI is one of the promising applications for HUDs. We have developed
an HUD-based CAI system for effectively presenting instructions of the
equipment in the transportable earth station. This chapter described HUDs in
CAI from a viewpoint of human-computer interaction based on the development
experience.
</summary>
    <author>
      <name>Kikuo Asai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">www.sciyo.com</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2456v1</id>
    <updated>2010-03-12T01:19:20Z</updated>
    <published>2010-03-12T01:19:20Z</published>
    <title>Piecemeal Journey To 'HALCYON' World Of Pervasive Computing : From past
  progress to future challenges</title>
    <summary>  Although 'Halcyon' means serene environment which pervasive computing aims
at, we have tried to present a different interpretation of this word. Through
our approach, we look at it in context of achieving future 'calm technology'.
The paper gives a general overview of the state of pervasive computing today,
proposes the 'HALCYON Model' and outlines the 'social' challenges faced by
system designers.
</summary>
    <author>
      <name>Rolly Seth</name>
    </author>
    <author>
      <name>Rishi Kapoor</name>
    </author>
    <author>
      <name>Hameed Al-Qaheri</name>
    </author>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <link href="http://arxiv.org/abs/1003.2456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.3633v1</id>
    <updated>2010-07-21T12:39:09Z</updated>
    <published>2010-07-21T12:39:09Z</published>
    <title>Alternatives to Mobile Keypad Design: Improved Text Feed</title>
    <summary>  In this paper we tried to focus on some of the problems with the mobile
keypad and text entering in these devices, and tried to give some possible
suggestions. We mainly took some of the basic Human Computer Interaction
principles and some general issues into consideration.
</summary>
    <author>
      <name>Satish Narayana Srirama</name>
    </author>
    <author>
      <name>M. A. A. Faruque</name>
    </author>
    <author>
      <name>M. A. S. Munni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference on Computer and Information Technology
  (ICCIT2003), 19-21 December, 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.3633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.3633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5701v1</id>
    <updated>2010-09-28T21:31:16Z</updated>
    <published>2010-09-28T21:31:16Z</published>
    <title>Changing User Attitudes to Reduce Spreadsheet Risk</title>
    <summary>  A business case study on how three simple guidelines:
  1. Make it easy to check (and maintain) 2. Make it safe to use 3. Keep
business logic out of code changed user attitudes and improved spreadsheet
quality in a financial services organisation.
</summary>
    <author>
      <name>Dermot Balson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages; ISBN 978-1-905404-50-6</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2010 133-137</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.5701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.6517v1</id>
    <updated>2012-02-29T11:17:10Z</updated>
    <published>2012-02-29T11:17:10Z</published>
    <title>Eye Pupil Location Using Webcam</title>
    <summary>  Three different algorithms used for eye pupil location were described and
tested. Algorithm efficiency comparison was based on human faces images taken
from the BioID database. Moreover all the eye localisation methods were
implemented in a dedicated application supporting eye movement based computer
control. In this case human face images were acquired by a webcam and processed
in a real-time.
</summary>
    <author>
      <name>Michal Ciesla</name>
    </author>
    <author>
      <name>Przemyslaw Koziol</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.6517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.6517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4944v1</id>
    <updated>2012-05-22T15:28:50Z</updated>
    <published>2012-05-22T15:28:50Z</published>
    <title>Emotion Detection from Text</title>
    <summary>  Emotion can be expressed in many ways that can be seen such as facial
expression and gestures, speech and by written text. Emotion Detection in text
documents is essentially a content - based classification problem involving
concepts from the domains of Natural Language Processing as well as Machine
Learning. In this paper emotion recognition based on textual data and the
techniques used in emotion detection are discussed.
</summary>
    <author>
      <name>Shiv Naresh Shivhare</name>
    </author>
    <author>
      <name>Saritha Khethawat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.0578v1</id>
    <updated>2012-09-04T09:44:07Z</updated>
    <published>2012-09-04T09:44:07Z</published>
    <title>Social Cheesecake: An UX-driven designed interface for managing contacts</title>
    <summary>  Social network management interfaces should consider separation of contexts
and tie strength. This paper shows the design process upon building the Social
Cheesecake, an interface that addresses both issues. Paper and screen
prototyping were used in the design process. Paper prototype interactions
helped to explore the metaphors in the domain, while screen prototype
consolidated the model. The prototype was finally built using HTML5 and
Javascript.
</summary>
    <author>
      <name>Alicia Díez</name>
    </author>
    <author>
      <name>Antonio Tapiador</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of IADIS WWW/Internet 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.0578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.0578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4965v1</id>
    <updated>2013-08-21T17:53:37Z</updated>
    <published>2013-08-21T17:53:37Z</published>
    <title>A proposal for a Chinese keyboard for cellphones, smartphones, ipads and
  tablets</title>
    <summary>  In this paper, we investigate the possibility to use two tilings of the
hyperbolic plane as basic frame for devising a way to input texts in Chinese
characters into messages of cellphones, smartphones, ipads and tablets.
</summary>
    <author>
      <name>Maurice Margenstern</name>
    </author>
    <author>
      <name>Lan Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 40 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.4965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="69U99, 94A99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3371v1</id>
    <updated>2013-11-14T03:12:54Z</updated>
    <published>2013-11-14T03:12:54Z</published>
    <title>Android Note Manager Application for People with Visual Impairment</title>
    <summary>  With the outburst of smart-phones today, the market is exploding with various
mobile applications. This paper proposes an application using which visually
impaired people can type a note in Grade 1 Braille and save it in the external
memory of their smart-phone. The application also shows intelligence by
activating reminders and/or calling certain contacts based on the content in
the notes.
</summary>
    <author>
      <name>Gayatri Venugopal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijmnct.2013.3502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijmnct.2013.3502" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.3371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5434v1</id>
    <updated>2013-11-21T15:00:46Z</updated>
    <published>2013-11-21T15:00:46Z</published>
    <title>The Well-tempered Compiler? The Aesthetics of Program Auralization</title>
    <summary>  In this chapter we are concerned with external auditory representations of
programs, also known as program auralization. As program auralization systems
tend to use musical representations they are necessarily affected by artistic
and aesthetic considerations. Therefore, it is instructive to explore program
auralization in the light of aesthetic computing principles.
</summary>
    <author>
      <name>Paul Vickers</name>
    </author>
    <author>
      <name>James L Alty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Aesthetic Computing (P. A. Fishwick, ed.), ch. 17, pp. 335-354,
  Boston, MA: MIT Press, 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.5434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4354v1</id>
    <updated>2014-05-17T06:01:03Z</updated>
    <published>2014-05-17T06:01:03Z</published>
    <title>Touch Survey: Comparison with Paper and Web Questionnaires</title>
    <summary>  We developed a prototype of touch-based survey tool for tablets and conducted
an experiment to compare interaction patterns of touch-based, PC-based, and
paper-based questionnaires. Our findings suggest that a touch-based interface
allows users to complete ranking questions easily, quickly, and accurately
although it can increase the time to complete a location input task for
well-known, prominent locations.
</summary>
    <author>
      <name>Tomoyo Sasao</name>
    </author>
    <author>
      <name>Shin'ichi Konomi</name>
    </author>
    <author>
      <name>Masatoshi Arikawa</name>
    </author>
    <author>
      <name>Hideyuki Fujita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01305v1</id>
    <updated>2015-07-05T23:53:59Z</updated>
    <published>2015-07-05T23:53:59Z</published>
    <title>Mixsourcing: a remix framework as a form of crowdsourcing</title>
    <summary>  In this paper, we introduce the concept of mixsourcing as a modality of
crowdsourcing focused on using remixing as a framework to get people to perform
creative tasks. We explore this idea through the design of a system that helped
us identify the promises and challenges of this peer-production modality.
</summary>
    <author>
      <name>Sarah Hallacher</name>
    </author>
    <author>
      <name>Jenny Rodenhouse</name>
    </author>
    <author>
      <name>Andres Monroy-Hernandez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In CHI 2013 Extended Abstracts on Human Factors in Computing Systems
  (CHI EA '13)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01662v1</id>
    <updated>2015-09-05T04:28:39Z</updated>
    <published>2015-09-05T04:28:39Z</published>
    <title>Tightly-Held and Ephemeral Psychometrics: Password and Passphrase
  Authentication Utilizing User-Supplied Constructs of Self</title>
    <summary>  This research investigates the role of passwords and passphrases as valid
authentication methodologies. Specifically, this research dispels earlier work
that ignores information-theoretic lessons learned from cognitive and social
psychology and psycholinguistics, and extends and enriches the current password
security model.
</summary>
    <author>
      <name>Christopher S. Pilson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1; H.1.2; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00244v1</id>
    <updated>2015-10-01T14:10:25Z</updated>
    <published>2015-10-01T14:10:25Z</published>
    <title>RDF Knowledge Graph Visualization From a Knowledge Extraction System</title>
    <summary>  In this paper, we present a system to visualize RDF knowledge graphs. These
graphs are obtained from a knowledge extraction system designed by
GEOLSemantics. This extraction is performed using natural language processing
and trigger detection. The user can visualize subgraphs by selecting some
ontology features like concepts or individuals. The system is also
multilingual, with the use of the annotated ontology in English, French, Arabic
and Chinese.
</summary>
    <author>
      <name>Fadhela Kerdjoudj</name>
    </author>
    <author>
      <name>Olivier Curé</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01665v2</id>
    <updated>2015-10-19T16:07:15Z</updated>
    <published>2015-10-06T17:00:34Z</published>
    <title>Smartphones in Mental Health: Detecting Depressive and Manic Episodes</title>
    <summary>  An observational study with patients diagnosed with bipolar disorder
investigates whether data from smartphone sensors can be used to recognize
bipolar disorder episodes and detect behavior changes that can signal an onset
of an episode using objective data.
</summary>
    <author>
      <name>Venet Osmani</name>
    </author>
    <author>
      <name>Agnes Gruenerbl</name>
    </author>
    <author>
      <name>Gernot Bahle</name>
    </author>
    <author>
      <name>Christian Haring</name>
    </author>
    <author>
      <name>Paul Lukowicz</name>
    </author>
    <author>
      <name>Oscar Mayora</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MPRV.2015.54</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MPRV.2015.54" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Pervasive Computing, vol.14, no. 3, pp. 10-13, July-Sept.
  2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.01665v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01665v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05901v3</id>
    <updated>2016-04-12T13:47:25Z</updated>
    <published>2016-03-18T16:09:09Z</published>
    <title>Emotion Classification from Noisy Speech - A Deep Learning Approach</title>
    <summary>  This paper investigates the performance of Deep Learning for speech emotion
classification when the speech is compounded with noise. It reports on the
classification accuracy and concludes with the future directions for achieving
greater robustness for emotion recognition from noisy speech.
</summary>
    <author>
      <name>Rajib Rana</name>
    </author>
    <link href="http://arxiv.org/abs/1603.05901v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05901v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01331v1</id>
    <updated>2016-04-05T17:00:59Z</updated>
    <published>2016-04-05T17:00:59Z</published>
    <title>A smartphone-based vision simulator</title>
    <summary>  Simulators, as tools that can clearly bring out the effect of impairment, are
invaluable in the design and development process of an assistive device.
Simulators are vital in meeting high standards of accessibility. Described is
our work on a smartphone-based vision simulator for diabetic retinopathy that
is economic, portable, flexible and easy-to-use.
</summary>
    <author>
      <name>Pragathi Praveena</name>
    </author>
    <author>
      <name>Jobin J Kavalam</name>
    </author>
    <author>
      <name>Namita Jacob</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 image, 3rd International Conference on Biomedical
  Engineering and Assistive Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05572v1</id>
    <updated>2016-04-19T14:01:57Z</updated>
    <published>2016-04-19T14:01:57Z</published>
    <title>Does Anonymity Increase the Chance to Get Feedback?</title>
    <summary>  To generate a hypothesis about the effects of anonymity on user participation
in online communities, comments on Youtube were analysed for effects of the
change from allowing pseudonyms to Google+ with its real name policy. Small
differences were detected, leading to the hypothesis that the option to remain
anonymous leads to a less active environment for getting feedback, with less
polite and less rude comments on the expense of neutral ones.
</summary>
    <author>
      <name>Malte Paskuda</name>
    </author>
    <author>
      <name>Myriam Lewkowicz</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09432v1</id>
    <updated>2016-05-30T22:05:36Z</updated>
    <published>2016-05-30T22:05:36Z</published>
    <title>Evaluating Crowdsourcing Participants in the Absence of Ground-Truth</title>
    <summary>  Given a supervised/semi-supervised learning scenario where multiple
annotators are available, we consider the problem of identification of
adversarial or unreliable annotators.
</summary>
    <author>
      <name>Ramanathan Subramanian</name>
    </author>
    <author>
      <name>Romer Rosales</name>
    </author>
    <author>
      <name>Glenn Fung</name>
    </author>
    <author>
      <name>Jennifer Dy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, Workshop on Human Computation for Science and
  Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02427v1</id>
    <updated>2016-06-08T07:14:24Z</updated>
    <published>2016-06-08T07:14:24Z</published>
    <title>VIF: Virtual Interactive Fiction (with a twist)</title>
    <summary>  Nowadays computer science can create digital worlds that deeply immerse
users; it can also process in real time brain activity to infer their inner
states. What marvels can we achieve with such technologies? Go back to
displaying text. And unfold a story that follows and molds users as never
before.
</summary>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc, UB</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pervasive Play - CHI '16 Workshop, May 2016, San Jose, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02661v1</id>
    <updated>2016-08-08T23:51:47Z</updated>
    <published>2016-08-08T23:51:47Z</published>
    <title>ActiveCrowd: A Framework for Optimized Multi-Task Allocation in Mobile
  Crowdsensing Systems</title>
    <summary>  Worker selection is a key issue in Mobile Crowd Sensing (MCS). While previous
worker selection approaches mainly focus on selecting a proper subset of
workers for a single MCS task, multi-task-oriented worker selection is
essential and useful for the efficiency of large-scale MCS platforms. This
paper proposes ActiveCrowd, a worker selection framework for multi-task MCS
environments.
</summary>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <author>
      <name>Wenle Wu</name>
    </author>
    <author>
      <name>Zhiwen Yu</name>
    </author>
    <author>
      <name>Qi Han</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00754v1</id>
    <updated>2016-09-02T21:47:47Z</updated>
    <published>2016-09-02T21:47:47Z</published>
    <title>A heuristic extending the Squarified treemapping algorithm</title>
    <summary>  A heuristic extending the Squarified Treemap technique for the representation
of hierarchical information as treemaps is presented. The original technique
gives high quality treemap views, since items are laid out with rectangles that
approximate squares, allowing easy comparison and selection operations. New key
steps, with a low computational impact, have been introduced to yield treemaps
with even better aspect ratios and higher homogeneity among items.
</summary>
    <author>
      <name>Antonio Cesarano</name>
    </author>
    <author>
      <name>FIlomena Ferrucci</name>
    </author>
    <author>
      <name>Mario Torre</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07577v1</id>
    <updated>2016-10-20T10:38:12Z</updated>
    <published>2016-10-20T10:38:12Z</published>
    <title>A new content format for immersive experiences</title>
    <summary>  The arrival of head-mounted displays (HMDs) to the consumer market requires a
novel content format that is, first, adapted to the specificities of immersive
displays and, second, that takes into account the current reality of
multi-display media consumption. We review the requirements for such content
format and report on existing initiatives, some of our own, towards
implementing such content format.
</summary>
    <author>
      <name>Joan Llobera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">position paper, Hot3d workshop at ICME 2016, Seattle</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00872v1</id>
    <updated>2016-11-03T03:53:25Z</updated>
    <published>2016-11-03T03:53:25Z</published>
    <title>A Decision Support System for Inbound Marketers: An Empirical Use of
  Latent Dirichlet Allocation Topic Model to Guide Infographic Designers</title>
    <summary>  Infographic is a type of information presentation that inbound marketers use.
I suggest a method that can allow the infographic designers to benchmark their
design against the previous viral infographics to measure whether a given
design decision can help or hurt the probability of the design becoming viral.
</summary>
    <author>
      <name>Meisam Hejazi Nia</name>
    </author>
    <link href="http://arxiv.org/abs/1611.00872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06114v1</id>
    <updated>2016-12-19T10:42:57Z</updated>
    <published>2016-12-19T10:42:57Z</published>
    <title>A real-time framework for visual feedback of articulatory data using
  statistical shape models</title>
    <summary>  We present a novel open-source framework for visualizing electromagnetic
articulography (EMA) data in real-time, with a modular framework and
anatomically accurate tongue and palate models derived by multilinear subspace
learning.
</summary>
    <author>
      <name>Kristy James</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DFKI</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander Hewer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DFKI</arxiv:affiliation>
    </author>
    <author>
      <name>Ingmar Steiner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DFKI</arxiv:affiliation>
    </author>
    <author>
      <name>Stefanie Wuhrer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MORPHEO</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17th Annual Conference of the International Speech Communication
  Association (Interspeech), Oct 2016, San Francisco, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05250v1</id>
    <updated>2017-02-17T08:20:37Z</updated>
    <published>2017-02-17T08:20:37Z</published>
    <title>Intelligent User Interfaces - A Tutorial</title>
    <summary>  IUIs aim to incorporate intelligent automated capabilities in human computer
interaction, where the net impact is a human-computer interaction that improves
performance or usability in critical ways. It also involves designing and
implementing an artificial intelligence (AI) component that effectively
leverages human skills and capabilities, so that human performance with an
application excels. IUIs embody capabilities that have traditionally been
associated more strongly with humans than with computers: how to perceive,
interpret, learn, use language, reason, plan, and decide.
</summary>
    <author>
      <name>Daniel Sonntag</name>
    </author>
    <link href="http://arxiv.org/abs/1702.05250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04574v1</id>
    <updated>2017-03-14T14:07:19Z</updated>
    <published>2017-03-14T14:07:19Z</published>
    <title>Causes of discomfort in stereoscopic content: a review</title>
    <summary>  This paper reviews the causes of discomfort in viewing stereoscopic content.
These include objective factors, such as misaligned images, as well as
subjective factors, such as excessive disparity. Different approaches to the
measurement of visual discomfort are also reviewed, in relation to the
underlying physiological and psychophysical processes. The importance of
understanding these issues, in the context of new display technologies, is
emphasized.
</summary>
    <author>
      <name>Kasim Terzic</name>
    </author>
    <author>
      <name>Miles Hansard</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03521v1</id>
    <updated>2017-04-09T20:18:25Z</updated>
    <published>2017-04-09T20:18:25Z</published>
    <title>Responsive Graphical User Interface (ReGUI) and its Implementation in
  MATLAB</title>
    <summary>  In this paper we introduce the responsive graphical user interface (ReGUI)
approach to creating applications, and demonstrate how this approach can be
implemented in MATLAB. The same general technique can be used in other
programming languages.
</summary>
    <author>
      <name>Matej Mikulszky</name>
    </author>
    <author>
      <name>Jana Pocsova</name>
    </author>
    <author>
      <name>Andrea Mojzisova</name>
    </author>
    <author>
      <name>Igor Podlubny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.2; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03973v1</id>
    <updated>2017-05-10T23:22:43Z</updated>
    <published>2017-05-10T23:22:43Z</published>
    <title>Transreality puzzle as new genres of entertainment technology</title>
    <summary>  The author considers a class of mechatronic puzzles falling in the
mixed-reality category, present examples of such devices, and propose a way to
categorize them. Close relationships of such devices with the Tangible User
Interface are described. The device designed by the author as an illustration
of a mixed reality puzzle is presented.
</summary>
    <author>
      <name>Ilya V Osipov</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08205v1</id>
    <updated>2017-06-26T02:07:11Z</updated>
    <published>2017-06-26T02:07:11Z</published>
    <title>Metrics for Bengali Text Entry Research</title>
    <summary>  With the intention of bringing uniformity to Bengali text entry research,
here we present a new approach for calculating the most popular English text
entry evaluation metrics for Bengali. To demonstrate our approach, we conducted
a user study where we evaluated four popular Bengali text entry techniques.
</summary>
    <author>
      <name>Sayan Sarcar</name>
    </author>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <author>
      <name>Ali Mazalek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted and presented as a position paper at ACM
  CHI 2015 workshop on "Text entry on the edge"</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03751v1</id>
    <updated>2017-07-05T13:50:20Z</updated>
    <published>2017-07-05T13:50:20Z</published>
    <title>New Symbols for Base-16 and Base-256 Numerals</title>
    <summary>  A new system of hexadecimal and base-256 numerals is proposed whose digit
shapes are based on binary numerals. The proposed numerals are implemented in
open source fonts and integrated into popular editors (Notepad++ and Eclipse)
to prove the concept.
</summary>
    <author>
      <name>MacKenzie Cumings</name>
    </author>
    <author>
      <name>Valdis Vītoliņš</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Engineering Technology
  (2017) pp. 205-212. ISSN 2229-3345</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.03751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08011v1</id>
    <updated>2017-07-25T14:19:23Z</updated>
    <published>2017-07-25T14:19:23Z</published>
    <title>Machine Intelligence, New Interfaces, and the Art of the Soluble</title>
    <summary>  Position: (1) Partial solutions to machine intelligence can lead to systems
which may be useful creating interesting and expressive musical works. (2) An
appropriate general goal for this field is augmenting human expression. (3) The
study of the aesthetics of human augmentation in musical performance is in its
infancy.
</summary>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6084/m9.figshare.5242045.v1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6084/m9.figshare.5242045.v1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2015 Workshop on Collaborating with Intelligent Machines:
  Interfaces for Creative Sound, April 18, 2015, Seoul, Republic of Korea</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05006v1</id>
    <updated>2017-08-16T09:40:53Z</updated>
    <published>2017-08-16T09:40:53Z</published>
    <title>A Survey of Augmented Reality Navigation</title>
    <summary>  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
</summary>
    <author>
      <name>Gaurav Bhorkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Seminar on Software Systems, Technologies and Security, Spring 2016,
  Aalto University</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08957v1</id>
    <updated>2017-09-26T12:03:36Z</updated>
    <published>2017-09-26T12:03:36Z</published>
    <title>Beyond Accessibility: Lifting Perceptual Limitations for Everyone</title>
    <summary>  We propose that accessibility research can lay the foundation for technology
that can be used to augment the perception of everyone. To show how this can be
achieved, we present three case studies of our research in which we demonstrate
our approaches for impaired colour vision, situational visual impairments and
situational hearing impairment.
</summary>
    <author>
      <name>Michael Mauderer</name>
    </author>
    <author>
      <name>Garreth W. Tigwell</name>
    </author>
    <author>
      <name>Benjamin M. Gorman</name>
    </author>
    <author>
      <name>David R. Flatla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01237v2</id>
    <updated>2019-10-31T01:13:51Z</updated>
    <published>2018-01-04T03:30:47Z</published>
    <title>A pairwise discriminative task for speech emotion recognition</title>
    <summary>  I have submitted a new version to arXiv:1910.11174. I forget to choose to
replace the old version, but submitted a new one. It's my mistake.
</summary>
    <author>
      <name>Zheng Lian</name>
    </author>
    <author>
      <name>Ya Li</name>
    </author>
    <author>
      <name>Jianhua Tao</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I have submitted a new version to arXiv:1910.11174. I forget to
  choose to replace the old version, but submitted a new one. It's my mistake</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01237v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01237v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01645v1</id>
    <updated>2018-03-05T13:01:42Z</updated>
    <published>2018-03-05T13:01:42Z</published>
    <title>You Are Okay: Towards User Interfaces for Improving Well-being</title>
    <summary>  Well-being is a relatively broad concept which can be succinctly described as
the state of being happy, healthy or successful. Interesting things happen when
bridging user interface design with the psychology of human well-being. This
position paper aims at providing a short on reflection the challenges and
opportunities in this context and presents concrete examples on how to tackle
these challenges and exploit the existing design opportunities.
</summary>
    <author>
      <name>Pedro F. Campos</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07947v1</id>
    <updated>2018-03-21T14:40:05Z</updated>
    <published>2018-03-21T14:40:05Z</published>
    <title>Crowd-Machine Collaboration for Item Screening</title>
    <summary>  In this paper we describe how crowd and machine classifier can be efficiently
combined to screen items that satisfy a set of predicates. We show that this is
a recurring problem in many domains, present machine-human (hybrid) algorithms
that screen items efficiently and estimate the gain over human-only or
machine-only screening in terms of performance and cost.
</summary>
    <author>
      <name>Evgeny Krivosheev</name>
    </author>
    <author>
      <name>Bahareh Harandizadeh</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <author>
      <name>Boualem Benatallah</name>
    </author>
    <link href="http://arxiv.org/abs/1803.07947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09145v1</id>
    <updated>2018-08-28T07:24:15Z</updated>
    <published>2018-08-28T07:24:15Z</published>
    <title>Psychological Frameworks for Persuasive Information and Communications
  Technologies</title>
    <summary>  When developing devices to encourage positive change in users, social
psychology can offer useful conceptual resources. This article outlines three
major theories from the discipline and discusses their implications for
designing persuasive technologies.
</summary>
    <author>
      <name>Joseph J. P. Simons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MPRV.2016.52</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MPRV.2016.52" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Simons, Joseph JP. "Psychological Frameworks for Persuasive
  Information and Communications Technologies." IEEE Pervasive Computing 15,
  no. 3 (2016): 68-76</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.09145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01564v1</id>
    <updated>2019-02-05T06:07:49Z</updated>
    <published>2019-02-05T06:07:49Z</published>
    <title>Animated Drag and Drop Interaction for Dynamic Multidimensional Graphs</title>
    <summary>  In this paper, we propose a new drag and drop interaction technique for
graphs. We designed this interaction to support analysis in complex
multidimensional and temporal graphs. The drag and drop interaction is enhanced
with an intuitive and controllable animation, in support of comparison tasks.
</summary>
    <author>
      <name>Benjamin Renoust</name>
    </author>
    <author>
      <name>Haolin Ren</name>
    </author>
    <author>
      <name>Guy Melançon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.01564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03239v1</id>
    <updated>2019-02-07T22:12:55Z</updated>
    <published>2019-02-07T22:12:55Z</published>
    <title>A Description of a Subtask Dataset with Glances</title>
    <summary>  This paper describes a set of data made available that contains detailed
subtask coding of interactions with several production vehicle human machine
interfaces (HMIs) on open roadways, along with accompanying eyeglance data.
</summary>
    <author>
      <name>B. D. Sawyer</name>
    </author>
    <author>
      <name>Sean Seaman</name>
    </author>
    <author>
      <name>Linda Angell</name>
    </author>
    <author>
      <name>Jon Dobres</name>
    </author>
    <author>
      <name>Bruce Mehler</name>
    </author>
    <author>
      <name>Bryan Reimer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper with two (2) json databases and two (2) csv data dictionaries</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01808v1</id>
    <updated>2019-03-05T13:21:44Z</updated>
    <published>2019-03-05T13:21:44Z</published>
    <title>Augmented Reality, Cyber-Physical Systems, and Feedback Control for
  Additive Manufacturing: A Review</title>
    <summary>  Our objective in this paper is to review the application of feedback ideas in
the area of additive manufacturing. Both the application of feedback control to
the 3D printing process, and the application of feedback theory to enable users
to interact better with machines, are reviewed. Where appropriate,
opportunities for future work are highlighted.
</summary>
    <author>
      <name>Hugo Lhachemi</name>
    </author>
    <author>
      <name>Ammar Malik</name>
    </author>
    <author>
      <name>Robert Shorten</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2019.2907287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2019.2907287" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access, vol. 7, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.01808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02612v1</id>
    <updated>2019-03-06T21:09:31Z</updated>
    <published>2019-03-06T21:09:31Z</published>
    <title>Visual Analysis of Photo Policy Misconfigurations Using Treemaps</title>
    <summary>  Online photo privacy is a major concern for social media users. Numerous
visualization tools have been proposed to help the users easily compose and
understand policies on social networks. However, these tools do not incorporate
the ability to quickly identify and fix unintended photo sharing. We propose a
tool that displays the photo albums w.r.t their policy misconfigurations using
a Treemap visualization.
</summary>
    <author>
      <name>Yousra Javed</name>
    </author>
    <author>
      <name>Mohamed Shehab</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.02612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06417v1</id>
    <updated>2019-04-12T21:01:01Z</updated>
    <published>2019-04-12T21:01:01Z</published>
    <title>The Future of Pedestrian Automated Vehicle Interactions</title>
    <summary>  While the increasing popularity of autonomous vehicles has garnered critical
media attention, less has been written about the field of pedestrian-automated
vehicle interactions and its challenges. Current research trends are discussed
as well as several areas receiving much less attention, but are still vital to
the field.
</summary>
    <author>
      <name>Lionel P. Robert Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3313115</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3313115" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">XRDS 25(3) (April 2019), 30-33. DOI:
  https://doi.org/10.1145/3313115</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.06417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07076v2</id>
    <updated>2020-02-27T16:49:12Z</updated>
    <published>2019-05-17T01:17:03Z</published>
    <title>TGView3D System Description: 3-Dimensional Visualization of Theory
  Graphs</title>
    <summary>  We describe TGView3D, an interactive 3D graph viewer optimized for exploring
theory graphs. To exploit the three spatial dimensions, it extends a
force-directed layout with a hierarchical component. Because of the limitations
of regular displays, the system also supports the use of a head-mounted display
and utilizes several virtual realty interaction concepts.
</summary>
    <author>
      <name>Richard Marcus</name>
    </author>
    <author>
      <name>Michael Kohlhase</name>
    </author>
    <author>
      <name>Florian Rabe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, fixed typo</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02637v1</id>
    <updated>2019-10-21T12:30:17Z</updated>
    <published>2019-10-21T12:30:17Z</published>
    <title>Homo Cyberneticus: The Era of Human-AI Integration</title>
    <summary>  This article is submitted and accepted as ACM UIST 2019 Visions. UIST Visions
is a venue for forward thinking ideas to inspire the community. The goal is not
to report research but to project and propose new research directions. This
article, entitled "Homo Cyberneticus: The Era of Human-AI Integration",
proposes HCI research directions, namely human-augmentation and
human-AI-integration.
</summary>
    <author>
      <name>Jun Rekimoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1235</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1235" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM UIST 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.02637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06541v2</id>
    <updated>2019-11-20T01:53:18Z</updated>
    <published>2019-11-15T09:50:43Z</published>
    <title>The Markup Language for Designing Gaze Controlled Applications</title>
    <summary>  The Gaze Interaction Markup Language (GIML) is presented, which is new
language for designing the gaze-controlled application and psychological
experiments (also by non-programmers).
</summary>
    <author>
      <name>J. Matulewski</name>
    </author>
    <author>
      <name>B. Bałaj</name>
    </author>
    <author>
      <name>I. Mościchowska</name>
    </author>
    <author>
      <name>A. Ignaczewska</name>
    </author>
    <author>
      <name>R. Linowiecki</name>
    </author>
    <author>
      <name>J. Dreszer</name>
    </author>
    <author>
      <name>W. Duch</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3, H.5.2" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09605v2</id>
    <updated>2019-11-22T16:22:11Z</updated>
    <published>2019-11-21T17:01:14Z</published>
    <title>A brief chronology of Virtual Reality</title>
    <summary>  In this article, we are going to review a brief history of the field of
Virtual Reality (VR), VR systems, and applications and discuss how they
evolved. After that, we will familiarize ourselves with the essential
components of VR experiences and common VR terminology. Finally, we discuss the
evolution of ubiquitous VR as a subfield of VR and its current trends.
</summary>
    <author>
      <name>Aryabrata Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint to other suitable (potential) journals</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.09605v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09605v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09303v2</id>
    <updated>2020-02-24T11:56:21Z</updated>
    <published>2020-02-20T16:16:28Z</published>
    <title>The DIDI dataset: Digital Ink Diagram data</title>
    <summary>  We are releasing a dataset of diagram drawings with dynamic drawing
information. The dataset aims to foster research in interactive graphical
symbolic understanding. The dataset was obtained using a prompted data
collection effort.
</summary>
    <author>
      <name>Philippe Gervais</name>
    </author>
    <author>
      <name>Thomas Deselaers</name>
    </author>
    <author>
      <name>Emre Aksan</name>
    </author>
    <author>
      <name>Otmar Hilliges</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09303v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09303v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05235v1</id>
    <updated>2020-04-10T21:37:18Z</updated>
    <published>2020-04-10T21:37:18Z</published>
    <title>Using Conformity to Probe Interaction Challenges in XR Collaboration</title>
    <summary>  The concept of a conformity spectrum is introduced to describe the degree to
which virtualization adheres to real world physical characteristics surrounding
the user. This is then used to examine interaction challenges when
collaborating across different levels of virtuality and conformity.
</summary>
    <author>
      <name>Jeremy Hartmann</name>
    </author>
    <author>
      <name>Hemant Bhaskar Surale</name>
    </author>
    <author>
      <name>Aakar Gupta</name>
    </author>
    <author>
      <name>Daniel Vogel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 Figures, CHI 2018 April 21 to 26, 2018, Montreal, QC,
  Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.05235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07792v1</id>
    <updated>2020-05-15T21:32:19Z</updated>
    <published>2020-05-15T21:32:19Z</published>
    <title>Technologies and Workflow of Creative Coding Projects: Examples from the
  Google DevArt Competition</title>
    <summary>  Recently, many artists and creative technologists created computer
programming as a goal to create something expressive instead of something
functional. In this paper, I analyzed 18 creative coding projects from the
Google DevArt competition and summarized the critical technologies, types of
content, and key workflows of these creative coding projects. The paper also
discusses the potential research opportunities for creative coding and art
technologies.
</summary>
    <author>
      <name>Youyang Hou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.07792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12665v1</id>
    <updated>2020-07-24T17:31:54Z</updated>
    <published>2020-07-24T17:31:54Z</published>
    <title>Considerations for Eye Tracking Experiments in Information Retrieval</title>
    <summary>  In this survey I discuss ophthalmic neurophysiology and the experimental
considerations that must be made to reduce possible noise in an eye-tracking
data stream. I also review the history, experiments, technological benefits and
limitations of eye-tracking within the information retrieval field. The
concepts of aware and adaptive user interfaces are also explored that humbly
make an attempt to synthesize work from the fields of industrial engineering
and psychophysiology with information retrieval.
</summary>
    <author>
      <name>Michael Segundo Ortiz</name>
    </author>
    <link href="http://arxiv.org/abs/2007.12665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.14734v1</id>
    <updated>2020-07-29T10:44:36Z</updated>
    <published>2020-07-29T10:44:36Z</published>
    <title>Maritime Design: A CSCW Territory?</title>
    <summary>  This paper focuses on remote-control and autonomous vessels from a
sociological perspective. We report that if CSCW research aims to shed light on
other disciplines, researchers should be reflexive insider that first position
themselves in such disciplines. Through reflexive practice, CSCW researchers
could connect communities of practice, thus narrowing the distance between
design and engineering.
</summary>
    <author>
      <name>Yushan Pan</name>
    </author>
    <author>
      <name>Hans Petter Hildre</name>
    </author>
    <link href="http://arxiv.org/abs/2007.14734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.00078v1</id>
    <updated>2020-11-30T20:02:19Z</updated>
    <published>2020-11-30T20:02:19Z</published>
    <title>Why Did the Robot Cross the Road? A User Study of Explanation in
  Human-Robot Interaction</title>
    <summary>  This work documents a pilot user study evaluating the effectiveness of
contrastive, causal and example explanations in supporting human understanding
of AI in a hypothetical commonplace human robot interaction HRI scenario. In
doing so, this work situates explainable AI XAI in the context of the social
sciences and suggests that HRI explanations are improved when informed by the
social sciences.
</summary>
    <author>
      <name>Zachary Taschdjian</name>
    </author>
    <link href="http://arxiv.org/abs/2012.00078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12002v1</id>
    <updated>2020-12-22T13:45:13Z</updated>
    <published>2020-12-22T13:45:13Z</published>
    <title>Trust in robot-mediated health information</title>
    <summary>  This paper outlines a social robot platform for providing health information.
In comparison with previous findings for accessing information online, the use
of a social robot may affect which factors users consider important when
evaluating the trustworthiness of health information provided.
</summary>
    <author>
      <name>David Cameron</name>
    </author>
    <author>
      <name>Marina Sarda Gou</name>
    </author>
    <author>
      <name>Laura Sbaffi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.12002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.16168v1</id>
    <updated>2021-03-30T08:49:45Z</updated>
    <published>2021-03-30T08:49:45Z</published>
    <title>Understanding Mental Models of AI through Player-AI Interaction</title>
    <summary>  Designing human-centered AI-driven applications require deep understandings
of how people develop mental models of AI. Currently, we have little knowledge
of this process and limited tools to study it. This paper presents the position
that AI-based games, particularly the player-AI interaction component, offer an
ideal domain to study the process in which mental models evolve. We present a
case study to illustrate the benefits of our approach for explainable AI.
</summary>
    <author>
      <name>Jennifer Villareale</name>
    </author>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2103.16168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.16168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12610v1</id>
    <updated>2021-05-26T15:07:12Z</updated>
    <published>2021-05-26T15:07:12Z</published>
    <title>POD: A Smartphone That Flies</title>
    <summary>  We present POD, a smartphone that flies, as a new way to achieve hands-free,
eyes-up mobile computing. Unlike existing drone-carried user interfaces, POD
features a smartphone-sized display and the computing and sensing power of a
modern smartphone. We share our experience in building a prototype of POD,
discuss the technical challenges facing it, and describe early results toward
addressing them.
</summary>
    <author>
      <name>Guojun Chen</name>
    </author>
    <author>
      <name>Noah Weiner</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2105.12610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14835v1</id>
    <updated>2021-06-28T16:23:38Z</updated>
    <published>2021-06-28T16:23:38Z</published>
    <title>Virtual Agents in Live Coding: A Short Review</title>
    <summary>  AI and live coding has been little explored. This article contributes with a
short review of different perspectives of using virtual agents in the practice
of live coding looking at past and present as well as pointing to future
directions.
</summary>
    <author>
      <name>Anna Xambó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint version submitted to eContact! (https://econtact.ca) for the
  special issue 21.1 - Take Back the Stage: Live coding, live audiovisual,
  laptop orchestra</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.14835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08584v1</id>
    <updated>2021-10-16T14:35:48Z</updated>
    <published>2021-10-16T14:35:48Z</published>
    <title>Surveying Wonderland for many more literature visualization techniques</title>
    <summary>  There are still many potential literature visualizations to be discovered. By
focusing on a single text, the author surveys many existing visualizations
across research domains, in the wild, and creates new visualizations. 58
techniques are indicated, suggesting a wider variety of visualizations beyond
research disciplines.
</summary>
    <author>
      <name>Richard Brath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. 58 figures, each a different visualization of Alice's
  Adventures in Wonderland</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.08584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04472v1</id>
    <updated>2021-11-04T03:21:03Z</updated>
    <published>2021-11-04T03:21:03Z</published>
    <title>Ten Conceptual Dimensions of Context</title>
    <summary>  This paper attempts to synthesize various conceptualizations of the term
"context" as found in computing literature. Ten conceptual dimensions of
context thus emerge -- location; user, task, and system characteristics;
physical, social, organizational, and cultural environments; time-related
aspects, and historical information. Together, the ten dimensions of context
provide a comprehensive view of the notion of context, and allow for a more
systematic examination of the influence of context and contextual information
on human-system or human-AI interactions.
</summary>
    <author>
      <name>Hashai Papneja</name>
    </author>
    <link href="http://arxiv.org/abs/2111.04472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.12795v1</id>
    <updated>2021-11-24T21:00:42Z</updated>
    <published>2021-11-24T21:00:42Z</published>
    <title>Picasso: Model-free Feature Visualization</title>
    <summary>  Today, Machine Learning (ML) applications can have access to tens of
thousands of features. With such feature sets, efficiently browsing and
curating subsets of most relevant features is a challenge. In this paper, we
present a novel approach to visualize up to several thousands of features in a
single image. The image not only shows information on individual features, but
also expresses feature interactions via the relative positioning of features.
</summary>
    <author>
      <name>Binh Vu</name>
    </author>
    <author>
      <name>Igor Markov</name>
    </author>
    <link href="http://arxiv.org/abs/2111.12795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00076v1</id>
    <updated>2021-11-30T20:21:19Z</updated>
    <published>2021-11-30T20:21:19Z</published>
    <title>Using Conversational Artificial Intelligence to Support Children's
  Search in the Classroom</title>
    <summary>  We present pathways of investigation regarding conversational user interfaces
(CUIs) for children in the classroom. We highlight anticipated challenges to be
addressed in order to advance knowledge on CUIs for children. Further, we
discuss preliminary ideas on strategies for evaluation.
</summary>
    <author>
      <name>Garrett Allen</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Maria Soledad Pera</name>
    </author>
    <author>
      <name>Ujwal Gadiraju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CUI@CSCW 2021 --
  https://www.conversationaluserinterfaces.org/workshops/CSCW2021/pdfs/2-Allen.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01919v1</id>
    <updated>2021-11-16T07:05:49Z</updated>
    <published>2021-11-16T07:05:49Z</published>
    <title>On Female Audience Sending Virtual Gifts to Male Streamers on Douyin</title>
    <summary>  Live streaming has become increasingly popular. Our study focuses on the
emerging Chinese female audiences who send virtual gifts to young male
streamers. We observe a reversed entertainer-viewer gender relationship. We aim
to study why they watch young male streamers, why they send gifts, and their
relationships with these streamers.
</summary>
    <author>
      <name>Huilian Sophie Qiu</name>
    </author>
    <link href="http://arxiv.org/abs/2112.01919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.15169v1</id>
    <updated>2021-12-30T18:52:12Z</updated>
    <published>2021-12-30T18:52:12Z</published>
    <title>Investigations of Smart Health Reliability</title>
    <summary>  A balanced investigation into the reliability of wireless smart health
devices when it comes to the collection of biometric data under varying
network/environmental conditions. Followed by a program implementation to begin
introductory analysis on measurement accuracy and data collection to gauge the
reliability of smart health devices.
</summary>
    <author>
      <name>Sharlet Claros</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Ting Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2112.15169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.15169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.16122v1</id>
    <updated>2022-03-30T08:03:45Z</updated>
    <published>2022-03-30T08:03:45Z</published>
    <title>Eff the ineffable: on the uncommunicability of a conceptually simple
  contribution to HCI methodology</title>
    <summary>  Given a simple concept that has the potential for a methodological
contribution to the field of HCI, the constraints of submission conventions
within the field may make it impossible to communicate the concept in a manner
that is intelligible to the reader.
</summary>
    <author>
      <name>Helen Oliver</name>
    </author>
    <author>
      <name>Richard Mortier</name>
    </author>
    <author>
      <name>Jon Crowcroft</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.16122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.16122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0025v1</id>
    <updated>2007-05-01T15:44:17Z</updated>
    <published>2007-05-01T15:44:17Z</published>
    <title>Can the Internet cope with stress?</title>
    <summary>  When will the Internet become aware of itself? In this note the problem is
approached by asking an alternative question: Can the Internet cope with
stress? By extrapolating the psychological difference between coping and
defense mechanisms a distributed software experiment is outlined which could
reject the hypothesis that the Internet is not a conscious entity.
</summary>
    <author>
      <name>Andreas Martin Lisewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.0025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0166v1</id>
    <updated>2008-03-03T01:40:31Z</updated>
    <published>2008-03-03T01:40:31Z</published>
    <title>Spreadsheet Validation and Analysis through Content Visualization</title>
    <summary>  Visualizing spreadsheet content provides analytic insight and visual
validation of large amounts of spreadsheet data. Oculus Excel Visualizer is a
point and click data visualization experiment which directly visualizes Excel
data and re-uses the layout and formatting already present in the spreadsheet.
</summary>
    <author>
      <name>Richard Brath</name>
    </author>
    <author>
      <name>Michael Peters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 11 Colour Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 175-183
  ISBN:1-905617-08-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.2.5; H.4.1; K.6.4; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2302v1</id>
    <updated>2011-12-10T20:30:52Z</updated>
    <published>2011-12-10T20:30:52Z</published>
    <title>A Door into Another World</title>
    <summary>  Is it possible to design programs which each user can change according to his
preferences? Not an illusion of such a thing that adaptive interface provides
but really an interface ruled by users. What is the main problem of such design
and what is the solution to this problem? This short article gives a glimpse
into the theory discussed in the book "The World of Movable Objects".
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.2302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03403v2</id>
    <updated>2015-08-30T13:28:19Z</updated>
    <published>2015-03-11T16:20:51Z</published>
    <title>Bublz! : Playing with Bubbles to Develop Mathematical Thinking</title>
    <summary>  We encounter mathematical problems in various forms in our lives, thus making
mathematical thinking an important human ability. In this paper, we present
Bublz!, a simple, click-driven game for children to engage in and develop
mathematical thinking in an enjoyable manner.
</summary>
    <author>
      <name>Dhruv Chand</name>
    </author>
    <author>
      <name>Karthik Gopalakrishnan</name>
    </author>
    <author>
      <name>Nisha KK</name>
    </author>
    <author>
      <name>Mudit Sinha</name>
    </author>
    <author>
      <name>Shreya Sriram</name>
    </author>
    <link href="http://arxiv.org/abs/1503.03403v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03403v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01819v1</id>
    <updated>2018-05-04T15:15:01Z</updated>
    <published>2018-05-04T15:15:01Z</published>
    <title>Time-on-Task Estimation with Log-Normal Mixture Model</title>
    <summary>  We describe a method of estimating a user's time-on-task in an online
learning environment. The method is agnostic of the details of the user's
mental activity and does not rely on any data except timestamps of user's
interactions, accounting for individual user differences. The method is
implemented in R (the code is open-source) and has been tested in the data from
a large sample of HarvardX MOOCs.
</summary>
    <author>
      <name>Ilia Rushkin</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02489v2</id>
    <updated>2018-05-30T13:03:59Z</updated>
    <published>2018-05-03T19:42:57Z</published>
    <title>Transformer for Emotion Recognition</title>
    <summary>  This paper describes the UMONS solution for the OMG-Emotion Challenge. We
explore a context-dependent architecture where the arousal and valence of an
utterance are predicted according to its surrounding context (i.e. the
preceding and following utterances of the video). We report an improvement when
taking into account context for both unimodal and multimodal predictions.
</summary>
    <author>
      <name>Jean-Benoit Delbrouck</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12346v1</id>
    <updated>2018-05-31T07:06:11Z</updated>
    <published>2018-05-31T07:06:11Z</published>
    <title>Crowdsourcing for Reminiscence Chatbot Design</title>
    <summary>  In this work-in-progress paper we discuss the challenges in identifying
effective and scalable crowd-based strategies for designing content,
conversation logic, and meaningful metrics for a reminiscence chatbot targeted
at older adults. We formalize the problem and outline the main research
questions that drive the research agenda in chatbot design for reminiscence and
for relational agents for older adults in general.
</summary>
    <author>
      <name>Svetlana Nikitina</name>
    </author>
    <author>
      <name>Florian Daniel</name>
    </author>
    <author>
      <name>Marcos Baez</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03660v1</id>
    <updated>2018-12-10T07:49:57Z</updated>
    <published>2018-12-10T07:49:57Z</published>
    <title>Investigating Key User Experiencing Engineering Aspects in
  Software-as-a-Service Service Delivery Model</title>
    <summary>  Software as a Service (SaaS) is well established as an effective model for
the development, deployment and customization of software. As it continues to
gain more momentum in the IT industry, many user experience challenges and
issues are being reported by the experts and end users.
</summary>
    <author>
      <name>Lakshmi Sirisha Revadi</name>
    </author>
    <author>
      <name>Xi Zheng</name>
    </author>
    <author>
      <name>Yupeng Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1812.03660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00916v1</id>
    <updated>2019-06-03T16:37:23Z</updated>
    <published>2019-06-03T16:37:23Z</published>
    <title>Software Adaptation and Generalization of Physically-Constrained Games</title>
    <summary>  In this paper we provide a case study of the use of relatively sophisticated
mathematics and algorithms to redefine and adapt a simple traditional
game/puzzle to exploit the computational power of smart devices. The focus here
is not so much on the end product as it is on the process and considerations
underpinning its development. Ancillary results of the venture include
generalizations of the circular-shift operator and examination of its
computational complexity.
</summary>
    <author>
      <name>Jeffrey Uhlmann</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02374v1</id>
    <updated>2020-09-04T19:42:40Z</updated>
    <published>2020-09-04T19:42:40Z</published>
    <title>Literal Encoding: Text is a first-class data encoding</title>
    <summary>  Digital humanities are rooted in text analysis. However, most visualization
paradigms use only categoric, ordered or quantitative data. Literal text must
be considered a base data type to encode into visualizations. Literal text
offers functional, perceptual, cognitive, semantic and operational benefits.
These are briefly illustrated with a subset of sample visualizations focused on
semantic word sequences, indicating benefits over standard graphs, maps,
treemaps, bar charts and narrative layouts.
</summary>
    <author>
      <name>Richard Brath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.02374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01328v1</id>
    <updated>2020-10-03T11:22:48Z</updated>
    <published>2020-10-03T11:22:48Z</published>
    <title>HCI Models for Digital Musical Instruments: Methodologies for Rigorous
  Testing of Digital Musical Instruments</title>
    <summary>  Here we present an analysis of literature relating to the evaluation
methodologies of Digital Musical Instruments (DMIs) derived from the field of
Human-Computer Interaction (HCI). We then apply choice aspects from these
existing evaluation models and apply them to an optimized evaluation for
assessing new DMIs.
</summary>
    <author>
      <name>Gareth W. Young</name>
    </author>
    <author>
      <name>Dave Murphy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CMMR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01569v1</id>
    <updated>2020-10-04T12:55:18Z</updated>
    <published>2020-10-04T12:55:18Z</published>
    <title>A Course on Controllers</title>
    <summary>  Over the last four years, we have developed a series of lectures, labs and
project assignments aimed at introducing enough technology so that students
from a mix of disciplines can design and build innovative interface devices.
</summary>
    <author>
      <name>Bill Verplank</name>
    </author>
    <author>
      <name>Craig Sapp</name>
    </author>
    <author>
      <name>Max Mathews</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176380</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176380" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01575v1</id>
    <updated>2020-10-04T12:57:33Z</updated>
    <published>2020-10-04T12:57:33Z</published>
    <title>Tangible Music Interfaces Using Passive Magnetic Tags</title>
    <summary>  The technologies behind passive resonant magnetically coupled tags are
introduced and their application as a musical controller is illustrated for
solo or group performances, interactive installations, and music toys.
</summary>
    <author>
      <name>Joseph A. Paradiso</name>
    </author>
    <author>
      <name>Kai-yuh Hsiao</name>
    </author>
    <author>
      <name>Ari Benbasat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176374</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176374" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07532v1</id>
    <updated>2020-11-15T14:00:27Z</updated>
    <published>2020-11-15T14:00:27Z</published>
    <title>Aquanims -- Area-Preserving Animated Transitions based on a Hydraulic
  Metaphor</title>
    <summary>  We propose "Aquanims" as new design metaphors for animated transitions that
preserve displayed areas during the transformation. As liquids are
incompressible fluids, we use a hydraulic metaphor to convey the sense of area
preservation during animated transitions. We study the design space of Aquanims
for rectangle-based charts.
</summary>
    <author>
      <name>Michael Aupetit</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04559v1</id>
    <updated>2021-02-08T22:24:40Z</updated>
    <published>2021-02-08T22:24:40Z</published>
    <title>Designing for Contestation: Insights from Administrative Law</title>
    <summary>  A paper presented at the Workshop on Contestability in Algorithmic Systems at
CSCW 2019. Challenging algorithmic decisions is important to decision subjects,
yet numerous factors can limit a person's ability to contest such decisions. We
propose that administrative law systems, which were created to ensure that
governments are kept accountable for their actions and decision making in
relation to individuals, can provide guidance on how to design contestation
systems for algorithmic decision-making.
</summary>
    <author>
      <name>Henrietta Lyons</name>
    </author>
    <author>
      <name>Eduardo Velloso</name>
    </author>
    <author>
      <name>Tim Miller</name>
    </author>
    <link href="http://arxiv.org/abs/2102.04559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04681v1</id>
    <updated>2021-07-09T21:06:04Z</updated>
    <published>2021-07-09T21:06:04Z</published>
    <title>A Survey on Personal Image Retrieval Systems</title>
    <summary>  The number of photographs taken worldwide is growing rapidly and steadily.
While a small subset of these images is annotated and shared by users through
social media platforms, due to the sheer number of images in personal photo
repositories (shared or not shared), finding specific images remains
challenging. This survey explores existing image retrieval techniques as well
as photo-organizer applications to highlight their relative strengths in
addressing this challenge.
</summary>
    <author>
      <name>Amit Kumar Nath</name>
    </author>
    <author>
      <name>Andy Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2107.04681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12696v1</id>
    <updated>2021-07-27T09:54:14Z</updated>
    <published>2021-07-27T09:54:14Z</published>
    <title>A tactile closed-loop device for musical interaction</title>
    <summary>  This paper presents a device implementing a closed tactile loop for musical
interaction, based on a small freely held magnet which serves as the medium for
both input and output. The component parts as well as an example of its
programmable behaviour are described.
</summary>
    <author>
      <name>Staas de Jong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176935</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176935" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.12696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03147v1</id>
    <updated>2022-01-10T03:20:49Z</updated>
    <published>2022-01-10T03:20:49Z</published>
    <title>Effective Representation to Capture Collaboration Behaviors between
  Explainer and User</title>
    <summary>  An explainable AI (XAI) model aims to provide transparency (in the form of
justification, explanation, etc) for its predictions or actions made by it.
Recently, there has been a lot of focus on building XAI models, especially to
provide explanations for understanding and interpreting the predictions made by
deep learning models. At UCLA, we propose a generic framework to interact with
an XAI model in natural language.
</summary>
    <author>
      <name>Arjun Akula</name>
    </author>
    <author>
      <name>Song-Chun Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2201.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07362v1</id>
    <updated>2022-01-03T09:27:41Z</updated>
    <published>2022-01-03T09:27:41Z</published>
    <title>GeoGebra Discovery in Context</title>
    <summary>  In our contribution we will reflect, through a collection of selected
examples, on the potential impact of the GeoGebra Discovery application on
different social and educational contexts.
</summary>
    <author>
      <name>Zoltán Kovács</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Private University College of Education of the Diocese of Linz, Linz, Austria</arxiv:affiliation>
    </author>
    <author>
      <name>Tomás Recio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad Antonio de Nebrija, Madrid, Spain</arxiv:affiliation>
    </author>
    <author>
      <name>M. Pilar Vélez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad Antonio de Nebrija, Madrid, Spain</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.352.16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.352.16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ADG 2021, arXiv:2112.14770</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 352, 2021, pp. 141-147</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.07362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08210v1</id>
    <updated>2021-12-23T11:29:49Z</updated>
    <published>2021-12-23T11:29:49Z</published>
    <title>Comparative Study of Cloud and Non-Cloud Gaming Platform: Apercu</title>
    <summary>  Nowadays game engines are imperative for building 3D applications and games.
This is for the reason that the engines appreciably reduce resources for
employing obligatory but intricate utilities. This paper elucidates about a
game engine and its foremost elements. It portrays a number of special kinds of
contemporary game engines by way of their aspects, procedure and deliberates
their stipulations with comparison.
</summary>
    <author>
      <name>Prerna Mishra</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 Figures, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.08210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11614v2</id>
    <updated>2022-05-25T08:40:40Z</updated>
    <published>2022-05-19T13:07:24Z</published>
    <title>Trust, Professional Vision and Diagnostic Work</title>
    <summary>  In this paper we consider some empirical materials from our ongoing research
into forms of everyday detection and diagnosis work in healthcare settings, and
how these relate to issues of trust; trust in people, in technology, processes
and in data.
</summary>
    <author>
      <name>Mark Rouncefield</name>
    </author>
    <author>
      <name>Rob Procter</name>
    </author>
    <author>
      <name>Peter Tolmie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.11614v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11614v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02325v1</id>
    <updated>2022-07-05T21:34:15Z</updated>
    <published>2022-07-05T21:34:15Z</published>
    <title>Demonstrating Eye Movement Biometrics in Virtual Reality</title>
    <summary>  Thanks to the eye-tracking sensors that are embedded in emerging consumer
devices like the Vive Pro Eye, we demonstrate that it is feasible to deliver
user authentication via eye movement biometrics.
</summary>
    <author>
      <name>Dillon Lohr</name>
    </author>
    <author>
      <name>Saide Johnson</name>
    </author>
    <author>
      <name>Samantha Aziz</name>
    </author>
    <author>
      <name>Oleg Komogortsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Demo presented at ETRA '22. Poster and video available at
  https://www.doi.org/10.18738/T8/WPEXGW</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.13550v1</id>
    <updated>2022-08-25T12:50:12Z</updated>
    <published>2022-08-25T12:50:12Z</published>
    <title>ProxiTrak: Intelligent Enablement of Social Distancing &amp; Contact Tracing
  for a Safer Workplace in the New Normal</title>
    <summary>  This paper describes an innovative solution that enables the enterprises to
bring their associates (or employees) back to physical workspaces for critical
operations in a safe manner in the wake of current COVID-19 pandemic.
</summary>
    <author>
      <name>Vivek Chandel</name>
    </author>
    <author>
      <name>Snehasis Banerjee</name>
    </author>
    <author>
      <name>Avik Ghose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CSI YITPA Region II Winning Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.13550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.13550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09291v1</id>
    <updated>2022-10-05T13:02:23Z</updated>
    <published>2022-10-05T13:02:23Z</published>
    <title>Embodying the Glitch: Perspectives on Generative AI in Dance Practice</title>
    <summary>  What role does the break from realism play in the potential for generative
artificial intelligence as a creative tool? Through exploration of glitch, we
examine the prospective value of these artefacts in creative practice. This
paper describes findings from an exploration of AI-generated "mistakes" when
using movement produced by a generative deep learning model as an inspiration
source in dance composition.
</summary>
    <author>
      <name>Benedikte Wallace</name>
    </author>
    <author>
      <name>Charles P. Martin</name>
    </author>
    <link href="http://arxiv.org/abs/2210.09291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.02208v1</id>
    <updated>2022-11-04T01:36:32Z</updated>
    <published>2022-11-04T01:36:32Z</published>
    <title>Automated Logging Drone: A Computer Vision Drone Implementation</title>
    <summary>  In recent years, Artificial Intelligence (AI) and Computer Vision (CV) have
become the pinnacle of technology with new developments seemingly every day.
This technology along with more powerful drone technology have made autonomous
surveillance more sought after. Here an overview of the Automated Logging Drone
(ALD) project is presented along with examples of how this project can be used
with more refining and added features.
</summary>
    <author>
      <name>Aaron Yagnik</name>
    </author>
    <author>
      <name>Adrian S. -W. Tam</name>
    </author>
    <link href="http://arxiv.org/abs/2211.02208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.02208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.09334v1</id>
    <updated>2022-11-17T04:49:51Z</updated>
    <published>2022-11-17T04:49:51Z</published>
    <title>Estimation of physical activities of people in offices from time-series
  point-cloud data</title>
    <summary>  This paper proposes an edge computing system that enables estimating physical
activities of people in offices from time-series point-cloud data, obtained by
using a light-detection-and-ranging (LIDAR) sensor network. The paper presents
that the proposed system successfully constructs the model for estimating the
number of typed characters from time-series point-cloud data, through an
experiment using real LIDAR sensors.
</summary>
    <author>
      <name>Koki Kizawa</name>
    </author>
    <author>
      <name>Ryoichi Shinkuma</name>
    </author>
    <author>
      <name>Gabriele Trovato</name>
    </author>
    <link href="http://arxiv.org/abs/2211.09334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03186v1</id>
    <updated>2022-12-06T17:59:47Z</updated>
    <published>2022-12-06T17:59:47Z</published>
    <title>Towards Better User Requirements: How to Involve Human Participants in
  XAI Research</title>
    <summary>  Human-Center eXplainable AI (HCXAI) literature identifies the need to address
user needs. This paper examines how existing XAI research involves human users
in designing and developing XAI systems and identifies limitations in current
practices, especially regarding how researchers identify user requirements.
Finally, we propose several suggestions on how to derive better user
requirements.
</summary>
    <author>
      <name>Thu Nguyen</name>
    </author>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. 36th Conference on Neural Information Processing Systems
  (NeurIPS 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.03186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09908v1</id>
    <updated>2023-01-24T10:35:28Z</updated>
    <published>2023-01-24T10:35:28Z</published>
    <title>Cross-lingual German Biomedical Information Extraction: from Zero-shot
  to Human-in-the-Loop</title>
    <summary>  This paper presents our project proposal for extracting biomedical
information from German clinical narratives with limited amounts of
annotations. We first describe the applied strategies in transfer learning and
active learning for solving our problem. After that, we discuss the design of
the user interface for both supplying model inspection and obtaining user
annotations in the interactive environment.
</summary>
    <author>
      <name>Siting Liang</name>
    </author>
    <author>
      <name>Mareike Hartmann</name>
    </author>
    <author>
      <name>Daniel Sonntag</name>
    </author>
    <link href="http://arxiv.org/abs/2301.09908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05172v1</id>
    <updated>2023-02-10T11:05:38Z</updated>
    <published>2023-02-10T11:05:38Z</published>
    <title>Social Virtual Reality Avatar Biosignal Animations as Availability
  Status Indicators</title>
    <summary>  In this position paper, we outline our research challenges in Affective
Interactive Systems, and present recent work on visualizing avatar biosignals
for social VR entertainment. We highlight considerations for how biosignals
animations in social VR spaces can (falsely) indicate users' availability
status.
</summary>
    <author>
      <name>Abdallah El Ali</name>
    </author>
    <author>
      <name>Sueyoon Lee</name>
    </author>
    <author>
      <name>Pablo Cesar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position paper for CHI '22 Workshop on "Social Presence in Virtual
  Event Spaces"</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.05172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.07539v2</id>
    <updated>2024-03-01T18:01:53Z</updated>
    <published>2023-03-13T23:51:33Z</published>
    <title>HCI Papers Cite HCI Papers, Increasingly So</title>
    <summary>  To measure how HCI papers are cited across disciplinary boundaries, we
collected a citation dataset of CHI, UIST, and CSCW papers published between
2010 and 2020. Our analysis indicates that HCI papers have been more and more
likely to be cited by HCI papers rather than by non-HCI papers.
</summary>
    <author>
      <name>Xiang 'Anthony' Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2303.07539v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07539v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14557v2</id>
    <updated>2025-04-12T06:44:25Z</updated>
    <published>2023-03-25T20:49:40Z</published>
    <title>Clo(o)k: Human-Time Interactions Through a Clock That "Looks"</title>
    <summary>  What if a clock could do more than tell time - what if it could look around?
This project explores the conceptualization, design, and construction of a
timepiece with visual perception capabilities, featuring three types of
human-time interactions. Informal observations during a demonstration highlight
its unique user experiences. https://www.zhuoyuelyu.com/clook
</summary>
    <author>
      <name>Zhuoyue Lyu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3706599.3721343</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3706599.3721343" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI EA '25: Extended Abstracts of the 2025 CHI Conference on Human
  Factors in Computing Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16790v1</id>
    <updated>2023-03-29T15:28:48Z</updated>
    <published>2023-03-29T15:28:48Z</published>
    <title>Advancing Inclusive Design in Multiple Dimensions</title>
    <summary>  Making technology design inclusive requires facing multiple challenges in
different dimensions: the populations we work with, who we are, what
interaction possibilities we consider, and what context we examine. We reflect
on these challenges and propose two main measures to achieve research
inclusiveness.
</summary>
    <author>
      <name>Gabriela Molina León</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the CUI@CHI Workshop on Inclusive Design of CUIs Across
  Modalities and Mobilities, CHI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.16790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09038v1</id>
    <updated>2023-05-15T21:57:41Z</updated>
    <published>2023-05-15T21:57:41Z</published>
    <title>Characterizing Image Accessibility on Wikipedia across Languages</title>
    <summary>  We make a first attempt to characterize image accessibility on Wikipedia
across languages, present new experimental results that can inform efforts to
assess description quality, and offer some strategies to improve Wikipedia's
image accessibility.
</summary>
    <author>
      <name>Elisa Kreiss</name>
    </author>
    <author>
      <name>Krishna Srinivasan</name>
    </author>
    <author>
      <name>Tiziano Piccardi</name>
    </author>
    <author>
      <name>Jesus Adolfo Hermosillo</name>
    </author>
    <author>
      <name>Cynthia Bennett</name>
    </author>
    <author>
      <name>Michael S. Bernstein</name>
    </author>
    <author>
      <name>Meredith Ringel Morris</name>
    </author>
    <author>
      <name>Christopher Potts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Wiki Workshop 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.09038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.06459v1</id>
    <updated>2023-06-10T14:49:02Z</updated>
    <published>2023-06-10T14:49:02Z</published>
    <title>Truth in Motion: The Unprecedented Risks and Opportunities of Extended
  Reality Motion Data</title>
    <summary>  Motion tracking "telemetry" data lies at the core of nearly all modern
extended reality (XR) and metaverse experiences. While generally presumed
innocuous, recent studies have demonstrated that motion data actually has the
potential to profile and deanonymize XR users, posing a significant threat to
security and privacy in the metaverse.
</summary>
    <author>
      <name>Vivek Nair</name>
    </author>
    <author>
      <name>Louis Rosenberg</name>
    </author>
    <author>
      <name>James F. O'Brien</name>
    </author>
    <author>
      <name>Dawn Song</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSEC.2023.3330392</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSEC.2023.3330392" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Security &amp; Privacy (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.06459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.06459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.14320v1</id>
    <updated>2023-08-28T06:04:33Z</updated>
    <published>2023-08-28T06:04:33Z</published>
    <title>Video Multimodal Emotion Recognition System for Real World Applications</title>
    <summary>  This paper proposes a system capable of recognizing a speaker's
utterance-level emotion through multimodal cues in a video. The system
seamlessly integrates multiple AI models to first extract and pre-process
multimodal information from the raw video input. Next, an end-to-end MER model
sequentially predicts the speaker's emotions at the utterance level.
Additionally, users can interactively demonstrate the system through the
implemented interface.
</summary>
    <author>
      <name>Sun-Kyung Lee</name>
    </author>
    <author>
      <name>Jong-Hwan Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.14320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.14320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.06298v2</id>
    <updated>2024-01-16T20:36:40Z</updated>
    <published>2023-12-11T11:02:07Z</published>
    <title>Musical Sonification of Daily Physical Activity Data: A Proof of Concept
  Study</title>
    <summary>  arXiv admin comment: This version has been removed by arXiv administrators as
the submitter did not have the rights to agree to the license at the time of
submission
</summary>
    <author>
      <name>Andrew Danso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin comment: This version has been removed by arXiv
  administrators as the submitter did not have the rights to agree to the
  license at the time of submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.06298v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.06298v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.11520v1</id>
    <updated>2023-12-12T22:19:00Z</updated>
    <published>2023-12-12T22:19:00Z</published>
    <title>Implementing biosensing based user preference visualisation in
  architectural spaces</title>
    <summary>  This study delves into the interplay between architectural spaces and human
emotions, leveraging the emergent field of neuroarchitecture. It examines the
functional and aesthetic influence of architectural design on individual users,
with a focus on biosensing data such as brainwave and eye tracking information
to understand user preferences.
</summary>
    <author>
      <name>Mi Kyoung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.11520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.11520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.00356v1</id>
    <updated>2023-12-31T00:24:55Z</updated>
    <published>2023-12-31T00:24:55Z</published>
    <title>Understanding Driver Agency in RideSharing</title>
    <summary>  Agency is an important human characteristic that users of automated complex
technologies are usually denied. This affects the user's experience leading to
decreased satisfaction and productivity. In this paper, we consider the
ridesharing context and interviewed 7 drivers to understand the controls that
would improve the agency they feel. The results show that they desire
transparency, community and an effective ability to seek redress.
</summary>
    <author>
      <name>Iyadunni Adenuga</name>
    </author>
    <author>
      <name>Benjamin Hanrahan</name>
    </author>
    <link href="http://arxiv.org/abs/2401.00356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.00356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.09654v1</id>
    <updated>2024-01-18T00:02:35Z</updated>
    <published>2024-01-18T00:02:35Z</published>
    <title>User Study: Comparison of Picture Passwords and Current Login Approaches</title>
    <summary>  In this research, we conduct a user study that compares different
computer/system authentication methods. More specifically, we look into
comparing regular password authentication with picture authentication. Picture
authentication means selecting a sequence of pictures from a set of pictures
(30). We present users with both interfaces; various metrics are tracked while
the participants conduct a variety of user authentication-related tasks. Other
metrics include user perception of security with such technologies.
</summary>
    <author>
      <name>Ignacio Astaburuaga</name>
    </author>
    <link href="http://arxiv.org/abs/2401.09654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.09654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.10953v1</id>
    <updated>2024-01-19T02:49:43Z</updated>
    <published>2024-01-19T02:49:43Z</published>
    <title>How customers' satisfaction change with the use of AR shopping
  application: A conceptuall model</title>
    <summary>  The paper proposes a conceptual model of how different perceived levels of
experiential AR application features have effects on customer experience, and
in turn their satisfaction and purchase behavior. In addition, it put forward
the mediation role of immersion between perceived levels of experiential AR
application features and customers experience.
</summary>
    <author>
      <name>Fariba Sanaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2023 AMA Winter Academic Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2023 AMA Winter Academic Conference Proceedings</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2401.10953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.10953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.10118v1</id>
    <updated>2024-03-15T09:08:15Z</updated>
    <published>2024-03-15T09:08:15Z</published>
    <title>CyberMoraba: A game-based approach enhancing cybersecurity awareness</title>
    <summary>  Numerous studies confirm Cybersecurity Awareness Games (CAGs) effectively
bolster organisational security against cyberattacks. This article introduces a
serious CAG, integrating the traditional South African Morabaraba board game
into cybersecurity education. Players adopt roles of defenders or attackers,
strategically placing tokens to enhance awareness. Evaluation shows positive
outcomes, enhancing understanding and enjoyment among participants.
</summary>
    <author>
      <name>Mike Nkongolo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.34190/iccws.19.1.1957</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.34190/iccws.19.1.1957" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 19th International Conference on Cyber Warfare and
  Security (ICCWS), 26 - 27 March 2024, Johannesburg, South Africa</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.10118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.10118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.07360v1</id>
    <updated>2024-04-10T21:35:17Z</updated>
    <published>2024-04-10T21:35:17Z</published>
    <title>Enhancing Accessibility in Soft Robotics: Exploring Magnet-Embedded
  Paper-Based Interactions</title>
    <summary>  This paper explores the implementation of embedded magnets to enhance
paper-based interactions. The integration of magnets in paper-based
interactions simplifies the fabrication process, making it more accessible for
building soft robotics systems. We discuss various interaction patterns
achievable through this approach and highlight their potential applications.
</summary>
    <author>
      <name>Ruhan Yang</name>
    </author>
    <author>
      <name>Ellen Yi-Luen Do</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM DIS 2023 Workshop 02: Soft Robotics and Programmable Materials
  for Human-Computer Interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.07360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.07360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13229v1</id>
    <updated>2024-04-20T01:41:53Z</updated>
    <published>2024-04-20T01:41:53Z</published>
    <title>Preserving History through Augmented Reality</title>
    <summary>  Extended reality can weave together the fabric of the past, present, and
future. A two-day design hackathon was held to bring the community together
through a love for history and a common goal to use technology for good.
Through interviewing an influential community elder, Emile Pitre, and
referencing his book Revolution to Evolution, my team developed an augmented
reality artifact to tell his story and preserve on revolutionary's legacy that
impacted the University of Washington's history forever.
</summary>
    <author>
      <name>Annie Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CHI 2024 arXiv:2404.05889</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.13229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.19546v1</id>
    <updated>2024-04-30T13:15:59Z</updated>
    <published>2024-04-30T13:15:59Z</published>
    <title>Designing Technology for Positive Solitude</title>
    <summary>  This paper discusses Life-Based Design (LBD) methodology within the context
of designing technologies for reaching a state of solitude, the state where a
person wishes to minimize her social contacts to get space or freedom.
</summary>
    <author>
      <name>Pertti Saariluoma</name>
    </author>
    <author>
      <name>Juhani Heinilä</name>
    </author>
    <author>
      <name>Erkki Kuisma</name>
    </author>
    <author>
      <name>Jaana Leikas</name>
    </author>
    <author>
      <name>Hannu Vilpponen</name>
    </author>
    <author>
      <name>Mari Ylikauppila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Modern Behavioral Science Journal, 1(1), 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.19546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.19546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.03999v1</id>
    <updated>2024-05-07T04:29:54Z</updated>
    <published>2024-05-07T04:29:54Z</published>
    <title>Interaction Design for Human-AI Choreography Co-creation</title>
    <summary>  Human-AI co-creation aims to combine human and AI strengths for artistic
results exceeding individual capabilities. Frameworks exist for painting,
music, and poetry, but choreography's embodied nature demands a dedicated
approach. This paper explores AI-assisted choreography techniques (e.g.,
generative ideation, embodied improvisation) and analyzes interaction design --
how humans and AI collaborate and communicate -- to inform the design
considerations of future human-AI choreography co-creation systems.
</summary>
    <author>
      <name>Yimeng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GenAICHI: CHI 2024 Workshop on Generative AI and HCI</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.03999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.03999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.04687v1</id>
    <updated>2024-05-07T21:59:57Z</updated>
    <published>2024-05-07T21:59:57Z</published>
    <title>Towards Human-AI Mutual Learning: A New Research Paradigm</title>
    <summary>  This paper describes a new research paradigm for studying human-AI
collaboration, named "human-AI mutual learning", defined as the process where
humans and AI agents preserve, exchange, and improve knowledge during human-AI
collaboration. We describe relevant methodologies, motivations, domain
examples, benefits, challenges, and future research agenda under this paradigm.
</summary>
    <author>
      <name>Xiaomei Wang</name>
    </author>
    <author>
      <name>Xiaoyu Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2405.04687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.04687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.17690v1</id>
    <updated>2024-05-27T22:46:17Z</updated>
    <published>2024-05-27T22:46:17Z</published>
    <title>Data Makes Better Data Scientists</title>
    <summary>  With the goal of identifying common practices in data science projects, this
paper proposes a framework for logging and understanding incremental code
executions in Jupyter notebooks. This framework aims to allow reasoning about
how insights are generated in data science and extract key observations into
best data science practices in the wild. In this paper, we show an early
prototype of this framework and ran an experiment to log a machine learning
project for 25 undergraduate students.
</summary>
    <author>
      <name>Jinjin Zhao</name>
    </author>
    <author>
      <name>Avidgor Gal</name>
    </author>
    <author>
      <name>Sanjay Krishnan</name>
    </author>
    <link href="http://arxiv.org/abs/2405.17690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.17690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02520v1</id>
    <updated>2024-06-04T17:41:20Z</updated>
    <published>2024-06-04T17:41:20Z</published>
    <title>Digital Privacy for Migrants: Exploring Current Research Trends and
  Future Prospects</title>
    <summary>  This paper explores digital privacy challenges for migrants, analyzing trends
from 2013 to 2023. Migrants face heightened risks such as government
surveillance and identity theft. Understanding these threats is vital for
raising awareness and guiding research towards effective solutions and policies
to protect migrant digital privacy.
</summary>
    <author>
      <name>Sarah Tabassum</name>
    </author>
    <author>
      <name>Cori Faklaris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11146v1</id>
    <updated>2024-06-17T02:20:50Z</updated>
    <published>2024-06-17T02:20:50Z</published>
    <title>Designing Interactions with Autonomous Physical Systems</title>
    <summary>  In this position paper, we present a collection of four different prototyping
approaches which we have developed and applied to prototype and evaluate
interfaces for and interactions around autonomous physical systems. Further, we
provide a classification of our approaches aiming to support other researchers
and designers in choosing appropriate prototyping platforms and
representations.
</summary>
    <author>
      <name>Marius Hoggenmueller</name>
    </author>
    <author>
      <name>Tram Thi Minh Tran</name>
    </author>
    <author>
      <name>Luke Hespanhol</name>
    </author>
    <author>
      <name>Martin Tomitsch</name>
    </author>
    <link href="http://arxiv.org/abs/2406.11146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.14397v2</id>
    <updated>2024-06-21T02:36:25Z</updated>
    <published>2024-06-20T15:17:40Z</published>
    <title>Jupyter Scatter: Interactive Exploration of Large-Scale Datasets</title>
    <summary>  Jupyter Scatter is a scalable, interactive, and interlinked scatterplot
widget for exploring datasets in Jupyter Notebook/Lab, Colab, and VS Code. Its
goal is to simplify the visual exploration, analysis, and comparison of
large-scale bivariate datasets. Jupyter Scatter can render up to twenty million
points, supports fast point selections, integrates with Pandas DataFrame and
Matplotlib, uses perceptually-effective default settings, and offers a
user-friendly API.
</summary>
    <author>
      <name>Fritz Lekschas</name>
    </author>
    <author>
      <name>Trevor Manz</name>
    </author>
    <link href="http://arxiv.org/abs/2406.14397v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.14397v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.11998v1</id>
    <updated>2024-06-14T03:19:03Z</updated>
    <published>2024-06-14T03:19:03Z</published>
    <title>Custom Cloth Creation and Virtual Try-on for Everyone</title>
    <summary>  This demo showcases a simple tool that utilizes AIGC technology, enabling
both professional designers and regular users to easily customize clothing for
their digital avatars. Customization options include changing clothing colors,
textures, logos, and patterns. Compared with traditional 3D modeling processes,
our approach significantly enhances efficiency and interactivity and reduces
production costs.
</summary>
    <author>
      <name>Pei Chen</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Sainan Sun</name>
    </author>
    <author>
      <name>Zhiyuan Chen</name>
    </author>
    <author>
      <name>Zhenkun Liu</name>
    </author>
    <author>
      <name>Shuhua Cao</name>
    </author>
    <author>
      <name>Li Yang</name>
    </author>
    <author>
      <name>Minghui Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.11998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.11998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.15326v2</id>
    <updated>2024-08-02T03:35:05Z</updated>
    <published>2024-07-22T02:12:42Z</published>
    <title>Intelligence Preschool Education System based on Multimodal Interaction
  Systems and AI</title>
    <summary>  Rapid progress in AI technologies has generated considerable interest in
their potential to address challenges in every field and education is no
exception. Improving learning outcomes and providing relevant education to all
have been dominant themes universally, both in the developed and developing
world. And they have taken on greater significance in the current era of
technology driven personalization.
</summary>
    <author>
      <name>Long Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2407.15326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.15326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.01176v1</id>
    <updated>2024-09-02T11:10:51Z</updated>
    <published>2024-09-02T11:10:51Z</published>
    <title>Space module with gyroscope and accelerometer integration</title>
    <summary>  MEIGA is a module specially designed for people with tetraplegia or anyone
who has very limited movement capacity in their upper limbs. MEIGA converts the
user's head movements into mouse movements. To simulate keystrokes, it uses
blinking, reading the movement of the cheek that occurs with it. The
performance, speed of movement of the mouse and its precision are practically
equivalent to their respective measurements using the hand.
</summary>
    <author>
      <name>Antonio Losada González</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Spanish language</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.01176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.01176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.09267v1</id>
    <updated>2024-09-14T02:26:38Z</updated>
    <published>2024-09-14T02:26:38Z</published>
    <title>Cross-Disciplinary Perspectives on Youth Digital Well-Being Research:
  Identifying Notable Developments, Persistent Gaps, and Future Directions</title>
    <summary>  This paper provides a broad, multi-disciplinary overview of key insights,
persistent gaps, and future paths in youth digital well-being research from the
perspectives of researchers who are conducting this work.
</summary>
    <author>
      <name>Katie Davis</name>
    </author>
    <author>
      <name>Morgan Anderson</name>
    </author>
    <author>
      <name>Chia-chen Yang</name>
    </author>
    <author>
      <name>Sophia Choukas-Bradley</name>
    </author>
    <author>
      <name>Beth T. Bell</name>
    </author>
    <author>
      <name>Petr Slovak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Adolescent Research</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.09267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.09267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17272v1</id>
    <updated>2024-09-10T08:19:54Z</updated>
    <published>2024-09-10T08:19:54Z</published>
    <title>Design and development of desktop braille printing machine at Fablab
  Nepal</title>
    <summary>  The development of a desktop Braille printing machine aims to create an
affordable, user-friendly device for visually impaired users. This document
outlines the entire process, from research and requirement analysis to
distribution and support, leveraging the content and guidelines from the GitHub
repository,https://github.com/fablabnepal1/Desktop-Braille-Printing-Machine.
</summary>
    <author>
      <name>Daya Bandhu Ghimire</name>
    </author>
    <author>
      <name>Pallab Shrestha</name>
    </author>
    <link href="http://arxiv.org/abs/2409.17272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.05165v1</id>
    <updated>2024-11-07T20:09:34Z</updated>
    <published>2024-11-07T20:09:34Z</published>
    <title>Haptic Dial based on Magnetorheological Fluid Having Bumpy Structure</title>
    <summary>  We proposed a haptic dial based on magnetorheological fluid (MRF) which
enhances performance by increasing the MRF-exposed area through concave shaft
and housing structure. We developed a breakout-style game to show that the
proposed haptic dial allows users to efficiently interact with virtual objects.
</summary>
    <author>
      <name>Seok Hun Lee</name>
    </author>
    <author>
      <name>Yong Hae Heo</name>
    </author>
    <author>
      <name>Seok-Han Lee</name>
    </author>
    <author>
      <name>Sang-Youn Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of proceedings of 6th International Conference AsiaHaptics 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.05165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.05165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05802v1</id>
    <updated>2024-12-08T03:49:26Z</updated>
    <published>2024-12-08T03:49:26Z</published>
    <title>Bidirectional Human-AI Learning in Real-Time Disoriented Balancing</title>
    <summary>  We present a real-time system that enables bidirectional human-AI learning
and teaching in a balancing task that is a realistic analogue of disorientation
during piloting and spaceflight. A human subject and autonomous AI model of
choice guide each other in maintaining balance using a visual inverted pendulum
(VIP) display. We show how AI assistance changes human performance and vice
versa.
</summary>
    <author>
      <name>Sheikh Mannan</name>
    </author>
    <author>
      <name>Nikhil Krishnaswamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, appearing at AAAI 2025 demos track</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.05802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.05802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.08360v1</id>
    <updated>2024-12-11T13:06:24Z</updated>
    <published>2024-12-11T13:06:24Z</published>
    <title>Agency and Morality as part of Text Entry AI Assistant Personas</title>
    <summary>  This paper discusses the need to move away from an instrumental view of text
composition AI assistants under direct control of the user, towards a more
agentic approach that is based on a value rationale. Based on an analysis of
moral dimensions of AI assistance in computer mediated communication, the paper
proposes basic guidelines for designing the agent's persona.
</summary>
    <author>
      <name>Andreas Komninos</name>
    </author>
    <link href="http://arxiv.org/abs/2412.08360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.08360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17714v1</id>
    <updated>2025-02-24T23:13:59Z</updated>
    <published>2025-02-24T23:13:59Z</published>
    <title>On the usability of generative AI: Human generative AI</title>
    <summary>  Generative AI systems are transforming content creation, but their usability
remains a key challenge. This paper examines usability factors such as user
experience, transparency, control, and cognitive load. Common challenges
include unpredictability and difficulties in fine-tuning outputs. We review
evaluation metrics like efficiency, learnability, and satisfaction,
highlighting best practices from various domains. Improving interpretability,
intuitive interfaces, and user feedback can enhance usability, making
generative AI more accessible and effective.
</summary>
    <author>
      <name>Anna Ravera</name>
    </author>
    <author>
      <name>Cristina Gena</name>
    </author>
    <link href="http://arxiv.org/abs/2502.17714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03968v1</id>
    <updated>2025-03-05T23:40:10Z</updated>
    <published>2025-03-05T23:40:10Z</published>
    <title>Preliminary Report: Enhancing Role Differentiation in Conversational HCI
  Through Chromostereopsis</title>
    <summary>  We propose leveraging chromostereopsis, a perceptual phenomenon inducing
depth perception through color contrast, as a novel approach to visually
differentiating conversational roles in text-based AI interfaces. This method
aims to implicitly communicate role hierarchy and add a subtle sense of
physical space.
</summary>
    <author>
      <name>Matteo Grella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary Report, 8 pages, 1 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16521v1</id>
    <updated>2025-03-17T02:16:41Z</updated>
    <published>2025-03-17T02:16:41Z</published>
    <title>Conversational Self-Play for Discovering and Understanding Psychotherapy
  Approaches</title>
    <summary>  This paper explores conversational self-play with LLMs as a scalable approach
for analyzing and exploring psychotherapy approaches, evaluating how well
AI-generated therapeutic dialogues align with established modalities.
</summary>
    <author>
      <name>Onno P Kampman</name>
    </author>
    <author>
      <name>Michael Xing</name>
    </author>
    <author>
      <name>Charmaine Lim</name>
    </author>
    <author>
      <name>Ahmad Ishqi Jabir</name>
    </author>
    <author>
      <name>Ryan Louie</name>
    </author>
    <author>
      <name>Jimmy Lee</name>
    </author>
    <author>
      <name>Robert JT Morris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI4X conference submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.17639v1</id>
    <updated>2025-03-22T03:57:49Z</updated>
    <published>2025-03-22T03:57:49Z</published>
    <title>Kintsugi-Inspired Design: Communicatively Reconstructing Identities
  Online After Trauma</title>
    <summary>  Trauma can disrupt one's sense of self and mental well-being, leading
survivors to reconstruct their identities in online communities. Drawing from
30 in-depth interviews, we present a sociotechnical process model that
illustrates the mechanisms of online identity reconstruction and the pathways
to integration. We introduce the concept of fractured identities, reflecting
the enduring impact of trauma on one's self-concept.
</summary>
    <author>
      <name>Casey Randazzo</name>
    </author>
    <author>
      <name>Tawfiq Ammari</name>
    </author>
    <link href="http://arxiv.org/abs/2503.17639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.17639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.03375v1</id>
    <updated>2025-04-04T11:45:45Z</updated>
    <published>2025-04-04T11:45:45Z</published>
    <title>Virtualizing a Collaboration Task as an Interactable Environment and
  Installing it on Real World</title>
    <summary>  This paper proposes a novel approach to scaling distributed collaboration in
mixed reality by virtualizing collaborative tasks as independent, installable
environments. By mapping group activities into dedicated virtual spaces that
adapt to each user's real-world context, the proposed method supports
consistent MR interactions, dynamic group engagement, and seamless task
transitions. Preliminary studies in individual ideation demonstrate enhanced
immersion and productivity, paving the way for future multi-user collaborative
systems.
</summary>
    <author>
      <name>Euijun Jung</name>
    </author>
    <author>
      <name>Youngki Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2504.03375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.03375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07475v1</id>
    <updated>2025-04-10T06:00:01Z</updated>
    <published>2025-04-10T06:00:01Z</published>
    <title>Proceedings of the Purposeful XR Workshop for CHI 2025</title>
    <summary>  This volume represents the proceedings of Workshop 27 on Purposeful XR:
Affordances, Challenges, and Speculations for an Ethical Future, held together
with the CHI conference on Human Factors in Computing Systems on MY 26th, 2025
in Yokohama, Japan.
</summary>
    <author>
      <name>Elizabeth Childs</name>
    </author>
    <author>
      <name>Samir Ghosh</name>
    </author>
    <author>
      <name>Sebastian Cmentowski</name>
    </author>
    <author>
      <name>Andrea Cuadra</name>
    </author>
    <author>
      <name>Rabindra Ratan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position papers for CHI Workshop 27</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13885v1</id>
    <updated>2025-04-04T10:41:10Z</updated>
    <published>2025-04-04T10:41:10Z</published>
    <title>User Satisfaction -- UX Design Strategies for Seamless Virtual
  Experience</title>
    <summary>  User Experience (UX) in virtual worlds is a fast-developing discipline that
requires creative design concepts to overcome the divide between physical and
virtual interaction. This research investigates primary principles and
techniques to improve UX in virtual experiences based on usability,
accessibility, user engagement, and technology advancements. It gives detailed
insight into trends, issues, and prospects for UX design of virtual
applications that guarantee an efficient, easy-to-use, and immersive
experience.
</summary>
    <author>
      <name>Harish Vijayakumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16824v1</id>
    <updated>2025-04-23T15:41:12Z</updated>
    <published>2025-04-23T15:41:12Z</published>
    <title>Nurturing Language Proficiency in Spanish.speaking children Through
  digital competence</title>
    <summary>  This article explores into the intricate design and meticulous construction
of a digital platform aimed at revolutionizing early-age English education,
particularly for Spanish-speaking children. The focus of this work used an
innovative methodologies, vibrant and engaging visuals, and a comprehensive
approach to phonics. The principles of usability, accessibility, and
user-centered design are intricately woven into every facet of the platform's
architecture.
</summary>
    <author>
      <name>Rhayza Jolley Rangel</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.03492v1</id>
    <updated>2025-05-06T12:48:38Z</updated>
    <published>2025-05-06T12:48:38Z</published>
    <title>Augmenting Human Cognition through Everyday AR</title>
    <summary>  As spatial computing and multimodal LLMs mature, AR is tending to become an
intuitive "thinking tool," embedding semantic and context-aware intelligence
directly into everyday environments. This paper explores how always-on AR can
seamlessly bridge digital cognition and physical affordances, enabling
proactive, context-sensitive interactions that enhance human task performance
and understanding.
</summary>
    <author>
      <name>Xiaoan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures. Position paper accepted to CHI'25 Workshop
  'Everyday AR through AI-in-the-Loop'</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.03492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.03492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.2603v1</id>
    <updated>2009-11-13T12:48:42Z</updated>
    <published>2009-11-13T12:48:42Z</published>
    <title>Cybermatter</title>
    <summary>  In this paper we examine several aspects of the impact of Cyberworld onto our
Reality conceptions, and their social implications.
</summary>
    <author>
      <name>Daniel Stern</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Seventh International Computer Ethics Conference (CEPE 2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.2603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.2603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00251v2</id>
    <updated>2017-01-26T15:53:55Z</updated>
    <published>2016-01-31T14:22:47Z</published>
    <title>Do we have privacy in the digital world?</title>
    <summary>  Not really.
</summary>
    <author>
      <name>Kaveh Bakhtiyari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.2492.5203/2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.2492.5203/2" rel="related"/>
    <link href="http://arxiv.org/abs/1602.00251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.03365v1</id>
    <updated>2021-05-07T16:24:47Z</updated>
    <published>2021-05-07T16:24:47Z</published>
    <title>Accelerating Entrepreneurial Decision-Making Through Hybrid Intelligence</title>
    <summary>  Accelerating Entrepreneurial Decision-Making Through Hybrid Intelligence
DESIGN PARADIGMS AND PRINCIPLES FOR DECISIONAL GUIDANCE IN ENTREPRENEURSHIP
</summary>
    <author>
      <name>Dominik Dellermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.17170/kobra-202004301196</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.17170/kobra-202004301196" rel="related"/>
    <link href="http://arxiv.org/abs/2105.03365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812015v1</id>
    <updated>1998-12-11T23:05:51Z</updated>
    <published>1998-12-11T23:05:51Z</published>
    <title>Adaptive Interaction Using the Adaptive Agent Oriented Software
  Architecture (AAOSA)</title>
    <summary>  User interfaces that adapt their characteristics to those of the user are
referred to as adaptive interfaces. We propose Adaptive Agent Oriented Software
Architecture (AAOSA) as a new way of designing adaptive interfaces. AAOSA is a
new approach to software design based on an agent-oriented architecture. In
this approach agents are considered adaptively communicating concurrent modules
which are divided into a white box module responsible for the communications
and learning, and a black box which is responsible for the independent
specialized processes of the agent. A distributed learning policy that makes
use of this architecture is used for purposes of system adaptability.
</summary>
    <author>
      <name>Babak Hodjat</name>
    </author>
    <author>
      <name>Makoto Amamiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9812015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.2; I.2.7; J.7; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9903019v2</id>
    <updated>1999-03-31T22:44:35Z</updated>
    <published>1999-03-30T21:28:07Z</published>
    <title>Workflow Automation with Lotus Notes for the Governmental Administrative
  Information System</title>
    <summary>  The paper presents an introductory overview of the workflow automation area,
outlining the main types, basic technologies, the essential features of
workflow applications. Two sorts of process models for the definition of
workflows (according to the conversation-based and activity-based
methodologies) are sketched. Later on, the nature of Lotus Notes and its
capabilities (as an environment for workflow management systems development)
are indicated. Concluding, the experience of automating administrative
workflows (developing a Subsystem of Inter-institutional Document Management of
the VADIS project) is briefly outlined.
</summary>
    <author>
      <name>Saulius Maskeliunas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Mathematics &amp; Informatics, Vilnius</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9903019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9903019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.1; H.5.3; J.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0101029v1</id>
    <updated>2001-01-26T07:27:02Z</updated>
    <published>2001-01-26T07:27:02Z</published>
    <title>Tap Tips: Lightweight Discovery of Touchscreen Targets</title>
    <summary>  We describe tap tips, a technique for providing touch-screen target location
hints. Tap tips are lightweight in that they are non-modal, appear only when
needed, require a minimal number of user gestures, and do not add to the
standard touchscreen gesture vocabulary. We discuss our implementation of tap
tips in an electronic guidebook system and some usability test results.
</summary>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <author>
      <name>Amy Hurst</name>
    </author>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/634067.634208</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/634067.634208" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Abstracts, ACM SIGCHI Conf. on Human Factors in Computing
  Systems, Seattle, WA, March 2001, 237-238. ACM Press.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0101029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0101029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.5.4; I.3.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0101035v2</id>
    <updated>2001-02-02T04:48:43Z</updated>
    <published>2001-01-28T07:14:46Z</published>
    <title>The Guidebook, the Friend, and the Room: Visitor Experience in a
  Historic House</title>
    <summary>  In this paper, we describe an electronic guidebook prototype and report on a
study of its use in a historic house. Supported by mechanisms in the guidebook,
visitors constructed experiences that had a high degree of interaction with
three entities: the guidebook, their companions, and the house and its
contents. For example, we found that most visitors played audio descriptions
played through speakers (rather than using headphones or reading textual
descriptions) to facilitate communication with their companions.
</summary>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <author>
      <name>Amy Hurst</name>
    </author>
    <author>
      <name>Margaret H. Szymanski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/634067.634229</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/634067.634229" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Abstracts, ACM SIGCHI Conf. on Human Factors in Computing
  Systems, Seattle, WA, March 2001, 273-274. ACM Press.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0101035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0101035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; H.5.2; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110006v1</id>
    <updated>2001-10-02T12:08:41Z</updated>
    <published>2001-10-02T12:08:41Z</published>
    <title>Electronic Commerce, Consumer Search and Retailing Cost Reduction</title>
    <summary>  This paper explains four things in a unified way. First, how e-commerce can
generate price equilibria where physical shops either compete with virtual
shops for consumers with Internet access, or alternatively, sell only to
consumers with no Internet access. Second, how these price equilibria might
involve price dispersion on-line. Third, why prices may be higher on-line.
Fourth, why established firms can, but need not, be more reluctant than newly
created firm to adopt e-commerce. For this purpose we develop a model where
e-commerce reduces consumers' search costs, involves trade-offs for consumers,
and reduces retailing costs.
</summary>
    <author>
      <name>Pedro Pereira</name>
    </author>
    <author>
      <name>Cristina Mazón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29th TPRC Conference, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.m Miscellaneous" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204030v4</id>
    <updated>2002-08-21T18:21:46Z</updated>
    <published>2002-04-12T15:07:46Z</published>
    <title>Fast Hands-free Writing by Gaze Direction</title>
    <summary>  We describe a method for text entry based on inverse arithmetic coding that
relies on gaze direction and which is faster and more accurate than using an
on-screen keyboard.
  These benefits are derived from two innovations: the writing task is matched
to the capabilities of the eye, and a language model is used to make
predictable words and phrases easier to write.
</summary>
    <author>
      <name>David J. Ward</name>
    </author>
    <author>
      <name>David J. C. MacKay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/418838a</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/418838a" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages. Final version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature 418, 2002 p. 838 (22nd August 2002) www.nature.com</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0204030v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204030v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2;K.4.2;H.1.1;H.5.1;I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207013v1</id>
    <updated>2002-07-04T16:02:41Z</updated>
    <published>2002-07-04T16:02:41Z</published>
    <title>A Compact Graph Model of Handwritten Images: Integration into
  Authentification and Recognition</title>
    <summary>  A novel algorithm for creating a mathematical model of curved shapes is
introduced. The core of the algorithm is based on building a graph
representation of the contoured image, which occupies less storage space than
produced by raster compression techniques. Different advanced applications of
the mathematical model are discussed: recognition of handwritten characters and
verification of handwritten text and signatures for authentification purposes.
Reducing the storage requirements due to the efficient mathematical model
results in faster retrieval and processing times. The experimental outcomes in
compression of contoured images and recognition of handwritten numerals are
given.
</summary>
    <author>
      <name>Denis V. Popel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, 1 tables, SSPR'02</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SSPR 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0207013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0210014v1</id>
    <updated>2002-10-16T07:42:30Z</updated>
    <published>2002-10-16T07:42:30Z</published>
    <title>Current state of the Sonix -- the IBR-2 instrument control software and
  plans for future developments</title>
    <summary>  The Sonix is the main control software for the IBR-2 instruments. This is a
modular configurable and flexible system created using the Varman (real time
database) and the X11/OS9 graphical package in the OS-9 environment. In the
last few years we were mostly focused on making this system more reliable and
user friendly. Because the VME hardware and software upgrade is rather
expensive we would like to replace existing VME + OS9 control computers with
the PC+Windows XP ones in the future. This could be done with the help of
VME-PCI adapters.
</summary>
    <author>
      <name>A. S. Kirilov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited talk at NOBUGS2002 Conference, NIST, Gaithersburg, MD NOBUGS
  abstract identifier NOBUGS2002/015 5 pages, pdf, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0210014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0210014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304001v1</id>
    <updated>2003-03-31T22:49:29Z</updated>
    <published>2003-03-31T22:49:29Z</published>
    <title>Design Guidelines for Landmarks to Support Navigation in Virtual
  Environments</title>
    <summary>  Unfamiliar, large-scale virtual environments are difficult to navigate. This
paper presents design guidelines to ease navigation in such virtual
environments. The guidelines presented here focus on the design and placement
of landmarks in virtual environments. Moreover, the guidelines are based
primarily on the extensive empirical literature on navigation in the real
world. A rationale for this approach is provided by the similarities between
navigational behavior in real and virtual environments.
</summary>
    <author>
      <name>Norman G. Vinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the SIGCHI conference on Human factors in computing
  systems: the CHI is the limit, p.278-285, May 15-20, 1999, Pittsburgh,
  Pennsylvania, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0304001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.4; I.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305003v1</id>
    <updated>2003-05-05T11:32:59Z</updated>
    <published>2003-05-05T11:32:59Z</published>
    <title>The Ubiquitous Interactor - Device Independent Access to Mobile Services</title>
    <summary>  The Ubiquitous Interactor (UBI) addresses the problems of design and
development that arise around services that need to be accessed from many
different devices. In UBI, the same service can present itself with different
user interfaces on different devices. This is done by separating interaction
between users and services from presentation. The interaction is kept the same
for all devices, and different presentation information is provided for
different devices. This way, tailored user interfaces for many different
devices can be created without multiplying development and maintenance work. In
this paper we describe the system design of UBI, the system implementation, and
two services implemented for the system: a calendar service and a stockbroker
service.
</summary>
    <author>
      <name>Stina Nylander</name>
    </author>
    <author>
      <name>Markus Bylund</name>
    </author>
    <author>
      <name>Annika Waern</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0305003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306087v1</id>
    <updated>2003-06-14T05:42:43Z</updated>
    <published>2003-06-14T05:42:43Z</published>
    <title>OO Model of the STAR offline production "Event Display" and its
  implementation based on Qt-ROOT</title>
    <summary>  The paper presents the "Event Display" package for the STAR offline
production as a special visualization tool to debug the reconstruction code.
This can be achieved if an author of the algorithm / code may build his/her own
custom Event Display alone from the base software blocks and re-used some
well-designed, easy to learn user-friendly patterns. For STAR offline
production Event Display ROOT with Qt lower level interface was chosen as the
base tools.
</summary>
    <author>
      <name>Valeri Fine</name>
    </author>
    <author>
      <name>Jerome Lauret</name>
    </author>
    <author>
      <name>Victor Perevoztchikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, Computing in High Energy Physics, CHEP2003, La
  Jolla California, USA, March 24-28</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7; D.1.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310013v1</id>
    <updated>2003-10-08T14:07:42Z</updated>
    <published>2003-10-08T14:07:42Z</published>
    <title>WebTeach in practice: the entrance test to the Engineering faculty in
  Florence</title>
    <summary>  We present the WebTeach project, formed by a web interface to database for
test management, a wiki site for the diffusion of teaching material and student
forums, and a suite for the generation of multiple-choice mathematical quiz
with automatic elaboration of forms. This system has been massively tested for
the entrance test to the Engineering Faculty of the University of Florence,
Italy
</summary>
    <author>
      <name>Franco Bagnoli</name>
    </author>
    <author>
      <name>Fabio Franci</name>
    </author>
    <author>
      <name>Francesco Mugelli</name>
    </author>
    <author>
      <name>Andrea Sterbini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311006v1</id>
    <updated>2003-11-07T02:17:34Z</updated>
    <published>2003-11-07T02:17:34Z</published>
    <title>How Push-To-Talk Makes Talk Less Pushy</title>
    <summary>  This paper presents an exploratory study of college-age students using
two-way, push-to-talk cellular radios. We describe the observed and reported
use of cellular radio by the participants. We discuss how the half-duplex,
lightweight cellular radio communication was associated with reduced
interactional commitment, which meant the cellular radios could be used for a
wide range of conversation styles. One such style, intermittent conversation,
is characterized by response delays. Intermittent conversation is surprising in
an audio medium, since it is typically associated with textual media such as
instant messaging. We present design implications of our findings.
</summary>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/958160.958187</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/958160.958187" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM SIGGROUP Conf. on Supporting Group Work, Sanibel Island,
  FL, Nov. 2003, 170-179. ACM Press.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0311006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.3; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312010v1</id>
    <updated>2003-12-04T05:26:51Z</updated>
    <published>2003-12-04T05:26:51Z</published>
    <title>Designing of a Community-based Translation Center</title>
    <summary>  Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.
</summary>
    <author>
      <name>Kathleen McDevitt</name>
    </author>
    <author>
      <name>Manuel A. Perez-Quinones</name>
    </author>
    <author>
      <name>Olga I. Padilla-Falto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312017v1</id>
    <updated>2003-12-09T03:44:07Z</updated>
    <published>2003-12-09T03:44:07Z</published>
    <title>An Exploratory Study of Mobile Computing Use by Knowledge Workers</title>
    <summary>  This paper describes some preliminary results from a 20-week study on the use
of Compaq iPAQ Personal Digital Assistants (PDAs) by 10 senior developers,
analysts, technical managers, and senior organisational managers. The goal of
the study was to identify what applications were used, how and where they were
used, the problems and issues that arose, and how use of the iPAQs changed over
the study period. The paper highlights some interesting uses of the iPAQs, and
identifies some of the characteristics of successful mobile applications.
</summary>
    <author>
      <name>Paul Prekop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in: Proceedings of the 2003 Australasian Computer Human
  Interaction Conference (OzCHI 2003), University of Queensland, Australia,
  November 26-28 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H1.2;H5.2;J1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401007v1</id>
    <updated>2004-01-12T18:45:47Z</updated>
    <published>2004-01-12T18:45:47Z</published>
    <title>Design of a Community-based Translation Center</title>
    <summary>  Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.
</summary>
    <author>
      <name>K. McDevitt</name>
    </author>
    <author>
      <name>M. A. Pérez-Quiñones</name>
    </author>
    <author>
      <name>O. I. Padilla-Falto</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0401007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.7; J.5; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402036v1</id>
    <updated>2004-02-16T21:56:03Z</updated>
    <published>2004-02-16T21:56:03Z</published>
    <title>Towards a Model-Based Framework for Integrating Usability and Software
  Engineering Life Cycles</title>
    <summary>  In this position paper we propose a process model that provides a development
infrastructure in which the usability engineering and software engineering life
cycles co-exist in complementary roles. We describe the motivation, hurdles,
rationale, arguments, and implementation plan for the need, specification, and
the usefulness of such a model.
</summary>
    <author>
      <name>Pardha S. Pyla</name>
    </author>
    <author>
      <name>Manuel A. Perez-Quinones</name>
    </author>
    <author>
      <name>James D. Arthur</name>
    </author>
    <author>
      <name>H. Rex Hartson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This document contains 8 pages with 5 figures. This appeared in
  Bridging the SE &amp; HCI Communities Workshop in INTERACT 2003
  (http://www.se-hci.org/bridging/interact)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.0; K.6.1; K.6.3; D.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405078v1</id>
    <updated>2004-05-23T09:53:12Z</updated>
    <published>2004-05-23T09:53:12Z</published>
    <title>Generative Programming of Graphical User Interfaces</title>
    <summary>  Generative Programming (GP) is a computing paradigm allowing automatic
creation of entire software families utilizing the configuration of elementary
and reusable components. GP can be projected on different technologies, e.g.
C++-templates, Java-Beans, Aspect-Oriented Programming (AOP), or Frame
technology. This paper focuses on Frame Technology, which aids the possible
implementation and completion of software components. The purpose of this paper
is to introduce the GP paradigm in the area of GUI application generation. It
demonstrates how automatically customized executable applications with GUI
parts can be generated from an abstract specification.
</summary>
    <author>
      <name>Max Schlee</name>
    </author>
    <author>
      <name>Jean Vanderdonckt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 12 figures, ACM Conference on Visual Interfaces AVI'2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.1, D.2.2, H.2.4, I.3.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405109v1</id>
    <updated>2004-05-28T02:59:54Z</updated>
    <published>2004-05-28T02:59:54Z</published>
    <title>Conversation Analysis and the User Experience</title>
    <summary>  We provide two case studies in the application of ideas drawn from
conversation analysis to the design of technologies that enhance the experience
of human conversation. We first present a case study of the design of an
electronic guidebook, focusing on how conversation analytic principles played a
role in the design process. We then discuss how the guidebook project has
inspired our continuing work in social, mobile audio spaces. In particular, we
describe some as yet unrealized concepts for adaptive audio spaces.
</summary>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Exploring Experience Methods Across Disciplines, ACM
  SIGCHI Conf. on Human Factors in Computing Systems (CHI 2004), Vienna,
  Austria, Apr. 2004.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0510034v1</id>
    <updated>2005-10-14T01:03:55Z</updated>
    <published>2005-10-14T01:03:55Z</published>
    <title>COMODI: On the Graphical User Interface</title>
    <summary>  We propose a series of features for the graphical user interface (GUI) of the
COmputational MOdule Integrator (COMODI) \cite{Synasc05a}\cite{COMODI}. In view
of the special requirements that a COMODI type of framework for scientific
computing imposes and inspiring from existing solutions that provide advanced
graphical visual programming environments, we identify those elements and
associated behaviors that will have to find their way into the first release of
COMODI.
</summary>
    <author>
      <name>Zsolt I. Lázár</name>
    </author>
    <author>
      <name>Andreea Fanea</name>
    </author>
    <author>
      <name>Dragoş Petraşcu</name>
    </author>
    <author>
      <name>Vladiela Ciobotariu-Boer</name>
    </author>
    <author>
      <name>Bazil Pârv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, to be published as proceedings of SYNASC 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0510034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0510034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.13; D.2.12; D.2.11; D.2.9; D.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601025v1</id>
    <updated>2006-01-09T13:52:32Z</updated>
    <published>2006-01-09T13:52:32Z</published>
    <title>Prop-Based Haptic Interaction with Co-location and Immersion: an
  Automotive Application</title>
    <summary>  Most research on 3D user interfaces aims at providing only a single sensory
modality. One challenge is to integrate several sensory modalities into a
seamless system while preserving each modality's immersion and performance
factors. This paper concerns manipulation tasks and proposes a visuo-haptic
system integrating immersive visualization, tactile force and tactile feedback
with co-location. An industrial application is presented.
</summary>
    <author>
      <name>Michael Ortega</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Sabine Coquillart</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans HAVE 2005 - IEEE International Workshop on Haptic Audio
  Visual Environments and their Applications</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0601025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604103v1</id>
    <updated>2006-04-25T19:53:19Z</updated>
    <published>2006-04-25T19:53:19Z</published>
    <title>Further Evaluationh of VLEs using HCI and Educational Metrics</title>
    <summary>  Under consideration are the general set of Human computer Interaction (HCI)
and Educational principles from prominent authors in the field and the
construction of a system for evaluating Virtual Learning Environments (VLEs)
with respect to the application of these HCI and Educational Principles. A
frequency analysis of principles is used to obtain the most significant set.
Metrics are devised to provide objective measures of these principles and a
consistent testing regime is introduced. These principles are used to analyse
the University VLE Blackboard. An open source VLE is also constructed with
similar content to Blackboard courses so that a systematic comparison can be
made. HCI and Educational metrics are determined for each VLE.
</summary>
    <author>
      <name>Vita Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0604103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605147v1</id>
    <updated>2006-05-30T15:32:41Z</updated>
    <published>2006-05-30T15:32:41Z</published>
    <title>Utilisation de la linguistique en reconnaissance de la parole : un
  état de l'art</title>
    <summary>  To transcribe speech, automatic speech recognition systems use statistical
methods, particularly hidden Markov model and N-gram models. Although these
techniques perform well and lead to efficient systems, they approach their
maximum possibilities. It seems thus necessary, in order to outperform current
results, to use additional information, especially bound to language. However,
introducing such knowledge must be realized taking into account specificities
of spoken language (hesitations for example) and being robust to possible
misrecognized words. This document presents a state of the art of these
researches, evaluating the impact of the insertion of linguistic information on
the quality of the transcription.
</summary>
    <author>
      <name>Stéphane Huet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA / INRIA Rennes, IRISA / INRIA Rennes</arxiv:affiliation>
    </author>
    <author>
      <name>Pascale Sébillot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA / INRIA Rennes</arxiv:affiliation>
    </author>
    <author>
      <name>Guillaume Gravier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA / INRIA Rennes</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0605147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606107v1</id>
    <updated>2006-06-26T15:58:40Z</updated>
    <published>2006-06-26T15:58:40Z</published>
    <title>Human Information Processing with the Personal Memex</title>
    <summary>  In this report, we describe the work done in a project that explored the
human information processing aspects of a personal memex (a memex to organize
personal information). In the project, we considered the use of the personal
memex, focusing on information recall, by three populations: people with Mild
Cognitive Impairment, those diagnosed with Macular Degeneration, and a
high-functioning population. The outcomes of the project included human
information processing-centered design guidelines for the memex interface, a
low-fidelity prototype, and an annotated bibliography for human information
processing, usability and design literature relating to the memex and the
populations we explored.
</summary>
    <author>
      <name>Ingrid Burbey</name>
    </author>
    <author>
      <name>Gyuhyun Kwon</name>
    </author>
    <author>
      <name>Uma Murthy</name>
    </author>
    <author>
      <name>Nicholas Polys</name>
    </author>
    <author>
      <name>Prince Vincent</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0606107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607072v1</id>
    <updated>2006-07-14T16:00:40Z</updated>
    <published>2006-07-14T16:00:40Z</published>
    <title>Effect of Interface Style in Peer Review Comments for UML Designs</title>
    <summary>  This paper presents our evaluation of using a Tablet-PC to provide
peer-review comments in the first year Computer Science course. Our exploration
consisted of an evaluation of how students write comments on other students'
assignments using three different methods: pen and paper, a Tablet-PC, and a
desktop computer. Our ultimate goal is to explore the effect that interface
style (Tablet vs. Desktop) has on the quality and quantity of the comments
provided.
</summary>
    <author>
      <name>Scott A. Turner</name>
    </author>
    <author>
      <name>Manuel A. Perez-Quinones</name>
    </author>
    <author>
      <name>Stephen H. Edwards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1; H.4; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611014v1</id>
    <updated>2006-11-03T14:34:57Z</updated>
    <published>2006-11-03T14:34:57Z</published>
    <title>Interactive Problem Solving in Prolog</title>
    <summary>  This paper presents an environment for solving Prolog problems which has been
implemented as a module for the virtual laboratory VILAB. During the problem
solving processes the learners get fast adaptive feedback. As a result
analysing the learner's actions the system suggests the use of suitable
auxiliary predicates which will also be checked for proper implementation. The
focus of the environment has been set on robustness and the integration in
VILAB.
</summary>
    <author>
      <name>Erik Braun</name>
    </author>
    <author>
      <name>Rainer Luetticke</name>
    </author>
    <author>
      <name>Ingo Gloeckner</name>
    </author>
    <author>
      <name>Hermann Helbig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, accepted for publication: Interactive computer
  aided learning (ICL) 2006, International Conference in Villach (Austria).
  Paper was not published because the authors were not able to participate on
  the conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.1; K.3.2; D.1.6; I.2.6; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611151v2</id>
    <updated>2007-02-01T09:05:31Z</updated>
    <published>2006-11-29T16:22:18Z</published>
    <title>Collaborative design : managing task interdependencies and multiple
  perspectives</title>
    <summary>  This paper focuses on two characteristics of collaborative design with
respect to cooperative work: the importance of work interdependencies linked to
the nature of design problems; and the fundamental function of design
cooperative work arrangement which is the confrontation and combination of
perspectives. These two intrinsic characteristics of the design work stress
specific cooperative processes: coordination processes in order to manage task
interdependencies, establishment of common ground and negotiation mechanisms in
order to manage the integration of multiple perspectives in design.
</summary>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Interacting With Computers 18, 1 (2006) 1-20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611151v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611151v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611152v2</id>
    <updated>2007-02-01T09:03:34Z</updated>
    <published>2006-11-29T16:24:20Z</published>
    <title>Viewpoints in co-design: a field study in concurrent engineering</title>
    <summary>  We present a field study aimed at analysing the use of viewpoints in
co-design meetings. A viewpoint is a representation characterised by a certain
combination of constraints. Three types of viewpoints are distinguished:
prescribed viewpoint, discipline-specific viewpoint and integrated viewpoint.
The contribution of our work consists in characterising the viewpoints of
various stakeholders involved in co-design ("design office" disciplines, and
production and maintenance disciplines), the dynamics of viewpoints
confrontation and the cooperative modes that enable these different viewpoints
to be integrated.
</summary>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <author>
      <name>Géraldine Martin</name>
    </author>
    <author>
      <name>Elisabeth Lavigne</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Design Studies 26, 3 (2005) 215-241</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611152v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611152v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612010v2</id>
    <updated>2007-02-01T09:04:37Z</updated>
    <published>2006-12-01T11:43:08Z</published>
    <title>Articulation entre composantes verbale et graphico-gestuelle de
  l'interaction dans des réunions de conception architecturale</title>
    <summary>  This study is focused on the role of external representations, e.g.,
skteches, in collaborative architectural design. In particular, we analyse (1)
the use of graphico-gestural modalities and, (2) the articulation modes between
graphico-gestural and verbal modalities in design interaction. We have
elaborated a first classification which distinguishes between two modes of
articulation, articulation in integrated activities versus articulation in
parallel activities.
</summary>
    <author>
      <name>Willemien Visser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, INRIA</arxiv:affiliation>
    </author>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, INRIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans SCAN'05 (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0612010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612016v2</id>
    <updated>2007-03-02T12:47:49Z</updated>
    <published>2006-12-04T13:48:22Z</published>
    <title>Memory of past designs: distinctive roles in individual and collective
  design</title>
    <summary>  Empirical studies on design have emphasised the role of memory of past
solutions. Design involves the use of generic knowledge as well as episodic
knowledge about past designs for analogous problems : in this way, it involves
the reuse of past designs. We analyse this mechanism of reuse from a
socio-cognitive viewpoint. According to a purely cognitive approach, reuse
involves cognitive mechanisms linked to the problem solving activity itself.
Our socio-cognitive approach accounts for these phenomena as well as reuse
mechanisms linked to cooperation, in particular coordination, and
confrontation/integration of viewpoints.
</summary>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cognitive Technology Journal 1, 8 (2003) 16-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0612016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612021v2</id>
    <updated>2007-03-04T20:14:42Z</updated>
    <published>2006-12-04T16:27:32Z</published>
    <title>Multimodality and parallelism in design interaction: co-designers'
  alignment and coalitions</title>
    <summary>  This paper presents an analysis of various forms of articulation between
graphico-gestural and verbal modalities in parallel interactions between
designers in a collaborative design situation. Based on our methodological
framework, we illustrate several forms of multimodal articulations, that is,
integrated and non-integrated, through extracts from a corpus on an
architectural design meeting. These modes reveal alignment or disalignment
between designers, with respect to the focus of their activities. They also
show different forms of coalition.
</summary>
    <author>
      <name>Françoise Détienne</name>
    </author>
    <author>
      <name>Willemien Visser</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans COOP'2006 Volume 137 (2006) 118-131</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0612021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612022v1</id>
    <updated>2006-12-04T16:28:44Z</updated>
    <published>2006-12-04T16:28:44Z</published>
    <title>Both Generic Design and Different Forms of Designing</title>
    <summary>  This paper defends an augmented cognitively oriented "generic-design
hypothesis": There are both significant similarities between the design
activities implemented in different situations and crucial differences between
these and other cognitive activities; yet, characteristics of a design
situation (i.e., related to the designers, the artefact, and other task
variables influencing these two) introduce specificities in the corresponding
design activities and cognitive structures that are used. We thus combine the
generic-design hypothesis with that of different "forms" of designing. In this
paper, outlining a number of directions that need further elaboration, we
propose a series of candidate dimensions underlying such forms of design.
</summary>
    <author>
      <name>Willemien Visser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Wonderground, the 2006 DRS (Design Research Society)
  International Conference (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0612022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701199v2</id>
    <updated>2007-05-24T20:45:24Z</updated>
    <published>2007-01-31T16:36:43Z</published>
    <title>A Virtual Logo Keyboard for People with Motor Disabilities</title>
    <summary>  In our society, people with motor impairments are oftentimes socially
excluded from their environment. This is unfortunate because every human being
should have the possibility to obtain the necessary conditions to live a normal
life. Although there is technology to assist people with motor impairments, few
systems are targeted for programming environments. We have created a system,
called Logo Keyboard, to assist people with motor disabilities to program with
the Logo programming language. With this special keyboard we can help more
people to get involved into computer programming and to develop projects in
different areas.
</summary>
    <author>
      <name>Stephane Norte</name>
    </author>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; K.3.1; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702004v1</id>
    <updated>2007-02-01T09:03:19Z</updated>
    <published>2007-02-01T09:03:19Z</published>
    <title>What model(s) for program understanding?</title>
    <summary>  The first objective of this paper is to present and discuss various types of
models of program understanding. They are discussed in relation to models of
text understanding. The second objective of this paper is to assess the effect
of purpose for reading, or more specifically programming task, on the cognitive
processes involved and representations constructed in program understanding.
This is done in the theoretical framework of van Dijk and Kintsch's model of
text understanding (1983).
</summary>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans UCIS'96, Conference on Using Complex Information Systems
  (1996)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702005v1</id>
    <updated>2007-02-01T09:03:59Z</updated>
    <published>2007-02-01T09:03:59Z</published>
    <title>An empirical study of software reuse by experts in object-oriented
  design</title>
    <summary>  This paper presents an empirical study of the software reuse activity by
expert designers in the context of object-oriented design. Our study focuses on
the three following aspects of reuse : (1) the interaction between some design
processes, e.g. constructing a problem representation, searching for and
evaluating solutions, and reuse processes, i.e. retrieving and using previous
solutions, (2) the mental processes involved in reuse, e.g. example-based
retrieval or bottom-up versus top-down expanding of the solution, and (3) the
mental representations constructed throughout the reuse activity, e.g. dynamic
versus static representations. Some implications of these results for the
specification of software reuse support environments are discussed.
</summary>
    <author>
      <name>Jean-Marie Burkhardt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA, LEI</arxiv:affiliation>
    </author>
    <author>
      <name>Françoise Détienne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans INTERACT'95 (1995)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.2542v1</id>
    <updated>2007-04-19T14:27:25Z</updated>
    <published>2007-04-19T14:27:25Z</published>
    <title>Narratives within immersive technologies</title>
    <summary>  The main goal of this project is to research technical advances in order to
enhance the possibility to develop narratives within immersive mediated
environments. An important part of the research is concerned with the question
of how a script can be written, annotated and realized for an immersive
context. A first description of the main theoretical framework and the ongoing
work and a first script example is provided. This project is part of the
program for presence research, and it will exploit physiological feedback and
Computational Intelligence within virtual reality.
</summary>
    <author>
      <name>Joan Llobera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.2542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3643v1</id>
    <updated>2007-04-27T00:42:22Z</updated>
    <published>2007-04-27T00:42:22Z</published>
    <title>Sabbath Day Home Automation: "It's Like Mixing Technology and Religion"</title>
    <summary>  We present a qualitative study of 20 American Orthodox Jewish families' use
of home automation for religious purposes. These lead users offer insight into
real-life, long-term experience with home automation technologies. We discuss
how automation was seen by participants to contribute to spiritual experience
and how participants oriented to the use of automation as a religious custom.
We also discuss the relationship of home automation to family life. We draw
design implications for the broader population, including surrender of control
as a design resource, home technologies that support long-term goals and
lifestyle choices, and respite from technology.
</summary>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <author>
      <name>Sally Augustin</name>
    </author>
    <author>
      <name>Brooke Foucault</name>
    </author>
    <link href="http://arxiv.org/abs/0704.3643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1725v2</id>
    <updated>2009-08-20T14:55:41Z</updated>
    <published>2007-08-13T15:00:37Z</published>
    <title>Design: One, but in different forms</title>
    <summary>  This overview paper defends an augmented cognitively oriented generic-design
hypothesis: there are both significant similarities between the design
activities implemented in different situations and crucial differences between
these and other cognitive activities; yet, characteristics of a design
situation (related to the design process, the designers, and the artefact)
introduce specificities in the corresponding cognitive activities and
structures that are used, and in the resulting designs. We thus augment the
classical generic-design hypothesis with that of different forms of designing.
We review the data available in the cognitive design research literature and
propose a series of candidates underlying such forms of design, outlining a
number of directions requiring further elaboration.
</summary>
    <author>
      <name>Willemien Visser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.destud.2008.11.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.destud.2008.11.004" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Design Studies 30, 3 (2009) 187-223</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1725v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1725v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0370v1</id>
    <updated>2007-09-04T09:20:03Z</updated>
    <published>2007-09-04T09:20:03Z</published>
    <title>An Integrated Simulation System for Human Factors Study</title>
    <summary>  It has been reported that virtual reality can be a useful tool for ergonomics
study. The proposed integrated simulation system aims at measuring operator's
performance in an interactive way for 2D control panel design. By incorporating
some sophisticated virtual reality hardware/software, the system allows natural
human-system and/or human-human interaction in a simulated virtual environment;
enables dynamic objective measurement of human performance; and evaluates the
quality of the system design in human factors perspective based on the
measurement. It can also be for operation training for some 2D control panels.
</summary>
    <author>
      <name>Ying Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DIE</arxiv:affiliation>
    </author>
    <author>
      <name>Wei Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DIE</arxiv:affiliation>
    </author>
    <author>
      <name>Fouad Bennis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Damien Chablat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans The Institute of Industrial Engineers Annual Conference - The
  Institute of Industrial Engineers Annual Conference, Orlando : \'Etats-Unis
  d'Am\'erique (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.0370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0426v1</id>
    <updated>2007-09-04T13:23:40Z</updated>
    <published>2007-09-04T13:23:40Z</published>
    <title>Do oral messages help visual search?</title>
    <summary>  A preliminary experimental study is presented, that aims at eliciting the
contribution of oral messages to facilitating visual search tasks on crowded
visual displays. Results of quantitative and qualitative analyses suggest that
appropriate verbal messages can improve both target selection time and
accuracy. In particular, multimodal messages including a visual presentation of
the isolated target together with absolute spatial oral information on its
location in the displayed scene seem most effective. These messages also got
top-ranking ratings from most subjects.
</summary>
    <author>
      <name>Noëlle Carbonell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Suzanne Kieffer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Natural Multimodal Dialogue Systems, Dordrecht (NL)
  Springer (Ed.) (2005) pp. 131-157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.0426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.1056v3</id>
    <updated>2007-09-17T21:08:35Z</updated>
    <published>2007-09-07T11:59:22Z</published>
    <title>A Sudoku Game for People with Motor Impairments</title>
    <summary>  Computer games are motivating and beneficial in learning different
educational skills. Most people use their fingers, hands, and arms when using a
computer game. However, for people with motor disabilities this task can be a
barrier. We present a new Sudoku game for people whose motion is impaired,
called Sudoku 4ALL. With this special interface a person can control the game
with the voice or with a single switch. Our research aims to cautiously search
for issues that might be appropriate for computational support and to build
enabling technologies that increase individuals' functional independence in a
game environment.
</summary>
    <author>
      <name>Stephane Norte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.1056v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.1056v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; K.4.2; K.8.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0847v1</id>
    <updated>2007-10-03T17:36:07Z</updated>
    <published>2007-10-03T17:36:07Z</published>
    <title>Emotion capture based on body postures and movements</title>
    <summary>  In this paper we present a preliminary study for designing interactive
systems that are sensible to human emotions based on the body movements. To do
so, we first review the literature on the various approaches for defining and
characterizing human emotions. After justifying the adopted characterization
space for emotions, we then focus on the movement characteristics that must be
captured by the system for being able to recognize the human emotions.
</summary>
    <author>
      <name>Alexis Clay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPSI</arxiv:affiliation>
    </author>
    <author>
      <name>Nadine Couture</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPSI</arxiv:affiliation>
    </author>
    <author>
      <name>Laurence Nigay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLIPS - IMAG</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Computing and
  e-systems 2007 (TIGERA'07), Hammamet : Tunisie (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4999v2</id>
    <updated>2007-11-08T08:56:00Z</updated>
    <published>2007-10-26T06:32:22Z</published>
    <title>L'analyse de l'expertise du point de vue de l'ergonomie cognitive</title>
    <summary>  This paper presents a review of methods for collecting and analysing data on
complex activities. Starting with methods developed for design, we examine the
possibility to transpose them to other complex activities, especially
activities referring to sensorial expertise. R\'esum\'e Ce texte pr\'esente une
revue de m\'ethodes pour recueillir et analyser des donn\'ees sur des
actvit\'es complexes. A partir de m\'ethodes d\'evelopp\'ees pour des
actvit\'es de conception, nous examinons la possibilit\'e de les transposer \`a
d'autres actvit\'es complexes, notamment des actvit\'es faisant \`a appel \`a
des expertises sensorielles.
</summary>
    <author>
      <name>Willemien Visser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Les expertises sensorielles : Nature et acquisition (2006)
  1-12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4999v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4999v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2183v1</id>
    <updated>2007-12-13T16:43:56Z</updated>
    <published>2007-12-13T16:43:56Z</published>
    <title>Apports des démarches d'inspection et des tests d'usage dans
  l'évaluation de l'accessibilité de E-services</title>
    <summary>  This article proposes to describe and compare the contributions of various
techniques of evaluation of the accessibility of E-services carried out
starting from (i) methods of inspection (on the basis of traditional ergonomic
criteria and accessibility) and (ii) of tests of use. It show that these are
the latter which show the best rate of identification of the problems of uses
for the poeple with disabilities
</summary>
    <author>
      <name>Marc-Eric Bobiller-Chaumon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRePS</arxiv:affiliation>
    </author>
    <author>
      <name>Françoise Sandoz-Guermond</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Actes du congr\`es ERGO IA'2006 - ERGO'IA : L'humain comme
  facteur de performance des syst\`emes complexes, Biarritz : France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.2183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.3215v1</id>
    <updated>2007-12-19T15:54:07Z</updated>
    <published>2007-12-19T15:54:07Z</published>
    <title>L'accessibilité des E-services aux personnes non-voyantes :
  difficultés d'usage et recommandations</title>
    <summary>  While taking into account handicapped people in the design of technologies
represents a social and political stake that becomes important (in particular
with the recent law on equal rights for all the citizens, March 2004), this
paper aims at evaluating the level of accessibility of two sites of E-services
thanks to tests of use and proposing a set of recommendations in order to
increase usability for the largest amount of people.
</summary>
    <author>
      <name>Françoise Sandoz-Guermond</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Marc-Eric Bobiller-Chaumon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRePS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans International Conference Proceedings of IHM'2006 - IIHM :
  Interaction Homme Machine, Montr\'eal : Canada (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.3215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.3433v1</id>
    <updated>2007-12-19T23:31:55Z</updated>
    <published>2007-12-19T23:31:55Z</published>
    <title>AccelKey Selection Method for Mobile Devices</title>
    <summary>  Portable Electronic Devices usually utilize a small screen with limited
viewing area and a keyboard with a limited number of keys. This makes it
difficult to perform quick searches in data arrays containing more than dozen
items such an address book or song list. In this article we present a new data
selection method which allows the user to quickly select an entry from a list
using 4-way navigation device such as joystick, trackball or 4-way key pad.
This method allows for quick navigation using just one hand, without looking at
the screen.
</summary>
    <author>
      <name>Vadim Zaliva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.3433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1033v1</id>
    <updated>2008-01-07T16:05:46Z</updated>
    <published>2008-01-07T16:05:46Z</published>
    <title>The What, Who, Where, When, Why and How of Context-Awareness</title>
    <summary>  The understanding of context and context-awareness is very important for the
areas of handheld and ubiquitous computing. Unfortunately, at present, there
has not been a satisfactory definition of these two concepts that would lead to
a more effective communication in humancomputer interaction. As a result, on
the one hand, application designers are not able to choose what context to use
in their applications and on the other, they cannot determine the type of
context-awareness behaviours their applications should exhibit. In this work,
we aim to provide answers to some fundamental questions that could enlighten us
on the definition of context and its functionality.
</summary>
    <author>
      <name>George Tsibidis</name>
    </author>
    <author>
      <name>Theodoros N. Arvanitis</name>
    </author>
    <author>
      <name>Chris Baber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted manuscript at the CHI 2000, April 3-2000, The Hague, The
  Netherlands</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.1033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3114v1</id>
    <updated>2008-01-21T00:33:15Z</updated>
    <published>2008-01-21T00:33:15Z</published>
    <title>Thinking is Bad: Implications of Human Error Research for Spreadsheet
  Research and Practice</title>
    <summary>  In the spreadsheet error community, both academics and practitioners
generally have ignored the rich findings produced by a century of human error
research. These findings can suggest ways to reduce errors; we can then test
these suggestions empirically. In addition, research on human error seems to
suggest that several common prescriptions and expectations for reducing errors
are likely to be incorrect. Among the key conclusions from human error research
are that thinking is bad, that spreadsheets are not the cause of spreadsheet
errors, and that reducing errors is extremely difficult.
</summary>
    <author>
      <name>Raymond R. Panko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages including references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 69-80
  ISBN 978-905617-58-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1; K.3; K.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4274v1</id>
    <updated>2008-01-28T13:55:55Z</updated>
    <published>2008-01-28T13:55:55Z</published>
    <title>Computational Models of Spreadsheet Development: Basis for Educational
  Approaches</title>
    <summary>  Among the multiple causes of high error rates in spreadsheets, lack of proper
training and of deep understanding of the computational model upon which
spreadsheet computations rest might not be the least issue. The paper addresses
this problem by presenting a didactical model focussing on cell interaction,
thus exceeding the atomicity of cell computations. The approach is motivated by
an investigation how different spreadsheet systems handle certain computational
issues implied from moving cells, copy-paste operations, or recursion.
</summary>
    <author>
      <name>Karin Hodnigg</name>
    </author>
    <author>
      <name>Markus Clermont</name>
    </author>
    <author>
      <name>Roland T. Mittermeir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 Pages, 4 figures, includes references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 153-168
  ISBN 1 902724 94 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3478v1</id>
    <updated>2008-02-24T01:57:01Z</updated>
    <published>2008-02-24T01:57:01Z</published>
    <title>It Ain't What You View, But The Way That You View It: documenting
  spreadsheets with Excelsior, semantic wikis, and literate programming</title>
    <summary>  I describe preliminary experiments in documenting Excelsior versions of
spreadsheets using semantic wikis and literate programming. The objective is to
create well-structured and comprehensive documentation, easy to use by those
unfamiliar with the spreadsheets documented. I discuss why so much
documentation is hard to use, and briefly explain semantic wikis and literate
programming; although parts of the paper are Excelsior-specific, these sections
may be of more general interest.
</summary>
    <author>
      <name>Jocelyn Paine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 131-142
  ISBN 978-905617-58-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.1; H.4.1; K.6.4; D.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3480v1</id>
    <updated>2008-02-24T02:10:03Z</updated>
    <published>2008-02-24T02:10:03Z</published>
    <title>Why Task-Based Training is Superior to Traditional Training Methods</title>
    <summary>  The risks of spreadsheet use do not just come from the misuse of formulae. As
such, training needs to go beyond this technical aspect of spreadsheet use and
look at the spreadsheet in its full business context. While standard training
is by and large unable to do this, task-based training is perfectly suited to a
contextual approach to training.
</summary>
    <author>
      <name>Kath McGuire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 191-196
  ISBN 978-905617-58-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.2.5; H.4.1; K.6.4; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0937v1</id>
    <updated>2008-04-06T23:20:15Z</updated>
    <published>2008-04-06T23:20:15Z</published>
    <title>Issues in Strategic Decision Modelling</title>
    <summary>  [Spreadsheet] Models are invaluable tools for strategic planning. Models help
key decision makers develop a shared conceptual understanding of complex
decisions, identify sensitivity factors and test management scenarios.
Different modelling approaches are specialist areas in themselves. Model
development can be onerous, expensive, time consuming, and often bewildering.
It is also an iterative process where the true magnitude of the effort, time
and data required is often not fully understood until well into the process.
This paper explores the traditional approaches to strategic planning modelling
commonly used in organisations and considers the application of a real-options
approach to match and benefit from the increasing uncertainty in today's
rapidly changing world.
</summary>
    <author>
      <name>Paula Jennings</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2003 111-116
  ISBN 1 86166 199 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.0937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.2.5; H.4.1; K.6.4; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0941v1</id>
    <updated>2008-04-07T00:20:24Z</updated>
    <published>2008-04-07T00:20:24Z</published>
    <title>Reducing Overconfidence in Spreadsheet Development</title>
    <summary>  Despite strong evidence of widespread errors, spreadsheet developers rarely
subject their spreadsheets to post-development testing to reduce errors. This
may be because spreadsheet developers are overconfident in the accuracy of
their spreadsheets. This conjecture is plausible because overconfidence is
present in a wide variety of human cognitive domains, even among experts. This
paper describes two experiments in overconfidence in spreadsheet development.
The first is a pilot study to determine the existence of overconfidence. The
second tests a manipulation to reduce overconfidence and errors. The
manipulation is modestly successful, indicating that overconfidence reduction
is a promising avenue to pursue.
</summary>
    <author>
      <name>Raymond R. Panko</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2003 49-66
  ISBN 1 86166 199 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.0941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4885v1</id>
    <updated>2008-04-30T18:44:52Z</updated>
    <published>2008-04-30T18:44:52Z</published>
    <title>SimDialog: A visual game dialog editor</title>
    <summary>  SimDialog is a visual editor for dialog in computer games. This paper
presents the design of SimDialog, illustrating how script writers and
non-programmers can easily create dialog for video games with complex branching
structures and dynamic response characteristics. The system creates dialog as a
directed graph. This allows for play using the dialog with a state-based cause
and effect system that controls selection of non-player character responses and
can provide a basic scoring mechanism for games.
</summary>
    <author>
      <name>C. Owen</name>
    </author>
    <author>
      <name>F. Biocca</name>
    </author>
    <author>
      <name>C. Bohil</name>
    </author>
    <author>
      <name>J. Conley</name>
    </author>
    <link href="http://arxiv.org/abs/0804.4885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2189v1</id>
    <updated>2008-05-15T00:27:15Z</updated>
    <published>2008-05-15T00:27:15Z</published>
    <title>Visual Checking of Spreadsheets</title>
    <summary>  The difference between surface and deep structures of a spreadsheet is a
major cause of difficulty in checking spreadsheets. After a brief survey of
current methods of checking (or debugging) spreadsheets, new visual methods of
showing the deep structures are presented. Illustrations are given on how these
visual methods can be employed in various interactive local and global
debugging strategies.
</summary>
    <author>
      <name>Ying Chen</name>
    </author>
    <author>
      <name>Hock Chuan Chan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 5 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 75-85
  ISBN:1 86166 158 4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.2189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0172v1</id>
    <updated>2008-06-01T21:01:15Z</updated>
    <published>2008-06-01T21:01:15Z</published>
    <title>EuSpRIG TEAM work:Tools, Education, Audit, Management</title>
    <summary>  Research on spreadsheet errors began over fifteen years ago. During that
time, there has been ample evidence demonstrating that spreadsheet errors are
common and nontrivial. Quite simply, spreadsheet error rates are comparable to
error rates in other human cognitive activities and are caused by fundamental
limitations in human cognition, not mere sloppiness. Nor does ordinary "being
careful" eliminate errors or reduce them to acceptable levels.
</summary>
    <author>
      <name>David Chadwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 1 Figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2003 1-6 ISBN
  1 86166 199 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.0172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0182v1</id>
    <updated>2008-06-01T23:14:22Z</updated>
    <published>2008-06-01T23:14:22Z</published>
    <title>Training Gamble leads to Corporate Grumble?</title>
    <summary>  Fifteen years of research studies have concluded unanimously that spreadsheet
errors are both common and non-trivial. Now we must seek ways to reduce
spreadsheet errors. Several approaches have been suggested, some of which are
promising and others, while appealing because they are easy to do, are not
likely to be effective. To date, only one technique, cell-by-cell code
inspection, has been demonstrated to be effective. We need to conduct further
research to determine the degree to which other techniques can reduce
spreadsheet errors.
</summary>
    <author>
      <name>David R. Chadwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 4 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2002 1-11
  ISBN 1 86166 182</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.0182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.2628v1</id>
    <updated>2008-07-16T18:31:14Z</updated>
    <published>2008-07-16T18:31:14Z</published>
    <title>Un cadre de conception pour réunir les modèles d'interaction et
  l'ingénierie des interfaces</title>
    <summary>  We present HIC (Human-system Interaction Container), a general framework for
the integration of advanced interaction in the software development process. We
show how this framework allows to reconcile the software development methods
(such MDA, MDE) with the architectural models of software design such as MVC or
PAC. We illustrate our approach thanks to two different types of implementation
for this concept in two different business areas: one software design pattern,
MVIC (Model View Interaction Control) and one architectural model, IM
(Interaction Middleware).
</summary>
    <author>
      <name>Jérôme Lard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Landragin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaTTice</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Grisvard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ATOL</arxiv:affiliation>
    </author>
    <author>
      <name>David Faure</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ing\'enierie des Syst\`emes d'Information (ISI) 12, 6 (2007) 67-91</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.2628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.2628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.2993v1</id>
    <updated>2008-07-18T15:45:47Z</updated>
    <published>2008-07-18T15:45:47Z</published>
    <title>Establishing and Measuring Standard Spreadsheet Practices for End-Users</title>
    <summary>  This paper offers a brief review of cognitive verbs typically used in the
literature to describe standard spreadsheet practices. The verbs identified are
then categorised in terms of Bloom's Taxonomy of Hierarchical Levels, and then
rated and arranged to distinguish some of their qualities and characteristics.
Some measurement items are then evaluated to see how well computerised test
question items validate or reinforce training or certification. The paper
considers how establishing standard practices in spreadsheet training and
certification can help reduce some of the risks associated with spreadsheets,
and help promote productivity.
</summary>
    <author>
      <name>Garry Cleere</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages, 5 Tables, 9 Colour Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2008 1-15
  ISBN 978-905617-69-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.2993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.2993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.2997v1</id>
    <updated>2008-07-18T16:00:34Z</updated>
    <published>2008-07-18T16:00:34Z</published>
    <title>Reducing Spreadsheet Risk with FormulaDataSleuth</title>
    <summary>  A new MS Excel application has been developed which seeks to reduce the risks
associated with the development, operation and auditing of Excel spreadsheets.
FormulaDataSleuth provides a means of checking spreadsheet formulas and data as
they are developed or used, enabling the users to identify actual or potential
errors quickly and thereby halt their propagation. In this paper, we will
describe, with examples, how the application works and how it can be applied to
reduce the risks associated with Excel spreadsheets.
</summary>
    <author>
      <name>Bill Bekenn</name>
    </author>
    <author>
      <name>Ray Hooper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 colour figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2008 33-44
  ISBN 978-905617-69-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.2997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.2997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3186v1</id>
    <updated>2008-07-20T20:58:37Z</updated>
    <published>2008-07-20T20:58:37Z</published>
    <title>New Guidelines For Spreadsheets</title>
    <summary>  Current prescriptions for spreadsheet style specify modular separation of
data, calcu1ation and output, based on the notion that writing a spreadsheet is
like writing a computer program. Instead of a computer programming style, this
article examines rules of style for text, graphics, and mathematics. Much
'common wisdom' in spreadsheets contradicts rules for these well-developed
arts. A case is made here for a new style for spreadsheets that emphasises
readability. The new style is described in detail with an example, and
contrasted with the programming style.
</summary>
    <author>
      <name>John F. Raffensperger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2001 61-76
  ISBN:1 86166 179 7</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.3186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.4623v1</id>
    <updated>2008-07-29T10:15:38Z</updated>
    <published>2008-07-29T10:15:38Z</published>
    <title>AceWiki: Collaborative Ontology Management in Controlled Natural
  Language</title>
    <summary>  AceWiki is a prototype that shows how a semantic wiki using controlled
natural language - Attempto Controlled English (ACE) in our case - can make
ontology management easy for everybody. Sentences in ACE can automatically be
translated into first-order logic, OWL, or SWRL. AceWiki integrates the OWL
reasoner Pellet and ensures that the ontology is always consistent. Previous
results have shown that people with no background in logic are able to add
formal knowledge to AceWiki without being instructed or trained in advance.
</summary>
    <author>
      <name>Tobias Kuhn</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 3rd Semantic Wiki Workshop, CEUR Workshop
  Proceedings, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.4623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.4623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1019v1</id>
    <updated>2008-09-05T12:59:06Z</updated>
    <published>2008-09-05T12:59:06Z</published>
    <title>Moving and resizing of the screen objects</title>
    <summary>  The shape and size of the objects, which we see on the screen, when the
application is running, are defined at the design time. By using some sort of
adaptive interface, developers give users a chance to resize these objects or
on rare occasion even change, but all these changes are predetermined by a
developer; user can't go out of the designer's scenario. Making each and all
elements moveable / resizable and giving users the full control of these
processes, changes the whole idea of applications; programs become user-driven
and significantly increase the effectiveness of users' work. This article is
about the instrument to turn any screen object into moveable / resizable.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.3571v1</id>
    <updated>2008-09-21T11:06:23Z</updated>
    <published>2008-09-21T11:06:23Z</published>
    <title>Evaluation of an Intelligent Assistive Technology for Voice Navigation
  of Spreadsheets</title>
    <summary>  An integral part of spreadsheet auditing is navigation. For sufferers of
Repetitive Strain Injury who need to use voice recognition technology this
navigation can be highly problematic. To counter this the authors have
developed an intelligent voice navigation system, iVoice, which replicates
common spreadsheet auditing behaviours through simple voice commands. This
paper outlines the iVoice system and summarizes the results of a study to
evaluate iVoice when compared to a leading voice recognition technology.
</summary>
    <author>
      <name>Derek Flood</name>
    </author>
    <author>
      <name>Kevin Mc Daid</name>
    </author>
    <author>
      <name>Fergal Mc Caffery</name>
    </author>
    <author>
      <name>Brian Bishop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 2 Colour Figures, 4 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2008 69-78
  ISBN 978-905617-69-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.3571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.3571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.3595v1</id>
    <updated>2008-09-21T17:45:08Z</updated>
    <published>2008-09-21T17:45:08Z</published>
    <title>Controlling End User Computing Applications - a case study</title>
    <summary>  We report the results of a project to control the use of end user computing
tools for business critical applications in a banking environment. Several
workstreams were employed in order to bring about a cultural change within the
bank towards the use of spreadsheets and other end-user tools, covering policy
development, awareness and skills training, inventory monitoring, user
licensing, key risk metrics and mitigation approaches. The outcomes of these
activities are discussed, and conclusions are drawn as to the need for
appropriate organisational models to guide the use of these tools.
</summary>
    <author>
      <name>Jamie Chambers</name>
    </author>
    <author>
      <name>John Hamill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2008 153-161
  ISBN 978-905617-69-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.3595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.3595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3076v1</id>
    <updated>2008-10-17T07:19:39Z</updated>
    <published>2008-10-17T07:19:39Z</published>
    <title>Combining Semantic Wikis and Controlled Natural Language</title>
    <summary>  We demonstrate AceWiki that is a semantic wiki using the controlled natural
language Attempto Controlled English (ACE). The goal is to enable easy creation
and modification of ontologies through the web. Texts in ACE can automatically
be translated into first-order logic and other languages, for example OWL.
Previous evaluation showed that ordinary people are able to use AceWiki without
being instructed.
</summary>
    <author>
      <name>Tobias Kuhn</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Poster and Demonstration Session at the 7th
  International Semantic Web Conference (ISWC2008), CEUR Workshop Proceedings,
  Volume 401, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0810.3076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0874v1</id>
    <updated>2008-12-04T07:35:48Z</updated>
    <published>2008-12-04T07:35:48Z</published>
    <title>Stroke Fragmentation based on Geometry Features and HMM</title>
    <summary>  Stroke fragmentation is one of the key steps in pen-based interaction. In
this letter, we present a unified HMM-based stroke fragmentation technique that
can do segment point location and primitive type determination simultaneously.
The geometry features included are used to evaluate local features, and the HMM
model is utilized to measure the global drawing context. Experiments prove that
the model can efficiently represent smooth curves as well as strokes made up of
arbitrary lines and circular arcs.
</summary>
    <author>
      <name>Guihuan Feng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Viard-Gaudin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0812.0874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.0498v1</id>
    <updated>2009-01-05T15:11:32Z</updated>
    <published>2009-01-05T15:11:32Z</published>
    <title>Towards the characterization of individual users through Web analytics</title>
    <summary>  We perform an analysis of the way individual users navigate in the Web. We
focus primarily in the temporal patterns of they return to a given page. The
return probability as a function of time as well as the distribution of time
intervals between consecutive visits are measured and found to be independent
of the level of activity of single users. The results indicate a rich variety
of individual behaviors and seem to preclude the possibility of defining a
characteristic frequency for each user in his/her visits to a single site.
</summary>
    <author>
      <name>Bruno Goncalves</name>
    </author>
    <author>
      <name>Jose J. Ramasco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-02469-6_102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-02469-6_102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures. To appear in Proceeding of Complex'09</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Complex Sciences, 2247-2254 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.0498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.0498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4261v1</id>
    <updated>2009-03-25T08:43:19Z</updated>
    <published>2009-03-25T08:43:19Z</published>
    <title>On-Line Tests</title>
    <summary>  This paper presents an interactive implementation which makes the link
between a human operator and a system of a administration of a relational
databases MySQL. This application conceived as a multimedia presentations is
illustrative for the way in which the transfer and the remaking of the
information between the human operator, the module of data processing and the
database which stores the informations can be solved (with help of the PHP
language and the web use).
</summary>
    <author>
      <name>Florentina Anica Pintea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, exposed on 4th International Conference "Actualities and
  Perspectives on Hardware and Software" - APHS2007, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 77-84</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.4261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4864v1</id>
    <updated>2009-05-29T13:49:44Z</updated>
    <published>2009-05-29T13:49:44Z</published>
    <title>Computer Based Interpretation of the Students' Evaluation of the
  Teaching Staff</title>
    <summary>  The goal of this paper is to offer a full support for universities and
quality assessment committees in retrieving the feedback from their students
regarding to their teaching staff. The computer based application presented
before ([Cri07]) collects data from the students. Another part of the
application, presented in this paper, processes this data and presents the
statistical results concerning each teacher.
</summary>
    <author>
      <name>Tiberiu Marius Karnyanszky</name>
    </author>
    <author>
      <name>Ovidiu Crista</name>
    </author>
    <author>
      <name>Catalin Tuican</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, exposed on 5th International Conference "Actualities and
  Perspectives on Hardware and Software" - APHS2009, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),181-188</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.4864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.3224v1</id>
    <updated>2009-06-17T16:07:01Z</updated>
    <published>2009-06-17T16:07:01Z</published>
    <title>Personal applications, based on moveable / resizable elements</title>
    <summary>  All the modern day applications have the interface, absolutely defined by the
developers. The use of adaptive interface or dynamic layout allows some
variations, but even all of them are predetermined on the design stage, because
the best reaction (from designer's view) on any possible users' movement was
hardcoded. But there is a different world of applications, totally constructed
on moveable / resizable elements; such applications turn the full control to
the users. The crucial thing in such programs is that not something but
everything must become moveable and resizable. This article describes the
features of such applications and the algorithm behind their design.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.3224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.3224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4125v1</id>
    <updated>2009-06-22T20:31:45Z</updated>
    <published>2009-06-22T20:31:45Z</published>
    <title>A Refined Experience Sampling Method to Capture Mobile User Experience</title>
    <summary>  This paper reviews research methods used to understand the user experience of
mobile technology. The paper presents an improvement of the Experience Sampling
Method and case studies supporting its design. The paper concludes with an
agenda of future work for improving research in this field.
  Keywords: Research methods, topology, case study, contrasting graph,
Experience Sampling Method
</summary>
    <author>
      <name>Mauro Cherubini</name>
    </author>
    <author>
      <name>Nuria Oliver</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Cherubini, M., and Oliver, N. A refined experience sampling method to
  capture mobile user experience. In Presented at the International Workshop of
  Mobile User Experience Research part of CHI'2009 (Boston, MA, USA, April 4-9
  2009), Y. Nakhimovsky, D. Eckles, and J. Riegelsberger, Eds. 12 pages, 5
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.4125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1245v1</id>
    <updated>2009-07-07T15:59:02Z</updated>
    <published>2009-07-07T15:59:02Z</published>
    <title>How Controlled English can Improve Semantic Wikis</title>
    <summary>  The motivation of semantic wikis is to make acquisition, maintenance, and
mining of formal knowledge simpler, faster, and more flexible. However, most
existing semantic wikis have a very technical interface and are restricted to a
relatively low level of expressivity. In this paper, we explain how AceWiki
uses controlled English - concretely Attempto Controlled English (ACE) - to
provide a natural and intuitive interface while supporting a high degree of
expressivity. We introduce recent improvements of the AceWiki system and user
studies that indicate that AceWiki is usable and useful.
</summary>
    <author>
      <name>Tobias Kuhn</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Fourth Semantic Wiki Workshop (SemWiki
  2009), co-located with 6th European Semantic Web Conference (ESWC 2009), CEUR
  Workshop Proceedings, Volume 464, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.1245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1251v1</id>
    <updated>2009-07-07T16:15:21Z</updated>
    <published>2009-07-07T16:15:21Z</published>
    <title>How to Evaluate Controlled Natural Languages</title>
    <summary>  This paper presents a general framework how controlled natural languages can
be evaluated and compared on the basis of user experiments. The subjects are
asked to classify given statements (in the language to be tested) as either
true or false with respect to a certain situation that is shown in a graphical
notation called "ontographs". A first experiment has been conducted that
applies this framework to the language Attempto Controlled English (ACE).
</summary>
    <author>
      <name>Tobias Kuhn</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Pre-Proceedings of the Workshop on Controlled Natural Language
  (CNL 2009), CEUR Workshop Proceedings, Volume 448, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.1251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0928v1</id>
    <updated>2009-08-06T18:33:10Z</updated>
    <published>2009-08-06T18:33:10Z</published>
    <title>Automated Spreadsheet Development</title>
    <summary>  Few major commercial or economic decisions are made today which are not
underpinned by analysis using spreadsheets. It is virtually impossible to avoid
making mistakes during their drafting and some of these errors remain, unseen
and uncorrected, until something turns the spotlight on them. By then it may be
too late. The challenge is to find a way of creating spreadsheets which will
preserve the benefit of their power and flexibility while making their creation
more transparent and safer. Full documentation and documented version and
quality control, section by section, of the eventual spreadsheet would be a
bonus. And if the whole process could be made quicker, too, that would be a
further bonus.
</summary>
    <author>
      <name>Angus Dunn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 3 Colour Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2009 39-46
  ISBN 978-1-905617-89-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.0928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3361v1</id>
    <updated>2009-08-24T05:34:57Z</updated>
    <published>2009-08-24T05:34:57Z</published>
    <title>WebNC: efficient sharing of web applications</title>
    <summary>  WebNC is a system for efficiently sharing, retrieving and viewing web
applications. Unlike existing screencasting and screensharing tools, WebNC is
optimized to work with web pages where a lot of scrolling happens. WebNC uses a
tile-based encoding to capture, transmit and deliver web applications, and
relies only on dynamic HTML and JavaScript. The resulting webcasts require very
little bandwidth and are viewable on any modern web browser including Firefox
and Internet Explorer as well as browsers on the iPhone and Android platforms.
</summary>
    <author>
      <name>Laurent Denoue</name>
    </author>
    <author>
      <name>Scott Carter</name>
    </author>
    <author>
      <name>John Adcock</name>
    </author>
    <author>
      <name>Gene Golovchinsky</name>
    </author>
    <author>
      <name>Andreas Girgensohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at WWW 2009, Madrid, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3523v1</id>
    <updated>2009-08-25T01:37:59Z</updated>
    <published>2009-08-25T01:37:59Z</published>
    <title>Cognitive Dimensions Analysis of Interfaces for Information Seeking</title>
    <summary>  Cognitive Dimensions is a framework for analyzing human-computer interaction.
It is used for meta-analysis, that is, for talking about characteristics of
systems without getting bogged down in details of a particular implementation.
In this paper, I discuss some of the dimensions of this theory and how they can
be applied to analyze information seeking interfaces. The goal of this analysis
is to introduce a useful vocabulary that practitioners and researchers can use
to describe systems, and to guide interface design toward more usable and
useful systems
</summary>
    <author>
      <name>Gene Golovchinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to HCIR'09 http://cuaslis.org/hcir2009/</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2455v1</id>
    <updated>2009-09-13T23:51:59Z</updated>
    <published>2009-09-13T23:51:59Z</published>
    <title>End User Computing in AIB Capital Markets: A Management Summary</title>
    <summary>  This paper is a management summary of how the area of End User Computing
(EUC) has been addressed by AIB Capital Markets. The development of an
effective policy is described, as well as the process by which a register of
critical EUC applications was assembled and how those applications were brought
into a controlled environment. A number of findings are included as well as
recommendations for others who would seek to run a similar project.
</summary>
    <author>
      <name>Andrew McGeady</name>
    </author>
    <author>
      <name>Joseph McGouran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages. Referenced &amp; submitted by GJC in Sept 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2008 25-31
  ISBN 978-905617-69-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.2455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3158v1</id>
    <updated>2009-09-17T13:05:49Z</updated>
    <published>2009-09-17T13:05:49Z</published>
    <title>The quantitative side of the Repertory Grid Technique: some concerns</title>
    <summary>  User experience (UX) evaluation is gaining increased interest lately, both
from academia and industry. In this paper we argue that UX evaluation needs to
fulfill two important requirements: scalability, i.e. the ability to provide
useful feedback in different stages of the design, and diversity, i.e. the
ability to reflect the di-versity of opinions that may exist in different
users. We promote the use of the Repertory Grid Technique as a promising UX
evaluation technique and discuss some of our concerns regarding the
quantitative side of its use.
</summary>
    <author>
      <name>Evangelos Karapanos</name>
    </author>
    <author>
      <name>Jean-Bernard Martens</name>
    </author>
    <link href="http://arxiv.org/abs/0909.3158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0039v2</id>
    <updated>2009-12-09T05:35:43Z</updated>
    <published>2009-10-30T23:31:38Z</published>
    <title>Beyond the Drawing Board: Toward More Effective Use of Whiteboard
  Content</title>
    <summary>  We developed a system that augments traditional office whiteboards with
computation for the purposes of retrieving, reusing, and sharing whiteboard
content. Our system automatically captures changes to whiteboard images,
detects significant changes, and identifies potential collaborative activities.
Users then browse and search the collection of images captured from their
camera or shared from other users' cameras based on aspects such as location,
time, collaboration, etc. We report on the results of a formative study and on
an evaluation of effectiveness of our system, and discuss additional
functionality that can be built on our framework.
</summary>
    <author>
      <name>Gene Golovchinsky</name>
    </author>
    <author>
      <name>Scott Carter</name>
    </author>
    <author>
      <name>Jacob Biehl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Modified acknowledgments</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.0039v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0039v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.1647v1</id>
    <updated>2009-11-09T11:49:50Z</updated>
    <published>2009-11-09T11:49:50Z</published>
    <title>An extendible User-Command Framework based on tagging system</title>
    <summary>  Memorizing the user commands has been a problem since long. In this study we
try to propose solutions to overcome two problems - the problem of selecting
appropriate commands names during application development and the problem of
memorizing these command names. The proposed solution includes a framework in
which the applications can plug into, to get their application commands and
corresponding tags in to the new command execution application.We also propose
a mechanism where user can generate her own set of tags for a command and share
those with peers.
</summary>
    <author>
      <name>Ajinkya Kale</name>
    </author>
    <author>
      <name>Ananth Chakravarthy</name>
    </author>
    <author>
      <name>Nitin Jadhav</name>
    </author>
    <link href="http://arxiv.org/abs/0911.1647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.1647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5404v1</id>
    <updated>2009-11-28T16:06:49Z</updated>
    <published>2009-11-28T16:06:49Z</published>
    <title>Laser Actuated Presentation System</title>
    <summary>  We present here a pattern sensitive PowerPoint presentation scheme. The
presentation is actuated by simple patterns drawn on the presentation screen by
a laser pointer. A specific pattern corresponds to a particular command
required to operate the presentation. Laser spot on the screen is captured by a
RGB webcam with a red filter mounted, and its location is identified at the
blue layer of each captured frame by estimating the mean position of the pixels
whose intensity is above a given threshold value. Measured Reliability,
Accuracy and Latency of our system are 90%, 10 pixels (in the worst case) and
38 ms respectively.
</summary>
    <author>
      <name>Atul Chowdhary</name>
    </author>
    <author>
      <name>Vivek Agrawal</name>
    </author>
    <author>
      <name>Subhajit Karmakar</name>
    </author>
    <author>
      <name>Sandip Sarkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.5404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5652v1</id>
    <updated>2009-11-30T14:19:48Z</updated>
    <published>2009-11-30T14:19:48Z</published>
    <title>Modeling Human Interaction to Design a Human-Computer Dialog System</title>
    <summary>  This article presents the Cogni-CISMeF project, which aims at improving the
health information search engine CISMeF, by including a conversational agent
that interacts with the user in natural language. To study the cognitive
processes involved during information search, a bottom-up methodology was
adopted. An experiment has been set up to obtain human dialogs related to such
searches. The analysis of these dialogs underlines the establishment of a
common ground and accommodation effects to the user. A model of artificial
agent is proposed, that guides the user by proposing examples, assistance and
choices.
</summary>
    <author>
      <name>Alain Loisel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Chaignaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Philippe Kotowicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Entreprise Information System,
  ICEIS'08, Barcelone : Spain (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.5652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.4882v1</id>
    <updated>2009-12-24T15:29:12Z</updated>
    <published>2009-12-24T15:29:12Z</published>
    <title>Interagir avec un contenu opératique : le projet d'opéra virtuel
  interactif Virtualis</title>
    <summary>  In this article, we present the interactive opera project on CD-ROM
Virtualis. This project includes a scientific dimension as well as artistic. It
gave us the opportunity to design a model of the opera performance using
formalisms from organization sciences. Moreover, our investigation on
interactions between a user and opera contents led us to use models of
relationships between entities based on physical forces, where the user is in a
way absent. We detail some aspects of a reading but also writing environment on
artistic complex contents between text, music and graphics.
</summary>
    <author>
      <name>Alain Bonardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STMS</arxiv:affiliation>
    </author>
    <author>
      <name>Francis Rousseaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STMS, CRESTIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Revue d'Interaction Homme Machine 2, 1 (2001) /</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.4882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.4882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3150v1</id>
    <updated>2010-01-18T21:17:38Z</updated>
    <published>2010-01-18T21:17:38Z</published>
    <title>Gaze and Gestures in Telepresence: multimodality, embodiment, and roles
  of collaboration</title>
    <summary>  This paper proposes a controlled experiment to further investigate the
usefulness of gaze awareness and gesture recognition in the support of
collaborative work at a distance. We propose to redesign experiments conducted
several years ago with more recent technology that would: a) enable to better
study of the integration of communication modalities, b) allow users to freely
move while collaborating at a distance and c) avoid asymmetries of
communication between collaborators.
</summary>
    <author>
      <name>Mauro Cherubini</name>
    </author>
    <author>
      <name>Rodrigo de Oliveira</name>
    </author>
    <author>
      <name>Nuria Oliver</name>
    </author>
    <author>
      <name>Christian Ferran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position paper, International Workshop New Frontiers in Telepresence
  2010, part of CSCW2010, Savannah, GA, USA, 7th of February, 2010.
  http://research.microsoft.com/en-us/events/nft2010/</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.3150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3967v1</id>
    <updated>2010-01-22T11:35:57Z</updated>
    <published>2010-01-22T11:35:57Z</published>
    <title>Spreadsheet good practice: is there any such thing?</title>
    <summary>  Various techniques for developing spreadsheet models greatly improve the
chance that the end result will not contain basic mechanical errors. However,
for every discipline in which a given technique is useful, there is likely to
be another in which the same technique works badly. As a result, the author
urges that EuSpRIG does not succumb to internal or external pressures to
champion a particular set of "best practices", because no such set is optimal
in all spreadsheet applications.
</summary>
    <author>
      <name>David Colver</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 3 Colour Figures. Referenced &amp; Submitted by GJC in Jan 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 ISBN 1
  902724 94 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4924v1</id>
    <updated>2010-03-25T14:45:28Z</updated>
    <published>2010-03-25T14:45:28Z</published>
    <title>Common Frame of reference in collaborative virtual environments and
  their impact on presence</title>
    <summary>  Virtual collaborative environment are 3D shared spaces in which people can
work together. To collaborate through these systems, users must have a shared
comprehension of the environment. The objective of this experimental study was
to determine if visual stable landmarks improve the construction of a common
representation of the virtual environment and thus facilitate collaboration.
This seems to increase the awareness of the partner's presence.
</summary>
    <author>
      <name>Amine Chellali</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Cédric Dumas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Isabelle Milleville-Pennel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Nouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 10th Annual International Workshop on Presence, Barcelone :
  Spain (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.0243v1</id>
    <updated>2010-04-01T21:52:36Z</updated>
    <published>2010-04-01T21:52:36Z</published>
    <title>Psychophysiological Correlations with Gameplay Experience Dimensions</title>
    <summary>  In this paper, we report a case study using two easy-to-deploy
psychophysiological measures - electrodermal activity (EDA) and heart rate (HR)
- and correlating them with a gameplay experience questionnaire (GEQ) in an
attempt to establish this mixed-methods approach for rapid application in a
commercial game development context. Results indicate that there is a
statistically significant correlation (p &lt; 0.01) between measures of
psychophysiological arousal (HR, EDA) and self-reported UX in games (GEQ), with
some variation between the EDA and HR measures. Results are consistent across
three major commercial First-Person Shooter (FPS) games.
</summary>
    <author>
      <name>Anders Drachen</name>
    </author>
    <author>
      <name>Lennart E. Nacke</name>
    </author>
    <author>
      <name>Georgios Yannakakis</name>
    </author>
    <author>
      <name>Anja Lee Pedersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2010 Workshop: Brain, Body, and Bytes</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91E30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.8.0; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.0258v1</id>
    <updated>2010-04-01T23:48:23Z</updated>
    <published>2010-04-01T23:48:23Z</published>
    <title>Trends and Techniques in Visual Gaze Analysis</title>
    <summary>  Visualizing gaze data is an effective way for the quick interpretation of eye
tracking results. This paper presents a study investigation benefits and
limitations of visual gaze analysis among eye tracking professionals and
researchers. The results were used to create a tool for visual gaze analysis
within a Master's project.
</summary>
    <author>
      <name>Sophie Stellmach</name>
    </author>
    <author>
      <name>Lennart E. Nacke</name>
    </author>
    <author>
      <name>Raimund Dachselt</name>
    </author>
    <author>
      <name>Craig A. Lindley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pages 89-93, The 5th Conference on Communication by Gaze Interaction
  - COGAIN 2009: Gaze Interaction For Those Who Want It Most, ISBN:
  978-87-643-0475-6</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A66" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.0438v1</id>
    <updated>2010-04-03T11:21:26Z</updated>
    <published>2010-04-03T11:21:26Z</published>
    <title>User-driven applications</title>
    <summary>  User-driven applications are the programs, in which the full control is given
to the users. Designers of such programs are responsible only for developing an
instrument for solving some task, but they do not enforce users to work with
this instrument according with the predefined scenario. Users' control of the
applications means that only users decide at any moment WHAT, WHEN, and HOW
must appear on the screen. Such applications can be constructed only on the
basis of moveable / resizable elements. Programs, based on such elements, have
very interesting features and open absolutely new possibilities. This article
describes the design of the user-driven applications and shows the consequences
of switching to such type of programs on the samples from different areas.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3182v2</id>
    <updated>2010-06-07T07:43:43Z</updated>
    <published>2010-05-18T13:03:47Z</published>
    <title>Haptics in computer music : a paradigm shift</title>
    <summary>  With an historical point of view combined with a bibliographic overview, the
article discusses the idea that haptic force feedback transducers correspond
with a paradigm shift in our real-time tools for creating music. So doing, il
shows that computer music may be regarded as a major field of research and
application for haptics.
</summary>
    <author>
      <name>Nicolas Castagné</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <author>
      <name>Claude Cadoz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE, ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Loup Florens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <author>
      <name>Annie Luciani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE, ICA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Document accompagn\'e du poster</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EuroHaptics 2004, Munich : Germany (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.3182v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3182v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4028v1</id>
    <updated>2010-05-21T17:46:45Z</updated>
    <published>2010-05-21T17:46:45Z</published>
    <title>Internet Banking System Prototype</title>
    <summary>  Internet Banking System refers to systems that enable bank customers to
access accounts and general information on bank products and services through a
personal computer or other intelligent device. Internet banking products and
services can include detailed account information for corporate customers as
well as account summery and transfer money. Ultimately, the products and
services obtained through Internet Banking may mirror products and services
offered through other bank delivery channels. In this paper, Internet Banking
System Prototype has been proposed in order to illustrate the services which is
provided by the Bank online services.
</summary>
    <author>
      <name>Rami Alnaqeib</name>
    </author>
    <author>
      <name>Hamdan. O. Alanazi</name>
    </author>
    <author>
      <name>Hamid. A. Jalab</name>
    </author>
    <author>
      <name>M. A. Zaidan</name>
    </author>
    <author>
      <name>Ali K. Hmood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.journalofcomputing.org</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 5, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.4028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5918v1</id>
    <updated>2010-09-29T15:38:54Z</updated>
    <published>2010-09-29T15:38:54Z</published>
    <title>Usability testing: a review of some methodological and technical aspects
  of the method</title>
    <summary>  The aim of this paper is to review some work conducted in the field of user
testing that aims at specifying or clarifying the test procedures and at
defining and developing tools to help conduct user tests. The topics that have
been selected were considered relevant for evaluating applications in the field
of medical and health care informatics. These topics are: the number of
participants that should take part in a user test, the test procedure, remote
usability evaluation, usability testing tools, and evaluating mobile
applications.
</summary>
    <author>
      <name>J. M. Christian Bastien</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InterPsy-ETIC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ijmedinf.2008.12.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ijmedinf.2008.12.004" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Medical Informatics 79 (2010) e18-e23</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.5918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3325v1</id>
    <updated>2010-10-16T06:16:08Z</updated>
    <published>2010-10-16T06:16:08Z</published>
    <title>Wireless Sensor Network based Future of Telecom Applications</title>
    <summary>  A system and method for enabling human beings to communicate by way of their
monitored brain activity. The brain activity of an individual is monitored and
transmitted to a remote location (e.g. by satellite). At the remote location,
the monitored brain activity is compared with pre-recorded normalized brain
activity curves, waveforms, or patterns to determine if a match or substantial
match is found. If such a match is found, then the computer at the remote
location determines that the individual was attempting to communicate the word,
phrase, or thought corresponding to the matched stored normalized signal.
</summary>
    <author>
      <name>Arun Dua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.3325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.3325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2538v1</id>
    <updated>2010-11-11T00:28:34Z</updated>
    <published>2010-11-11T00:28:34Z</published>
    <title>mVideoCast: Mobile, real time ROI detection and streaming</title>
    <summary>  A variety of applications are emerging to support streaming video from mobile
devices. However, many tasks can benefit from streaming specific content rather
than the full video feed which may include irrelevant, private, or distracting
content. We describe a system that allows users to capture and stream targeted
video content captured with a mobile device. The application incorporates a
variety of automatic and interactive techniques to identify and segment desired
content in the camera view, allowing the user to publish a more focused video.
</summary>
    <author>
      <name>Scott Carter</name>
    </author>
    <author>
      <name>Laurent Denoue</name>
    </author>
    <author>
      <name>John Adcock</name>
    </author>
    <link href="http://arxiv.org/abs/1011.2538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.0467v1</id>
    <updated>2010-12-02T16:01:21Z</updated>
    <published>2010-12-02T16:01:21Z</published>
    <title>MT4j - A Cross-platform Multi-touch Development Framework</title>
    <summary>  This article describes requirements and challenges of crossplatform
multi-touch software engineering, and presents the open source framework
Multi-Touch for Java (MT4j) as a solution. MT4j is designed for rapid
development of graphically rich applications on a variety of contemporary
hardware, from common PCs and notebooks to large-scale ambient displays, as
well as different operating systems. The framework has a special focus on
making multi-touch software development easier and more efficient. Architecture
and abstractions used by MT4j are described, and implementations of several
common use cases are presented.
</summary>
    <author>
      <name>Uwe Laufs</name>
    </author>
    <author>
      <name>Christopher Ruff</name>
    </author>
    <author>
      <name>Jan Zibuschka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM EICS 2010, Workshop: Engineering patterns for multi-touch
  interfaces (2010), p. 52-57</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.0467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.0467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; D.2.11; D.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.4559v1</id>
    <updated>2010-12-21T07:39:45Z</updated>
    <published>2010-12-21T07:39:45Z</published>
    <title>A Force-Directed Method for Large Crossing Angle Graph Drawing</title>
    <summary>  Recent empirical research has indicated that human graph reading performance
improves when crossing angles increase. However, crossing angle has not been
used as an aesthetic criterion for graph drawing algorithms so far. In this
paper, we introduce a force-directed method that aims to construct graph
drawings with large crossing angles. Experiments indicate that our method
significantly increases crossing angles. Surprisingly, the experimental results
further demonstrate that the resulting drawings produced by our method have
fewer edge crossings, a shorter total edge length and more uniform edge
lengths, compared to classical spring algorithms.
</summary>
    <author>
      <name>Peter Eades</name>
    </author>
    <author>
      <name>Weidong Huang</name>
    </author>
    <author>
      <name>Seok-Hee Hong</name>
    </author>
    <link href="http://arxiv.org/abs/1012.4559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0234v1</id>
    <updated>2010-12-31T12:06:11Z</updated>
    <published>2010-12-31T12:06:11Z</published>
    <title>Dynamic Feature Description in Human Action Recognition</title>
    <summary>  This work aims to present novel description methods for human action
recognition. Generally, a video sequence can be represented as a collection of
spatial temporal words by detecting space-time interest points and describing
the unique features around the detected points (Bag of Words representation).
Interest points as well as the cuboids around them are considered informative
for feature description in terms of both the structural distribution of
interest points and the information content inside the cuboids. Our proposed
description approaches are based on this idea and making the feature
descriptors more discriminative.
</summary>
    <author>
      <name>Ruoyun Gao</name>
    </author>
    <author>
      <name>Michael S. Lew</name>
    </author>
    <author>
      <name>Ling Shao</name>
    </author>
    <link href="http://arxiv.org/abs/1101.0234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0405v1</id>
    <updated>2011-03-02T12:10:26Z</updated>
    <published>2011-03-02T12:10:26Z</published>
    <title>Analysis of the User Acceptance for Implementing ISO/IEC 27001:2005 in
  Turkish Public Organizations</title>
    <summary>  This study aims to develop a model for the user acceptance for implementing
the information security standard (i.e. ISO 27001) in Turkish public
organizations. The results of the surveys performed in Turkey reveal that the
legislation on information security public which organizations have to obey is
significantly related with the user acceptance during ISO 27001 implementation
process. The fundamental components of our user acceptance model are perceived
usefulness, attitude towards use, social norms, and performance expectancy.
</summary>
    <author>
      <name>Tolga Mataracioglu</name>
    </author>
    <author>
      <name>Sevgi Ozkan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.0405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0868v1</id>
    <updated>2011-06-05T01:11:08Z</updated>
    <published>2011-06-05T01:11:08Z</published>
    <title>Python GUI Scripting Interface for Running Atomic Physics Applications</title>
    <summary>  We create a Python GUI scripting interface working under Windows in addition
to (UNIX/Linux). The GUI has been built around the Python open-source
programming language. We use the Python's GUI library that so called Python
Mega Widgets (PMW) and based on Tkinter Python module
(http://www.freenetpages.co.uk/hp/alan.gauld/tutgui.htm). The new GUI was
motivated primarily by the desire of more updated operations, more flexibility
incorporating future and current improvements in producing atomic data.
Furthermore it will be useful for a variety of applications of atomic physics,
plasma physics and astrophysics and will help in calculating various atomic
properties.
</summary>
    <author>
      <name>Amani Tahat</name>
    </author>
    <author>
      <name>Mofleh Tahat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Python Papers Source Codes 3: 2, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.0868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.5308v1</id>
    <updated>2011-06-27T06:24:34Z</updated>
    <published>2011-06-27T06:24:34Z</published>
    <title>Clasificarea distribuita a mesajelor de e-mail</title>
    <summary>  A basic component in Internet applications is the electronic mail and its
various implications. The paper proposes a mechanism for automatically
classifying emails and create dynamic groups that belong to these messages.
Proposed mechanisms will be based on natural language processing techniques and
will be designed to facilitate human-machine interaction in this direction.
</summary>
    <author>
      <name>Florin Pop</name>
    </author>
    <author>
      <name>Diana Petrescu</name>
    </author>
    <author>
      <name>Ştefan Trauşan-Matu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISSN 1453-1305</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A Treia Conferin\c{t}\u{a} Na\c{t}ional\u{a} de Interac\c{t}iune
  Om-Calculator 2006, Informatica Economica, vol. X, Bucuresti, pp. 79-82, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.5308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.5308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5323v1</id>
    <updated>2011-09-25T04:41:35Z</updated>
    <published>2011-09-25T04:41:35Z</published>
    <title>Squiggle - A Glyph Recognizer for Gesture Input</title>
    <summary>  Squiggle is a template-based glyph recognizer in the lineage of `$1
Recognizer' and `Protractor'. It seeks a good fit linear affine mapping between
the input and template glyphs which are represented as a list of milestone
points along the glyph path. The algorithm can recognize input glyphs invariant
of rotation, scaling, skew, and reflection symmetries. In practice the
algorithm is fast and robust enough to recognize user-generated glyphs as they
are being drawn in real time, and to project `shadows' of the matching
templates as feedback.
</summary>
    <author>
      <name>Jeremy Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.4.7; I.5.5; G.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6288v1</id>
    <updated>2011-09-28T18:35:51Z</updated>
    <published>2011-09-28T18:35:51Z</published>
    <title>Using Stereoscopic 3D Technologies for the Diagnosis and Treatment of
  Amblyopia in Children</title>
    <summary>  The 3D4Amb project aims at developing a system based on the stereoscopic 3D
techonlogy, like the NVIDIA 3D Vision, for the diagnosis and treatment of
amblyopia in young children. It exploits the active shutter technology to
provide binocular vision, i.e. to show different images to the amblyotic (or
lazy) and the normal eye. It would allow easy diagnosis of amblyopia and its
treatment by means of interactive games or other entertainment activities. It
should not suffer from the compliance problems of the classical treatment, it
is suitable to domestic use, and it could at least partially substitute
occlusion or patching of the normal eye.
</summary>
    <author>
      <name>Angelo Gargantini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the HEALTHINF 2011 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.6288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.3172v2</id>
    <updated>2012-02-02T23:51:49Z</updated>
    <published>2012-01-16T07:57:34Z</published>
    <title>Assessing the Value of 3D Reconstruction in Building Construction</title>
    <summary>  3-dimensional (3D) reconstruction is an emerging field in image processing
and computer vision that aims to create 3D visualizations/ models of objects/
scenes from image sets. However, its commercial applications and benefits are
yet to be fully explored. In this paper, we describe ongoing work towards
assessing the value of 3D reconstruction in the building construction domain.
We present preliminary results from a user study, where our objective is to
understand the use of visual information in building construction in order to
determine problems with the use of visual information and identify potential
benefits and scenarios for the use of 3D reconstruction.
</summary>
    <author>
      <name>Uma Murthy</name>
    </author>
    <author>
      <name>David Boardman</name>
    </author>
    <author>
      <name>Chirag Garg</name>
    </author>
    <link href="http://arxiv.org/abs/1201.3172v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3172v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.6251v1</id>
    <updated>2012-01-27T18:30:11Z</updated>
    <published>2012-01-27T18:30:11Z</published>
    <title>Real-time jam-session support system</title>
    <summary>  We propose a method for the problem of real time chord accompaniment of
improvised music. Our implementation can learn an underlying structure of the
musical performance and predict next chord. The system uses Hidden Markov Model
to find the most probable chord sequence for the played melody and then a
Variable Order Markov Model is used to a) learn the structure (if any) and b)
predict next chord. We implemented our system in Java and MAX/Msp and compared
and evaluated using objective (prediction accuracy) and subjective
(questionnaire) evaluation methods.
</summary>
    <author>
      <name>Panagiotis Tigas</name>
    </author>
    <link href="http://arxiv.org/abs/1201.6251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3926v1</id>
    <updated>2012-02-17T15:03:54Z</updated>
    <published>2012-02-17T15:03:54Z</published>
    <title>Exploring Geometric Shapes with Touch</title>
    <summary>  We propose a new technique to help users to explore geometric shapes without
vision. This technique is based on a guidance using directional cues with a pin
array. This is an alternative to the usual technique that consists of raising
the pins corresponding to dark pixels around the cursor. In this paper we
compare the exploration of geometric shapes with our new technique in unimanual
and bimanual conditions. The users made fewer errors in unimanual condition
than in bimanual condition. However they did not explore the shapes more
quickly and there was no difference in confidence in their answer.
</summary>
    <author>
      <name>Thomas Pietrzak</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Crossan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIST</arxiv:affiliation>
    </author>
    <author>
      <name>Stephen A. Brewster</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIST</arxiv:affiliation>
    </author>
    <author>
      <name>Benoît Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITA</arxiv:affiliation>
    </author>
    <author>
      <name>Isabelle Pecci</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-03655-2_18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-03655-2_18" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Interact 2009 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.3926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.6104v1</id>
    <updated>2012-02-28T02:04:04Z</updated>
    <published>2012-02-28T02:04:04Z</published>
    <title>On the Convergence of Collaboration and Knowledge Management</title>
    <summary>  Collaboration technology typically focuses on collaboration and group
processes (cooperation, communication, coordination and coproduction).
Knowledge Management (KM) technology typically focuses on content (creation,
storage, sharing and use of data, information and knowledge). Yet, to achieve
their common goals, teams and organizations need both KM and collaboration
technology to make that more effective and efficient. This paper is interested
in knowledge management and collaboration regarding their convergence and their
integration. First, it contributes to a better understanding of the knowledge
management and collaboration concepts. Second, it focuses on KM and
collaboration convergence by presenting the different interpretation of this
convergence. Third, this paper proposes a generic framework of collaborative
knowledge management.
</summary>
    <author>
      <name>Nesrine Ben yahia</name>
    </author>
    <author>
      <name>Narjès Bellamine</name>
    </author>
    <author>
      <name>Henda Ben Ghézala</name>
    </author>
    <link href="http://arxiv.org/abs/1202.6104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.6104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4332v1</id>
    <updated>2012-04-19T12:10:10Z</updated>
    <published>2012-04-19T12:10:10Z</published>
    <title>Designing generalisation evaluation function through human-machine
  dialogue</title>
    <summary>  Automated generalisation has known important improvements these last few
years. However, an issue that still deserves more study concerns the automatic
evaluation of generalised data. Indeed, many automated generalisation systems
require the utilisation of an evaluation function to automatically assess
generalisation outcomes. In this paper, we propose a new approach dedicated to
the design of such a function. This approach allows an imperfectly defined
evaluation function to be revised through a man-machine dialogue. The user
gives its preferences to the system by comparing generalisation outcomes.
Machine Learning techniques are then used to improve the evaluation function.
An experiment carried out on buildings shows that our approach significantly
improves generalisation evaluation functions defined by users.
</summary>
    <author>
      <name>Patrick Taillandier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UMMISCO</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Gaffuri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">COGIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GIScience, Zurich : Switzerland (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.4332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6325v2</id>
    <updated>2012-06-03T08:45:46Z</updated>
    <published>2012-04-27T20:10:16Z</published>
    <title>CELL: Connecting Everyday Life in an archipeLago</title>
    <summary>  We explore the design of a seamless broadcast communication system that
brings together the distributed community of remote secondary education
schools. In contrast to higher education, primary and secondary education
establishments should remain distributed, in order to maintain a balance of
urban and rural life in the developing and the developed world. We plan to
deploy an ambient and social interactive TV platform (physical installation,
authoring tools, interactive content) that supports social communication in a
positive way. In particular, we present the physical design and the conceptual
model of the system.
</summary>
    <author>
      <name>Konstantinos Chorianopoulos</name>
    </author>
    <author>
      <name>Vassiliki Tsaknaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some errors</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.6325v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6325v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.0751v1</id>
    <updated>2012-05-03T16:09:07Z</updated>
    <published>2012-05-03T16:09:07Z</published>
    <title>Integrated Development Environment Gesture for modeling workflow
  diagrams</title>
    <summary>  The current software development tools show the same form of interaction as
when they started back, in the mid 70's. However, since the appearance of
visual languages and due to their own nature, they can be handled by tools
which have different input methods to conventional ones. By incorporating new
motion detection technology, it is intended that new forms of interaction are
established. Interactions which respond to the free movement of hands,
therefore the software's developer will have a substantial improvement in the
user experience.
</summary>
    <author>
      <name>Carlos Alberto Fernandez-y-Fernandez</name>
    </author>
    <author>
      <name>Jose Angel Quintanar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ISBN: 978-607-607-082-6</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congreso Internacional de Investigacion e Innovacion en Ingenieria
  de Software (Conisoft 2012), Guadalajara, Jalisco, April 25-27, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.0751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2476v1</id>
    <updated>2012-05-11T10:46:50Z</updated>
    <published>2012-05-11T10:46:50Z</published>
    <title>Open Data Visualization: Keeping Traces of the Exploration Process</title>
    <summary>  This paper describes a system to support the visual exploration of Open Data.
During his/her interactive experience with the graphics, the user can easily
store the current complete state of the visualization application (called a
viewpoint). Next, he/she can compose sequences of these viewpoints (called
scenarios) that can easily be reloaded. This feature allows to keep traces of a
former exploration process, which can be useful in single user (to support
investigation carried out in multiple sessions) as well as in collaborative
setting (to share points of interest identified in the data set).
</summary>
    <author>
      <name>Benoît Otjacques</name>
    </author>
    <author>
      <name>Mickaël Stefas</name>
    </author>
    <author>
      <name>Maël Cornil</name>
    </author>
    <author>
      <name>Fernand Feltz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.2476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3205v1</id>
    <updated>2012-05-14T21:47:06Z</updated>
    <published>2012-05-14T21:47:06Z</published>
    <title>Cumulative Revision Map</title>
    <summary>  Unlike static documents, version-controlled documents are edited by one or
more authors over a certain period of time. Examples include large scale
computer code, papers authored by a team of scientists, and online discussion
boards. Such collaborative revision process makes traditional document modeling
and visualization techniques inappropriate. In this paper we propose a new
visualization technique for version-controlled documents that reveals
interesting authoring patterns in papers, computer code and Wikipedia articles.
The revealed authoring patterns are useful for the readers, participants in the
authoring process, and supervisors.
</summary>
    <author>
      <name>Seungyeon Kim</name>
    </author>
    <author>
      <name>Joshua V. Dillon</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1968v4</id>
    <updated>2012-10-02T07:54:53Z</updated>
    <published>2012-06-09T20:04:38Z</published>
    <title>A novel 2.5D approach for interfacing with web applications</title>
    <summary>  Web applications need better user interface to be interactive and attractive.
A new approach/concept of dimensional enhancement - 2.5D "a 2D display of a
virtual 3D environment", which can be implemented in social networking sites
and further in other system applications.
</summary>
    <author>
      <name>Saurabh Sarkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An approach offering a new idea for Human Computer Interaction,
  including 3 pages and 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1968v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1968v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4206v1</id>
    <updated>2012-06-19T13:36:59Z</updated>
    <published>2012-06-19T13:36:59Z</published>
    <title>Correlating Pedestrian Flows and Search Engine Queries</title>
    <summary>  An important challenge for ubiquitous computing is the development of
techniques that can characterize a location vis-a-vis the richness and
diversity of urban settings. In this paper we report our work on correlating
urban pedestrian flows with Google search queries. Using longitudinal data we
show pedestrian flows at particular locations can be correlated with the
frequency of Google search terms that are semantically relevant to those
locations. Our approach can identify relevant content, media, and
advertisements for particular locations.
</summary>
    <author>
      <name>Vassilis Kostakos</name>
    </author>
    <author>
      <name>Simo Hosio</name>
    </author>
    <author>
      <name>Jorge Goncalves</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0063980</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0063980" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS ONE 8(5): e63980, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.4206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1821v1</id>
    <updated>2012-07-07T18:58:39Z</updated>
    <published>2012-07-07T18:58:39Z</published>
    <title>Beyond Experience Sampling: Evaluating Personal Informatics with
  Technology-Assisted Reconstruction</title>
    <summary>  Experience Sampling has been considered the golden standard of in-situ
measurement, yet, at the expense of high burden to participants. In this paper
we propose Technology-Assisted Reconstruction (TAR), a methodological approach
that combines passive logging of users' behaviors with use of these data in
assisting the reconstruction of behaviors and experiences. Through a number of
recent and ongoing projects we will discuss how TAR may be employed for the
evaluation of personal informatics systems, but also, conversely, how ideas
from the field of personal informatics may contribute towards the development
of new methodologies for in-situ evaluation.
</summary>
    <author>
      <name>Evangelos Karapanos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In adjunct proceedings of the conference on Human factors in
  computing systems (CHI 2012), Workshop on Personal Informatics in Practice:
  Improving Quality of Life Through Data. Austin, Canada</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.1821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3155v1</id>
    <updated>2012-09-14T11:27:30Z</updated>
    <published>2012-09-14T11:27:30Z</published>
    <title>Augmenting Customer Journey Maps with quantitative empirical data: a
  case on EEG and eye tracking</title>
    <summary>  This paper introduces the use of electroencephalography (EEG) and eye
tracking in exploring customer experiences in service design. These tools are
expected to allow designers to generate customer journeys from empirical data
leading to new visualization methods and therefore improvements in service
design deliverables.
</summary>
    <author>
      <name>Rui Alves</name>
    </author>
    <author>
      <name>Veranika Lim</name>
    </author>
    <author>
      <name>Evangelos Niforatos</name>
    </author>
    <author>
      <name>Monchu Chen</name>
    </author>
    <author>
      <name>Evangelos Karapanos</name>
    </author>
    <author>
      <name>Nuno Jardim Nunes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 3 figures. Published in DIS2012, in Newcastle, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4982v1</id>
    <updated>2012-09-22T10:36:11Z</updated>
    <published>2012-09-22T10:36:11Z</published>
    <title>Using multimodal speech production data to evaluate articulatory
  animation for audiovisual speech synthesis</title>
    <summary>  The importance of modeling speech articulation for high-quality audiovisual
(AV) speech synthesis is widely acknowledged. Nevertheless, while
state-of-the-art, data-driven approaches to facial animation can make use of
sophisticated motion capture techniques, the animation of the intraoral
articulators (viz. the tongue, jaw, and velum) typically makes use of simple
rules or viseme morphing, in stark contrast to the otherwise high quality of
facial modeling. Using appropriate speech production data could significantly
improve the quality of articulatory animation for AV synthesis.
</summary>
    <author>
      <name>Ingmar Steiner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA, Trinity College Dublin</arxiv:affiliation>
    </author>
    <author>
      <name>Korin Richmond</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSTR</arxiv:affiliation>
    </author>
    <author>
      <name>Slim Ouni</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Symposium on Facial Analysis and Animation
  (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.4982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1634v2</id>
    <updated>2012-11-12T23:55:25Z</updated>
    <published>2012-11-07T18:40:58Z</published>
    <title>Annotations for Supporting Collaboration through Artifacts</title>
    <summary>  Shared artifacts and environments play a prominent role in shaping the
collaboration between their users. This article describes this role and
explains how annotations can provide a bridge between direct communication and
collaboration through artifacts. The various functions of annotations are
discussed through examples that represent some of the important trends in
annotation research. Ultimately, some of the research issues are briefly
discussed, followed by my perspective on the future of asynchronous distributed
collaborative systems with respect to annotations.
</summary>
    <author>
      <name>Syavash Nobarany</name>
    </author>
    <link href="http://arxiv.org/abs/1211.1634v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1634v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2412v1</id>
    <updated>2012-11-11T12:36:36Z</updated>
    <published>2012-11-11T12:36:36Z</published>
    <title>Work Integrated Learning (WIL) In Virtual Reality (VR)</title>
    <summary>  The focus of this report is to initially discuss the concepts WIL and VR,
their main characteristics and current applications. Moreover, the pros and
cons of VWIL are also analyzed. Finally, the report presents some
recommendation including further researches into areas where VWIL has potential
to be successful in the future.
</summary>
    <author>
      <name>Waleed Abdullah Al Shehri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 2, September 2012 ISSN (Online): 1694-0814 www.IJCSI.org</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.2412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6411v1</id>
    <updated>2012-11-24T01:21:25Z</updated>
    <published>2012-11-24T01:21:25Z</published>
    <title>New Heuristics for Interfacing Human Motor System using Brain Waves</title>
    <summary>  There are many new forms of interfacing human users to machines. We persevere
here electric mechanical form of interaction between human and machine. The
emergence of brain-computer interface allows mind-to-movement systems. The
story of the Pied Piper inspired us to devise some new heuristics for
interfacing human motor system using brain waves by combining head helmet and
LumbarMotionMonitor For the simulation we use java GridGain Brain responses of
classified subjects during training indicates that Probe can be the best
stimulus to rely on in distinguishing between knowledgeable and not
knowledgeable
</summary>
    <author>
      <name>Mohammed El-Dosuky</name>
    </author>
    <author>
      <name>Ahmed EL-Bassiouny</name>
    </author>
    <author>
      <name>Taher Hamza</name>
    </author>
    <author>
      <name>Magdy Rashad</name>
    </author>
    <link href="http://arxiv.org/abs/1211.6411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6799v1</id>
    <updated>2012-11-29T02:34:28Z</updated>
    <published>2012-11-29T02:34:28Z</published>
    <title>Context Visualization for Social Bookmark Management</title>
    <summary>  We present the design of a new social bookmark manager, named GalViz, as part
of the interface of the GiveA-Link system. Unlike the interfaces of traditional
social tagging tools, which usually display information in a list view, GalViz
visualizes tags, resources, social links, and social context in an interactive
network, combined with the tag cloud. Evaluations through a scenario case study
and log analysis provide evidence of the effectiveness of our design.
</summary>
    <author>
      <name>Lilian Weng</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.6799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0573v1</id>
    <updated>2012-12-12T15:56:34Z</updated>
    <published>2012-12-12T15:56:34Z</published>
    <title>Coordinates: Probabilistic Forecasting of Presence and Availability</title>
    <summary>  We present methods employed in Coordinate, a prototype service that supports
collaboration and communication by learning predictive models that provide
forecasts of users s AND availability.We describe how data IS collected about
USER activity AND proximity FROM multiple devices, IN addition TO analysis OF
the content OF users, the time of day, and day of week. We review applications
of presence forecasting embedded in the Priorities application and then present
details of the Coordinate service that was informed by the earlier efforts.
</summary>
    <author>
      <name>Eric J. Horvitz</name>
    </author>
    <author>
      <name>Paul Koch</name>
    </author>
    <author>
      <name>Carl Kadie</name>
    </author>
    <author>
      <name>Andy Jacobs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1649v1</id>
    <updated>2013-02-07T06:47:54Z</updated>
    <published>2013-02-07T06:47:54Z</published>
    <title>Eye-GUIDE (Eye-Gaze User Interface Design) Messaging for
  Physically-Impaired People</title>
    <summary>  Eye-GUIDE is an assistive communication tool designed for the paralyzed or
physically impaired people who were unable to move parts of their bodies
especially people whose communications are limited only to eye movements. The
prototype consists of a camera and a computer. Camera captures images then it
will be send to the computer, where the computer will be the one to interpret
the data. Thus, Eye-GUIDE focuses on camera-based gaze tracking. The proponent
designed the prototype to perform simple tasks and provides graphical user
interface in order the paralyzed or physically impaired person can easily use
it.
</summary>
    <author>
      <name>Rommel Anacan</name>
    </author>
    <author>
      <name>James Greggory Alcayde</name>
    </author>
    <author>
      <name>Retchel Antegra</name>
    </author>
    <author>
      <name>Leah Luna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdps.2013.4104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdps.2013.4104" rel="related"/>
    <link href="http://arxiv.org/abs/1302.1649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.2759v1</id>
    <updated>2013-02-12T11:17:20Z</updated>
    <published>2013-02-12T11:17:20Z</published>
    <title>Exploration of Recent Advances in the Field of Brain Computer Interfaces</title>
    <summary>  A new approach for implementing number of expressions, emotions and, actions
to operate objects through the thoughts of brain using a Non-Invasive Brain
Computing Interface (BCI) technique has been proposed. In this paper a survey
on brain and its operations are presented. The steps involved in the brain
signal processing are discussed. The current systems are able to present few
expressions and emotions on a single device. The proposed system provides the
extended number of expressions on multiple numbers of objects.
</summary>
    <author>
      <name>M. Rajyalakshmi</name>
    </author>
    <author>
      <name>T. Kameswara Rao</name>
    </author>
    <author>
      <name>T. V. Prasad</name>
    </author>
    <link href="http://arxiv.org/abs/1302.2759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.2759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1332v1</id>
    <updated>2013-04-04T11:43:09Z</updated>
    <published>2013-04-04T11:43:09Z</published>
    <title>What really happened on September 15th 2008? Getting The Most from Your
  Personal Information with Memacs</title>
    <summary>  Combining and summarizing meta-data from various kinds of data sources is one
possible solution to the data fragmentation we are suffering from. Multiple
projects have addressed this issue already. This paper presents a new approach
named Memacs. It automatically generates a detailed linked diary of our digital
artifacts scattered across local files of multiple formats as well as data
silos of the internet. Being elegantly simple and open, Memacs uses already
existing visualization features of GNU Emacs and Org-mode to provide a
promising platform for life-logging, Quantified Self movement, and people
looking for advanced Personal Information Management (PIM) in general.
</summary>
    <author>
      <name>Karl Voit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 21 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.1332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.2; H.4.1; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3940v2</id>
    <updated>2013-05-09T10:48:55Z</updated>
    <published>2013-04-14T19:48:07Z</published>
    <title>Unveiling the link between logical fallacies and web persuasion</title>
    <summary>  In the last decade Human-Computer Interaction (HCI) has started to focus
attention on forms of persuasive interaction where computer technologies have
the goal of changing users behavior and attitudes according to a predefined
direction. In this work, we hypothesize a strong connection between logical
fallacies (forms of reasoning which are logically invalid but cognitively
effective) and some common persuasion strategies adopted within web
technologies. With the aim of empirically evaluating our hypothesis, we carried
out a pilot study on a sample of 150 e-commerce websites.
</summary>
    <author>
      <name>Antonio Lieto</name>
    </author>
    <author>
      <name>Fabiana Vernero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, in proceedings of the WebSci'13 Conference,
  Paris, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3940v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3940v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1943v1</id>
    <updated>2013-07-08T04:41:39Z</updated>
    <published>2013-07-08T04:41:39Z</published>
    <title>Proof in Context -- Web Editing with Rich, Modeless Contextual Feedback</title>
    <summary>  The Agora system is a prototypical Wiki for formal mathematics: a web-based
system for collaborating on formal mathematics, intended to support informal
documentation of formal developments. This system requires a reusable proof
editor component, both for collaborative editing of documents, and for
embedding in the resulting documents. This paper describes the design of
Agora's asynchronous editor, that is generic enough to support different tools
working on editor content and providing contextual information, with
interactive theorem proverss being a special, but important, case described in
detail for the Coq theorem prover.
</summary>
    <author>
      <name>Carst Tankink</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for Computing and Information Science, Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.118.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.118.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings UITP 2012, arXiv:1307.1528</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 118, 2013, pp. 42-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.1943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.2018v1</id>
    <updated>2013-07-08T10:16:00Z</updated>
    <published>2013-07-08T10:16:00Z</published>
    <title>OntoFM: A Personal Ontology-based File Manager for the Desktop</title>
    <summary>  Personal ontologies have been proposed as a means to support the semantic
management of user information. Assuming that a personal ontology system is in
use, new tools have to be developed at user interface level to exploit the
enhanced capabilities offered by the system. In this work, we present an
ontology-based file manager that allows semantic searching on the user's
personal information space. The file manager exploits the ontology relations to
present files associated with specific concepts, proposes new related concepts
to users, and helps them explore the information space and locate the required
file.
</summary>
    <author>
      <name>Jenny Rompa</name>
    </author>
    <author>
      <name>Giorgos Lepouras</name>
    </author>
    <author>
      <name>Costas Vassilakis</name>
    </author>
    <author>
      <name>Christos Tryfonopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISWC 2011 Demo</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.2018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0322v1</id>
    <updated>2013-08-01T19:58:28Z</updated>
    <published>2013-08-01T19:58:28Z</published>
    <title>Social Data Mining through Distributed Mobile Sensing</title>
    <summary>  In this article, we present a distributed framework for collecting and
analyzing environmental and location data recorded by human users (carriers)
with the use of portable sensors. We demonstrate the data mining analysis
potential among the recorded environmental and location variables, as well as
the potential for classification analysis of human activities. We recognize
that the success of such an experimental framework relies on the adoption rate
by its candidate user network; thus, we have built our experimental prototype
on top of hardware equipment already embedded within the potential users'
everyday routine - i.e. hardware sensors installed on modern mobile phones.
Finally, we present preliminary analysis results on our collected data sample,
as well as potential further work directions and proposed use case scenarios.
</summary>
    <author>
      <name>John Gekas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3745v1</id>
    <updated>2013-08-17T00:33:54Z</updated>
    <published>2013-08-17T00:33:54Z</published>
    <title>Computational Properties of Fiction Writing and Collaborative Work</title>
    <summary>  From the earliest days of computing, there have been tools to help shape
narrative. Spell-checking, word counts, and readability analysis, give today's
novelists tools that Dickens, Austen, and Shakespeare could only have dreamt
of. However, such tools have focused on the word, or phrase levels. In the last
decade, research focus has shifted to support for collaborative editing of
documents. This work considers more sophisticated attempts to visualise the
semantics, pace and rhythm within a narrative through data mining. We describe
real life applications in two related domains.
</summary>
    <author>
      <name>Joseph Reddington</name>
    </author>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <author>
      <name>Douglas Cowie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91C20, 62H30, 76M27" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5; H.1.2; H.3.3; I.5.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.6368v1</id>
    <updated>2013-08-29T05:40:45Z</updated>
    <published>2013-08-29T05:40:45Z</published>
    <title>Incremental Grid-like Layout Using Soft and Hard Constraints</title>
    <summary>  We explore various techniques to incorporate grid-like layout conventions
into a force-directed, constraint-based graph layout framework. In doing so we
are able to provide high-quality layout---with predominantly axis-aligned
edges---that is more flexible than previous grid-like layout methods and which
can capture layout conventions in notations such as SBGN (Systems Biology
Graphical Notation). Furthermore, the layout is easily able to respect
user-defined constraints and adapt to interaction in online systems and diagram
editors such as Dunnart.
</summary>
    <author>
      <name>Steve Kieffer</name>
    </author>
    <author>
      <name>Tim Dwyer</name>
    </author>
    <author>
      <name>Kim Marriott</name>
    </author>
    <author>
      <name>Michael Wybrow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Graph Drawing 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.6368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.6368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1758v1</id>
    <updated>2013-10-07T12:45:48Z</updated>
    <published>2013-10-07T12:45:48Z</published>
    <title>GENIUS: Generating Usable User Interfaces</title>
    <summary>  In this report we describe the implementation and approach developed during
the GENIUS Project. The GENIUS project is about the generation of usable user
interfaces. It tries to cope with issues related to automatic generation where,
usually end-user complain about the poor quality (in term of usability) of
generated UI. To solve this issue GENIUS relies on Model-Driven Engineering
principles and several MDE tools. Notably, it consists in a set of metamodels
specific to the interaction, a set of model transformation embedding usability
criteria and an environment for model execution/ interpretation.
</summary>
    <author>
      <name>Jean-Sebastien Sottet</name>
    </author>
    <author>
      <name>Alain Vagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Public Research Center Henri Tudor- Technical Report of the FNR's
  project GENIUS</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3111v1</id>
    <updated>2013-10-11T12:56:08Z</updated>
    <published>2013-10-11T12:56:08Z</published>
    <title>Keyboard for inputting Chinese language</title>
    <summary>  As the structure of Chinese characters are very different, it is very
difficult to input Chinese characters into computer quickly and conveniently.
The conventional keyboard does not support the pictorial characters in Chinese
language. There are 3000 to 6000 commonly used pictorial Chinese characters
(Hanzi).
  There are a few existing systems which include "PinYin" (phonetic) system, a
combination of the PinYin system and character form techniques, whole character
encoding, stroke input encoding, and stoke form encoding. Each of the methods
have their own advantages and disadvantages. This article describes two
inventions on inputting Chinese language through a standard keyboard.
</summary>
    <author>
      <name>Umakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2139/ssrn.932271</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2139/ssrn.932271" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in TRIZsite Journal, Apr 2005, also available in
  http://papers.ssrn.com/abstract=932271</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3165v1</id>
    <updated>2013-10-11T15:21:05Z</updated>
    <published>2013-10-11T15:21:05Z</published>
    <title>10 Inventions on Key Guides and Keyboard Templates</title>
    <summary>  A keyboard has many function keys and each function key can have multiple
functions when used with control, shift and alt keys, it is difficult for a
user to remember the functionality of the function keys. We need a mechanism to
indicate the operations assigned to each function key for different software
programs. A keyboard guide or template is used for this purpose.
  This article illustrates 10 inventions on keyboard key guide and function key
templates selected from US patent database. Various mechanisms of keyboard
templates have been proposed, including static, dynamic, manual, mechanical,
onscreen display and others.
</summary>
    <author>
      <name>Umakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2139/ssrn.932276</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2139/ssrn.932276" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in TRIZsite Journal, June 2005, also available at
  http://papers.ssrn.com/abstract=932276</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3849v1</id>
    <updated>2013-10-11T14:49:38Z</updated>
    <published>2013-10-11T14:49:38Z</published>
    <title>10 Inventions on laptop keyboards -A study based on US patents</title>
    <summary>  A desktop keyboard has several sections like character key section,
navigation key section, numeric key section, and function key section etc. each
consisting of several number of keys. However, a laptop computer does not have
so much of space to accommodate all these keys into the keyboard. There are
several considerations while designing a laptop keyboard.
  This article illustrates 10 inventions on keyboards for laptop and portable
computers. The inventions are selected from US patent database. The inventions
try to improve various aspects of a laptop keyboard, such as reducing size,
folding and concealing, ergonomic features, improving quality and reducing
cost.
</summary>
    <author>
      <name>Umakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2139/ssrn.932274</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2139/ssrn.932274" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in TRIZsite Journal, May 2005, also available at
  http://papers.ssrn.com/abstract=932274. arXiv admin note: substantial text
  overlap with arXiv:1307.5426, arXiv:1310.3268, arXiv:1310.3070</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3672v1</id>
    <updated>2013-11-14T21:06:50Z</updated>
    <published>2013-11-14T21:06:50Z</published>
    <title>Hierarchical Model of Human Guidance Performance Based on Interaction
  Patterns in Behavior</title>
    <summary>  This paper describes a framework for the investigation and modeling of human
spatial guidance behavior in complex environments. The model is derived from
the concept of interaction patterns, which represent the invariances or
symmetries inherent in the interactions between an agent and its environment.
These patterns provide the basic elements needed for the formalization of
spatial behavior and determine a natural hierarchy that can be unified under a
hierarchical hidden Markov model.
</summary>
    <author>
      <name>Berenice Mettler</name>
    </author>
    <author>
      <name>Zhaodan Kong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd International Conference on Application and Theory of Automation
  in Command and Control Systems (ICARUS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.3672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1690v1</id>
    <updated>2014-01-08T13:22:42Z</updated>
    <published>2014-01-08T13:22:42Z</published>
    <title>Tendencies, Dead-ends, and Promising Ways. From Interface Ideas to New
  Programs</title>
    <summary>  The mechanism of communication between users and devices is called interface.
From time to time changes in interface significantly improve our work with
computers even without any serious changes in programs themselves. Main ideas
in PCs interface were introduced many years ago and since then there are no
significant changes, while new devices show promising ways by using direct
manipulation of screen objects. Users' direct action with all the screen
objects of our ordinary PCs turns standard screens into touch screens of very
high resolution and not only changes the interface of familiar programs but
creates the new type of programs: user-driven applications.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.1690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; D.2.2; H.1.2; I.3.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5289v1</id>
    <updated>2014-01-21T12:21:48Z</updated>
    <published>2014-01-21T12:21:48Z</published>
    <title>Graphical Interface for Visually Impaired People Based on Bi-stable
  Solenoids</title>
    <summary>  In this paper a concept for hardware realization of graphic tactile display
for visually impaired peoples is presented. For realization of tactile
actuators bi-stable, solenoids and PIC based control board are used. The
selected algorithm for series activation of each row of display allows using
minimal number of active components to set and reset the solenoids. Finally, a
program algorithm of control board is discussed. The project is funded by
Bulgarian National Science Fund NSF Grant D ID 02 14, 2009 2013
</summary>
    <author>
      <name>Stanislav Simeonov</name>
    </author>
    <author>
      <name>Neli Simeonova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7321/jscse</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7321/jscse" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, The International Journal of Soft Computing and
  Software Engineering, ISSN:2251-7545</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.5289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.7821v1</id>
    <updated>2014-01-30T12:29:46Z</updated>
    <published>2014-01-30T12:29:46Z</published>
    <title>Spatial Modelling Techniques in Microsoft Excel</title>
    <summary>  We begin by considering the expectations of the creators of VisiCalc, the
first spreadsheet. The emphasis is on the nature of the spreadsheet grid. The
grid is taken as a presentational method for showing a solution to a Sudoku
puzzle. We consider methods or approaches for the solution. We look at the
relationship between this model and academic papers on the methods for
describing and categorising end-user models generally. We consider whether the
type of model described here should be categorised separately. The complexity
of the model is reviewed in the context of commendations to minimise overly
sophisticated presentational constructs and formulae.
</summary>
    <author>
      <name>Stephen Allen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 colour figures, 1 table, Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.7821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0200v1</id>
    <updated>2014-02-02T15:15:30Z</updated>
    <published>2014-02-02T15:15:30Z</published>
    <title>SPHERE: Meaningful and Inclusive Sensor-Based Home Healthcare</title>
    <summary>  Given current demographic and health trends, and their economic implications,
home healthcare technology has become a fertile area for research and
development. Motivated by the need for a radical reform of healthcare
provision, SPHERE is a large-scale Interdisciplinary Research Collaboration
that aims to develop home sensor systems to monitor people's health and
wellbeing in the home. This paper outlines the unique circumstances of
designing healthcare technology for the home environment, with a particular
focus on how to ensure future systems are meaningful to and desirable for the
intended users.
</summary>
    <author>
      <name>Alison Burrows</name>
    </author>
    <author>
      <name>Rachel Gooberman-Hill</name>
    </author>
    <author>
      <name>Ian Craddock</name>
    </author>
    <author>
      <name>David Coyle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the ACM CSCW 2014 workshop on Designing with Users for
  Domestic environments: Methods, Challenges, Lessons Learned</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.0200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0693v1</id>
    <updated>2014-02-04T11:13:56Z</updated>
    <published>2014-02-04T11:13:56Z</published>
    <title>A survey on Human Computer Interaction Mechanism Using Finger Tracking</title>
    <summary>  Human Computer Interaction (HCI) is a field in which developer makes a user
friendly system. User can interact with a computer system without using any
conventional peripheral devices. Marker is used to recognize hand movement
accurately &amp; successfully. Researchers establish the mechanism to interact with
computer system using computer vision. The interaction is better than normal
static keyboard and mouse. This paper represents most of innovative mechanisms
of the finger tracking used to interact with a computer system using computer
vision.
</summary>
    <author>
      <name>Kinjal N. Shah</name>
    </author>
    <author>
      <name>Kirit R. Rathod</name>
    </author>
    <author>
      <name>Shardul J. Agravat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V7P148</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V7P148" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures, International Journal of Computer Trends and
  Technology (IJCTT)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCTT 7(3):174-177, January 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1243v1</id>
    <updated>2014-02-06T04:30:39Z</updated>
    <published>2014-02-06T04:30:39Z</published>
    <title>Destination Information Management System for Tourist</title>
    <summary>  The use of information and communication technology in our day to day
activities is now unavoidable. In tourism developments, destination information
and management systems are used to guide visitors and provide information to
both visitors and management of the tour sites. In this paper, information and
navigation system was designed for tourists, taking some Niger state of Nigeria
tourism destinations into account. The information management system was
designed using Java Applet (NetBeans IDE 6.1), Hypertext MarkUp Language
(HTML), Personal Home Page (PHP), Java script and MySQL as the back-end
integration database. Two different MySQL servers were used, the MySQL query
browser and the WAMP5 server to compare the effectiveness of the system
developed.
</summary>
    <author>
      <name>Shafii Muhammad Abdulhamid</name>
    </author>
    <author>
      <name>Gana Usman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Computer Science and Telecommunications 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.1243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4724v1</id>
    <updated>2014-02-19T16:36:58Z</updated>
    <published>2014-02-19T16:36:58Z</published>
    <title>Breaking Barriers: Assistive Technology Tool as Educational Software to
  support Writing</title>
    <summary>  The preliminary report by Siriraj Hospital suggested that 6% of population
who are students in Thailand could be estimated to have learning disabilities.
It is therefore necessary for our institute to develop suitable ICT
technologies to assist the education of these learning disabilities children.
We therefore developed a program called Thai Word Prediction Program. Thai Word
Prediction program aims to assist students with learning disabilities in their
writing. After the usability engineering, we conducted the experiment with
students with learning disabilities at the School in Bangkok. Hence, the
results indicated that all three students with learning disabilities in this
study improved their ability of writing by 50%, 81.89% and 100% respectively.
</summary>
    <author>
      <name>Onintra Poobrasert</name>
    </author>
    <author>
      <name>Waragorn Gestubtim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. IJCA 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.4724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4722v1</id>
    <updated>2014-03-19T07:40:55Z</updated>
    <published>2014-03-19T07:40:55Z</published>
    <title>Mouse Control using a Web Camera based on Colour Detection</title>
    <summary>  In this paper we present an approach for Human computer Interaction (HCI),
where we have tried to control the mouse cursor movement and click events of
the mouse using hand gestures. Hand gestures were acquired using a camera based
on colour detection technique. This method mainly focuses on the use of a Web
Camera to develop a virtual human computer interaction device in a cost
effective manner.
</summary>
    <author>
      <name>Abhik Banerjee</name>
    </author>
    <author>
      <name>Abhirup Ghosh</name>
    </author>
    <author>
      <name>Koustuvmoni Bharadwaj</name>
    </author>
    <author>
      <name>Hemanta Saikia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V9P104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V9P104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 1 flowchart, "Published with International
  Journal of Computer Trends and Technology (IJCTT)"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT)
  V9(1):15-20,March 2014.ISSN:2231-2803 Published by Seventh Sense Research
  Group</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.4722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5058v1</id>
    <updated>2014-03-20T07:50:54Z</updated>
    <published>2014-03-20T07:50:54Z</published>
    <title>Towards an Interaction-based Integration of MKM Services into End-User
  Applications</title>
    <summary>  The Semantic Alliance (SAlly) Framework, first presented at MKM 2012, allows
integration of Mathematical Knowledge Management services into typical
applications and end-user workflows. From an architecture allowing invasion of
spreadsheet programs, it grew into a middle-ware connecting spreadsheet, CAD,
text and image processing environments with MKM services. The architecture
presented in the original paper proved to be quite resilient as it is still
used today with only minor changes.
  This paper explores extensibility challenges we have encountered in the
process of developing new services and maintaining the plugins invading
end-user applications. After an analysis of the underlying problems, I present
an augmented version of the SAlly architecture that addresses these issues and
opens new opportunities for document type agnostic MKM services.
</summary>
    <author>
      <name>Constantin Jucovschi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.5058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6669v1</id>
    <updated>2014-03-26T13:53:26Z</updated>
    <published>2014-03-26T13:53:26Z</published>
    <title>Lost Again in Shibuya: Exploration and Awareness in a Labyrinth</title>
    <summary>  Existing digital technologies in urban settings tend to focus narrowly on
concerns around wayfinding, safety, and consumption. In this paper, we examine
pedestrian experiences based on the data collected through field observations
as well as intensive interviews with nine pedestrians in the Shibuya area of
Tokyo, and suggest an alternative approach to blending technologies and urban
activities. Our focus is on social and cognitive aspects of pedestrians who get
lost and explore a labyrinth of sidewalks. We use the data to discuss the
activities that are often ignored or inadequately supported by existing
systems.
</summary>
    <author>
      <name>Shin'ichi Konomi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.6669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5248v1</id>
    <updated>2014-04-21T17:25:15Z</updated>
    <published>2014-04-21T17:25:15Z</published>
    <title>Intelligent Remote Control for TV Program based on Emotion in Arabic
  Speech</title>
    <summary>  Recommender systems for TV program have been studied for the realization of
personalized TV Electronic Program Guides. In this paper, we propose automatic
emotion Arabic speech recognition in order to achieve an intelligent remote
control. In addition, the TV can estimate our interests and preferences by
observing our behavior to watch and have a conversation on topics that might be
interesting to us.
</summary>
    <author>
      <name>M. Meddeb</name>
    </author>
    <author>
      <name>H. Karray</name>
    </author>
    <author>
      <name>Adel M. Alimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Scientific Research &amp; Engineering
  Technology (IJSET), ISSN: (2277-1581) volume 1, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6750v1</id>
    <updated>2014-04-27T13:57:05Z</updated>
    <published>2014-04-27T13:57:05Z</published>
    <title>10 Inventions on Command Buttons in a Graphical User Interface</title>
    <summary>  A command button may contain a textual label or a graphic image or both. It
may be static or animated. There can be many different features to make a
command button attractive and effective. As command button is a typical GUI
element, most improvement on GUI in general will also be applicable to command
buttons. Besides, there are also inventions to improve various aspects of
command buttons in specific. This article illustrates 10 selected inventions
from US patent database. Each invention is followed by a TRIZ based analysis in
brief.
</summary>
    <author>
      <name>Umakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2139/ssrn.949240</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2139/ssrn.949240" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mishra, Umakant, 10 Inventions on Command Buttons in a Graphical
  User Interface, (December 6, 2006) Available at SSRN:
  http://ssrn.com/abstract=949240</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.6750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6757v1</id>
    <updated>2014-04-27T14:29:01Z</updated>
    <published>2014-04-27T14:29:01Z</published>
    <title>Inventions on expressing emotions In Graphical User Interface</title>
    <summary>  The conventional GUI is more mechanical and does not recognize or communicate
emotions. The modern GUIs are trying to infer the likely emotional state and
personality of the user and communicate through a corresponding emotional
state.
  Emotions are expressed in graphical icons, sounds, pictures and other means.
The emotions are found to be useful in especially in communication software,
interactive learning systems, robotics and other adaptive environments. Various
mechanisms have been developed to express emotions through graphical user
interfaces. This article illustrates some interesting inventions selected from
US patent database.
</summary>
    <author>
      <name>Umakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2139/ssrn.949250</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2139/ssrn.949250" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures. Umakant Mishra, Inventions on Expressing Emotions
  in Graphical User Interface, (December 6, 2006), Available at SSRN:
  http://ssrn.com/abstract=949250</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6765v1</id>
    <updated>2014-04-27T15:18:54Z</updated>
    <published>2014-04-27T15:18:54Z</published>
    <title>Inventions on GUI for Eye Cursor Controls Systems</title>
    <summary>  Operating a GUI through eyeball is a complex mechanism and not used as often
as mouse or trackball. But there are situations where eye-mouse devices can
play a tremendous role especially where the hands of the user are not available
or busy to perform other activities. The difficulties of implementing an
eye-cursor control system are many. The article illustrates some inventions on
eye-cursor control system, which attempt to eliminate the difficulties of the
prior art mechanisms.
</summary>
    <author>
      <name>Umakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2139/ssrn.1264687</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2139/ssrn.1264687" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mishra, Umakant, Inventions on GUI for Eye Cursor Control Systems
  (September 7, 2007), Available at SSRN: http://ssrn.com/abstract=1264687 or
  http://dx.doi.org/10.2139/ssrn.1264687</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.6765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3758v1</id>
    <updated>2014-05-15T06:55:21Z</updated>
    <published>2014-05-15T06:55:21Z</published>
    <title>Search Interfaces for Mathematicians</title>
    <summary>  Access to mathematical knowledge has changed dramatically in recent years,
therefore changing mathematical search practices. Our aim with this study is to
scrutinize professional mathematicians' search behavior. With this
understanding we want to be able to reason why mathematicians use which tool
for what search problem in what phase of the search process. To gain these
insights we conducted 24 repertory grid interviews with mathematically inclined
people (ranging from senior professional mathematicians to non-mathematicians).
From the interview data we elicited patterns for the user group
"mathematicians" that can be applied when understanding design issues or
creating new designs for mathematical search interfaces.
</summary>
    <author>
      <name>Andrea Kohlhase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference article "CICM'14: International Conference on Computer
  Mathematics 2014", DML-Track: Digital Math Libraries 17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.3758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5249v1</id>
    <updated>2014-05-20T22:08:32Z</updated>
    <published>2014-05-20T22:08:32Z</published>
    <title>Hidden Markov Model for Inferring Learner Task Using Mouse Movement</title>
    <summary>  One of the issues of e-learning web based application is to understand how
the learner interacts with an e-learning application to perform a given task.
This study proposes a methodology to analyze learner mouse movement in order to
infer the task performed. To do this, a Hidden Markov Model is used for
modeling the interaction of the learner with an e-learning application. The
obtained results show the ability of our model to analyze the interaction in
order to recognize the task performed by the learner.
</summary>
    <author>
      <name>Elbahi Anis</name>
    </author>
    <author>
      <name>Mohamed Ali Mahjoub</name>
    </author>
    <author>
      <name>Mohamed Nazih Omri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth International Conference on Information and Communication
  Technology and Accessibility (ICTA), 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4803v1</id>
    <updated>2014-06-18T17:25:06Z</updated>
    <published>2014-06-18T17:25:06Z</published>
    <title>Reorganization of Links to Improve User Navigation</title>
    <summary>  Website can be easily design but to efficient user navigation is not a easy
task since user behavior is keep changing and developer view is quite different
from what user wants, so to improve navigation one way is reorganization of
website structure. For reorganization here proposed strategy is farthest first
traversal clustering algorithm perform clustering on two numeric parameters and
for finding frequent traversal path of user Apriori algorithm is used. Our aim
is to perform reorganization with fewer changes in website structure.
</summary>
    <author>
      <name>Deepshree A. Vadeyar</name>
    </author>
    <author>
      <name>Yogish H. K</name>
    </author>
    <link href="http://arxiv.org/abs/1406.4803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0434v2</id>
    <updated>2014-07-14T12:18:54Z</updated>
    <published>2014-07-02T01:05:33Z</published>
    <title>Towards a Practical Architecture for India Centric Internet of Things</title>
    <summary>  An effective architecture for the Internet of Things (IoT), particularly for
an emerging nation like India with limited technology penetration at the
national scale, should be based on tangible technology advances in the present,
practical application scenarios of social and entrepreneurial value, and
ubiquitous capabilities that make the realization of IoT affordable and
sustainable. Humans, data, communication and devices play key roles in the IoT
ecosystem that we perceive. In a push towards this sustainable and practical
IoT Architecture for India, we synthesize ten design paradigms to consider.
</summary>
    <author>
      <name>Prasant Misra</name>
    </author>
    <author>
      <name>Yogesh Simmhan</name>
    </author>
    <author>
      <name>Jay Warrior</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Internet of Things Newsletter, January, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.0434v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0434v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0843v1</id>
    <updated>2014-07-03T10:09:57Z</updated>
    <published>2014-07-03T10:09:57Z</published>
    <title>The Gamification Design Problem</title>
    <summary>  Under the assumptions that (i) gamification consists of various types of
users that experience game design elements differently; and (ii) gamification
is deployed in order to achieve some goal in the broadest sense, we pose the
gamification problem as that of assigning each user a game design element that
maximizes their expected contribution in order to achieve that goal. We show
that this problem reduces to a statistical learning problem and suggest matrix
factorization as one solution when user interaction data is given. The
hypothesis is that predictive models as intelligent tools for supporting users
in decision-making may also have potential to support the design process in
gamification.
</summary>
    <author>
      <name>Michael Meder</name>
    </author>
    <author>
      <name>Brijnesh-Johannes Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1173v1</id>
    <updated>2014-08-06T03:16:21Z</updated>
    <published>2014-08-06T03:16:21Z</published>
    <title>A Review Paper on Oculus Rift-A Virtual Reality Headset</title>
    <summary>  Oculus rift: Virtual reality (VR) is a burgeoning field that has the inherent
potential of manipulating peoples mind with a superlative 3D experience. Oculus
rift is one such application that assists in achieving the same. With the
fleeting enhancements in VR it now seems very feasible to provide the user with
experiences that were earlier thought to be merely a dream or a nightmare.
</summary>
    <author>
      <name>Parth Rajesh Desai</name>
    </author>
    <author>
      <name>Pooja Nikhil Desai</name>
    </author>
    <author>
      <name>Komal Deepak Ajmera</name>
    </author>
    <author>
      <name>Khushbu Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.1173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0128v1</id>
    <updated>2014-08-30T15:45:19Z</updated>
    <published>2014-08-30T15:45:19Z</published>
    <title>Through the Frosted Glass: Security Problems in a Translucent UI</title>
    <summary>  Translucency is now a common design element in at least one popular mobile
operating system. This raises security concerns as it can make it harder for
users to correctly identify and interpret trusted interaction elements. In this
paper, we demonstrate this security problem using the example of the Safari
browser in the latest iOS version on Apple tablets and phones (iOS7), and
discuss technical challenges of an attack as well as solutions to these
challenges. We conclude with a survey-based user study, where we seek to
quantify the security impact, and find that further investigation is warranted.
</summary>
    <author>
      <name>Arne Renkema-Padmos</name>
    </author>
    <author>
      <name>Jerome Baum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2208v1</id>
    <updated>2014-09-08T05:09:49Z</updated>
    <published>2014-09-08T05:09:49Z</published>
    <title>A wireless hand-held platform for robotic behavior control</title>
    <summary>  The need for customizable properties in autonomous robotic platforms, such as
in-home nursing care for the elderly and parallel implementations of
human-to-machine control interfaces creates an opportunity to introduce methods
deploying commonly available mobile devices running robotic command
applications in managed code. This paper will discuss a human-to-machine
interface and demonstrate a prototype consisting of a mobile device running a
configurable application communicating with a mobile robot using a managed,
type-safe language, C#.NET, over Bluetooth.
</summary>
    <author>
      <name>Christopher A. Tucker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.2208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3993v1</id>
    <updated>2014-09-13T21:43:45Z</updated>
    <published>2014-09-13T21:43:45Z</published>
    <title>Clear, Concise and Effective UI: Opinion and Suggestions</title>
    <summary>  The most important aspect of any Software is the operability for the intended
audience. This factor of operability is encompassed in the user interface,
which serves as the only window to the features of the system. It is thus
essential that the User Interface provided is robust, concise and lucid.
Presently there are no properly defined rules or guidelines for user interface
design enabling a perfect design, since such a system cannot be perceived. This
article aims at providing suggestions in the design of the User Interface,
which would make it easier for the user to navigate through the system features
and also the developers to guide the users towards better utilization of the
features.
</summary>
    <author>
      <name>Rishabh Jain</name>
    </author>
    <author>
      <name>Rupanta Rwiteej Dutta</name>
    </author>
    <author>
      <name>Rajat Tandon</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5282v1</id>
    <updated>2014-09-18T12:25:04Z</updated>
    <published>2014-09-18T12:25:04Z</published>
    <title>Sonification Aesthetics and Listening for Network Situational Awareness</title>
    <summary>  This paper looks at the problem of using sonification to enable network
administrators to maintaining situational awareness about their network
environment. Network environments generate a lot of data and the need for
continuous monitoring means that sonification systems must be designed in such
a way as to maximise acceptance while minimising annoyance and listener
fatigue. It will be argued that solutions based on the concept of the
soundscape offer an ecological advantage over other sonification designs.
</summary>
    <author>
      <name>Paul Vickers</name>
    </author>
    <author>
      <name>Christopher Laing</name>
    </author>
    <author>
      <name>Mohamed Debashi</name>
    </author>
    <author>
      <name>Tom Fairfax</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/2.1.4225.6648</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/2.1.4225.6648" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop paper presented at SoniHED --- Conference on Sonification of
  Health and Environmental Data, York, UK, 12 September, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.5282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2828v1</id>
    <updated>2014-10-10T16:12:20Z</updated>
    <published>2014-10-10T16:12:20Z</published>
    <title>A Study on Placement of Social Buttons in Web Pages</title>
    <summary>  With the explosion of social media in the last few years, web pages nowadays
include different social network buttons where users can express if they
support or recommend content. Those social buttons are very visual and their
presentations, along with the counters, mark the importance of the social
network and the interest on the content. In this paper, we analyze the presence
of four types of social buttons (Facebook, Twitter, Google+1, and LinkedIn) in
a large collection of web pages that we tracked over a period of time. We
report on the distribution and counts along with some characteristics per
domain. Finally, we outline some research directions.
</summary>
    <author>
      <name>Omar Alonso</name>
    </author>
    <author>
      <name>Vasilis Kandylas</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5907v1</id>
    <updated>2014-10-22T03:36:53Z</updated>
    <published>2014-10-22T03:36:53Z</published>
    <title>Replacing the computer mouse</title>
    <summary>  In a few months the computer mouse will be half-a-century-old. It is known to
have many drawbacks, the main ones being: loss of productivity due to constant
switching between keyboard and mouse, and health issues such as RSI. Like the
keyboard, it is an unnatural human-computer interface. However the vast
majority of computer users still use computer mice nowadays.
  In this article, we explore computer mouse alternatives. Our research shows
that moving the mouse cursor can be done efficiently with camera-based head
tracking system such as the SmartNav device, and mouse clicks can be emulated
in many complementary ways. We believe that computer users can increase their
productivity and improve their long-term health by using these alternatives.
</summary>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8221v1</id>
    <updated>2014-10-30T01:08:18Z</updated>
    <published>2014-10-30T01:08:18Z</published>
    <title>PIDE for Asynchronous Interaction with Coq</title>
    <summary>  This paper describes the initial progress towards integrating the Coq proof
assistant with the PIDE architecture initially developed for Isabelle. The
architecture is aimed at asynchronous, parallel interaction with proof
assistants, and is tied in heavily with a plugin that allows the jEdit editor
to work with Isabelle.
  We have made some generalizations to the PIDE architecture to accommodate for
more provers than just Isabelle, and adapted Coq to understand the core
protocol: this delivered a working system in about two man-months.
</summary>
    <author>
      <name>Carst Tankink</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria Saclay - Île-de-France</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.167.9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.167.9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings UITP 2014, arXiv:1410.7850</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 167, 2014, pp. 73-83</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.8221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.8221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4179v1</id>
    <updated>2014-12-13T02:11:44Z</updated>
    <published>2014-12-13T02:11:44Z</published>
    <title>What About Feedback?</title>
    <summary>  The role of immediate feedback in-group conversations has received scant
attention in the recent literature. While studies from the early 1990's
suggested that "added information" in the form of non-verbal cues would allow
video conferencing to "augment" the audio-only conference in terms of
effectiveness, stunningly little follow-on research has been done reflective of
the current state of computer mediated communication, video conferencing, "live
walls", etc. This article contrasts three studies of immediate feedback in
in-person settings as the basis for suggesting a new research program -
research to look at potential effects of augmenting video-conferencing with an
immediate feedback channel.
</summary>
    <author>
      <name>Terrence Letiche</name>
    </author>
    <author>
      <name>Michael Lissack</name>
    </author>
    <link href="http://arxiv.org/abs/1412.4179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6378v1</id>
    <updated>2014-12-19T15:34:01Z</updated>
    <published>2014-12-19T15:34:01Z</published>
    <title>Wyrm, A Pythonic Toolbox for Brain-Computer Interfacing</title>
    <summary>  A Brain-Computer Interface (BCI) is a system that measures central nervous
system activity and translates the recorded data into an output suitable for a
computer to use as an input signal. Such a BCI system consists of three parts,
the signal acquisition, the signal processing and the feedback/stimulus
presentation. In this paper we present Wyrm, a signal processing toolbox for
BCI in Python. Wyrm is applicable to a broad range of neuroscientific problems
and capable for running online experiments in real time and off-line data
analysis and visualisation.
</summary>
    <author>
      <name>Bastian Venthur</name>
    </author>
    <author>
      <name>Benjamin Blankertz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7932v1</id>
    <updated>2014-12-26T12:50:07Z</updated>
    <published>2014-12-26T12:50:07Z</published>
    <title>Home Automation Using SSVEP &amp; Eye-Blink Detection Based Brain-Computer
  Interface</title>
    <summary>  In this paper, we present a novel brain computer interface based home
automation system using two responses - Steady State Visually Evoked Potential
(SSVEP) and the eye-blink artifact, which is augmented by a Bluetooth based
indoor localization system, to greatly increase the number of controllable
devices. The hardware implementation of this system to control a table lamp and
table fan using brain signals has also been discussed and state-of-the-art
results have been achieved.
</summary>
    <author>
      <name>Kratarth Goel</name>
    </author>
    <author>
      <name>Raunaq Vohra</name>
    </author>
    <author>
      <name>Anant Kamath</name>
    </author>
    <author>
      <name>Veeky Baths</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SMC.2014.6974563</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SMC.2014.6974563" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 table, published at IEEE SMC 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04156v1</id>
    <updated>2015-01-17T04:55:34Z</updated>
    <published>2015-01-17T04:55:34Z</published>
    <title>User involvement in the partner program of the educational social
  network</title>
    <summary>  The paper describes experiments to attract active online system users to the
partner program. The objective is to grow the number of users by involving
existing system users in viral mechanics. Several examples of user motivation
are given, along with the specific interface implementations and viral
mechanics. Viral K-factor was used as the metrics for the resulting system
growth assessment. Specific examples show both positive and negative outcomes.
Growth of the target system parameters is discussed.
</summary>
    <author>
      <name>Ilya V. Osipov</name>
    </author>
    <author>
      <name>Anna Y. Prasikova</name>
    </author>
    <author>
      <name>Alex A. Volinsky</name>
    </author>
    <link href="http://arxiv.org/abs/1501.04156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05696v2</id>
    <updated>2015-08-29T14:35:18Z</updated>
    <published>2015-01-23T01:59:39Z</published>
    <title>Anima: Adaptive Personalized Software Keyboard</title>
    <summary>  We present a Software Keyboard for smart touchscreen devices that learns its
owner's unique dictionary in order to produce personalized typing predictions.
The learning process is accelerated by analysing user's past typed
communication. Moreover, personal temporal user behaviour is captured and
exploited in the prediction engine. Computational and storage issues are
addressed by dynamically forgetting words that the user no longer types. A
prototype implementation is available at Google Play Store.
</summary>
    <author>
      <name>Panos Sakkos</name>
    </author>
    <author>
      <name>Dimitrios Kotsakos</name>
    </author>
    <author>
      <name>Ioannis Katakis</name>
    </author>
    <author>
      <name>Dimitrios Gunopulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.05696v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05696v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04326v1</id>
    <updated>2015-02-15T16:05:21Z</updated>
    <published>2015-02-15T16:05:21Z</published>
    <title>Interface Transformation from Ruling to Obedience</title>
    <summary>  This article is about one feature which was partly introduced 30 years ago
with the development of multi windows operating systems. It is about the
movability of screen objects not according to some predetermined algorithm but
by the direct user action. Many years ago it was introduced on a very limited
basis and nothing was improved since then. Smartphones and tablets give direct
access to screen elements but on a very limited set of commands (scroll and
zoom). There is an easy to use algorithm which turns any screen object into
movable / resizable. This algorithm uses only mouse to turn screens of normal
PCs into touchscreens, but this simple change means a revolution in our work
with computers.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.04326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04744v1</id>
    <updated>2015-02-16T22:50:03Z</updated>
    <published>2015-02-16T22:50:03Z</published>
    <title>Where's My Drink? Enabling Peripheral Real World Interactions While
  Using HMDs</title>
    <summary>  Head Mounted Displays (HMDs) allow users to experience virtual reality with a
great level of immersion. However, even simple physical tasks like drinking a
beverage can be difficult and awkward while in a virtual reality experience. We
explore mixed reality renderings that selectively incorporate the physical
world into the virtual world for interactions with physical objects. We
conducted a user study comparing four rendering techniques that balances
immersion in a virtual world with ease of interaction with the physical world.
Finally, we discuss the pros and cons of each approach, suggesting guidelines
for future rendering techniques that bring physical objects into virtual
reality.
</summary>
    <author>
      <name>Pulkit Budhiraja</name>
    </author>
    <author>
      <name>Rajinder Sodhi</name>
    </author>
    <author>
      <name>Brett Jones</name>
    </author>
    <author>
      <name>Kevin Karsch</name>
    </author>
    <author>
      <name>Brian Bailey</name>
    </author>
    <author>
      <name>David Forsyth</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00747v1</id>
    <updated>2015-04-03T05:25:07Z</updated>
    <published>2015-04-03T05:25:07Z</published>
    <title>Software for Wearable Devices: Challenges and Opportunities</title>
    <summary>  Wearable devices are a new form of mobile computer system that provides
exclusive and user-personalized services. Wearable devices bring new issues and
challenges to computer science and technology. This paper summarizes the
development process and the categories of wearable devices. In addition, we
present new key issues arising in aspects of wearable devices, including
operating systems, database management system, network communication protocol,
application development platform, privacy and security, energy consumption,
human-computer interaction, software engineering, and big data.
</summary>
    <author>
      <name>He Jiang</name>
    </author>
    <author>
      <name>Xin Chen</name>
    </author>
    <author>
      <name>Shuwei Zhang</name>
    </author>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Weiqiang Kong</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, for Compsac 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.00747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01049v1</id>
    <updated>2015-04-04T20:34:11Z</updated>
    <published>2015-04-04T20:34:11Z</published>
    <title>3D visual analysis of seabed on smartphone</title>
    <summary>  We create a 'virtual-seabed' platform to realize the 3D visual analysis of
seabed on smartphone. The 3D seabed platform is based on a 'section-drilling'
model, implementing visualization and analysis of the integrated data of seabed
on the 3D browser on smartphone. Some 3D visual analysis functions are
developed. This work presents a thorough and interesting way of presenting
seabed data on smartphone, which raises many application possibilities. This
platform is another practical proof based on our WebVRGIS platform.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Tianyun Su</name>
    </author>
    <author>
      <name>Xiaoming Li</name>
    </author>
    <author>
      <name>Shengzhong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE Pacific Visualization Symposium (PacificVis)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01051v1</id>
    <updated>2015-04-04T20:44:33Z</updated>
    <published>2015-04-04T20:44:33Z</published>
    <title>WebVRGIS Based City Bigdata 3D Visualization and Analysis</title>
    <summary>  This paper shows the WEBVRGIS platform overlying multiple types of data about
Shenzhen over a 3d globe. The amount of information that can be visualized with
this platform is overwhelming, and the GIS-based navigational scheme allows to
have great flexibility to access the different available data sources. For
example,visualising historical and forecasted passenger volume at stations
could be very helpful when overlaid with other social data.
</summary>
    <author>
      <name>Xiaoming Li</name>
    </author>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Baoyun Zhang</name>
    </author>
    <author>
      <name>Weixi Wang</name>
    </author>
    <author>
      <name>Shengzhong Feng</name>
    </author>
    <author>
      <name>Jinxing Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE Pacific Visualization Symposium (PacificVis)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03370v2</id>
    <updated>2015-07-29T08:40:22Z</updated>
    <published>2015-04-13T21:46:32Z</published>
    <title>Preprint Serious Game Based Dysphonic Rehabilitation Tool</title>
    <summary>  This is the preprint version of our paper on 2015 International Conference on
Virtual Rehabilitation (ICVR2015). The purpose of this work is designing and
implementing a rehabilitation software for dysphonic patients. Constant
training is a key factor for this type of therapy. The patient can play the
game as well as conduct the voice training simultaneously guided by therapists
at clinic or exercise independently at home. The voice information can be
recorded and extracted for evaluating the long-time rehabilitation progress.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Chantal Esteve</name>
    </author>
    <author>
      <name>Javier Chirivella</name>
    </author>
    <author>
      <name>Pablo Gagliardo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on 2015 International
  Conference on Virtual Rehabilitation (ICVR2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03370v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03370v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03371v2</id>
    <updated>2015-07-29T08:37:31Z</updated>
    <published>2015-04-13T21:52:07Z</published>
    <title>Preprint Imagining In-Air Interaction for Hemiplegia Sufferer</title>
    <summary>  This is the preprint version of our paper on 2015 International Conference on
Virtual Rehabilitation (ICVR2015). In this paper, we described the imagination
scenarios of a touch-less interaction technology for hemiplegia, which can
support either hand or foot interaction with the smartphone or head mounted
device (HMD). The computer vision interaction technology is implemented in our
previous work, which provides a core support for gesture interaction by
accurately detecting and tracking the hand or foot gesture. The patients
interact with the application using hand/foot gesture motion in the camera
view.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Haibo Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on 2015 International
  Conference on Virtual Rehabilitation (ICVR2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03731v1</id>
    <updated>2015-04-14T21:36:53Z</updated>
    <published>2015-04-14T21:36:53Z</published>
    <title>Safety enhancement through situation-aware user interfaces</title>
    <summary>  Due to their privileged position halfway between the physical and the cyber
universes, user interfaces may play an important role in preventing,
tolerating, and learning from scenarios potentially affecting mission safety
and the user's quality of experience. This vision is embodied here in the main
ideas and a proof-of-concepts implementation of user interfaces that combine
dynamic profiling with context- and situation-awareness and autonomic software
adaptation.
</summary>
    <author>
      <name>Vincenzo De Florio</name>
    </author>
    <author>
      <name>Chris Blondia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Proc. of the 7th Int.l IET System Safety Conference,
  Edinburgh, UK, 15-18 October 2012. IET</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00092v1</id>
    <updated>2015-05-01T05:18:57Z</updated>
    <published>2015-05-01T05:18:57Z</published>
    <title>On the Effect of Human-Computer Interfaces on Language Expression</title>
    <summary>  Language expression is known to be dependent on attributes intrinsic to the
author. To date, however, little attention has been devoted to the effect of
interfaces used to articulate language on its expression. Here we study a large
corpus of text written using different input devices and show that writers
unconsciously prefer different letters depending on the interplay between their
individual traits (e.g., hand laterality and injuries) and the layout of
keyboards. Our results show, for the first time, how the interplay between
technology and its users modifies language expression.
</summary>
    <author>
      <name>Dan Pelleg</name>
    </author>
    <author>
      <name>Elad Yom-Tov</name>
    </author>
    <author>
      <name>Evgeniy Gabrilovich</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00111v1</id>
    <updated>2015-05-01T07:46:53Z</updated>
    <published>2015-05-01T07:46:53Z</published>
    <title>Mobile Crowd Sensing and Computing: When Participatory Sensing Meets
  Participatory Social Media</title>
    <summary>  With the development of mobile sensing and mobile social networking
techniques, Mobile Crowd Sensing and Computing (MCSC), which leverages
heterogeneous crowdsourced data for large-scale sensing, has become a leading
paradigm. Built on top of the participatory sensing vision, MCSC has two
characterizing features: (1) it leverages heterogeneous crowdsourced data from
two data sources: participatory sensing and participatory social media; and (2)
it presents the fusion of human and machine intelligence (HMI) in both the
sensing and computing process. This paper characterizes the unique features and
challenges of MCSC. We further present early efforts on MCSC to demonstrate the
benefits of aggregating heterogeneous crowdsourced data.
</summary>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Daqing Zhang</name>
    </author>
    <author>
      <name>Zhiwen Yu</name>
    </author>
    <author>
      <name>Alvin Chin</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00489v1</id>
    <updated>2015-05-03T22:45:19Z</updated>
    <published>2015-05-03T22:45:19Z</published>
    <title>The Distant Heart: Mediating Long-Distance Relationships through
  Connected Computational Jewelry</title>
    <summary>  In the world where increasingly mobility and long-distance relationships with
family, friends and loved-ones became commonplace, there exists a gap in
intimate interpersonal communication mediated by technology. Considering the
advances in the field of mediation of relationships through technology, as well
as prevalence of use of jewelry as love-tokens for expressing a wish to be
remembered and to evoke the presence of the loved-one, developments in the new
field of computational jewelry offer some truly exciting possibilities. In this
paper we investigate the role that the jewelry-like form factor of prototypes
can play in the context of studying effects of computational jewelry in
mediating long-distance relationships.
</summary>
    <author>
      <name>Yulia Silina</name>
    </author>
    <author>
      <name>Hamed Haddadi</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01319v3</id>
    <updated>2015-11-10T10:02:28Z</updated>
    <published>2015-05-06T10:56:12Z</published>
    <title>The History of Mobile Augmented Reality</title>
    <summary>  This document summarizes the major milestones in mobile Augmented Reality
between 1968 and 2014. Major parts of the list were compiled by the member of
the Christian Doppler Laboratory for Handheld Augmented Reality in 2010 (author
list in alphabetical order) for the ISMAR society. Later in 2013 it was
updated, and more recent work was added during preparation of this report.
Permission is granted to copy and modify.
</summary>
    <author>
      <name>Clemens Arth</name>
    </author>
    <author>
      <name>Raphael Grasset</name>
    </author>
    <author>
      <name>Lukas Gruber</name>
    </author>
    <author>
      <name>Tobias Langlotz</name>
    </author>
    <author>
      <name>Alessandro Mulloni</name>
    </author>
    <author>
      <name>Daniel Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01319v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01319v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04333v2</id>
    <updated>2015-06-16T21:32:30Z</updated>
    <published>2015-06-13T23:23:21Z</published>
    <title>Towards Scalable Visual Exploration of Very Large RDF Graphs</title>
    <summary>  In this paper, we outline our work on developing a disk-based infrastructure
for efficient visualization and graph exploration operations over very large
graphs. The proposed platform, called graphVizdb, is based on a novel technique
for indexing and storing the graph. Particularly, the graph layout is indexed
with a spatial data structure, i.e., an R-tree, and stored in a database. In
runtime, user operations are translated into efficient spatial operations
(i.e., window queries) in the backend.
</summary>
    <author>
      <name>Nikos Bikakis</name>
    </author>
    <author>
      <name>John Liagouris</name>
    </author>
    <author>
      <name>Maria Krommyda</name>
    </author>
    <author>
      <name>George Papastefanatos</name>
    </author>
    <author>
      <name>Timos Sellis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th Extended Semantic Web Conference (ESWC 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05573v1</id>
    <updated>2015-06-18T07:47:49Z</updated>
    <published>2015-06-18T07:47:49Z</published>
    <title>Emergence of synchrony in an Adaptive Interaction Model</title>
    <summary>  In a Human-Computer Interaction context, we aim to elaborate an adaptive and
generic interaction model in two different use cases: Embodied Conversational
Agents and Creative Musical Agents for musical improvisation. To reach this
goal, we'll try to use the concepts of adaptation and synchronization to
enhance the interactive abilities of our agents and guide the development of
our interaction model, and will try to make synchrony emerge from non-verbal
dimensions of interaction.
</summary>
    <author>
      <name>Kevin Sanlaville</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Gérard Assayag</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Bevilacqua</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Catherine Pelachaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Intelligent Virtual Agents 2015 Doctoral Consortium, Aug 2015, Delft,
  Netherlands. IVA Doctoral Consortium, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.05573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01292v1</id>
    <updated>2015-07-05T22:56:01Z</updated>
    <published>2015-07-05T22:56:01Z</published>
    <title>ScratchR: Sharing User-generated Programmable Media</title>
    <summary>  In this paper, I describe a platform for sharing programmable media on the
web called ScratchR. As the backbone of an on-line community of creative
learners, ScratchR will give members access to an audience and inspirational
ideas from each other. ScratchR seeks to support different states of
participation: from passive consumption to active creation. This platform is
being evaluated with a group of middle-school students and a larger community
of beta testers.
</summary>
    <author>
      <name>Andrés Monroy-Hernández</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1297277.1297315</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1297277.1297315" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 6th international conference on Interaction
  Design and Children (IDC 2007). ACM, New York, NY, USA, 167-168</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01299v1</id>
    <updated>2015-07-05T23:34:15Z</updated>
    <published>2015-07-05T23:34:15Z</published>
    <title>NewsPad: Designing for Collaborative Storytelling in Neighborhoods</title>
    <summary>  This paper introduces design explorations in neighborhood collaborative
storytelling. We focus on blogs and citizen journalism, which have been
celebrated as a means to meet the reporting needs of small local communities.
These bloggers have limited capacity and social media feeds seldom have the
context or readability of news stories. We present NewsPad, a content editor
that helps communities create structured stories, collaborate in real time,
recruit contributors, and syndicate the editing process. We evaluate NewsPad in
four pilot deployments and find that the design elicits collaborative story
creation.
</summary>
    <author>
      <name>J. Nathan Matias</name>
    </author>
    <author>
      <name>Andrés Monroy-Hernández</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2559206.2581354</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2559206.2581354" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NewsPad: designing for collaborative storytelling in neighborhoods.
  In Proceedings of the extended abstracts of the 32nd annual ACM conference on
  Human factors in computing systems (CHI EA 2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01882v1</id>
    <updated>2015-07-07T17:10:16Z</updated>
    <published>2015-07-07T17:10:16Z</published>
    <title>Demandance</title>
    <summary>  A demandance is a psychological "pull" exerted by a stimulus. It is closely
related to the theory of "affordance". I introduce the theory of demandance,
offer some motivating examples, briefly explore its psychological basis, and
examine some implications of the theory. I exemplify some of the positive and
negative implications of demandances for design, with special attention to
young children and the design of educational products and practices. I suggest
that demandance offers an approach to one of the persistent mysteries of the
theory of affordance, specifically: Given that there may be many affordances in
any particular setting, how do we choose which to actually act upon?
</summary>
    <author>
      <name>Jeff Shrager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02024v1</id>
    <updated>2015-08-09T14:14:33Z</updated>
    <published>2015-08-09T14:14:33Z</published>
    <title>Preprint Virtual Reality Based GIS Analysis Platform</title>
    <summary>  This is the preprint version of our paper on ICONIP2015. The proposed
platform supports the integrated VRGIS functions including 3D spatial analysis
functions, 3D visualization for spatial process and serves for 3D globe and
digital city. The 3D analysis and visualization of the concerned city massive
information are conducted in the platform. The amount of information that can
be visualized with this platform is overwhelming, and the GIS based
navigational scheme allows to have great flexibility to access the different
available data sources.
</summary>
    <author>
      <name>Weixi Wang</name>
    </author>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Xiaoming Li</name>
    </author>
    <author>
      <name>Weiping Xu</name>
    </author>
    <author>
      <name>Baoyun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on ICONIP2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03653v1</id>
    <updated>2015-04-25T01:08:32Z</updated>
    <published>2015-04-25T01:08:32Z</published>
    <title>AnimalCatcher: a digital camera to capture various reactions of animals</title>
    <summary>  People often have difficulty to take pictures of animals, since animals
usually do not react with cameras nor understand verbal directions. To solve
this problem, we developed a new interaction technique, AnimalCatcher, which
can attract animals' attention easily. The AnimalCatcher shoots various sounds
using directional speaker to capture various reactions of animals. This paper
describes concepts, implementation, and example pictures taken in a zoo.
</summary>
    <author>
      <name>Koji Tsukada</name>
    </author>
    <author>
      <name>Maho Oki</name>
    </author>
    <author>
      <name>Kazutaka Kurihara</name>
    </author>
    <author>
      <name>Yuko Furudate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Written in Japanese, 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07053v1</id>
    <updated>2015-08-27T23:03:17Z</updated>
    <published>2015-08-27T23:03:17Z</published>
    <title>SentenceRacer: A Game with a Purpose for Image Sentence Annotation</title>
    <summary>  Recently datasets that contain sentence descriptions of images have enabled
models that can automatically generate image captions. However, collecting
these datasets are still very expensive. Here, we present SentenceRacer, an
online game that gathers and verifies descriptions of images at no cost.
Similar to the game hangman, players compete to uncover words in a sentence
that ultimately describes an image. SentenceRacer both generates and verifies
that the sentences are accurate descriptions. We show that SentenceRacer
generates annotations of higher quality than those generated on Amazon
Mechanical Turk (AMT).
</summary>
    <author>
      <name>Kenji Hata</name>
    </author>
    <author>
      <name>Sherman Leung</name>
    </author>
    <author>
      <name>Ranjay Krishna</name>
    </author>
    <author>
      <name>Michael S. Bernstein</name>
    </author>
    <author>
      <name>Li Fei-Fei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, 2 tables, potential CSCW poster submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00159v2</id>
    <updated>2015-09-07T15:55:27Z</updated>
    <published>2015-09-01T07:03:35Z</published>
    <title>Preprint Virtual Reality Assistant Technology for Learning Primary
  Geography</title>
    <summary>  This is the preprint version of our paper on ICWL2015. A virtual reality
based enhanced technology for learning primary geography is proposed, which
synthesizes several latest information technologies including virtual
reality(VR), 3D geographical information system(GIS), 3D visualization and
multimodal human-computer-interaction (HCI). The main functions of the proposed
system are introduced, i.e. Buffer analysis, Overlay analysis, Space convex
hull calculation, Space convex decomposition, 3D topology analysis and 3D space
intersection detection. The multimodal technologies are employed in the system
to enhance the immersive perception of the users.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Xiaoming Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on ICWL2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.00159v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00159v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05452v1</id>
    <updated>2015-09-17T21:52:12Z</updated>
    <published>2015-09-17T21:52:12Z</published>
    <title>Composing vibrotactile music: A multisensory experience with the
  Emoti-chair</title>
    <summary>  The Emoti-Chair is a novel technology to enhance entertainment through
vibrotactile stimulation. We assessed the experience of this technology in two
workshops. In the first workshop, deaf film-makers experimented with creating
vibetracks for a movie clip using a professional movie editing software. In the
second workshop, trained opera singers sang and felt their voice through the
Emoti-Chair. Participants in both workshops generally found the overall
experience to be exciting and they were motivated to use the Chair for upcoming
projects.
</summary>
    <author>
      <name>Anant Baijal</name>
    </author>
    <author>
      <name>Julia Kim</name>
    </author>
    <author>
      <name>Carmen Branje</name>
    </author>
    <author>
      <name>Frank Russo</name>
    </author>
    <author>
      <name>Deborah I. Fels</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HAPTIC.2012.6183839</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HAPTIC.2012.6183839" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HAPTICS Symposium 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.05452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06774v1</id>
    <updated>2015-09-22T20:35:54Z</updated>
    <published>2015-09-22T20:35:54Z</published>
    <title>Preprint: Bringing immersive enjoyment to hyperbaric oxygen chamber
  users using virtual reality glasses</title>
    <summary>  This is the preprint version of our paper on REHAB2015. This paper proposed a
novel immersive entertainment system for the users of hyperbaric oxygen therapy
chamber. The system is a hybrid of hardware and software, the scheme is
described in this paper. The hardware is combined by a HMD (i.e. virtual
reality glasses shell), a smartphone and a waterproof bag. The software is able
to transfer the stereoscopic images of the 3D game to the screen of the
smartphone synchronously. The comparison and selection of the hardware are
discussed according to the practical running scene of the clinical hyperbaric
oxygen treatment. Finally, a preliminary guideline for designing this kind of
system is raised accordingly.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on REHAB2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06776v1</id>
    <updated>2015-09-22T20:43:51Z</updated>
    <published>2015-09-22T20:43:51Z</published>
    <title>Preprint: Intuitive Evaluation of Kinect2 based Balance Measurement
  Software</title>
    <summary>  This is the preprint version of our paper on REHAB2015. A balance measurement
software based on Kinect2 sensor is evaluated by comparing to golden standard
balance measure platform intuitively. The software analysis the tracked body
data from the user by Kinect2 sensor and get user's center of mass(CoM) as well
as its motion route on a plane. The software is evaluated by several comparison
tests, the evaluation results preliminarily prove the reliability of the
software.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Vicente Penades</name>
    </author>
    <author>
      <name>Sonia Blasco</name>
    </author>
    <author>
      <name>Javier Chirivella</name>
    </author>
    <author>
      <name>Pablo Gagliardo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on REHAB2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06783v1</id>
    <updated>2015-09-22T20:59:19Z</updated>
    <published>2015-09-22T20:59:19Z</published>
    <title>Preprint: Comparing Kinect2 based Balance Measurement Software to Wii
  Balance Board</title>
    <summary>  This is the preprint version of our paper on REHAB2015. A balance measurement
software based on Kinect2 sensor is evaluated by comparing to Wii balance board
in numerical analysis level, and further improved according to the
consideration of BFP (Body fat percentage) values of the user. Several person
with different body types are involved into the test. The algorithm is improved
by comparing the body type of the user to the 'golden- standard' body type. The
evaluation results of the optimized algorithm preliminarily prove the
reliability of the software.
</summary>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <author>
      <name>Vicente Penades</name>
    </author>
    <author>
      <name>Sonia Blasco</name>
    </author>
    <author>
      <name>Javier Chirivella</name>
    </author>
    <author>
      <name>Pablo Gagliardo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on REHAB2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00519v1</id>
    <updated>2015-10-02T08:14:24Z</updated>
    <published>2015-10-02T08:14:24Z</published>
    <title>Interface Between Market and Science</title>
    <summary>  At the beginning, programming was inspired by the search of the best
solutions. At that time some fundamental stones like famous languages and
object oriented and structured programming were laid. It was found later that
applications could generate huge profits; after it marketing departments
started to decide what was right and wrong. Programs are ruled by developers
but declared user-friendly; millions of users are going mad trying to get the
needed results from these applications. Research goes on and new results can be
opposite to business view. History shows that not science has to adjust to
business, but eventually business will have to adapt to the results of the
research work.
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01050v1</id>
    <updated>2015-10-05T07:05:27Z</updated>
    <published>2015-10-05T07:05:27Z</published>
    <title>Learning about End-User Development for Smart Homes by "Eating Our Own
  Dog Food"</title>
    <summary>  SPOK is an End-User Development Environment that permits people to monitor,
control, and configure smart home services and devices. SPOK has been deployed
for more than 4 months in the homes of 5 project team members for testing and
refinement, prior to longitudinal experiments in the homes of families not
involved in the project. This article reports on the lessons learned in this
initial deployment.
</summary>
    <author>
      <name>Joelle Coutaz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRIMA</arxiv:affiliation>
    </author>
    <author>
      <name>James L. Crowley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRIMA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2015, Conference on Computer Human Interaction, Apr 2015,
  Seoul, South Korea. Workshop on End-User Development for IOT Era,.
  \&amp;lt;https://chi2015.acm.org/\&amp;gt;</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.01050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01942v1</id>
    <updated>2015-10-07T13:47:12Z</updated>
    <published>2015-10-07T13:47:12Z</published>
    <title>Helping Domain Experts Build Speech Translation Systems</title>
    <summary>  We present a new platform, "Regulus Lite", which supports rapid development
and web deployment of several types of phrasal speech translation systems using
a minimal formalism. A distinguishing feature is that most development work can
be performed directly by domain experts. We motivate the need for platforms of
this type and discuss three specific cases: medical speech translation,
speech-to-sign-language translation and voice questionnaires. We briefly
describe initial experiences in developing practical systems.
</summary>
    <author>
      <name>Manny Rayner</name>
    </author>
    <author>
      <name>Alejandro Armando</name>
    </author>
    <author>
      <name>Pierrette Bouillon</name>
    </author>
    <author>
      <name>Sarah Ebling</name>
    </author>
    <author>
      <name>Johanna Gerlach</name>
    </author>
    <author>
      <name>Sonia Halimi</name>
    </author>
    <author>
      <name>Irene Strasly</name>
    </author>
    <author>
      <name>Nikos Tsourakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure, to appear in Proc. Future and Emerging Trends in
  Language Technology 2015, Seville, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05879v1</id>
    <updated>2015-10-20T13:16:07Z</updated>
    <published>2015-10-20T13:16:07Z</published>
    <title>What's the point? Frame-wise Pointing Gesture Recognition with
  Latent-Dynamic Conditional Random Fields</title>
    <summary>  We use Latent-Dynamic Conditional Random Fields to perform skeleton-based
pointing gesture classification at each time instance of a video sequence,
where we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently,
we determine continuous time sequences of arbitrary length that form individual
pointing gestures and this way reliably detect pointing gestures at a false
positive detection rate of 0.63%.
</summary>
    <author>
      <name>Christian Wittner</name>
    </author>
    <author>
      <name>Boris Schauerte</name>
    </author>
    <author>
      <name>Rainer Stiefelhagen</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05672v2</id>
    <updated>2016-01-28T21:12:54Z</updated>
    <published>2015-11-18T07:06:55Z</published>
    <title>Could We Distinguish Child Users from Adults Using Keystroke Dynamics?</title>
    <summary>  Significant portion of contemporary computer users are children, who are
vulnerable to threats coming from the Internet. To protect children from such
threats, in this study, we investigate how successfully typing data can be used
to distinguish children from adults. For this purpose, we collect a dataset
comprising keystroke data of 100 users and show that distinguishing child
Internet users from adults is possible using Keystroke Dynamics with equal
error rates less than 10 percent. However the error rates increase
significantly when there are impostors in the system.
</summary>
    <author>
      <name>Yasin Uzun</name>
    </author>
    <author>
      <name>Kemal Bicakci</name>
    </author>
    <author>
      <name>Yusuf Uzunay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05672v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05672v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07500v1</id>
    <updated>2015-11-23T22:49:08Z</updated>
    <published>2015-11-23T22:49:08Z</published>
    <title>Planning in the Wild: Modeling Tools for PDDL</title>
    <summary>  Even though there are sophisticated AI planning algorithms, many integrated,
large-scale projects do not use planning. One reason seems to be the missing
support by engineering tools such as syntax highlighting and visualization. We
propose myPDDL - a modular toolbox for efficiently creating PDDL domains and
problems. To evaluate myPDDL, we compare it to existing knowledge engineering
tools for PDDL and experimentally assess its usefulness for novice PDDL users.
</summary>
    <author>
      <name>Volker Strobel</name>
    </author>
    <author>
      <name>Alexandra Kirsch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-11206-0_27</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-11206-0_27" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Strobel, Volker, and Alexandra Kirsch. "Planning in the Wild:
  Modeling Tools for PDDL." KI 2014: Advances in Artificial Intelligence.
  Springer International Publishing, 2014. 273-284</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.07500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03199v1</id>
    <updated>2015-12-10T10:30:19Z</updated>
    <published>2015-12-10T10:30:19Z</published>
    <title>Graph-theoretic autofill</title>
    <summary>  Imagine a website that asks the user to fill in a web form and -- based on
the input values -- derives a relevant figure, for instance an expected salary,
a medical diagnosis or the market value of a house. How to deal with missing
input values at run-time? Besides using fixed defaults, a more sophisticated
approach is to use predefined dependencies (logical or correlational) between
different fields to autofill missing values in an iterative way. Directed
loopless graphs (in which cycles are allowed) are the ideal mathematical model
to formalize these dependencies. We present two new graph-theoretic approaches
to filling missing values at run-time.
</summary>
    <author>
      <name>Michael Mayer</name>
    </author>
    <author>
      <name>Dominic van der Zypen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01497v1</id>
    <updated>2016-01-07T11:52:47Z</updated>
    <published>2016-01-07T11:52:47Z</published>
    <title>Family of 2-simplex cognitive tools and their application for
  decision-making and its justifications</title>
    <summary>  Urgency of application and development of cognitive graphic tools for usage
in intelligent systems of data analysis, decision making and its justifications
is given. Cognitive graphic tool "2-simplex prism" and examples of its usage
are presented. Specificity of program realization of cognitive graphics tools
invariant to problem areas is described. Most significant results are given and
discussed. Future investigations are connected with usage of new approach to
rendering, cross-platform realization, cognitive features improving and
expanding of n-simplex family.
</summary>
    <author>
      <name>Anna Yankovskaya</name>
    </author>
    <author>
      <name>Artem Yamshanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.01497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01815v1</id>
    <updated>2016-01-08T10:13:55Z</updated>
    <published>2016-01-08T10:13:55Z</published>
    <title>Design and implementation of a user interface for a multi-device
  spatially-aware mobile system</title>
    <summary>  The aim of this thesis was the design and development of an interactive
system enhancing collaborative sensemaking. The system design was based on
related work research and preliminary user study. The system is based on
multiple spatially-aware mobile devices. The spatial awareness is established
with a motion tracking system. The information about position of tablets is
received by a server. The server communicates with the devices and manages the
content of each device. The implemented system supports managing the elements
of information across the devices basing on their relative spatial arrangement.
The evaluation of the system was done by a user study.
</summary>
    <author>
      <name>Przemysław Kucharski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, Lodz University of Technology, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.01815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04066v1</id>
    <updated>2016-01-15T01:04:23Z</updated>
    <published>2016-01-15T01:04:23Z</published>
    <title>Human Computer Symbiosis</title>
    <summary>  Human Computer Symbiosis is similar to Human Computer Interaction in the
sense that it is about how humans and computer interact with each other. For
this interaction to be made there needs to be a symbiotic relationship between
man and computer. Man can interact with computer in many ways, either just by
typing with the keyboard or surfing the web. The cyber-physical-socio space is
an important aspect to be looked into when referring to the interaction between
man and computer. This paper investigates various aspects related to human
computer symbiosis. Alongside the aspects related to the topic, this paper
would also look into the limitations of Human Computer Symbiosis and evaluate
some previously proposed solutions.
</summary>
    <author>
      <name>Eluyefa Olanrewaju Andrew</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06359v1</id>
    <updated>2016-01-24T08:45:10Z</updated>
    <published>2016-01-24T08:45:10Z</published>
    <title>Usability Evaluation of Dwell-free Eye Typing Techniques</title>
    <summary>  Dwelling is an essential task to be performed to select keys from an
on-screen keyboard present in the eye typing interface. This selection task can
be performed by fixing eye gaze on a key for a prolonged time. Spending
sufficient amount of time on each key effectively decreases the overall eye
typing rate. To address the problem, researchers proposed mechanisms, which
diminish the dwell time. We conducted a within-subject usability evaluation of
four dwell-free eye typing techniques. The results of first-time usability
study, longitudinal study and subjective evaluation conducted with 15
participants confirm the superiority of controlled eye movement based advanced
eye typing method (Adv-EyeK) than the other three techniques.
</summary>
    <author>
      <name>Sayan Sarcar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages (with reference), accepted in IDHF 2014 conference held in
  Kochi, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08358v1</id>
    <updated>2016-02-26T15:02:07Z</updated>
    <published>2016-02-26T15:02:07Z</published>
    <title>Remote Heart Rate Sensing and Projection to Renew Traditional Board
  Games and Foster Social Interactions</title>
    <summary>  While physiological sensors enter the mass market and reach the general
public, they are still mainly employed to monitor health -- whether it is for
medical purpose or sports. We describe an application that uses heart rate
feedback as an incentive for social interactions. A traditional board game has
been "augmented" through remote physiological sensing, using webcams.
Projection helped to conceal the technological aspects from users. We detail
how players reacted -- stressful situations could emerge when users are
deprived from their own signals -- and we give directions for game designers to
integrate physiological sensors.
</summary>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc, LaBRI, UB</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2851581.2892391</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2851581.2892391" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI '16 Extended Abstracts, May 2016, San Jose, United States.
  2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08750v1</id>
    <updated>2016-02-28T18:31:31Z</updated>
    <published>2016-02-28T18:31:31Z</published>
    <title>Filtering Video Noise as Audio with Motion Detection to Form a Musical
  Instrument</title>
    <summary>  Even though they differ in the physical domain, digital video and audio share
many characteristics. Both are temporal data streams often stored in buffers
with 8-bit values. This paper investigates a method for creating harmonic
sounds with a video signal as input. A musical instrument is proposed, that
utilizes video in both a sound synthesis method, and in a controller interface
for selecting musical notes at specific velocities. The resulting instrument
was informally determined by the author to sound both pleasant and interesting,
but hard to control, and therefore suited for synth pad sounds.
</summary>
    <author>
      <name>Carl Thomé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Received the 2015 best paper award in the KTH Royal Institute of
  Technology course "Musical Communication and Music Technology"</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01367v1</id>
    <updated>2016-03-04T07:40:58Z</updated>
    <published>2016-03-04T07:40:58Z</published>
    <title>Motivating Healthy Water Intake through Prompting, Historical
  Information, and Implicit Feedback</title>
    <summary>  We describe Hydroprompt, a prototype for sensing and motivating healthy water
intake in work environments. In a 3-week field deployment of Hydroprompt, we
evaluate the effectiveness of three approaches to behavior change: historical
information enabling users to compare their water intake lev- els across
different times of day and days of week, implicit feedback providing subtle
cues to users on the current hydration levels, and explicit prompting at-
tempting to remind participants when hydration falls below acceptable levels or
when substantial amount of time has elapsed since the last sip.
</summary>
    <author>
      <name>Davide Neves</name>
    </author>
    <author>
      <name>Donovan Costa</name>
    </author>
    <author>
      <name>Marcio Oliveira</name>
    </author>
    <author>
      <name>Ruben Jardim</name>
    </author>
    <author>
      <name>Ruben Gouveia</name>
    </author>
    <author>
      <name>Evangelos Karapanos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Adjunct Proceedings of Persuasive Technology 2016, Salzburg,
  Austria</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01369v1</id>
    <updated>2016-03-04T07:45:13Z</updated>
    <published>2016-03-04T07:45:13Z</published>
    <title>Designing for Different Stages in Behavior Change</title>
    <summary>  The behavior change process is a dynamic journey with different informational
and motivational needs across its different stages; yet current technologies
for behavior change are static. In our recent deployment of Habito, an activity
tracking mobile app, we found individuals "readiness" to behavior change (or
the stage of behavior change they were in) to be a strong predictor of
adoption. Individuals in the contemplation and preparation stages had an
adoption rate of 56%, whereas individuals in precontemplation, action or
maintenance stages had an adoption rate of only 20%. In this position paper we
argue for behavior change technologies that are tailored to the different
stages of behavior change.
</summary>
    <author>
      <name>Evangelos Karapanos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the workshop "Personalization in Persuasive
  Technology", Persuasive Technology 2016, Salzburg, Austria</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04581v1</id>
    <updated>2016-03-15T07:36:10Z</updated>
    <published>2016-03-15T07:36:10Z</published>
    <title>Introspectibles: Tangible Interaction to Foster Introspection</title>
    <summary>  Digital devices are now ubiquitous and have the potential to be used to
support positive changes in human lives and promote psychological well-being.
This paper presents three interactive systems that we created focusing on
introspection activities, leveraging tangible interaction and spatial augmented
reality. More specifically, we describe anthropomorphic augmented avatars that
display the users' inner states using physiological sensors. We also present a
first prototype of an augmented sandbox specifically dedicated to promoting
mindfulness activities.
</summary>
    <author>
      <name>Renaud Gervais</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Joan Sol Roo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UB, Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Martin Hachet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in CHI '16 - SIGCHI Conference on Human Factors in Computing System -
  Computing and Mental Health Workshop, May 2016, San Jose, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09533v1</id>
    <updated>2016-03-31T11:36:19Z</updated>
    <published>2016-03-31T11:36:19Z</published>
    <title>VapeTracker: Tracking Vapor Consumption to Help E-cigarette Users Quit</title>
    <summary>  Despite current controversy over e-cigarettes as a smoking cessation aid, we
present early work based on a web survey (N=249) that shows that some
e-cigarette users (46.2%) want to quit altogether, and that behavioral feedback
that can be tracked can fulfill that purpose. Based on our survey findings, we
designed VapeTracker, an early prototype that can attach to any e-cigarette
device to track vaping activity. We discuss our future research on vaping
cessation, addressing how to improve our VapeTracker prototype, ambient
feedback mechanisms, and the future inclusion of behavior change models to
support quitting e-cigarettes.
</summary>
    <author>
      <name>Abdallah El Ali</name>
    </author>
    <author>
      <name>Andrii Matviienko</name>
    </author>
    <author>
      <name>Yannick Feld</name>
    </author>
    <author>
      <name>Wilko Heuten</name>
    </author>
    <author>
      <name>Susanne Boll</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CHI 2016 EA</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.M" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04389v1</id>
    <updated>2016-04-15T07:42:39Z</updated>
    <published>2016-04-15T07:42:39Z</published>
    <title>Composing applications with OntoCompo</title>
    <summary>  In this paper, we present an ontology-based approach to compose applications
while preserving their former look. Our composition process relies on the
manipulation of User Interfaces (UI) and on several ontologies describing
relationships between tasks, UI and Functionalities. Our tool, called
OntoCompo, supports compositions realized by the developer thanks to the
selection, extraction and positioning of UI elements to constitute the
newapplication.
</summary>
    <author>
      <name>Christian Brel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S, SPARKS</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Renevier-Gonin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SPARKS, I3S</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2044354.2044384</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2044354.2044384" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">d\'emonstration \`a la conf\'erence IHM 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IHM 2011, 23\`eme Conf\'erence Francophone Sur l'Interaction Homme
  Machine, Oct 2011, Biot, France. ACM, pp.141-144, 2011, IHM '11 Proceedings
  of the 23rd Conference on l'Interaction Homme-Machine</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.04389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05797v1</id>
    <updated>2016-04-20T02:54:21Z</updated>
    <published>2016-04-20T02:54:21Z</published>
    <title>Towards the Holodeck: Fully Immersive Virtual Reality Visualisation of
  Scientific and Engineering Data</title>
    <summary>  In this paper, we describe the development and operating principles of an
immersive virtual reality (VR) visualisation environment that is designed
around the use of consumer VR headsets in an existing wide area motion capture
suite. We present two case studies in the application areas of visualisation of
scientific and engineering data. Each of these case studies utilise a different
render engine, namely a custom engine for one case and a commercial game engine
for the other. The advantages and appropriateness of each approach are
discussed along with suggestions for future work.
</summary>
    <author>
      <name>Stefan Marks</name>
    </author>
    <author>
      <name>Javier E. Estevez</name>
    </author>
    <author>
      <name>Andy M. Connor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2683405.2683424</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2683405.2683424" rel="related"/>
    <link href="http://arxiv.org/abs/1604.05797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06158v1</id>
    <updated>2016-04-21T02:02:13Z</updated>
    <published>2016-04-21T02:02:13Z</published>
    <title>Augmented Body: Changing Interactive Body Play</title>
    <summary>  This paper investigates the player's body as a system capable of unfamiliar
interactive movement achieved through digital mediation in a playful
environment. Body interactions in both digital and non-digital environments can
be considered as a perceptually manipulative exploration of self. This implies
a player may alter how they perceive their body and its operations in order to
create a new playful and original experience. This paper therefore questions
how player interaction can change as their perception of their body changes
using augmentative technology.
</summary>
    <author>
      <name>Matthew Martin</name>
    </author>
    <author>
      <name>James Charlton</name>
    </author>
    <author>
      <name>Andy M. Connor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2677758.2677790</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2677758.2677790" rel="related"/>
    <link href="http://arxiv.org/abs/1604.06158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06159v1</id>
    <updated>2016-04-21T02:07:27Z</updated>
    <published>2016-04-21T02:07:27Z</published>
    <title>Social Play Spaces for Active Community Engagement</title>
    <summary>  This paper puts forward the perspective that social play spaces are
opportunities to utilise both technology and body for the benefit of community
culture and engagement. Co-located social gaming coupled with tangible
interfaces offer active participant engagement and the development of the local
video game scene. This paper includes a descriptive account of Rabble Room
Arcade, an experimental social event combining custom-built physical interface
devices and multiplayer video games.
</summary>
    <author>
      <name>Jenna Gavin</name>
    </author>
    <author>
      <name>Ben Kenobi</name>
    </author>
    <author>
      <name>Andy M. Connor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2677758.2677789</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2677758.2677789" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1604.05793</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08284v1</id>
    <updated>2016-04-28T01:57:11Z</updated>
    <published>2016-04-28T01:57:11Z</published>
    <title>Talk&amp;Learn: Improving Conversation Experience and Creating Opportunities
  for Foreign Language Learning</title>
    <summary>  Existing Real-Time Translation Interfaces (RTTI) do not provide experience as
natural and efficient as monolingual communication. Also, such systems do not
provide functions supporting language learning. This results in the waste of
both time and potential language context. In order to overcome the above
limitations, we propose a solution named "Talk&amp;Learn". Its core idea is to
rearrange ("Delay-Match") the real-time videos and translated texts or
speeches, so as to gain better naturalness and efficiency. At the same time,
this will create extra free time for users. So we further propose to utilize
the free time for contextual language learning.
</summary>
    <author>
      <name>Yaohua Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07733v1</id>
    <updated>2016-05-25T04:57:42Z</updated>
    <published>2016-05-25T04:57:42Z</published>
    <title>On model architecture for a children's speech recognition interactive
  dialog system</title>
    <summary>  This report presents a general model of the architecture of information
systems for the speech recognition of children. It presents a model of the
speech data stream and how it works. The result of these studies and presented
veins architectural model shows that research needs to be focused on
acoustic-phonetic modeling in order to improve the quality of children's speech
recognition and the sustainability of the systems to noise and changes in
transmission environment. Another important aspect is the development of more
accurate algorithms for modeling of spontaneous child speech.
</summary>
    <author>
      <name>Radoslava Kraleva</name>
    </author>
    <author>
      <name>Velin Kralev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, in proc. of conference FMNS 2009, Blagoevgrad,
  Bulgaria</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Third International Scientific Conference "Mathematics and Natural
  Sciences", Vol. (1), pp. 106-111, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.07733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07760v1</id>
    <updated>2016-05-25T07:26:02Z</updated>
    <published>2016-05-25T07:26:02Z</published>
    <title>Challenges in Mobile Multi-Device Ecosystems</title>
    <summary>  Coordinated multi-display environments from the desktop, second-screen to
gigapixel display walls are increasingly common. Personal and intimate mobile
and wearable devices such as head-mounted displays, smartwatches, smartphones
and tablets are rarely part of such multi-device ecosystems. With this paper,
we contribute to a better understanding about factors that impede the creation
and use of such mobile multi-device ecosystems. We base our findings on
literature research and an expert survey. Specifically, we present grounded
challenges relevant for the design, development and use of mobile multi-device
environments.
</summary>
    <author>
      <name>Jens Grubert</name>
    </author>
    <author>
      <name>Matthias Kranz</name>
    </author>
    <author>
      <name>Aaron Quigley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s13678-016-0007-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s13678-016-0007-y" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">mUX: The Journal of Mobile User Experience, 5(1), 1-22, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.07760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08035v1</id>
    <updated>2016-05-25T12:01:46Z</updated>
    <published>2016-05-25T12:01:46Z</published>
    <title>Notes on Pervasive Virtuality</title>
    <summary>  This paper summarizes current notes about a new mixed-reality paradigm that
we named as "pervasive virtuality". This paradigm has emerged recently in
industry and academia through different initiatives. In this paper we intend to
explore this new area by proposing a set of features that we identified as
important or helpful to realize pervasive virtuality in games and entertainment
applications.
</summary>
    <author>
      <name>Luis Valente</name>
    </author>
    <author>
      <name>Bruno Feijo</name>
    </author>
    <author>
      <name>Alexandre Ribeiro Silva</name>
    </author>
    <author>
      <name>Esteban Clua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report MCC01/16 (Monografias em Ci\^encia da Computa\c{c}\~ao,
  May 2016, PUC-Rio, ISSN 0103-9741), discussion paper in progress to IFIP ICEC
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02711v1</id>
    <updated>2016-06-08T14:21:53Z</updated>
    <published>2016-06-08T14:21:53Z</published>
    <title>ChinMotion Rapidly Enables 3D Computer Interaction after Tetraplegia</title>
    <summary>  Individuals with severe paralysis require hands-free interfaces to control
assistive devices that can improve their quality of life. We present
ChinMotion, an interface that noninvasively harnesses preserved chin, lip and
tongue sensorimotor function after tetraplegia to convey intuitive control
commands. After two hours of practice, ChinMotion enables superior
point-and-click performance over existing interfaces and it facilitates
accurate 3D control of a virtual robotic arm.
</summary>
    <author>
      <name>Ferran Galán</name>
    </author>
    <author>
      <name>Stuart N. Baker</name>
    </author>
    <author>
      <name>Monica A. Perez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The .ps file contains main manuscript and supplementary information.
  The .ps file is accompanied with ancillary files (supplementary files)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07487v2</id>
    <updated>2016-07-03T20:04:55Z</updated>
    <published>2016-06-23T21:36:36Z</published>
    <title>The VGLC: The Video Game Level Corpus</title>
    <summary>  Levels are a key component of many different video games, and a large body of
work has been produced on how to procedurally generate game levels. Recently,
Machine Learning techniques have been applied to video game level generation
towards the purpose of automatically generating levels that have the properties
of the training corpus. Towards that end we have made available a corpora of
video game levels in an easy to parse format ideal for different machine
learning and other game AI research purposes.
</summary>
    <author>
      <name>Adam James Summerville</name>
    </author>
    <author>
      <name>Sam Snodgrass</name>
    </author>
    <author>
      <name>Michael Mateas</name>
    </author>
    <author>
      <name>Santiago Ontañón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proceedings of the 7th Workshop on Procedural Content
  Generation</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08065v2</id>
    <updated>2018-03-13T20:16:55Z</updated>
    <published>2016-06-26T18:40:57Z</published>
    <title>Face Card: An Information-sharing Framework on Google Glass</title>
    <summary>  Wearable devices such as Google Glass can provide an efficient way to get
around users information. We present Face Card, a system builds on Google Glass
to provide information-sharing service with around people. With a look at
Google Glass, users can quickly get information of nearby and coming users.
Utilizing Bluetooth Low Energy (BLE) and proper user interface, Face Card
demonstrates the potential of being an efficient information sharing system
framework.
</summary>
    <author>
      <name>Weiren Wang</name>
    </author>
    <author>
      <name>Miseon Park</name>
    </author>
    <author>
      <name>Yuanzhe Fan</name>
    </author>
    <author>
      <name>Thad Starner</name>
    </author>
    <author>
      <name>Gregory D. Abowd</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper needs to be revised. I need to discuss this my professors</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01752v1</id>
    <updated>2016-07-06T19:26:20Z</updated>
    <published>2016-07-06T19:26:20Z</published>
    <title>CrowdCafe - Mobile Crowdsourcing Platform</title>
    <summary>  In this paper we present a mobile crowdsourcing platform CrowdCafe, where
people can perform microtasks using their smartphones while they ride a bus,
travel by train, stand in a queue or wait for an appointment. These microtasks
are executed in exchange for rewards provided by local stores, such as coffee,
desserts and bus tickets. We present the concept, the implementation and the
evaluation by conducting a study with 52 participants, having 1108 tasks
completed.
</summary>
    <author>
      <name>Pavel Kucherbaev</name>
    </author>
    <author>
      <name>Azad Abad</name>
    </author>
    <author>
      <name>Stefano Tranquillini</name>
    </author>
    <author>
      <name>Florian Daniel</name>
    </author>
    <author>
      <name>Maurizio Marchese</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Was published before as a part of the phd thesis by Pavel Kucherbaev
  http://eprints-phd.biblio.unitn.it/1716/</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00945v2</id>
    <updated>2016-10-27T01:03:25Z</updated>
    <published>2016-09-04T15:20:44Z</published>
    <title>MmmTurkey: A Crowdsourcing Framework for Deploying Tasks and Recording
  Worker Behavior on Amazon Mechanical Turk</title>
    <summary>  Internal HITs on Mechanical Turk can be programmatically restrictive, and as
a result, many requesters turn to using external HITs as a more flexible
alternative. However, creating such HITs can be redundant and time-consuming.
We present MmmTurkey, a framework that enables researchers to not only quickly
create and manage external HITs, but more significantly also capture and record
detailed worker behavioral data characterizing how each worker completes a
given task.
</summary>
    <author>
      <name>Brandon Dang</name>
    </author>
    <author>
      <name>Miles Hutson</name>
    </author>
    <author>
      <name>Matt Lease</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00945v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00945v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01070v1</id>
    <updated>2016-09-05T09:17:38Z</updated>
    <published>2016-09-05T09:17:38Z</published>
    <title>Toward Crowdsourced User Studies for Software Evaluation</title>
    <summary>  This work-in-progress paper describes a vision, i.e., that of fast and
reliable software user experience studies conducted with the help from the
crowd. Commonly, user studies are controlled in-lab activities that require the
instruction, monitoring, interviewing and compensation of a number of
participants that are typically hard to recruit. The goal of this work is to
study which user study methods can instead be crowdsourced to generic audiences
to enable the conduct of user studies without the need for expensive lab
experiments. The challenge is understanding how to conduct crowdsourced studies
without giving up too many of the guarantees in-lab settings are able to
provide.
</summary>
    <author>
      <name>Florian Daniel</name>
    </author>
    <author>
      <name>Pavel Kucherbaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Works-in-Progress paper of HCOMP 2016, Austin, Texas</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01348v1</id>
    <updated>2016-09-05T22:52:02Z</updated>
    <published>2016-09-05T22:52:02Z</published>
    <title>Incentive Engineering Framework for Crowdsourcing Systems</title>
    <summary>  Significant effort has been made to understand user motivation and to elicit
user participation in crowdsourcing systems. However, incentive engineering,
i.e., designing incentives that can purposefully motivate users, is still an
open question and remains one of the key challenges of crowdsourcing
initiatives. In this work in progress, we propose a general and systematic
incentive engineering framework that system designers can use to implement
appropriate incentives in order to effect desirable user behaviours.
</summary>
    <author>
      <name>Nhat V. Q. Truong</name>
    </author>
    <author>
      <name>Sebastian Stein</name>
    </author>
    <author>
      <name>Long Tran-Thanh</name>
    </author>
    <author>
      <name>Nicholas R. Jennings</name>
    </author>
    <link href="http://arxiv.org/abs/1609.01348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01614v1</id>
    <updated>2016-09-06T15:42:56Z</updated>
    <published>2016-09-06T15:42:56Z</published>
    <title>Modeling The Adaption Rule in Context-aware Systems</title>
    <summary>  Context awareness is increasingly gaining applicability in interactive
ubiquitous mobile computing systems. Each context-aware application has its own
set of behaviors to react to context modifications. This paper is concerned
with the context modeling and the development methodology for context-aware
systems. We proposed a rule-based approach and use the adaption tree to model
the adaption rule of context-aware systems. We illustrate this idea in an
arithmetic game application.
</summary>
    <author>
      <name>Mao Zheng</name>
    </author>
    <author>
      <name>Qian Xu</name>
    </author>
    <author>
      <name>Hao Fan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijasuc.2016.7401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijasuc.2016.7401" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 tables, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Ad hoc, Sensor &amp; Ubiquitous Computing,
  Vol.7, No.3/4, August 2016, ISSN : 0976 - 1764 (Online); 0976 - 2205 (Print),</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.01614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02182v1</id>
    <updated>2016-09-07T20:41:57Z</updated>
    <published>2016-09-07T20:41:57Z</published>
    <title>Feedback and Timing in a Crowdsourcing Game</title>
    <summary>  The present research examines two problems inherent to the creation of
crowdsourcing games: how to give feedback when the right answer is not always
known by the game and how much time to give players without sacrificing data
quality. Taken together, the present research provides an important first step
in considering how to create fun, challenging crowdsourcing games that generate
quality data.
</summary>
    <author>
      <name>Gili Freedman</name>
    </author>
    <author>
      <name>Sukdith Punjasthitkul</name>
    </author>
    <author>
      <name>Max Seidman</name>
    </author>
    <author>
      <name>Mary Flanagan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented as Work in Progress Poster at Human Computation 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03050v1</id>
    <updated>2016-09-10T13:22:14Z</updated>
    <published>2016-09-10T13:22:14Z</published>
    <title>Dropout Prediction in Crowdsourcing Markets</title>
    <summary>  Crowdsourcing environments have shown promise in solving diverse tasks in
limited cost and time. This type of business model involves both the expert and
non-expert workers. Interestingly, the success of such models depends on the
volume of the total number of workers. But, the survival of the fittest
controls the stability of these workers. Here, we show that the crowd workers
who fail to win jobs successively loose interest and might dropout over time.
Therefore, dropout prediction in such environments is a promising task. In this
paper, we establish that it is possible to predict the dropouts in a
crowdsourcing market from the success rate based on the arrival pattern of
workers.
</summary>
    <author>
      <name>Malay Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Works in Progress, Third AAAI Conference on Human Computation and
  Crowdsourcing (HCOMP 2015), San Diego, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04657v1</id>
    <updated>2016-09-15T14:06:57Z</updated>
    <published>2016-09-15T14:06:57Z</published>
    <title>Results of a Collective Awareness Platforms Investigation</title>
    <summary>  In this paper we provide two introductory analyses of CAPs, based exclusively
on the analysis of documents found on the Internet. The first analysis allowed
us to investigate the world of CAPs, in particular for what concerned their
status (dead or alive), the scope of those platforms and the typology of users.
In order to develop a more accurate model of CAPs, and to understand more
deeply the motivation of the users and the type of expected payoff, we analysed
those CAPs from the above list that are still alive and we used two models
developed for what concerned the virtual community and the collective
intelligence.
</summary>
    <author>
      <name>Giovanna Pacini</name>
    </author>
    <author>
      <name>Franco Bagnoli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-45982-0_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-45982-0_2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">INSCI 2016, LNCS 9934, pp. 19-26, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.04657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02445v1</id>
    <updated>2016-10-07T23:29:23Z</updated>
    <published>2016-10-07T23:29:23Z</published>
    <title>Instagram Post Data Analysis</title>
    <summary>  Because of the spread of the Internet, social platforms become big data
pools. From there we can learn about the trends, culture and hot topics. This
project focuses on analyzing the data from Instagram. It shows the relationship
of Instagram filter data with location and number of likes to give users filter
suggestion on achieving more likes based on their location. It also analyzes
the popular hashtags in different locations to show visual culture differences
between different cities.
</summary>
    <author>
      <name>Steve Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02857v1</id>
    <updated>2016-10-10T11:44:30Z</updated>
    <published>2016-10-10T11:44:30Z</published>
    <title>Accounting for Availability Biases in Information Visualization</title>
    <summary>  The availability heuristic is a strategy that people use to make quick
decisions but often lead to systematic errors. We propose three ways that
visualization could facilitate unbiased decision-making. First, visualizations
can alter the way our memory stores the events for later recall, so as to
improve users' long-term intuitions. Second, the known biases could lead to new
visualization guidelines. Third, we suggest the design of decision-making tools
that are inspired by heuristics, e.g. suggesting intuitive approximations,
rather than target to present exhaustive comparisons of all possible outcomes,
or automated solutions for choosing decisions.
</summary>
    <author>
      <name>Evanthia Dimara</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AVIZ</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Dragicevic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AVIZ</arxiv:affiliation>
    </author>
    <author>
      <name>Anastasia Bezerianos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI, ILDA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VIS 2014, 2014, Paris, France. 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03704v1</id>
    <updated>2016-10-12T13:28:14Z</updated>
    <published>2016-10-12T13:28:14Z</published>
    <title>A low-cost indoor and outdoor terrestrial autonomous navigation model</title>
    <summary>  In this paper, a method for low-cost system design oriented to indoor and
outdoor autonomous navigation is illustrated. In order to provide a motivation
for the solution here presented, a brief discussion of the typical drawbacks of
state-of-the-art technologies is reported. Finally, an application of such a
method for the design of a navigation system for blindfolded people is shown.
</summary>
    <author>
      <name>Gianluca Susi</name>
    </author>
    <author>
      <name>Alessandro Cristini</name>
    </author>
    <author>
      <name>Mario Salerno</name>
    </author>
    <author>
      <name>Emiliano Daddario</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TELFOR.2014.7034499</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TELFOR.2014.7034499" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 2 tables. Presented at IEEE TELFOR 22nd
  Telecommunications Forum, Belgrade, Serbia, 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE TELFOR 22nd Telecommunications Forum, Belgrade, Serbia, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.03704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.2; D.2.10; C.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04758v1</id>
    <updated>2016-10-15T16:28:17Z</updated>
    <published>2016-10-15T16:28:17Z</published>
    <title>Sensing Emotions in Text Messages: An Application and Deployment Study
  of EmotionPush</title>
    <summary>  Instant messaging and push notifications play important roles in modern
digital life. To enable robust sense-making and rich context awareness in
computer mediated communications, we introduce EmotionPush, a system that
automatically conveys the emotion of received text with a colored push
notification on mobile devices. EmotionPush is powered by state-of-the-art
emotion classifiers and is deployed for Facebook Messenger clients on Android.
The study showed that the system is able to help users prioritize interactions.
</summary>
    <author>
      <name>Shih-Ming Wang</name>
    </author>
    <author>
      <name>Chun-Hui Li</name>
    </author>
    <author>
      <name>Yu-Chun Lo</name>
    </author>
    <author>
      <name>Ting-Hao K. Huang</name>
    </author>
    <author>
      <name>Lun-Wei Ku</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. COLING 2016 Demo paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09743v1</id>
    <updated>2016-10-31T00:19:00Z</updated>
    <published>2016-10-31T00:19:00Z</published>
    <title>An Exploration of Graphical Password Authentication for Children</title>
    <summary>  In this paper, we explore graphical passwords as a child-friendly alternative
for user authentication. We evaluate the usability of three variants of the
PassTiles graphical password scheme for children, and explore the similarities
and differences in performance and preferences between children and adults
while using these schemes. Children were most successful at recalling passwords
containing images of distinct objects. Both children and adults prefer
graphical passwords to their existing schemes, but password memorization
strategies differ considerably between the two groups. Based on our findings,
we provide recommendations for designing more child-friendly authentication
schemes.
</summary>
    <author>
      <name>Hala Assal</name>
    </author>
    <author>
      <name>Ahsan Imran</name>
    </author>
    <author>
      <name>Sonia Chiasson</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.6.5; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07714v1</id>
    <updated>2016-12-22T17:28:38Z</updated>
    <published>2016-12-22T17:28:38Z</published>
    <title>Understanding Tree: a tool to estimate one's understanding of knowledge</title>
    <summary>  People learn whenever and wherever possible, and whatever they like or
encounter--Mathematics, Drama, Art, Languages, Physics, Philosophy, and so on.
With the bursting of knowledge, evaluation of one's possession of knowledge
becomes increasingly difficult. There are a lot of demands to evaluate one's
understanding of a piece of knowledge. Assessment of understanding of knowledge
is conventionally through tests or interviews, but they have some limitations
such as low-efficiency and not-comprehensive. This paper proposes a method
called Understanding Tree to estimate one's understanding of knowledge, by
keeping track of his/her learning activities. It overcomes some limitations of
traditional methods, hence complements traditional methods.
</summary>
    <author>
      <name>Gangli Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1612.07714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.M" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00487v1</id>
    <updated>2017-01-02T12:16:11Z</updated>
    <published>2017-01-02T12:16:11Z</published>
    <title>The leveled approach. Using and evaluating text mining tools
  AVResearcherXL and Texcavator for historical research on public perceptions
  of drugs</title>
    <summary>  We introduce our explorative historical leveled approach that we use to
understand drug debates in the Royal Dutch Library's digital newspaper archive.
In this approach we alternate between distant reading and close reading.
Furthermore, we use this approach to evaluate two text mining tools:
AVResearcherXL and Texcavator.
</summary>
    <author>
      <name>Berrie van der Molen</name>
    </author>
    <author>
      <name>Lars Buitinck</name>
    </author>
    <author>
      <name>Toine Pieters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, extended abstract of a lightning talk delivered at the 2nd
  IFIP International Workshop on Computational History and Data-driven
  Humanities on 25 May 2016 (Trinity College Dublin, Ireland)</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.00487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06270v2</id>
    <updated>2017-01-24T19:24:11Z</updated>
    <published>2017-01-23T05:45:55Z</published>
    <title>Plexus: An Interactive Visualization Tool for Analyzing Public Emotions
  from Twitter Data</title>
    <summary>  Social media is often used by researchers as an approach to obtaining
real-time data on people's activities and thoughts. Twitter, as one of the most
popular social networking services nowadays, provides copious information
streams on various topics and events. Mining and analyzing Tweets enable us to
find public reactions and emotions to activities or objects. This paper
presents an interactive visualization tool that identifies and visualizes
people's emotions on any two related topics by streaming and processing data
from Twitter. The effectiveness of this visualization was evaluated and
demonstrated by a feasibility study with 14 participants.
</summary>
    <author>
      <name>Xiaodong Wu</name>
    </author>
    <author>
      <name>Lyn Bartram</name>
    </author>
    <author>
      <name>Chris Shaw</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, Conference ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08879v1</id>
    <updated>2017-01-31T00:24:10Z</updated>
    <published>2017-01-31T00:24:10Z</published>
    <title>Robotic Haptic Proxies for Collaborative Virtual Reality</title>
    <summary>  We propose a new approach for interaction in Virtual Reality (VR) using
mobile robots as proxies for haptic feedback. This approach allows VR users to
have the experience of sharing and manipulating tangible physical objects with
remote collaborators. Because participants do not directly observe the robotic
proxies, the mapping between them and the virtual objects is not required to be
direct. In this paper, we describe our implementation, various scenarios for
interaction, and a preliminary user study.
</summary>
    <author>
      <name>Zhenyi He</name>
    </author>
    <author>
      <name>Fengyuan Zhu</name>
    </author>
    <author>
      <name>Aaron Gaudette</name>
    </author>
    <author>
      <name>Ken Perlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 page, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02178v1</id>
    <updated>2017-02-07T19:42:17Z</updated>
    <published>2017-02-07T19:42:17Z</published>
    <title>Refining StreamBED Through Expert Interviews, Design Feedback, and a Low
  Fidelity Prototype</title>
    <summary>  StreamBED is an embodied VR training for citizen scientists to make
qualitative stream assessments. Early findings garnered positive feedback about
training qualitative assessment using a virtual representation of different
stream spaces, but presented field-specific challenges; novice biologists had
trouble interpreting qualitative protocols, and needed substantive guidance to
look for and interpret environment cues. In order to address these issues in
the redesign, this work uses research through design (RTD) methods to consider
feedback from expert stream biologists, firsthand stream monitoring experience,
discussions with education and game designers, and feedback from a low fidelity
prototype. The qualitative findings found that training should facilitate
personal narratives, maximize realism, and should use social dynamics to
scaffold learning.
</summary>
    <author>
      <name>Alina Striner</name>
    </author>
    <author>
      <name>Jennifer Preece</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06236v1</id>
    <updated>2017-02-21T01:45:07Z</updated>
    <published>2017-02-21T01:45:07Z</published>
    <title>Transitioning Between Audience and Performer: Co-Designing Interactive
  Music Performances with Children</title>
    <summary>  Live interactions have the potential to meaningfully engage audiences during
musical performances, and modern technologies promise unique ways to facilitate
these interactions. This work presents findings from three co-design sessions
with children that investigated how audiences might want to interact with live
music performances, including design considerations and opportunities. Findings
from these sessions also formed a Spectrum of Audience Interactivity in live
musical performances, outlining ways to encourage interactivity in music
performances from the child perspective.
</summary>
    <author>
      <name>Alina Striner</name>
    </author>
    <author>
      <name>Brenna McNally</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3027063.3053171</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3027063.3053171" rel="related"/>
    <link href="http://arxiv.org/abs/1702.06236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07099v2</id>
    <updated>2017-02-24T18:52:52Z</updated>
    <published>2017-02-23T05:22:16Z</published>
    <title>Carina: Interactive Million-Node Graph Visualization using Web Browser
  Technologies</title>
    <summary>  We are working on a scalable, interactive visualization system, called
Carina, for people to explore million-node graphs. By using latest web browser
technologies, Carina offers fast graph rendering via WebGL, and works across
desktop (via Electron) and mobile platforms. Different from most existing graph
visualization tools, Carina does not store the full graph in RAM, enabling it
to work with graphs with up to 69M edges. We are working to improve and
open-source Carina, to offer researchers and practitioners a new, scalable way
to explore and visualize large graph datasets.
</summary>
    <author>
      <name>Dezhi Fang</name>
    </author>
    <author>
      <name>Matthew Keezer</name>
    </author>
    <author>
      <name>Jacob Williams</name>
    </author>
    <author>
      <name>Kshitij Kulkarni</name>
    </author>
    <author>
      <name>Robert Pienta</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3041021.3054234</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3041021.3054234" rel="related"/>
    <link href="http://arxiv.org/abs/1702.07099v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07099v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07480v1</id>
    <updated>2017-02-24T07:22:37Z</updated>
    <published>2017-02-24T07:22:37Z</published>
    <title>Automation in Human-Machine Networks: How Increasing Machine Agency
  Affects Human Agency</title>
    <summary>  Efficient human-machine networks require productive interaction between human
and machine actors. In this study, we address how a strengthening of machine
agency, for example through increasing levels of automation, affect the human
actors of the networks. Findings from case studies within air traffic
management, crisis management, and crowd evacuation are presented, exemplifying
how automation may strengthen the agency of human actors in the network through
responsibility sharing and task allocation, and serve as a needed prerequisite
of innovation and change.
</summary>
    <author>
      <name>Asbjørn Følstad</name>
    </author>
    <author>
      <name>Vegard Engen</name>
    </author>
    <author>
      <name>Ida Maria Haugstveit</name>
    </author>
    <author>
      <name>Brian Pickering</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00549v2</id>
    <updated>2018-06-19T03:52:00Z</updated>
    <published>2017-03-01T23:41:57Z</published>
    <title>Statistical Verification of Computational Rapport Model</title>
    <summary>  Rapport plays an important role during communication because it can help
people understand each other's feelings or ideas and leads to a smooth
communication. Computational rapport model has been proposed based on theory in
previous work. But there lacks solid verification. In this paper, we apply
structural equation model (SEM) to the theoretical model on both dyads of
friend and stranger. The results indicate some unfavorable paths. Based on the
results and more literature, we modify the original model to integrate more
nonverbal behaviors, including gaze and smile. Fit indices and other
examination show the goodness of our new models, which can give us more insight
into rapport management during conversation.
</summary>
    <author>
      <name>Xuhai Xu</name>
    </author>
    <author>
      <name>Justine Cassell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Incomplete project</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00549v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00549v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00818v1</id>
    <updated>2017-03-02T15:01:59Z</updated>
    <published>2017-03-02T15:01:59Z</published>
    <title>Evaluating Singleplayer and Multiplayer in Human Computation Games</title>
    <summary>  Human computation games (HCGs) can provide novel solutions to intractable
computational problems, help enable scientific breakthroughs, and provide
datasets for artificial intelligence. However, our knowledge about how to
design and deploy HCGs that appeal to players and solve problems effectively is
incomplete. We present an investigatory HCG based on Super Mario Bros. We used
this game in a human subjects study to investigate how different social
conditions---singleplayer and multiplayer---and scoring
mechanics---collaborative and competitive---affect players' subjective
experiences, accuracy at the task, and the completion rate. In doing so, we
demonstrate a novel design approach for HCGs, and discuss the benefits and
tradeoffs of these mechanics in HCG design.
</summary>
    <author>
      <name>Kristin Siu</name>
    </author>
    <author>
      <name>Matthew Guzdial</name>
    </author>
    <author>
      <name>Mark O. Riedl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01377v1</id>
    <updated>2017-03-04T01:46:52Z</updated>
    <published>2017-03-04T01:46:52Z</published>
    <title>Learning styles: Literature versus machine learning</title>
    <summary>  Every teacher understands that different students benefit from different
activities. Recent advances in data processing allow us to detect and use
behavioral variability for adapting to a student. This approach allows us to
optimize learning process but does not focus on understanding it. Conversely,
classical findings in educational sciences allow us to understand the learner
but are hard to embed in a large scale adaptive system. In this study we design
and build a framework to investigate when the two approaches coincide.
</summary>
    <author>
      <name>Farah Bouassida</name>
    </author>
    <author>
      <name>Łukasz Kidziński</name>
    </author>
    <author>
      <name>Pierre Dillenbourg</name>
    </author>
    <link href="http://arxiv.org/abs/1703.01377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02968v1</id>
    <updated>2017-03-08T18:16:21Z</updated>
    <published>2017-03-08T18:16:21Z</published>
    <title>Sigil3D: A Crowdsourcing Platform for Interactive 3D Content</title>
    <summary>  In this paper we propose applying the crowdsourcing approach to a software
platform that uses a modern and state-of-the-art 3D game engine. This platform
could facilitate the generation and manipulation of interactive 3D environments
by a community of users producing different content such as cultural heritage,
scientific virtual labs, games, novel art forms and virtual museums.
</summary>
    <author>
      <name>Andrea Barillari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Daniele Bernardini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Pierluigi Crescenzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Università di Firenze, Italy</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">translated from the paper published in the conference proceedings for
  GARR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09847v2</id>
    <updated>2017-04-18T03:50:19Z</updated>
    <published>2017-03-29T00:27:01Z</published>
    <title>Designing Privacy for You : A User Centric Approach For Privacy</title>
    <summary>  Privacy directly concerns the user as the data owner (data- subject) and
hence privacy in systems should be implemented in a manner which concerns the
user (user-centered). There are many concepts and guidelines that support
development of privacy and embedding privacy into systems. However, none of
them approaches privacy in a user- centered manner. Through this research we
propose a framework that would enable developers and designers to grasp privacy
in a user-centered manner and implement it along with the software development
life cycle.
</summary>
    <author>
      <name>Awanthika Senarath</name>
    </author>
    <author>
      <name>Nalin A. G. Arachchilage</name>
    </author>
    <author>
      <name>Jill Slay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, HCI International 2017 Vancouver, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09847v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09847v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03507v1</id>
    <updated>2017-05-09T19:34:17Z</updated>
    <published>2017-05-09T19:34:17Z</published>
    <title>Analysis of Information Technologies Used to Insure Working Efficiency
  of Personnel</title>
    <summary>  The work is devoted to a modern state, methods and tools of monitoring,
assessment and prediction of the indicators showing physical condition of a
person and his/her capabilities to perform work duties. The work contains an
analysis of existing gadgets and software that allow tracking physical
condition of personnel at the working place. The analysis showing significant
interconnections and factors that determine a necessary level of working
capacity and productivity of personnel allows organizing Work &amp; Rest Schedule
of employees in an effective manner.
</summary>
    <author>
      <name>V. Ya. Vilisov</name>
    </author>
    <author>
      <name>D. A. Dyatlova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07490v1</id>
    <updated>2017-05-21T19:08:09Z</updated>
    <published>2017-05-21T19:08:09Z</published>
    <title>MindDesktop: a general purpose brain computer interface</title>
    <summary>  Recent advances in electroencephalography (EEG) and electromyography (EMG)
enable communication for people with severe disabilities. In this paper we
present a system that enables the use of regular computers using an
off-the-shelf EEG/EMG headset, providing a pointing device and virtual keyboard
that can be used to operate any Windows based system, minimizing the user
effort required for interacting with a personal computer. Effectiveness of the
proposed system is evaluated by a usability study, indicating decreasing
learning curve for completing various tasks. The proposed system is available
in the link provided.
</summary>
    <author>
      <name>Ori Ossmy</name>
    </author>
    <author>
      <name>Ofir Tam</name>
    </author>
    <author>
      <name>Rami Puzis</name>
    </author>
    <author>
      <name>Lior Rokach</name>
    </author>
    <author>
      <name>Ohad Inbar</name>
    </author>
    <author>
      <name>Yuval Elovici</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01248v1</id>
    <updated>2017-06-05T09:23:47Z</updated>
    <published>2017-06-05T09:23:47Z</published>
    <title>Aiding autobiographic memory by using wearable devices</title>
    <summary>  In this paper, we investigate the effectiveness of two distinct techniques
(Special Moment Approach &amp; Spatial Frequency Approach) for reviewing the
lifelogs, which were collected by lifeloggers who were willing to use a
wearable camera and a bracelet simultaneously for two days. Generally, Special
moment approach is a technique for extracting episodic events and Spatial
frequency approach is a technique for associating visual with temporal and
location information, especially heat map is applied as the spatial data for
expressing frequency awareness. Based on that, the participants were asked to
fill in two post-study questionnaires for evaluating the effectiveness of those
two techniques and their combination. The preliminary result showed the
positive potential of exploring individual lifelogs using our approaches.
</summary>
    <author>
      <name>Jingyi Wang</name>
    </author>
    <author>
      <name>Jiro Tanaka</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01523v1</id>
    <updated>2017-06-05T20:05:10Z</updated>
    <published>2017-06-05T20:05:10Z</published>
    <title>Cognitive Depletion in the Wild: a Case Study of NMR Spectroscopy
  Analysis</title>
    <summary>  NMR spectroscopy analysis is a detail-oriented analytic feat that typically
requires specific domain expertise and hours of concentration. This work
presents an ethnographic-style study of this analysis process in the context of
evaluating the symptoms of cognitive depletion. The repeated, non-trivial
decisions required by and the time-consuming nature of NMR spectroscopy
analysis make it an ideal, real-world scenario to study the symptoms of
cognitive depletion, its effect on workflow and performance, and potential
strategies for mitigating its deleterious effects.
</summary>
    <author>
      <name>Lyndsey Franklin</name>
    </author>
    <author>
      <name>Nathan Hodas</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01919v1</id>
    <updated>2017-06-06T18:31:30Z</updated>
    <published>2017-06-06T18:31:30Z</published>
    <title>Understanding Cognitive Depletion in Novice NMR Analysts</title>
    <summary>  We present the results of a user study with novice NMR analysts (N=19)
involving a gamified simulation of the NMR analysis process. Participants
solved randomly generated spectrum puzzles for up to three hours. We used eye
tracking, event logging, and observations to record symptoms of cognitive
depletion while participants worked. Analysis of results indicate that we can
detect both signs of learning and signs of cognitive depletion in participants
over the course of the three hours. Participants' break strategies did not
predict or reflect game scores, but certain symptoms appear predictive of
breaks.
</summary>
    <author>
      <name>Lyndsey Franklin</name>
    </author>
    <author>
      <name>Kyungsik Han</name>
    </author>
    <author>
      <name>Zhuanyi Huang</name>
    </author>
    <author>
      <name>Dustin Arendt</name>
    </author>
    <author>
      <name>Nathan Hodas</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05718v1</id>
    <updated>2017-06-18T20:25:47Z</updated>
    <published>2017-06-18T20:25:47Z</published>
    <title>An exploration to visualize finite element data with a DSL</title>
    <summary>  The scientific community use PDEs to model a range of problems. The people in
this domain are interested in visualizing their results, but existing
mechanisms for visualization can not handle the full richness of computations
in the domain. We did an exploration to see how Diderot, a domain specific
language for scientific visualization and image analysis, could be used to
solve this problem.
  We demonstrate our first and modest approach of visualizing FE data with
Diderot and provide examples. Using Diderot, we do a simple sampling and a
volume rendering of a FE field. These examples showcase Diderot's ability to
provide a visualization result for Firedrake. This paper describes the
extension of the Diderot language to include FE data.
</summary>
    <author>
      <name>Charisee Chiw</name>
    </author>
    <author>
      <name>Gordon Kindlmann</name>
    </author>
    <author>
      <name>John Reppy</name>
    </author>
    <link href="http://arxiv.org/abs/1706.05718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.10060v1</id>
    <updated>2017-06-30T08:35:06Z</updated>
    <published>2017-06-30T08:35:06Z</published>
    <title>Turned 70? It is time to start editing Wikipedia</title>
    <summary>  Success of Wikipedia would not be possible without the contributions of
millions of anonymous Internet users who edit articles, correct mistakes, add
links or pictures. At the same time Wikipedia editors are currently overworked
and there is always more tasks waiting to be completed than people willing to
volunteer. The paper explores the possibility of involving the elderly in the
Wikipedia editing process. Older adults were asked to complete various tasks on
Wikipedia. Based on the observations made during these activities as well as
in-depth interviews, a list of recommendation has been crafted. It turned out
that older adults are willing to contribute to Wikiepdia but substantial
changes have to be made in the Wikipedia editor.
</summary>
    <author>
      <name>Radoslaw Nielek</name>
    </author>
    <author>
      <name>Marta Lutostanska</name>
    </author>
    <author>
      <name>Wieslaw Kopec</name>
    </author>
    <author>
      <name>Adam Wierzbicki</name>
    </author>
    <link href="http://arxiv.org/abs/1706.10060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02654v1</id>
    <updated>2017-07-09T22:45:43Z</updated>
    <published>2017-07-09T22:45:43Z</published>
    <title>Vision-Based Classification of Social Gestures in Videochat Sessions</title>
    <summary>  This paper describes the design and evaluation of the vision-based
classification of social gestures, such as handshake, hug, high-five, etc. This
is a component of the mediated social touch systems, which can be incorporated
into ShareTable and SqueezeBands system to achieve automated gestures
recognition and transmission of the touch between the users in real time. The
results from our pilot study show the recognition accuracy of each gestures,
and they indicate that significant future work is necessary to improve its
practical feasibility in the mediated social touch applications.
</summary>
    <author>
      <name>Yuan Yao</name>
    </author>
    <author>
      <name>Svetlana Yarosh</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00076v1</id>
    <updated>2017-07-31T21:33:32Z</updated>
    <published>2017-07-31T21:33:32Z</published>
    <title>Capturing the Connections: Unboxing Internet of Things Devices</title>
    <summary>  Based upon a study of how to capture data from Internet of Things (IoT)
devices, this paper explores the challenges for data centric design
ethnography. Often purchased to perform specific tasks, IoT devices exist in a
complex ecosystem. This paper describes a study that used a variety of methods
to capture the interactions an IoT device engaged in when it was first setup.
The complexity of the study that is explored through the annotated
documentation across video and router activity, presents the ethnographic
challenges that designers face in an age of connected things.
</summary>
    <author>
      <name>Kami Vaniea</name>
    </author>
    <author>
      <name>Ella Tallyn</name>
    </author>
    <author>
      <name>Chris Speed</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01944v1</id>
    <updated>2017-08-06T22:20:02Z</updated>
    <published>2017-08-06T22:20:02Z</published>
    <title>Rookie: A unique approach for exploring news archives</title>
    <summary>  News archives are an invaluable primary source for placing current events in
historical context. But current search engine tools do a poor job at uncovering
broad themes and narratives across documents. We present Rookie: a practical
software system which uses natural language processing (NLP) to help readers,
reporters and editors uncover broad stories in news archives. Unlike prior
work, Rookie's design emerged from 18 months of iterative development in
consultation with editors and computational journalists. This process lead to a
dramatically different approach from previous academic systems with similar
goals. Our efforts offer a generalizable case study for others building
real-world journalism software using NLP.
</summary>
    <author>
      <name>Abram Handler</name>
    </author>
    <author>
      <name>Brendan O'Connor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at KDD 2017: Data Science + Journalism workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03892v2</id>
    <updated>2018-01-19T09:21:05Z</updated>
    <published>2017-08-13T11:55:02Z</published>
    <title>EmoTxt: A Toolkit for Emotion Recognition from Text</title>
    <summary>  We present EmoTxt, a toolkit for emotion recognition from text, trained and
tested on a gold standard of about 9K question, answers, and comments from
online interactions. We provide empirical evidence of the performance of
EmoTxt. To the best of our knowledge, EmoTxt is the first open-source toolkit
supporting both emotion recognition from text and training of custom emotion
classification models.
</summary>
    <author>
      <name>Fabio Calefato</name>
    </author>
    <author>
      <name>Filippo Lanubile</name>
    </author>
    <author>
      <name>Nicole Novielli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. 7th Affective Computing and Intelligent Interaction
  (ACII'17), San Antonio, TX, USA, Oct. 23-26, 2017, p. 79-80, ISBN:
  978-1-5386-0563-9</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. 7th Affective Computing and Intelligent Interaction
  (ACII'17), San Antonio, TX, USA, Oct. 23-26, 2017, p. 79-80, ISBN:
  978-1-5386-0563-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.03892v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03892v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04139v1</id>
    <updated>2017-08-10T21:03:21Z</updated>
    <published>2017-08-10T21:03:21Z</published>
    <title>PhyShare: Sharing Physical Interaction in Virtual Reality</title>
    <summary>  We present PhyShare, a new haptic user interface based on actuated robots.
Virtual reality has recently been gaining wide adoption, and an effective
haptic feedback in these scenarios can strongly support user's sensory in
bridging virtual and physical world. Since participants do not directly observe
these robotic proxies, we investigate the multiple mappings between physical
robots and virtual proxies that can utilize the resources needed to provide a
well rounded VR experience. PhyShare bots can act either as directly touchable
objects or invisible carriers of physical objects, depending on different
scenarios. They also support distributed collaboration, allowing remotely
located VR collaborators to share the same physical feedback.
</summary>
    <author>
      <name>Zhenyi He</name>
    </author>
    <author>
      <name>Fengyuan Zhu</name>
    </author>
    <author>
      <name>Ken Perlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. arXiv admin note: text overlap with arXiv:1701.08879</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05805v1</id>
    <updated>2017-08-19T05:04:05Z</updated>
    <published>2017-08-19T05:04:05Z</published>
    <title>Design Space of Programming Tools on Mobile Touchscreen Devices</title>
    <summary>  While mobile touchscreen devices are ubiquitous and present opportunities for
novel applications, they have seen little adoption as tools for computer
programming. In this literature survey, we bring together the diverse research
work on programming-related tasks supported by mobile touchscreen devices to
explore the design space for applying them to programming situations. We used
the Grounded theory approach to identify themes and classify previous work. We
present these themes and how each paper contributes to the theme, and we
outline the remaining challenges in and opportunities for using mobile
touchscreen devices in programming applications.
</summary>
    <author>
      <name>Poorna Talkad Sukumar</name>
    </author>
    <author>
      <name>Ronald Metoyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, includes one-page table</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; D.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07762v1</id>
    <updated>2017-08-25T14:36:07Z</updated>
    <published>2017-08-25T14:36:07Z</published>
    <title>Chisio: A Compound Graph Editing and Layout Framework</title>
    <summary>  We introduce a new free, open-source compound graph editing and layout
framework named Chisio, based on the Eclipse Graph Editing Framework (GEF) and
written in Java. Chisio can be used as a finished graph editor with its
easy-to-use graphical interface. The framework has an architecture suitable for
easy customization of the tool for end-user's specific needs as well. Chisio
comes with a variety of graph layout algorithms, most supporting compound
structures and non-uniform node dimensions. Furthermore, new algorithms are
straightforward to add, making Chisio an ideal test environment for layout
algorithm developers.
</summary>
    <author>
      <name>Cihan Kucukkececi</name>
    </author>
    <author>
      <name>Ugur Dogrusoz</name>
    </author>
    <author>
      <name>Esat Belviranli</name>
    </author>
    <author>
      <name>Alptug Dilek</name>
    </author>
    <link href="http://arxiv.org/abs/1708.07762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09654v1</id>
    <updated>2017-08-31T10:25:28Z</updated>
    <published>2017-08-31T10:25:28Z</published>
    <title>Identifying Unsafe Videos on Online Public Media using Real-time
  Crowdsourcing</title>
    <summary>  Due to the significant growth of social networking and human activities
through the web in recent years, attention to analyzing big data using
real-time crowdsourcing has increased. This data may appear in the form of
streaming images, audio or videos. In this paper, we address the problem of
deciding the appropriateness of streaming videos in public media with the help
of crowdsourcing in real-time.
</summary>
    <author>
      <name>Sankar Kumar Mridha</name>
    </author>
    <author>
      <name>Braznev Sarkar</name>
    </author>
    <author>
      <name>Sujoy Chatterjee</name>
    </author>
    <author>
      <name>Malay Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Works-in-Progress, Fifth AAAI Conference on Human Computation and
  Crowdsourcing (HCOMP 2017), Quebec City, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.09654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00098v1</id>
    <updated>2017-08-31T22:15:12Z</updated>
    <published>2017-08-31T22:15:12Z</published>
    <title>AudExpCreator: A GUI-based Matlab tool for designing and creating
  auditory experiments with the Psychophysics Toolbox</title>
    <summary>  We present AudExpCreator, a GUI-based Matlab tool for designing and creating
auditory experiments. AudExpCreator allows users to generate auditory
experiments that run on Matlab's Psychophysics Toolbox without having to write
any code; rather, users simply follow instructions in GUIs to specify desired
design parameters. The software comprises five auditory study types, including
behavioral studies and integration with EEG and physiological response
collection systems. Advanced features permit more complicated experimental
designs as well as maintenance and update of previously created experiments.
AudExpCreator alleviates programming barriers while providing a free,
open-source alternative to commercial experimental design software.
</summary>
    <author>
      <name>Duc T. Nguyen</name>
    </author>
    <author>
      <name>Blair Kaneshiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.00098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00111v1</id>
    <updated>2017-08-31T23:47:37Z</updated>
    <published>2017-08-31T23:47:37Z</published>
    <title>Good Usability Practices in Scientific Software Development</title>
    <summary>  Scientific software often presents very particular requirements regarding
usability, which is often completely overlooked in this setting. As
computational science has emerged as its own discipline, distinct from
theoretical and experimental science, it has put new requirements on future
scientific software developments. In this paper, we discuss the background of
these problems and introduce nine aspects of good usability. We also highlight
best practices for each aspect with an emphasis on applications in
computational science.
</summary>
    <author>
      <name>Francisco Queiroz</name>
    </author>
    <author>
      <name>Raniere Silva</name>
    </author>
    <author>
      <name>Jonah Miller</name>
    </author>
    <author>
      <name>Sandor Brockhauser</name>
    </author>
    <author>
      <name>Hans Fangohr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6084/m9.figshare.5331814.v3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6084/m9.figshare.5331814.v3" rel="related"/>
    <link href="http://arxiv.org/abs/1709.00111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00293v1</id>
    <updated>2017-08-31T15:18:46Z</updated>
    <published>2017-08-31T15:18:46Z</published>
    <title>Revisited Experimental Comparison of Node-Link and Matrix
  Representations</title>
    <summary>  Visualizing network data is applicable in domains such as biology,
engineering, and social sciences. We report the results of a study comparing
the effectiveness of the two primary techniques for showing network data:
node-link diagrams and adjacency matrices. Specifically, an evaluation with a
large number of online participants revealed statistically significant
differences between the two visualizations. Our work adds to existing research
in several ways. First, we explore a broad spectrum of network tasks, many of
which had not been previously evaluated. Second, our study uses a large
dataset, typical of many real-life networks not explored by previous studies.
Third, we leverage crowdsourcing to evaluate many tasks with many participants.
</summary>
    <author>
      <name>Mershack Okoe</name>
    </author>
    <author>
      <name>Radu Jianu</name>
    </author>
    <author>
      <name>Stephen Kobourov</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00965v1</id>
    <updated>2017-09-04T14:00:59Z</updated>
    <published>2017-09-04T14:00:59Z</published>
    <title>Feasibility of Corneal Imaging for Handheld Augmented Reality</title>
    <summary>  Smartphones are a popular device class for mobile Augmented Reality but
suffer from a limited input space. Around-device interaction techniques aim at
extending this input space using various sensing modalities. In this paper we
present our work towards extending the input area of mobile devices using
front-facing device-centered cameras that capture reflections in the cornea. As
current generation mobile devices lack high resolution front-facing cameras, we
study the feasibility of around-device interaction using corneal reflective
imaging based on a high resolution camera. We present a workflow, a technical
prototype and a feasibility evaluation.
</summary>
    <author>
      <name>Daniel Schneider</name>
    </author>
    <author>
      <name>Jens Grubert</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00966v1</id>
    <updated>2017-09-04T14:01:45Z</updated>
    <published>2017-09-04T14:01:45Z</published>
    <title>Towards Around-Device Interaction using Corneal Imaging</title>
    <summary>  Around-device interaction techniques aim at extending the input space using
various sensing modalities on mobile and wearable devices. In this paper, we
present our work towards extending the input area of mobile devices using
front-facing device-centered cameras that capture reflections in the human eye.
As current generation mobile devices lack high resolution front-facing cameras
we study the feasibility of around-device interaction using corneal reflective
imaging based on a high resolution camera. We present a workflow, a technical
prototype and an evaluation, including a migration path from high resolution to
low resolution imagers. Our study indicates, that under optimal conditions a
spatial sensing resolution of 5 cm in the vicinity of a mobile phone is
possible.
</summary>
    <author>
      <name>Daniel Schneider</name>
    </author>
    <author>
      <name>Jens Grubert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3132272.3134127</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3132272.3134127" rel="related"/>
    <link href="http://arxiv.org/abs/1709.00966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01293v1</id>
    <updated>2017-09-05T09:04:05Z</updated>
    <published>2017-09-05T09:04:05Z</published>
    <title>Authoring and Living Next-Generation Location-Based Experiences</title>
    <summary>  Authoring location-based experiences involving multiple participants,
collaborating or competing in both indoor and outdoor mixed realities, is
extremely complex and bound to serious technical challenges. In this work, we
present the first results of the MAGELLAN European project and how these
greatly simplify this creative process using novel authoring, augmented reality
(AR) and indoor geolocalisation techniques.
</summary>
    <author>
      <name>Olivier Balet</name>
    </author>
    <author>
      <name>Boriana Koleva</name>
    </author>
    <author>
      <name>Jens Grubert</name>
    </author>
    <author>
      <name>Kwang Moo Yi</name>
    </author>
    <author>
      <name>Marco Gunia</name>
    </author>
    <author>
      <name>Angelos Katsis</name>
    </author>
    <author>
      <name>Julien Castet</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01772v1</id>
    <updated>2017-10-04T19:19:39Z</updated>
    <published>2017-10-04T19:19:39Z</published>
    <title>CELIO: An application development framework for interactive spaces</title>
    <summary>  Developing applications for interactive space is different from developing
cross-platform applications for personal computing. Input, output, and
architectural variations in each interactive space introduce big overhead in
terms of cost and time for developing, deploying and maintaining applications
for interactive spaces. Often, these applications become on-off experience tied
to the deployed spaces. To alleviate this problem and enable rapid responsive
space design applications similar to responsive web design, we present CELIO
application development framework for interactive spaces. The framework is
micro services based and neatly decouples application and design specifications
from hardware and architecture specifications of an interactive space. In this
paper, we describe this framework and its implementation details. Also, we
briefly discuss the use cases developed using this framework.
</summary>
    <author>
      <name>Yedendra B. Shrinivasan</name>
    </author>
    <author>
      <name>Yunfeng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02173v1</id>
    <updated>2017-10-05T18:27:56Z</updated>
    <published>2017-10-05T18:27:56Z</published>
    <title>Clustrophile: A Tool for Visual Clustering Analysis</title>
    <summary>  While clustering is one of the most popular methods for data mining, analysts
lack adequate tools for quick, iterative clustering analysis, which is
essential for hypothesis generation and data reasoning. We introduce
Clustrophile, an interactive tool for iteratively computing discrete and
continuous data clusters, rapidly exploring different choices of clustering
parameters, and reasoning about clustering instances in relation to data
dimensions. Clustrophile combines three basic visualizations -- a table of raw
datasets, a scatter plot of planar projections, and a matrix diagram (heatmap)
of discrete clusterings -- through interaction and intermediate visual
encoding. Clustrophile also contributes two spatial interaction techniques,
$\textit{forward projection}$ and $\textit{backward projection}$, and a
visualization method, $\textit{prolines}$, for reasoning about two-dimensional
projections obtained through dimensionality reductions.
</summary>
    <author>
      <name>Çağatay Demiralp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD IDEA'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03320v2</id>
    <updated>2018-03-27T22:45:25Z</updated>
    <published>2017-10-09T21:20:00Z</published>
    <title>A Common Framework for Audience Interactivity</title>
    <summary>  Audience interactivity is interpreted differently across domains. This
research develops a framework to describe audience interactivity across a broad
range of experiences. We build on early work characterizing child audience
interactivity experiences, expanding on these findings with an extensive review
of literature in theater, games, and theme parks, paired with expert interviews
in those domains. The framework scaffolds interactivity as nested spheres of
audience influence, and comprises a series of dimensions of audience
interactivity including a Spectrum of Audience Interactivity. This framework
aims to develop a common taxonomy for researchers and practitioners working
with audience interactivity experiences.
</summary>
    <author>
      <name>Alina Striner</name>
    </author>
    <author>
      <name>Sasha Azad</name>
    </author>
    <author>
      <name>Chris Martens</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03320v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03320v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06615v1</id>
    <updated>2017-10-18T08:24:07Z</updated>
    <published>2017-10-18T08:24:07Z</published>
    <title>Amending the Characterization of Guidance in Visual Analytics</title>
    <summary>  At VAST 2016, a characterization of guidance has been presented. It includes
a definition of guidance and a model of guidance based on van Wijk's model of
visualization. This note amends the original characterization of guidance in
two aspects. First, we provide a clarification of what guidance actually is
(and is not). Second, we insert into the model a conceptually relevant link
that was missing in the original version.
</summary>
    <author>
      <name>Davide Ceneda</name>
    </author>
    <author>
      <name>Theresia Gschwandtner</name>
    </author>
    <author>
      <name>Thorsten May</name>
    </author>
    <author>
      <name>Silvia Miksch</name>
    </author>
    <author>
      <name>Hans-Jörg Schulz</name>
    </author>
    <author>
      <name>Marc Streit</name>
    </author>
    <author>
      <name>Christian Tominski</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06925v1</id>
    <updated>2017-10-18T20:26:37Z</updated>
    <published>2017-10-18T20:26:37Z</published>
    <title>Visualizing Sensor Network Coverage with Location Uncertainty</title>
    <summary>  We present an interactive visualization system for exploring the coverage in
sensor networks with uncertain sensor locations. We consider a simple case of
uncertainty where the location of each sensor is confined to a discrete number
of points sampled uniformly at random from a region with a fixed radius.
Employing techniques from topological data analysis, we model and visualize
network coverage by quantifying the uncertainty defined on its simplicial
complex representations. We demonstrate the capabilities and effectiveness of
our tool via the exploration of randomly distributed sensor networks.
</summary>
    <author>
      <name>Tim Sodergren</name>
    </author>
    <author>
      <name>Jessica Hair</name>
    </author>
    <author>
      <name>Jeff M. Phillips</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09901v1</id>
    <updated>2017-10-26T20:15:46Z</updated>
    <published>2017-10-26T20:15:46Z</published>
    <title>Optimal Crowdsourced Classification with a Reject Option in the Presence
  of Spammers</title>
    <summary>  We explore the design of an effective crowdsourcing system for an $M$-ary
classification task. Crowd workers complete simple binary microtasks whose
results are aggregated to give the final decision. We consider the scenario
where the workers have a reject option so that they are allowed to skip
microtasks when they are unable to or choose not to respond to binary
microtasks. We present an aggregation approach using a weighted majority voting
rule, where each worker's response is assigned an optimized weight to maximize
crowd's classification performance.
</summary>
    <author>
      <name>Qunwei Li</name>
    </author>
    <author>
      <name>Pramod K. Varshney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00334v1</id>
    <updated>2017-11-30T08:27:08Z</updated>
    <published>2017-11-30T08:27:08Z</published>
    <title>Enabling Embodied Analogies in Intelligent Music Systems</title>
    <summary>  The present methodology is aimed at cross-modal machine learning and uses
multidisciplinary tools and methods drawn from a broad range of areas and
disciplines, including music, systematic musicology, dance, motion capture,
human-computer interaction, computational linguistics and audio signal
processing. Main tasks include: (1) adapting wisdom-of-the-crowd approaches to
embodiment in music and dance performance to create a dataset of music and
music lyrics that covers a variety of emotions, (2) applying
audio/language-informed machine learning techniques to that dataset to identify
automatically the emotional content of the music and the lyrics, and (3)
integrating motion capture data from a Vicon system and dancers performing on
that music.
</summary>
    <author>
      <name>Fabio Paolizzo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03329v1</id>
    <updated>2017-12-09T01:47:29Z</updated>
    <published>2017-12-09T01:47:29Z</published>
    <title>Adaptive Interface for Accommodating Colour-Blind Users by Using
  Ishihara Test</title>
    <summary>  Imperative visual data frequently vanishes when color applications are seen
by partially color blind users. A new method for adaptive interface for
accommodating color blind users is presented. The method presented here has two
sections: 1) test client perceivability by utilizing Ishihara plates. 2) change
the interface color scheme to accommodate color blind users if necessary. We
demonstrate how the method works via a simple interface and evaluate the
efficiency of our method by experimenting it on 100 users.
</summary>
    <author>
      <name>Abu Zohran Qaiser</name>
    </author>
    <author>
      <name>Muhammad Taha Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04652v1</id>
    <updated>2017-12-13T08:19:08Z</updated>
    <published>2017-12-13T08:19:08Z</published>
    <title>Software Engineering Solutions To Support Vertical Transportation</title>
    <summary>  In this paper we introduce the core results of the project on visualisation
and analysis of data collected from the vertical transport facilities. The aim
of the project was to provide better user experience as well as to help
building maintenance staff to increase productivity of their work. We
elaborated a web-based system for vertical transportation, to cover the needs
of (1) staff working on building maintenance, (2) people who are regularly
using the facilities in the corresponding buildings.
</summary>
    <author>
      <name>Alber J. Christianto</name>
    </author>
    <author>
      <name>Peng Chen</name>
    </author>
    <author>
      <name>Osheen Walawedura</name>
    </author>
    <author>
      <name>Annie Vuong</name>
    </author>
    <author>
      <name>Jun Feng</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Maria Spichkova</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06179v1</id>
    <updated>2017-12-17T21:20:13Z</updated>
    <published>2017-12-17T21:20:13Z</published>
    <title>Organic Visualization of Document Evolution</title>
    <summary>  Recent availability of data of writing processes at keystroke-granularity has
enabled research on the evolution of document writing. A natural step is to
develop systems that can actually show this data and make it understandable.
Here we propose a data structure that captures a document's fine-grained
history and an organic visualization that serves as an interface to it. We
evaluate a proof-of-concept implementation of the system through a pilot study
with documents written by students at a public university. Our results are
promising and reveal facets such as general strategies adopted, local edition
density and hierarchical structure of the final text.
</summary>
    <author>
      <name>Ignacio Perez-Messina</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <author>
      <name>Eduardo Graells-Garrido</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. Short paper accepted at the 23rd ACM Conference on
  Intelligent User Interfaces</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08084v1</id>
    <updated>2017-12-21T16:57:35Z</updated>
    <published>2017-12-21T16:57:35Z</published>
    <title>AVEID: Automatic Video System for Measuring Engagement In Dementia</title>
    <summary>  Engagement in dementia is typically measured using behavior observational
scales (BOS) that are tedious and involve intensive manual labor to annotate,
and are therefore not easily scalable. We propose AVEID, a low cost and
easy-to-use video-based engagement measurement tool to determine the engagement
level of a person with dementia (PwD) during digital interaction. We show that
the objective behavioral measures computed via AVEID correlate well with
subjective expert impressions for the popular MPES and OME BOS, confirming its
viability and effectiveness. Moreover, AVEID measures can be obtained for a
variety of engagement designs, thereby facilitating large-scale studies with
PwD populations.
</summary>
    <author>
      <name>Viral Parekh</name>
    </author>
    <author>
      <name>Pin Sym Foong</name>
    </author>
    <author>
      <name>Shendong Zhao</name>
    </author>
    <author>
      <name>Ramanathan Subramanian</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09929v1</id>
    <updated>2017-12-28T16:49:40Z</updated>
    <published>2017-12-28T16:49:40Z</published>
    <title>On the Challenges of Detecting Rude Conversational Behaviour</title>
    <summary>  In this study, we aim to identify moments of rudeness between two
individuals. In particular, we segment all occurrences of rudeness in
conversations into three broad, distinct categories and try to identify each.
We show how machine learning algorithms can be used to identify rudeness based
on acoustic and semantic signals extracted from conversations. Furthermore, we
make note of our shortcomings in this task and highlight what makes this
problem inherently difficult. Finally, we provide next steps which are needed
to ensure further success in identifying rudeness in conversations.
</summary>
    <author>
      <name>Karan Grewal</name>
    </author>
    <author>
      <name>Khai N. Truong</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00061v1</id>
    <updated>2017-12-30T00:01:08Z</updated>
    <published>2017-12-30T00:01:08Z</published>
    <title>Multichannel Robot Speech Recognition Database: MChRSR</title>
    <summary>  In real human robot interaction (HRI) scenarios, speech recognition
represents a major challenge due to robot noise, background noise and
time-varying acoustic channel. This document describes the procedure used to
obtain the Multichannel Robot Speech Recognition Database (MChRSR). It is
composed of 12 hours of multichannel evaluation data recorded in a real mobile
HRI scenario. This database was recorded with a PR2 robot performing different
translational and azimuthal movements. Accordingly, 16 evaluation sets were
obtained re-recording the clean set of the Aurora 4 database in different
movement conditions.
</summary>
    <author>
      <name>José Novoa</name>
    </author>
    <author>
      <name>Juan Pablo Escudero</name>
    </author>
    <author>
      <name>Josué Fredes</name>
    </author>
    <author>
      <name>Jorge Wuth</name>
    </author>
    <author>
      <name>Rodrigo Mahu</name>
    </author>
    <author>
      <name>Néstor Becerra Yoma</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03261v1</id>
    <updated>2018-01-10T08:05:40Z</updated>
    <published>2018-01-10T08:05:40Z</published>
    <title>Exploring Stereotypes and Biased Data with the Crowd</title>
    <summary>  The goal of our research is to contribute information about how useful the
crowd is at anticipating stereotypes that may be biasing a data set without a
researcher's knowledge. The results of the crowd's prediction can potentially
be used during data collection to help prevent the suspected stereotypes from
introducing bias to the dataset. We conduct our research by asking the crowd on
Amazon's Mechanical Turk (AMT) to complete two similar Human Intelligence Tasks
(HITs) by suggesting stereotypes relating to their personal experience. Our
analysis of these responses focuses on determining the level of diversity in
the workers' suggestions and their demographics. Through this process we begin
a discussion on how useful the crowd can be in tackling this difficult problem
within machine learning data collection.
</summary>
    <author>
      <name>Zeyuan Hu</name>
    </author>
    <author>
      <name>Julia Strout</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05085v1</id>
    <updated>2018-01-16T01:10:54Z</updated>
    <published>2018-01-16T01:10:54Z</published>
    <title>Excuse me! Perception of Abrupt Direction Changes Using Body Cues and
  Paths on Mixed Reality Avatars</title>
    <summary>  We evaluate two methods of signalling abrupt direction changes of a robotic
platform using a Mixed Reality avatar. The "Body" method uses gaze, gesture and
torso direction to point to upcoming waypoints. The "Path" method visualises
the change in direction using an angled path on the ground. We compare these
two methods using a controlled user study and show that each method has its
strengths depending on the situation. Overall the "Path" method was slightly
more accurate in communicating the direction change of the robot but
participants overall preferred the "Body" method.
</summary>
    <author>
      <name>Nicholas Katzakis</name>
    </author>
    <author>
      <name>Frank Steinicke</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07185v1</id>
    <updated>2018-01-22T16:30:51Z</updated>
    <published>2018-01-22T16:30:51Z</published>
    <title>White Noise from the White Goods? Conceptual and Empirical Perspectives
  on Ambient Domestic Computing</title>
    <summary>  Within this chapter we consider the emergence of ambient domestic computing
systems, both conceptually and empirically. We critically assess visions of
post-desktop computing, paying particular attention to one contemporary trend:
the internet of things (IoT). We examine the contested nature of this term,
looking at the historical trajectory of similar technologies, and the
regulatory issues they can pose, particularly in the home. We also look to the
emerging regulatory solution of privacy by design, unpacking practical
challenges it faces. The novelty of our contribution stems from a turn to
practice through a set of empirical perspectives. We present findings that
document the practical experiences and viewpoints of leading experts in
technology law and design.
</summary>
    <author>
      <name>Lachlan Urquhart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09626v1</id>
    <updated>2018-01-29T17:03:34Z</updated>
    <published>2018-01-29T17:03:34Z</published>
    <title>Human-Machine Inference Networks For Smart Decision Making:
  Opportunities and Challenges</title>
    <summary>  The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines
complementary cognitive strengths of humans and machines in an intelligent
manner to tackle various inference tasks and achieves higher performance than
either humans or machines by themselves. While inference performance
optimization techniques for human-only or sensor-only networks are quite
mature, HuMaINs require novel signal processing and machine learning solutions.
In this paper, we present an overview of the HuMaINs architecture with a focus
on three main issues that include architecture design, inference algorithms
including security/privacy challenges, and application areas/use cases.
</summary>
    <author>
      <name>Aditya Vempaty</name>
    </author>
    <author>
      <name>Bhavya Kailkhura</name>
    </author>
    <author>
      <name>Pramod K. Varshney</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.10249v1</id>
    <updated>2018-01-30T22:54:06Z</updated>
    <published>2018-01-30T22:54:06Z</published>
    <title>The Reification of an Incorrect and Inappropriate Spreadsheet Model</title>
    <summary>  Once information is loaded into a spreadsheet, it acquires properties that it
may not deserve. These properties include believability, correctness,
appropriateness, concreteness, integrity, tangibility, objectivity and
authority. The information becomes reified. We describe a case study through
which we were able to observe at close hand the reification of a demonstrably
incorrect and inappropriate spreadsheet model within a small non profit
organisation.
</summary>
    <author>
      <name>Grenville J. Croll</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, 4 Colour Figures, 2 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the EuSpRIG 2017 Conference "Spreadsheet Risk
  Management", Imperial College, London, pp63-76 ISBN: 978-1-905404-54-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.10249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.10249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00507v1</id>
    <updated>2018-01-20T03:51:30Z</updated>
    <published>2018-01-20T03:51:30Z</published>
    <title>The effects of anger on automated long-term-spectra based
  speaker-identification</title>
    <summary>  Forensic speaker identification has traditionally considered approaches based
on long term spectra analysis as especially robust, given that they work well
for short recordings, are not sensitive to changes in the intensity of the
sample, and continue to function in the presence of noise and limited passband.
We find, however, that anger induces a significant distortion of the acoustic
signal for long term spectra analysis purposes. Even moderate anger offsets
speaker identification results by 33% in the direction of a different speaker
altogether. Thus, caution should be exercised when applying this tool.
</summary>
    <author>
      <name>Diana Valverde-Méndez</name>
    </author>
    <author>
      <name>Manuel Ortega-Rodríguez</name>
    </author>
    <author>
      <name>Hugo Solís-Sánchez</name>
    </author>
    <author>
      <name>Ariadna Venegas-Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05534v1</id>
    <updated>2018-02-13T22:07:27Z</updated>
    <published>2018-02-13T22:07:27Z</published>
    <title>Opportunity in Conflict: Understanding Tension Among Key Groups on the
  Trail</title>
    <summary>  This paper examines the question of who technology users on the trail are,
what their technological uses and needs are, and what conflicts exist between
different trail users regarding technology use and experience, toward
understanding how experiences of trail users contribute to designers. We argue
that exploring these tensions provide opportunities for design that can be used
to both mitigate conflicts and improve community on the trail.
</summary>
    <author>
      <name>Lindah Kotut</name>
    </author>
    <author>
      <name>Michael Horning</name>
    </author>
    <author>
      <name>Steve Harrison</name>
    </author>
    <author>
      <name>D. Scott McCrickard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop Paper Submitted to CHI HCI Outdoors: Understanding
  Human-Computer Interaction in the Outdoors (2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00296v1</id>
    <updated>2018-03-01T10:45:12Z</updated>
    <published>2018-03-01T10:45:12Z</published>
    <title>Dišimo: Anchoring Our Breath</title>
    <summary>  We present a system that raises awareness about users' inner state.
Di\v{s}imo is a multimodal ambient display that provides feedback about one's
stress level, which is assessed through heart rate monitoring. Upon detecting a
low heart rate variability for a prolonged period of time, Di\v{s}imo plays an
audio track, setting the pace of a regular and deep breathing. Users can then
choose to take a moment to focus on their breath. By doing so, they will
activate the Di\v{s}imo devices belonging to their close ones, who can then
join for a shared relaxation session.
</summary>
    <author>
      <name>Jelena Mladenovic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDC, Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDC</arxiv:affiliation>
    </author>
    <author>
      <name>Jessica Cauchard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3170427.3186517</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3170427.3186517" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI '18 Interactivity - SIGCHI Conference on Human Factors in
  Computing System, Apr 2018, Montreal, Canada</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.00296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00459v1</id>
    <updated>2018-03-01T15:41:52Z</updated>
    <published>2018-03-01T15:41:52Z</published>
    <title>Challenges and opportunities in visual interpretation of Big Data</title>
    <summary>  We live in a world where data generation is omnipresent. Innovations in
computer hardware in the last few decades coupled with increasingly reliable
connectivity among them have fueled this phenomenon. We are constantly creating
and consuming data across digital devices of varying form factors. Leveraging
huge quantities of data involves making interpretations from it. However,
interpreting data is still a difficult task. We need data analysts to help make
decisions. These experts apply their domain knowledge, understanding of the
problem space and numerical analysis to draw inferences from the data in order
to support decision making. Existing tools and techniques for interference
serve users making decisions with hard constraints. Consumer systems are often
built to support exploratory data analysis in mind rather than sense making.
</summary>
    <author>
      <name>Gourab Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03092v1</id>
    <updated>2018-03-05T02:46:48Z</updated>
    <published>2018-03-05T02:46:48Z</published>
    <title>Carnap: An Open Framework for Formal Reasoning in the Browser</title>
    <summary>  This paper presents an overview of Carnap, a free and open framework for the
development of formal reasoning applications. Carnap's design emphasizes
flexibility, extensibility, and rapid prototyping. Carnap-based applications
are written in Haskell, but can be compiled to JavaScript to run in standard
web browsers. This combination of features makes Carnap ideally suited for
educational applications, where ease-of-use is crucial for students and
adaptability to different teaching strategies and classroom needs is crucial
for instructors. The paper describes Carnap's implementation, along with its
current and projected pedagogical applications.
</summary>
    <author>
      <name>Graham Leach-Krouse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kansas State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.267.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.267.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ThEdu'17, arXiv:1803.00722</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 267, 2018, pp. 70-88</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.03092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04099v1</id>
    <updated>2018-03-12T03:12:42Z</updated>
    <published>2018-03-12T03:12:42Z</published>
    <title>Context-aware Human Intent Inference for Improving Human Machine
  Cooperation</title>
    <summary>  The ability of human beings to precisely recog- nize others intents is a
significant mental activity in reasoning about actions, such as, what other
people are doing and what they will do next. Recent research has revealed that
human intents could be inferred by measuring human cognitive activities through
heterogeneous body and brain sensors (e.g., sensors for detecting physiological
signals like ECG, brain signals like EEG and IMU sensors like accelerometers
and gyros etc.). In this proposal, we aim at developing a computa- tional
framework for enabling reliable and precise real-time human intent recognition
by measuring human cognitive and physiological activities through the
heterogeneous body and brain sensors for improving human machine interactions,
and serving intent-based human activity prediction.
</summary>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1803.04099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04818v2</id>
    <updated>2018-03-14T08:32:02Z</updated>
    <published>2018-03-13T14:07:40Z</published>
    <title>A Survey on Deep Learning Toolkits and Libraries for Intelligent User
  Interfaces</title>
    <summary>  This paper provides an overview of prominent deep learning toolkits and, in
particular, reports on recent publications that contributed open source
software for implementing tasks that are common in intelligent user interfaces
(IUI). We provide a scientific reference for researchers and software engineers
who plan to utilise deep learning techniques within their IUI research and
development projects.
</summary>
    <author>
      <name>Jan Zacharias</name>
    </author>
    <author>
      <name>Michael Barz</name>
    </author>
    <author>
      <name>Daniel Sonntag</name>
    </author>
    <link href="http://arxiv.org/abs/1803.04818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05805v1</id>
    <updated>2018-03-15T15:26:06Z</updated>
    <published>2018-03-15T15:26:06Z</published>
    <title>Sonifying stochastic walks on biomolecular energy landscapes</title>
    <summary>  Translating the complex, multi-dimensional data from simulations of
biomolecules to intuitive knowledge is a major challenge in computational
chemistry and biology. The so-called "free energy landscape" is amongst the
most fundamental concepts used by scientists to understand both static and
dynamic properties of biomolecular systems. In this paper we use Markov models
to design a strategy for mapping features of this landscape to sonic
parameters, for use in conjunction with visual display techniques such as
structural animations and free energy diagrams.
</summary>
    <author>
      <name>Robert E. Arbon</name>
    </author>
    <author>
      <name>Alex J. Jones</name>
    </author>
    <author>
      <name>Lars A. Bratholm</name>
    </author>
    <author>
      <name>Tom Mitchell</name>
    </author>
    <author>
      <name>David R. Glowacki</name>
    </author>
    <link href="http://arxiv.org/abs/1803.05805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01382v1</id>
    <updated>2018-04-03T01:17:16Z</updated>
    <published>2018-04-03T01:17:16Z</published>
    <title>Vanlearning: A Machine Learning SaaS Application for People Without
  Programming Backgrounds</title>
    <summary>  Although we have tons of machine learning tools to analyze data, most of them
require users have some programming backgrounds. Here we introduce a SaaS
application which allows users analyze their data without any coding and even
without any knowledge of machine learning. Users can upload, train, predict and
download their data by simply clicks their mouses. Our system uses data
pre-processor and validator to relieve the computational cost of our server.
The simple architecture of Vanlearning helps developers can easily maintain and
extend it.
</summary>
    <author>
      <name>Chaochen Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.01382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02527v1</id>
    <updated>2018-04-07T07:52:04Z</updated>
    <published>2018-04-07T07:52:04Z</published>
    <title>Visual Analytics for Explainable Deep Learning</title>
    <summary>  Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions.
</summary>
    <author>
      <name>Jaegul Choo</name>
    </author>
    <author>
      <name>Shixia Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computer Graphics and Applications, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.9.c" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02657v1</id>
    <updated>2018-04-08T09:18:56Z</updated>
    <published>2018-04-08T09:18:56Z</published>
    <title>Emotion Orientated Recommendation System for Hiroshima Tourist by Fuzzy
  Petri Net</title>
    <summary>  We developed an Android Smartophone application software for tourist
information system. Especially, the agent system recommends the sightseeing
spot and local hospitality corresponding to the current feelings. The system
such as concierge can estimate user's emotion and mood by Emotion Generating
Calculations and Mental State Transition Network. In this paper, the system
decides the next candidates for spots and foods by the reasoning of fuzzy Petri
Net in order to make more smooth communication between human and smartphone.
The system was developed for Hiroshima Tourist Information and described some
hospitality about the concierge system.
</summary>
    <author>
      <name>Takumi Ichimura</name>
    </author>
    <author>
      <name>Issei Tachibana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IWCIA.2013.6624776</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IWCIA.2013.6624776" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures, Proc. of IEEE 6th International Workshop on
  Computational Intelligence and Applications (IWCIA2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03263v1</id>
    <updated>2018-04-09T22:31:06Z</updated>
    <published>2018-04-09T22:31:06Z</published>
    <title>Visualization Tool for Environmental Sensing and Public Health Data</title>
    <summary>  To assist residents affected by oil and gas development, public health
professionals in a non-profit organization have collected community data,
including symptoms, air quality, and personal stories. However, the
organization was unable to aggregate and visualize these data computationally.
We present the Environmental Health Channel, an interactive web-based tool for
visualizing environmental sensing and public health data. This tool enables
discussing and disseminating scientific evidence to reveal local environmental
and health impacts of industrial activities.
</summary>
    <author>
      <name>Yen-Chia Hsu</name>
    </author>
    <author>
      <name>Jennifer Cross</name>
    </author>
    <author>
      <name>Paul Dille</name>
    </author>
    <author>
      <name>Illah Nourbakhsh</name>
    </author>
    <author>
      <name>Leann Leiter</name>
    </author>
    <author>
      <name>Ryan Grode</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by 2018 ACM Conference Companion Publication on Designing
  Interactive Systems (DIS 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04855v1</id>
    <updated>2018-04-13T09:28:18Z</updated>
    <published>2018-04-13T09:28:18Z</published>
    <title>Activity Self-Tracking with Smart Phones: How to Approach Odd
  Measurements?</title>
    <summary>  Tracking physical activity reliably is becoming central to many research
efforts. In the last years specialized hardware has been proposed to measure
movement. However, asking study participants to carry additional devices has
drawbacks. We focus on using mobile devices as motion sensors. In the paper we
detail several issues that we found while using this technique in a
longitudinal study involving hundreds of participants for several months. We
hope to sparkle a lively discussion at the workshop and attract interest in
this method from other researchers.
</summary>
    <author>
      <name>Gabriela Villalobos-Zúñiga</name>
    </author>
    <author>
      <name>Mauro Cherubini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A Short Workshop on Next Steps Towards Long Term Self Tracking,
  CHI'18 Montreal, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08732v1</id>
    <updated>2018-04-23T20:57:47Z</updated>
    <published>2018-04-23T20:57:47Z</published>
    <title>StreamBED: Training Citizen Scientists to Make Qualitative Judgments
  Using Embodied Virtual Reality Training</title>
    <summary>  Environmental citizen science frequently relies on experience-based
assessment, however volunteers are not trained to make qualitative judgments.
Embodied learning in virtual reality (VR) has been explored as a way to train
behavior, but has not fully been considered as a way to train judgment. This
preliminary research explores embodied learning in VR through the design,
evaluation, and redesign of StreamBED, a water quality monitoring training
environment that teaches volunteers to make qualitative assessments by
exploring, assessing and comparing virtual watersheds.
</summary>
    <author>
      <name>Alina Striner</name>
    </author>
    <author>
      <name>Jennifer Preece</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Interface Prototype, Qualitative Judgments, Immersion, Citizen&#10;  Science, Training" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08737v1</id>
    <updated>2018-04-23T21:08:10Z</updated>
    <published>2018-04-23T21:08:10Z</published>
    <title>"It was Colonel Mustard in the Study with the Candlestick": Using
  Artifacts to Create An Alternate Reality Game-The Unworkshop</title>
    <summary>  Workshops are used for academic social networking, but connections can be
superficial and result in few enduring collaborations. This unworkshop offers a
novel interactive format to create deep connections, peer- learning, and
produces a technology-enhanced experience. Participants will generate
interactive technological artifacts before the unworkshop, which will be used
together and orchestrated at the unworkshop to engage all participants in an
alternate reality game set in local places at the conference.
</summary>
    <author>
      <name>Alina Striner</name>
    </author>
    <author>
      <name>Lennart E. Nacke</name>
    </author>
    <author>
      <name>Elizabeth Bonsignore</name>
    </author>
    <author>
      <name>Matthew Louis Mauriello</name>
    </author>
    <author>
      <name>Zachary O. Toups</name>
    </author>
    <author>
      <name>Carlea Holl-Jensen</name>
    </author>
    <author>
      <name>Heather Kelley</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Game Design, Prototyping, Playful Design, Design Methods, Design&#10;  Research, Improvisation, Workshop" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10202v1</id>
    <updated>2018-04-26T08:11:16Z</updated>
    <published>2018-04-26T08:11:16Z</published>
    <title>Sounding Board: A User-Centric and Content-Driven Social Chatbot</title>
    <summary>  We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa
Prize. The system architecture consists of several components including spoken
language processing, dialogue management, language generation, and content
management, with emphasis on user-centric and content-driven design. We also
share insights gained from large-scale online logs based on 160,000
conversations with real-world users.
</summary>
    <author>
      <name>Hao Fang</name>
    </author>
    <author>
      <name>Hao Cheng</name>
    </author>
    <author>
      <name>Maarten Sap</name>
    </author>
    <author>
      <name>Elizabeth Clark</name>
    </author>
    <author>
      <name>Ari Holtzman</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, NAACL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.10202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00006v1</id>
    <updated>2018-05-31T13:38:08Z</updated>
    <published>2018-05-31T13:38:08Z</published>
    <title>Towards Affective Drone Swarms: A Preliminary Crowd-Sourced Study</title>
    <summary>  Drone swarms are teams of autonomous un-manned aerial vehicles that act as a
collective entity. We are interested in humanizing drone swarms, equipping them
with the ability to emotionally affect human users through their non-verbal
motions. Inspired by recent findings in how observers are emotionally touched
by watching dance moves, we investigate the questions of whether and how
coordinated drone swarms' motions can achieve emotive impacts on general
audience. Our preliminary study on Amazon Mechanical Turk led to a number of
interesting findings, including both promising results and challenges.
</summary>
    <author>
      <name>Truong-Huy D. Nguyen</name>
    </author>
    <author>
      <name>Kasper Grispino</name>
    </author>
    <author>
      <name>Damian Lyons</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00923v1</id>
    <updated>2018-06-04T02:01:33Z</updated>
    <published>2018-06-04T02:01:33Z</published>
    <title>How Content Volume on Landing Pages Influences Consumer Behavior</title>
    <summary>  Does more information elicit users compliance and engagement, or the other
way around? This paper explores the relationship between content strategy and
user experience (UX). Specifically, we examine how the amount of information
provided on marketing web pages, often called landing pages,impact users
willingness to provide their e-mail address (a behavior called conversion in
marketing terms). We describe the results of two large-scale online experiments
(n= 535 and n= 27,900) conducted in real-world commercial settings. The
observed results indicate a negative correlation between the amount of
information on a web page and users decision-making and engagement.
</summary>
    <author>
      <name>Nim Dvir</name>
    </author>
    <author>
      <name>Ruti Gafni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.28945/4016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.28945/4016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">InSITE 2018- Informing Science and Information Technology Education
  Conference, La Verne, California</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09115v1</id>
    <updated>2018-06-24T09:42:43Z</updated>
    <published>2018-06-24T09:42:43Z</published>
    <title>The Perception of Humanoid Robots for Domestic Use in Saudi Arabia</title>
    <summary>  We propose a research to investigate Saudi peoples' perception of humanoid
domestic robots and attitude towards the possibility of having one in their
house. Through a series of questionnaires, semi-structured interviews, focus
groups, and participatory design sessions, this research will explore Saudi
peoples' level of acceptance towards domestic robots, the tasks and
responsibilities they would feel comfortable assigning to these robots, their
preferred appearance of domestic robots, and the cultural stereotypes they feel
a domestic robot must mimic.
</summary>
    <author>
      <name>Ohoud Alharbi</name>
    </author>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In CHI 2018 Workshop on Exploring Participatory Design Methods to
  Engage with Arab Communities (April 22, 2018). Montr\'eal, QC, Canada, 6
  pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11060v1</id>
    <updated>2018-06-28T16:19:37Z</updated>
    <published>2018-06-28T16:19:37Z</published>
    <title>Malicious User Experience Design Research for Cybersecurity</title>
    <summary>  This paper explores the factors and theory behind the user-centered research
that is necessary to create a successful game-like prototype, and user
experience, for malicious users in a cybersecurity context. We explore what is
known about successful addictive design in the fields of video games and
gambling to understand the allure of breaking into a system, and the joy of
thwarting the security to reach a goal or a reward of data. Based on the
malicious user research, game user research, and using the GameFlow framework,
we propose a novel malicious user experience design approach
</summary>
    <author>
      <name>Adam Trowbridge</name>
    </author>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Jessica Westbrook</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3285002.3285010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3285002.3285010" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NSPW 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.11060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04606v1</id>
    <updated>2018-07-12T13:41:40Z</updated>
    <published>2018-07-12T13:41:40Z</published>
    <title>A Survey Investigating Usage of Virtual Personal Assistants</title>
    <summary>  Despite significant improvements in automatic speech recognition and spoken
language understanding - human interaction with Virtual Personal Assistants
(VPAs) through speech remains irregular and sporadic. According to recent
studies, currently the usage of VPAs is constrained to basic tasks such as
checking facts, playing music, and obtaining weather updates.In this paper, we
present results of a survey (N = 118) that analyses usage of VPAs by frequent
and infrequent users. We investigate how usage experience, performance
expectations, and privacy concerns differ between these two groups. The results
indicate that, compared with infrequent users, frequent users of VPAs are more
satisfied with their assistants, more eager to use them in a variety of
settings, yet equally concerned about their privacy.
</summary>
    <author>
      <name>Mateusz Dubiel</name>
    </author>
    <author>
      <name>Martin Halvey</name>
    </author>
    <author>
      <name>Leif Azzopardi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06641v2</id>
    <updated>2018-08-15T22:54:46Z</updated>
    <published>2018-07-17T19:56:12Z</published>
    <title>Beyond Heuristics: Learning Visualization Design</title>
    <summary>  In this paper, we describe a research agenda for deriving design principles
directly from data. We argue that it is time to go beyond manually curated and
applied visualization design guidelines. We propose learning models of
visualization design from data collected using graphical perception studies and
build tools powered by the learned models. To achieve this vision, we need to
1) develop scalable methods for collecting training data, 2) collect different
forms of training data, 3) advance interpretability of machine learning models,
and 4) develop adaptive models that evolve as more data becomes available.
</summary>
    <author>
      <name>Bahador Saket</name>
    </author>
    <author>
      <name>Dominik Moritz</name>
    </author>
    <author>
      <name>Halden Lin</name>
    </author>
    <author>
      <name>Victor Dibia</name>
    </author>
    <author>
      <name>Cagatay Demiralp</name>
    </author>
    <author>
      <name>Jeffrey Heer</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09825v1</id>
    <updated>2018-07-12T02:27:19Z</updated>
    <published>2018-07-12T02:27:19Z</published>
    <title>An Affective Robot Companion for Assisting the Elderly in a Cognitive
  Game Scenario</title>
    <summary>  Being able to recognize emotions in human users is considered a highly
desirable trait in Human-Robot Interaction (HRI) scenarios. However, most
contemporary approaches rarely attempt to apply recognized emotional features
in an active manner to modulate robot decision-making and dialogue for the
benefit of the user. In this position paper, we propose a method of
incorporating recognized emotions into a Reinforcement Learning (RL) based
dialogue management module that adapts its dialogue responses in order to
attempt to make cognitive training tasks, like the 2048 Puzzle Game, more
enjoyable for the users.
</summary>
    <author>
      <name>Nikhil Churamani</name>
    </author>
    <author>
      <name>Alexander Sutherland</name>
    </author>
    <author>
      <name>Pablo Barros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on Intelligent Assistive Computing, IEEE
  World Congress on Computational Intelligence (WCCI) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03413v1</id>
    <updated>2018-08-10T05:23:37Z</updated>
    <published>2018-08-10T05:23:37Z</published>
    <title>Inverse Augmented Reality: A Virtual Agent's Perspective</title>
    <summary>  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
</summary>
    <author>
      <name>Zhenliang Zhang</name>
    </author>
    <author>
      <name>Dongdong Weng</name>
    </author>
    <author>
      <name>Haiyan Jiang</name>
    </author>
    <author>
      <name>Yue Liu</name>
    </author>
    <author>
      <name>Yongtian Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04414v1</id>
    <updated>2018-08-13T19:35:44Z</updated>
    <published>2018-08-13T19:35:44Z</published>
    <title>Large Graph Exploration via Subgraph Discovery and Decomposition</title>
    <summary>  We are developing an interactive graph exploration system called Graph
Playground for making sense of large graphs. Graph Playground offers a fast and
scalable edge decomposition algorithm, based on iterative vertex-edge peeling,
to decompose million-edge graphs in seconds. Graph Playground introduces a
novel graph exploration approach and a 3D representation framework that
simultaneously reveals (1) peculiar subgraph structure discovered through the
decomposition's layers, (e.g., quasi-cliques), and (2) possible vertex roles in
linking such subgraph patterns across layers.
</summary>
    <author>
      <name>James Abello</name>
    </author>
    <author>
      <name>Fred Hohman</name>
    </author>
    <author>
      <name>Varun Bezzam</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05852v1</id>
    <updated>2018-08-08T22:44:56Z</updated>
    <published>2018-08-08T22:44:56Z</published>
    <title>Real-time fMRI-based Brain Computer Interface: A Review</title>
    <summary>  In recent years, the rapid development of neuroimaging technology has been
providing many powerful tools for cognitive neuroscience research. Among them,
the functional magnetic resonance imaging (fMRI), which has high spatial
resolution, acceptable temporal resolution, simple calibration, and short
preparation time, has been widely used in brain research. Compared with the
electroencephalogram (EEG), real-time fMRI-based brain computer interface
(rtfMRI-BCI) not only can perform decoding analysis across the whole brain to
control external devices, but also allows a subject to voluntarily
self-regulate specific brain regions. This paper reviews the basic architecture
of rtfMRI-BCI, the emerging machine learning based data analysis approaches
(also known as multi-voxel pattern analysis), and the applications and recent
advances of rtfMRI-BCI.
</summary>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06019v2</id>
    <updated>2018-09-06T04:08:07Z</updated>
    <published>2018-08-17T23:17:25Z</published>
    <title>Visualizing a Million Time Series with the Density Line Chart</title>
    <summary>  Data analysts often need to work with multiple series of
data---conventionally shown as line charts---at once. Few visual
representations allow analysts to view many lines simultaneously without
becoming overwhelming or cluttered. In this paper, we introduce the DenseLines
technique to calculate a discrete density representation of time series.
DenseLines normalizes time series by the arc length to compute accurate
densities. The derived density visualization allows users both to see the
aggregate trends of multiple series and to identify anomalous extrema.
</summary>
    <author>
      <name>Dominik Moritz</name>
    </author>
    <author>
      <name>Danyel Fisher</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06080v1</id>
    <updated>2018-08-18T12:35:58Z</updated>
    <published>2018-08-18T12:35:58Z</published>
    <title>CrowdTruth 2.0: Quality Metrics for Crowdsourcing with Disagreement</title>
    <summary>  Typically crowdsourcing-based approaches to gather annotated data use
inter-annotator agreement as a measure of quality. However, in many domains,
there is ambiguity in the data, as well as a multitude of perspectives of the
information examples. In this paper, we present ongoing work into the
CrowdTruth metrics, that capture and interpret inter-annotator disagreement in
crowdsourcing. The CrowdTruth metrics model the inter-dependency between the
three main components of a crowdsourcing system -- worker, input data, and
annotation. The goal of the metrics is to capture the degree of ambiguity in
each of these three components. The metrics are available online at
https://github.com/CrowdTruth/CrowdTruth-core .
</summary>
    <author>
      <name>Anca Dumitrache</name>
    </author>
    <author>
      <name>Oana Inel</name>
    </author>
    <author>
      <name>Lora Aroyo</name>
    </author>
    <author>
      <name>Benjamin Timmermans</name>
    </author>
    <author>
      <name>Chris Welty</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08713v1</id>
    <updated>2018-08-27T07:29:05Z</updated>
    <published>2018-08-27T07:29:05Z</published>
    <title>Remote Biofeedback Sharing, Opportunities and Challenges</title>
    <summary>  Biofeedback is commonly used to regulate one's state, for example to manage
stress. The underlying idea is that by perceiving a feedback about their
physiological activity, a user can act upon it. In this paper we describe
through two recent projects how biofeedback could be leveraged to share one's
state at distance. Such extension of biofeedback could answer to the need of
belonging, further widening the applications of the technology in terms of
well-being.
</summary>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDC</arxiv:affiliation>
    </author>
    <author>
      <name>Jessica Cauchard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IDC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3267305.3267701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3267305.3267701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WellComp - UbiComp/ISWC'18 Adjunct, Oct 2018, Singapore, Singapore.
  http://ubicomp.org/ubicomp2018/</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00931v1</id>
    <updated>2018-10-01T19:39:19Z</updated>
    <published>2018-10-01T19:39:19Z</published>
    <title>Wikidata: A New Paradigm of Human-Bot Collaboration?</title>
    <summary>  Wikidata is a collaborative knowledge graph which has already drawn the
attention of practitioners and researchers. It is the work of a community of
volunteers, supported by policies, guidelines and automatic programs (bots)
which perform a broad range of tasks, doing the lion's share of the work on the
platform. In this paper, we highlight some of the most salient aspects of
human-bot collaboration in Wikidata. We argue that the combination of automated
and semi-automated work produces new challenges with respect to other online
collaboration platforms.
</summary>
    <author>
      <name>Alessandro Piscopo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the workshop 'The Changing Contours of "Participation"
  in Data-driven, Algorithmic Ecosystems: Challenges, Tactics, and an Agenda',
  at CSCW 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.00931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04144v1</id>
    <updated>2018-10-03T09:31:14Z</updated>
    <published>2018-10-03T09:31:14Z</published>
    <title>A Brief Survey on Autonomous Vehicle Possible Attacks, Exploits and
  Vulnerabilities</title>
    <summary>  Advanced driver assistance systems are advancing at a rapid pace and all
major companies started investing in developing the autonomous vehicles. But
the security and reliability is still uncertain and debatable. Imagine that a
vehicle is compromised by the attackers and then what they can do. An attacker
can control brake, accelerate and even steering which can lead to catastrophic
consequences. This paper gives a very short and brief overview of most of the
possible attacks on autonomous vehicle software and hardware and their
potential implications.
</summary>
    <author>
      <name>Amara Dinesh Kumar</name>
    </author>
    <author>
      <name>Koti Naga Renu Chebrolu</name>
    </author>
    <author>
      <name>Vinayakumar R</name>
    </author>
    <author>
      <name>Soman KP</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages,1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.04144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04943v1</id>
    <updated>2018-10-11T10:32:54Z</updated>
    <published>2018-10-11T10:32:54Z</published>
    <title>Interactive Cognitive Assessment Tools: A Case Study on Digital Pens for
  the Clinical Assessment of Dementia</title>
    <summary>  Interactive cognitive assessment tools may be valuable for doctors and
therapists to reduce costs and improve quality in healthcare systems. Use cases
and scenarios include the assessment of dementia. In this paper, we present our
approach to the semi-automatic assessment of dementia. We describe a case study
with digital pens for the patients including background, problem description
and possible solutions. We conclude with lessons learned when implementing
digital tests, and a generalisation for use outside the cognitive impairments
field.
</summary>
    <author>
      <name>Daniel Sonntag</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.10771v1</id>
    <updated>2018-10-25T08:43:29Z</updated>
    <published>2018-10-25T08:43:29Z</published>
    <title>An Incremental Truth Inference Approach to Aggregate Crowdsourcing
  Contributions in Games with a Purpose</title>
    <summary>  We introduce our approach for incremental truth inference over the
contributions provided by players of Games with a Purpose: we motivate the need
for such a method with the specificity of GWAP vs. traditional crowdsourcing;
we explain and formalize the proposed process and we explain its positive
consequences; finally, we illustrate the results of an experimental comparison
with state-of-the-art approaches, performed on data collected through two
different GWAPs, thus showing the properties of our proposed framework.
</summary>
    <author>
      <name>Irene Celino</name>
    </author>
    <author>
      <name>Gloria Re Calegari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.10771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.13206v1</id>
    <updated>2018-10-31T10:47:10Z</updated>
    <published>2018-10-31T10:47:10Z</published>
    <title>A speech-based driver assisting module for Intelligent Transport System</title>
    <summary>  Aim of this research is to transform images of roadside traffic panels to
speech to assist the vehicle driver, which is a new approach in the
state-of-the-art of the advanced driver assistance systems. The designed system
comprises of three modules, where the first module is used to capture and
detect the text area in traffic panels, second module is responsible for
converting the image of the detected text area to editable text and the last
module is responsible for transforming the text to speech. Additionally, during
experiments, we developed a corpus of 250 images of traffic panels for two
Indian languages.
</summary>
    <author>
      <name>Himangshu Sarma</name>
    </author>
    <author>
      <name>Navanath Saharia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Conference on Contemporary Computing and
  Informatics (IC3I), India, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.13206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.13206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00101v1</id>
    <updated>2018-10-31T20:26:00Z</updated>
    <published>2018-10-31T20:26:00Z</published>
    <title>A Process-driven View on Summative Evaluation of Visual Analytics
  Solutions</title>
    <summary>  Many evaluation methods have been applied to assess the usefulness of visual
analytics solutions. These methods are branching from a variety of origins with
different assumptions, and goals. We provide a high-level overview of the
process employed in each method using the generic evaluation model "GEM" that
generalizes the process of usefulness evaluation. The model treats evaluation
methods as processes that generate evidence of usefulness as output. Our model
serves three purposes: It educate new VA practitioners about the heterogeneous
evaluation practices in the field, it highlights potential risks in the process
of evaluation which reduces their validity and It provide a guideline to elect
suitable evaluation method.
</summary>
    <author>
      <name>Mosab Khayat</name>
    </author>
    <author>
      <name>Arif Ghafoor</name>
    </author>
    <link href="http://arxiv.org/abs/1811.00101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03401v1</id>
    <updated>2018-11-08T13:32:53Z</updated>
    <published>2018-11-08T13:32:53Z</published>
    <title>Your Eyes Say You're Lying: An Eye Movement Pattern Analysis for Face
  Familiarity and Deceptive Cognition</title>
    <summary>  Eye movement patterns reflect human latent internal cognitive activities. We
aim to discover eye movement patterns during face recognition under different
cognitions of information concealing. These cognitions include the degrees of
face familiarity and deception or not, namely telling the truth when observing
familiar and unfamiliar faces, and deceiving in front of familiar faces. We
apply Hidden Markov models with Gaussian emission to generalize regions and
trajectories of eye fixation points under the above three conditions. Our
results show that both eye movement patterns and eye gaze regions become
significantly different during deception compared with truth-telling. We show
the feasibility of detecting deception and further cognitive activity
classification using eye movement patterns.
</summary>
    <author>
      <name>Jiaxu Zuo</name>
    </author>
    <author>
      <name>Tom Gedeon</name>
    </author>
    <author>
      <name>Zhenyue Qin</name>
    </author>
    <link href="http://arxiv.org/abs/1811.03401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07206v1</id>
    <updated>2018-11-17T18:28:44Z</updated>
    <published>2018-11-17T18:28:44Z</published>
    <title>On Human Robot Interaction using Multiple Modes</title>
    <summary>  Humanoid robots have apparently similar body structure like human beings. Due
to their technical design, they are sharing the same workspace with humans.
They are placed to clean things, to assist old age people, to entertain us and
most importantly to serve us. To be acceptable in the household, they must have
higher level of intelligence than industrial robots and they must be social and
capable of interacting people around it, who are not supposed to be robot
specialist. All these come under the field of human robot interaction (HRI).
There are various modes like speech, gesture, behavior etc. through which human
can interact with robots. To solve all these challenges, a multimodel technique
has been introduced where gesture as well as speech is used as a mode of
interaction.
</summary>
    <author>
      <name>Neha Baranwal</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07271v2</id>
    <updated>2018-12-19T23:20:03Z</updated>
    <published>2018-11-18T04:37:32Z</published>
    <title>Ethical Dimensions of Visualization Research</title>
    <summary>  Visualizations have a potentially enormous influence on how data are used to
make decisions across all areas of human endeavor. However, it is not clear how
this power connects to ethical duties: what obligations do we have when it
comes to visualizations and visual analytics systems, beyond our duties as
scientists and engineers? Drawing on historical and contemporary examples, I
address the moral components of the design and use of visualizations, identify
some ongoing areas of visualization research with ethical dilemmas, and propose
a set of additional moral obligations that we have as designers, builders, and
researchers of visualizations.
</summary>
    <author>
      <name>Michael Correll</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3290605.3300418</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3290605.3300418" rel="related"/>
    <link href="http://arxiv.org/abs/1811.07271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07273v1</id>
    <updated>2018-11-18T04:59:38Z</updated>
    <published>2018-11-18T04:59:38Z</published>
    <title>Design and Assessment for Hybrid Courses: Insights and Overviews</title>
    <summary>  Technology is influencing education, providing new delivery and assessment
models. A combination between online and traditional course, the hybrid
(blended) course, may present a solution with many benefits as it provides a
gradual transition towards technology enabled education. This research work
provides a set of definitions for several course delivery approaches, and
evaluates five years of data from a course that has been converted from
traditional face-to-face delivery, to hybrid delivery. The collected
experimental data proves that the revised course, in the hybrid delivery mode,
is at least as good, if not better, than it previously was and it provides some
benefits in terms of student retention.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Stephen White</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advances in Life Sciences (2015),
  vol.7(3), pp.122-131</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.07273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.08244v1</id>
    <updated>2018-11-20T13:42:38Z</updated>
    <published>2018-11-20T13:42:38Z</published>
    <title>Interplay of Game Incentives, Player Profiles and Task Difficulty in
  Games with a Purpose</title>
    <summary>  How to take multiple factors into account when evaluating a Game with a
Purpose? How is player behaviour or participation influenced by different
incentives? How does player engagement impact their accuracy in solving tasks?
In this paper, we present a detailed investigation of multiple factors
affecting the evaluation of a GWAP and we show how they impact on the achieved
results. We inform our study with the experimental assessment of a GWAP
designed to solve a multinomial classification task.
</summary>
    <author>
      <name>Gloria Re Calegari</name>
    </author>
    <author>
      <name>Irene Celino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-03667-6_20</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-03667-6_20" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures, 21st International Conference on Knowledge
  Engineering and Knowledge Management (EKAW 2018)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNAI Volume 11313 (2018), pp. 306-321</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.08244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.08833v1</id>
    <updated>2018-11-20T06:00:02Z</updated>
    <published>2018-11-20T06:00:02Z</published>
    <title>Beyond the Desktop: Emerging Technologies for Supporting 3D
  Collaborative Teams</title>
    <summary>  The emergence of several trends, including the increased availability of
wireless networks, miniaturization of electronics and sensing technologies, and
novel input and output devices, is creating a demand for integrated, full-time
displays for use across a wide range of applications, including collaborative
environments. In this paper, we present and discuss emerging visualization
methods we are developing particularly as they relate to deployable displays
and displays worn on the body to support mobile users.
</summary>
    <author>
      <name>Jannick Rolland</name>
    </author>
    <author>
      <name>Ozan Cakmakci</name>
    </author>
    <author>
      <name>Jeff Covelli</name>
    </author>
    <author>
      <name>Cali Fidopiastis</name>
    </author>
    <author>
      <name>Florian Fournier</name>
    </author>
    <author>
      <name>Ricardo Martins</name>
    </author>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Denise Nicholson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12008-007-0027-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12008-007-0027-z" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Interactive Design and Manufacturing
  (2007), Vol. 4(1), pp. 239-241</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.08833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10819v1</id>
    <updated>2018-11-27T05:01:43Z</updated>
    <published>2018-11-27T05:01:43Z</published>
    <title>Isabelle/jEdit as IDE for Domain-specific Formal Languages and Informal
  Text Documents</title>
    <summary>  Isabelle/jEdit is the main application of the Prover IDE (PIDE) framework and
the default user-interface of Isabelle, but it is not limited to theorem
proving. This paper explores possibilities to use it as a general IDE for
formal languages that are defined in user-space, and embedded into informal
text documents. It covers overall document structure with auxiliary files and
document antiquotations, formal text delimiters and markers for interpretation
(via control symbols). The ultimate question behind this: How far can we
stretch a plain text editor like jEdit in order to support semantic text
processing, with support by the underlying PIDE framework?
</summary>
    <author>
      <name>Makarius Wenzel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.284.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.284.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings F-IDE 2018, arXiv:1811.09014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 284, 2018, pp. 71-84</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.10819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00350v1</id>
    <updated>2018-12-31T18:35:26Z</updated>
    <published>2018-12-31T18:35:26Z</published>
    <title>Responsive Equilibrium for Self-Adaptive Ubiquitous Interaction</title>
    <summary>  This work attempts to unify two domains: the Game Theory for cooperative
control systems and the Responsive Web Design, under the umbrella of
crowdsourcing for information gain on Ubiquous Sytems related to different
devices (as PC, Tablet, Mobile,...) This paper proposes a framework for
adapting DOM objects components for a disaggregated system, which dynamically
composes web pages for different kind of devices including ubiquitous/pervasive
computing systems. It introduces the notions of responsive webdesign for
non-cooperative Nash equilibrium proposing an algorithm (RE-SAUI) for the
dynamic interface based on the game theory.
</summary>
    <author>
      <name>Massimiliano Dal Mas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, for details see: http://www.maxdalmas.com</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.00350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03B65, 68P05, 68P10, 68Q55, 68T30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m; H.3.1; I.2.3; I.2.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.01001v1</id>
    <updated>2019-01-04T07:36:05Z</updated>
    <published>2019-01-04T07:36:05Z</published>
    <title>Identifying Barriers to Adoption for Rust through Online Discourse</title>
    <summary>  Rust is a low-level programming language known for its unique approach to
memory-safe systems programming and for its steep learning curve. To understand
what makes Rust difficult to adopt, we surveyed the top Reddit and Hacker News
posts and comments about Rust; from these online discussions, we identified
three hypotheses about Rust's barriers to adoption. We found that certain key
features, idioms, and integration patterns were not easily accessible to new
users.
</summary>
    <author>
      <name>Anna Zeng</name>
    </author>
    <author>
      <name>Will Crichton</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 9th Workshop on Evaluation and Usability of
  Programming Languages and Tools, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.01001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.01920v1</id>
    <updated>2019-01-07T16:59:14Z</updated>
    <published>2019-01-07T16:59:14Z</published>
    <title>Data is Personal: Attitudes and Perceptions of Data Visualization in
  Rural Pennsylvania</title>
    <summary>  Many of the guidelines that inform how designers create data visualizations
originate in studies that unintentionally exclude populations that are most
likely to be among the 'data poor'. In this paper, we explore which factors may
drive attention and trust in rural populations with diverse economic and
educational backgrounds - a segment that is largely underrepresented in the
data visualization literature. In 42 semi-structured interviews in rural
Pennsylvania (USA), we find that a complex set of factors intermix to inform
attitudes and perceptions about data visualization - including educational
background, political affiliation, and personal experience. The data and
materials for this research can be found at https://osf.io/uxwts/
</summary>
    <author>
      <name>Evan M. Peck</name>
    </author>
    <author>
      <name>Sofia E. Ayuso</name>
    </author>
    <author>
      <name>Omar El-Etr</name>
    </author>
    <link href="http://arxiv.org/abs/1901.01920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05835v1</id>
    <updated>2019-01-16T02:32:13Z</updated>
    <published>2019-01-16T02:32:13Z</published>
    <title>Unobtrusive and Multimodal Approach for Behavioral Engagement Detection
  of Students</title>
    <summary>  We propose a multimodal approach for detection of students' behavioral
engagement states (i.e., On-Task vs. Off-Task), based on three unobtrusive
modalities: Appearance, Context-Performance, and Mouse. Final behavioral
engagement states are achieved by fusing modality-specific classifiers at the
decision level. Various experiments were conducted on a student dataset
collected in an authentic classroom.
</summary>
    <author>
      <name>Nese Alyuz</name>
    </author>
    <author>
      <name>Eda Okur</name>
    </author>
    <author>
      <name>Utku Genc</name>
    </author>
    <author>
      <name>Sinem Aslan</name>
    </author>
    <author>
      <name>Cagri Tanriover</name>
    </author>
    <author>
      <name>Asli Arslan Esme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th Women in Machine Learning Workshop (WiML 2017), co-located with
  the 31st Conference on Neural Information Processing Systems (NeurIPS 2017),
  Long Beach, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.05835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01117v1</id>
    <updated>2019-02-04T10:44:50Z</updated>
    <published>2019-02-04T10:44:50Z</published>
    <title>Exploring Temporal Dependencies in Multimodal Referring Expressions with
  Mixed Reality</title>
    <summary>  In collaborative tasks, people rely both on verbal and non-verbal cues
simultaneously to communicate with each other. For human-robot interaction to
run smoothly and naturally, a robot should be equipped with the ability to
robustly disambiguate referring expressions. In this work, we propose a model
that can disambiguate multimodal fetching requests using modalities such as
head movements, hand gestures, and speech. We analysed the acquired data from
mixed reality experiments and formulated a hypothesis that modelling temporal
dependencies of events in these three modalities increases the model's
predictive power. We evaluated our model on a Bayesian framework to interpret
referring expressions with and without exploiting a temporal prior.
</summary>
    <author>
      <name>Elena Sibirtseva</name>
    </author>
    <author>
      <name>Ali Ghadirzadeh</name>
    </author>
    <author>
      <name>Iolanda Leite</name>
    </author>
    <author>
      <name>Mårten Björkman</name>
    </author>
    <author>
      <name>Danica Kragic</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02996v1</id>
    <updated>2019-02-08T09:53:53Z</updated>
    <published>2019-02-08T09:53:53Z</published>
    <title>SYM: Toward a New Tool in User's Mood Determination</title>
    <summary>  Even though the emotional state is increasingly taken into account in
scientific studies aimed at determining user experience of user acceptance,
there are still only a few normalized tools. In this article, we decided to
focus on mood determination as we consider this affective state to be more
pervasive and more understandable by the person who is experiencing it. Thus,
we propose a prototypical tool called SYM (Spot Your Mood) as a new tool in
user mood determination to be used in many different situations.
</summary>
    <author>
      <name>Willy Yvart</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DeVisu</arxiv:affiliation>
    </author>
    <author>
      <name>Charles-Alexandre Delestage</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DeVisu</arxiv:affiliation>
    </author>
    <author>
      <name>Sylvie Leleu-Merviel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DeVisu</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EmoVis 2016, ACM IUI 2016 Workshop on Emotion and Visualization,
  Mar 2016, Sonoma, United States. pp.23-28, 2016, Proceedings of EmoVis 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.02996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04929v1</id>
    <updated>2019-02-13T14:40:08Z</updated>
    <published>2019-02-13T14:40:08Z</published>
    <title>Integrating Neurophysiological Sensors and Driver Models for Safe and
  Performant Automated Vehicle Control in Mixed Traffic</title>
    <summary>  In future mixed traffic Highly Automated Vehicles (HAV) will have to resolve
interactions with human operated traffic. A particular problem for HAVs is
detection of human states influencing safety critical decisions and driving
behavior of humans. We demonstrate the value proposition of neurophysiological
sensors and driver models for optimizing performance of HAVs under safety
constraints in mixed traffic applications.
</summary>
    <author>
      <name>Werner Damm</name>
    </author>
    <author>
      <name>Martin Fränzle</name>
    </author>
    <author>
      <name>Andreas Lüdtke</name>
    </author>
    <author>
      <name>Jochem W. Rieger</name>
    </author>
    <author>
      <name>Alexander Trende</name>
    </author>
    <author>
      <name>Anirudh Unni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 Figures, submitted to HFIV'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.07807v1</id>
    <updated>2019-02-20T23:14:41Z</updated>
    <published>2019-02-20T23:14:41Z</published>
    <title>Simulating Forces - Learning Through Touch, Virtual Laboratories</title>
    <summary>  With the expansion of e-learning course curricula and the affordability of
haptic devices, at-home virtual laboratories are emerging as an increasingly
viable option for e-learners. We outline three novel haptic simulations for the
introductory physics concepts of friction, the Coriolis Effect, and Precession.
These simulations provide force feedback through one or more Novint Falcon
devices, allowing students to "feel" the forces at work in a controlled
learning environment. This multi-modal approach to education (beyond the
audiovisual) may lead to increased interest and immersion for e-learners and
appeal to the kinesthetic learners who may struggle in a traditional e-learning
course setting.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Faith-Anne L. Kocadag</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IARIA, 2013, pp.55-58</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.07807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09022v1</id>
    <updated>2019-02-24T22:05:57Z</updated>
    <published>2019-02-24T22:05:57Z</published>
    <title>Designing for Health Chatbots</title>
    <summary>  Building conversational agents have many technical, design and linguistic
challenges. Other more complex elements include using emotionally intelligent
conversational agent to build trust with the individuals. In this chapter, we
introduce the nature of conversational user interfaces (CUIs) for health and
describe UX design principles informed by a systematic literature review of
relevant research works. We analyze scientific literature in conversational
interfaces and chatterbots, providing a survey of major studies and describing
UX design principles and interaction patterns.
</summary>
    <author>
      <name>Ahmed Fadhil</name>
    </author>
    <author>
      <name>Gianluca Schiavo</name>
    </author>
    <link href="http://arxiv.org/abs/1902.09022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09289v1</id>
    <updated>2019-02-25T14:29:43Z</updated>
    <published>2019-02-25T14:29:43Z</published>
    <title>A Virtual Teaching Assistant for Personalized Learning</title>
    <summary>  In this extended abstract, we propose an intelligent system that can be used
as a Personalized Virtual Teaching Assistant (PVTA) to improve the students
learning experience both for online and on-site courses. We show the
architecture of such system, which is composed of an instance of IBM Watson
Assistant and a server, and present an initial implementation, consisting in a
chatbot that can be questioned about the content and the organization of the
RecSys course, an introductory course on recommender systems.
</summary>
    <author>
      <name>Luca Benedetto</name>
    </author>
    <author>
      <name>Paolo Cremonesi</name>
    </author>
    <author>
      <name>Manuel Parenti</name>
    </author>
    <link href="http://arxiv.org/abs/1902.09289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09749v1</id>
    <updated>2019-02-26T06:20:44Z</updated>
    <published>2019-02-26T06:20:44Z</published>
    <title>Analyzing the Use of Camera Glasses in the Wild</title>
    <summary>  Camera glasses enable people to capture point-of-view videos using a common
accessory, hands-free. In this paper, we investigate how, when, and why people
used one such product: Spectacles. We conducted 39 semi-structured interviews
and surveys with 191 owners of Spectacles. We found that the form factor
elicits sustained usage behaviors, and opens opportunities for new use-cases
and types of content captured. We provide a usage typology, and highlight
societal and individual factors that influence the classification of behaviors.
</summary>
    <author>
      <name>Taryn Bipat</name>
    </author>
    <author>
      <name>Maarten Willem Bos</name>
    </author>
    <author>
      <name>Rajan Vaish</name>
    </author>
    <author>
      <name>Andrés Monroy-Hernández</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3290605.3300651</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3290605.3300651" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 37th Annual ACM Conference on Human Factors in
  Computing Systems (CHI 2019). ACM, New York, NY, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.09749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.00283v1</id>
    <updated>2019-03-01T13:18:12Z</updated>
    <published>2019-03-01T13:18:12Z</published>
    <title>Visualizing Multiple Process Attributes in one 3D Process Representation</title>
    <summary>  Business process models are usually visualized using 2D representations.
However, multiple attributes contained in the models such as time, data, and
resources can quickly lead to cluttered and complex representations. To address
these challenges, this paper proposes techniques utilizing the 3D space (e.g.,
visualizing swim lanes as third dimension). All techniques are implemented in a
3D process viewer. On top of showing the feasibility of the proposed
techniques, the 3D process viewer served as live demonstration after which 42
participants completed a survey. The survey results support that 3D
representations are well-suited to convey information on multiple attributes in
business process models.
</summary>
    <author>
      <name>Manuel Gall</name>
    </author>
    <author>
      <name>Stefanie Rinderle-Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1903.00283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.00438v1</id>
    <updated>2019-03-01T17:58:06Z</updated>
    <published>2019-03-01T17:58:06Z</published>
    <title>Web-Based 3D and Haptic Interactive Environments for e-Learning,
  Simulation, and Training</title>
    <summary>  Knowledge creation occurs in the process of social interaction. As our
service-based society is evolving into a knowledge-based society there is an
acute need for more effective collaboration and knowledge-sharing systems to be
used by geographically scattered people. We present the use of Web3D components
and standards, such as X3D, in combination with the haptic (tactile) paradigm,
for the development of new communication channels for e-Learning and
simulation.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Ivan Sopin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-01344-7_26</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-01344-7_26" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISBN:978-3-642-01343-0</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.00438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01219v1</id>
    <updated>2019-03-01T07:36:19Z</updated>
    <published>2019-03-01T07:36:19Z</published>
    <title>Low-cost VR Collaborative System equipped with Haptic Feedback</title>
    <summary>  In this paper, we present a low-cost virtual reality (VR) collaborative
system equipped with a haptic feedback sensation system. This system is
composed of a Kinect sensor for bodies and gestures detection, a
microcontroller and vibrators to simulate outside interactions, and smartphone
powered cardboard, all of this are put into a network implemented with Unity 3D
game engine. CCS CONCEPTS $\bullet$ Interaction paradigms $\rightarrow$ Virtual
reality; Collaborative interaction; $\bullet$ Hardware $\rightarrow$ Sensors
and actuators; Wireless devices; KEYWORDS collaborative virtual reality, haptic
feedback system.
</summary>
    <author>
      <name>Samir Benbelkacem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CDTA</arxiv:affiliation>
    </author>
    <author>
      <name>Abdelkader Bellarbi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CDTA</arxiv:affiliation>
    </author>
    <author>
      <name>Nadia Zenati-Henda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CDTA</arxiv:affiliation>
    </author>
    <author>
      <name>Ahmed Bentaleb</name>
    </author>
    <author>
      <name>Ahmed Bellabaci</name>
    </author>
    <author>
      <name>Samir Otmane</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">the 24th ACM Symposium, Nov 2018, Tokyo, Japan. ACM Press, pp.1-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.01219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02723v1</id>
    <updated>2019-03-07T04:29:50Z</updated>
    <published>2019-03-07T04:29:50Z</published>
    <title>Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality</title>
    <summary>  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
</summary>
    <author>
      <name>Zhenliang Zhang</name>
    </author>
    <author>
      <name>Cong Wang</name>
    </author>
    <author>
      <name>Dongdong Weng</name>
    </author>
    <author>
      <name>Yue Liu</name>
    </author>
    <author>
      <name>Yongtian Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VR Poster</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.02723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02978v1</id>
    <updated>2019-03-07T15:14:42Z</updated>
    <published>2019-03-07T15:14:42Z</published>
    <title>Integrating Artificial and Human Intelligence for Efficient Translation</title>
    <summary>  Current advances in machine translation increase the need for translators to
switch from traditional translation to post-editing of machine-translated text,
a process that saves time and improves quality. Human and artificial
intelligence need to be integrated in an efficient way to leverage the
advantages of both for the translation task. This paper outlines approaches at
this boundary of AI and HCI and discusses open research questions to further
advance the field.
</summary>
    <author>
      <name>Nico Herbig</name>
    </author>
    <author>
      <name>Santanu Pal</name>
    </author>
    <author>
      <name>Josef van Genabith</name>
    </author>
    <author>
      <name>Antonio Krüger</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.03268v1</id>
    <updated>2019-03-08T03:50:21Z</updated>
    <published>2019-03-08T03:50:21Z</published>
    <title>Haptic Simulator for Liver Diagnostics through Palpation</title>
    <summary>  Mechanical properties of biological tissue for both histological and
pathological considerations are often required in disease diagnostics. Such
properties can be simulated and explored with haptic technology. Development of
cost effective haptic-based simulators and their introduction in the minimally
invasive surgery learning cycle is still in its infancy. Receiving pretraining
in a core set of surgical skills can reduce skill acquisition time and risks.
We present the development of a visuo-haptic simulator module designed to train
internal organs disease diagnostics through palpation. The module is part of a
set of tools designed to train and improve basic surgical skills for minimally
invasive surgery.
</summary>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <author>
      <name>Crenguta M. Bogdan</name>
    </author>
    <author>
      <name>Adrian Seitan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/978--1--61499--022--2--156</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/978--1--61499--022--2--156" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Medicine Meets Virtual Reality 19, 2012, pp.156-160</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.03268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07195v1</id>
    <updated>2019-03-17T23:12:45Z</updated>
    <published>2019-03-17T23:12:45Z</published>
    <title>Older Adults and Voice Interaction: A Pilot Study with Google Home</title>
    <summary>  In this paper we present the results of an exploratory study examining the
potential of voice assistants (VA) for some groups of older adults in the
context of Smart Home Technology (SHT). To research the aspect of older adults'
interaction with voice user interfaces (VUI) we organized two workshops and
gathered insights concerning possible benefits and barriers to the use of VA
combined with SHT by older adults. Apart from evaluating the participants'
interaction with the devices during the two workshops we also discuss some
improvements to the VA interaction paradigm.
</summary>
    <author>
      <name>Jarosław Kowalski</name>
    </author>
    <author>
      <name>Anna Jaskulska</name>
    </author>
    <author>
      <name>Kinga Skorupska</name>
    </author>
    <author>
      <name>Katarzyna Abramczuk</name>
    </author>
    <author>
      <name>Cezary Biele</name>
    </author>
    <author>
      <name>Wiesław Kopeć</name>
    </author>
    <author>
      <name>Krzysztof Marasek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3290607.3312973</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3290607.3312973" rel="related"/>
    <link href="http://arxiv.org/abs/1903.07195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.09866v1</id>
    <updated>2019-03-23T18:58:38Z</updated>
    <published>2019-03-23T18:58:38Z</published>
    <title>Referring to the recently seen: reference and perceptual memory in
  situated dialog</title>
    <summary>  From theoretical linguistic and cognitive perspectives, situated dialog
systems are interesting as they provide ideal test-beds for investigating the
interaction between language and perception. At the same time there are a
growing number of practical applications, for example robotic systems and
driver-less cars, where spoken interfaces, capable of situated dialog, promise
many advantages. To date, however much of the work on situated dialog has
focused resolving anaphoric or exophoric references. This paper, by contrast,
opens up the question of how perceptual memory and linguistic references
interact, and the challenges that this poses to computational models of
perceptually grounded dialog.
</summary>
    <author>
      <name>John D. Kelleher</name>
    </author>
    <author>
      <name>Simon Dobnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 Pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.09866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00472v2</id>
    <updated>2019-04-02T07:43:57Z</updated>
    <published>2019-03-31T20:11:22Z</published>
    <title>Positive Personas: Integrating Well-being Determinants into Personas</title>
    <summary>  System design for well-being needs an appropriate tool to help designers to
determine relevant requirements that can help human well-being to flourish.
Personas come as a simple yet powerful tool in the early development stage of
the user interface design. Considering well-being determinants in the early
design process provide benefits for both the user and the development team.
Therefore, in this short paper, we performed a literature study to provide a
conceptual model of well-being in personas and propose positive design
interventions in the personas creation process.
</summary>
    <author>
      <name>Irawan Nurhas</name>
    </author>
    <author>
      <name>Stefan Geisler</name>
    </author>
    <author>
      <name>Jan Pawlowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18420/muc2017-mci-0356</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18420/muc2017-mci-0356" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, conference MuC 2017, Regensburg</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mensch und Computer 2017-Tagungsband: Spielend einfach
  interagieren</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.00472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.01629v1</id>
    <updated>2019-03-08T04:27:33Z</updated>
    <published>2019-03-08T04:27:33Z</published>
    <title>Challenges in the Deployment of Visuo-Haptic Virtual Environments on the
  Internet</title>
    <summary>  Haptic sensory feedback has been shown to complement the visual and auditory
senses, improve user performance and provide a greater sense of togetherness in
collaborative and interactive virtual environments. However, we are faced with
numerous challenges when deploying these systems over the present day Internet.
The most significant of these challenges are the network performance
limitations of the Wide Area Networks. In this paper, we offer a structured
examination of the current challenges in the deployment of haptic-based
distributed systems by analyzing the recent advances in the understanding of
these challenges and the progress that has been made to overcome them.
</summary>
    <author>
      <name>Jonathan Norman</name>
    </author>
    <author>
      <name>Felix G. Hamza-Lup</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCNT.2010.88</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCNT.2010.88" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer and Network Technology, 2010, pp. 33-37</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.01629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.01629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.01664v1</id>
    <updated>2019-04-02T20:51:27Z</updated>
    <published>2019-04-02T20:51:27Z</published>
    <title>Mirroring to Build Trust in Digital Assistants</title>
    <summary>  We describe experiments towards building a conversational digital assistant
that considers the preferred conversational style of the user. In particular,
these experiments are designed to measure whether users prefer and trust an
assistant whose conversational style matches their own. To this end we
conducted a user study where subjects interacted with a digital assistant that
responded in a way that either matched their conversational style, or did not.
Using self-reported personality attributes and subjects' feedback on the
interactions, we built models that can reliably predict a user's preferred
conversational style.
</summary>
    <author>
      <name>Katherine Metcalf</name>
    </author>
    <author>
      <name>Barry-John Theobald</name>
    </author>
    <author>
      <name>Garrett Weinberg</name>
    </author>
    <author>
      <name>Robert Lee</name>
    </author>
    <author>
      <name>Ing-Marie Jonsson</name>
    </author>
    <author>
      <name>Russ Webb</name>
    </author>
    <author>
      <name>Nicholas Apostoloff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02679v2</id>
    <updated>2019-04-11T16:00:53Z</updated>
    <published>2019-04-04T17:32:49Z</published>
    <title>Visualizing Attention in Transformer-Based Language Representation
  Models</title>
    <summary>  We present an open-source tool for visualizing multi-head self-attention in
Transformer-based language representation models. The tool extends earlier work
by visualizing attention at three levels of granularity: the attention-head
level, the model level, and the neuron level. We describe how each of these
views can help to interpret the model, and we demonstrate the tool on the BERT
model and the OpenAI GPT-2 model. We also present three use cases for analyzing
GPT-2: detecting model bias, identifying recurring patterns, and linking
neurons to model behavior.
</summary>
    <author>
      <name>Jesse Vig</name>
    </author>
    <link href="http://arxiv.org/abs/1904.02679v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02679v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.04015v1</id>
    <updated>2019-04-08T12:45:51Z</updated>
    <published>2019-04-08T12:45:51Z</published>
    <title>Implementation of a Daemon for OpenBCI</title>
    <summary>  This document describes a technical study of the electroencephalographic
(EEG) headset OpenBCI (New York, US). In comparison to research grade EEG, the
OpenBCI headset is affordable thus suitable for the general public use. In this
study we designed a daemon, that is, a background and continuous task
communicating with the headset, acquiring, filtering and analyzing the EEG
data. This study was promoted by the IHMTEK Company (Vienne, France) in 2016
within a thesis on the integration of EEG-based brain-computer interfaces in
virtual reality for the general public.
</summary>
    <author>
      <name>Maxime Chabance</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IHMTEK</arxiv:affiliation>
    </author>
    <author>
      <name>Grégoire Cattan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIPSA-Services, IHMTEK</arxiv:affiliation>
    </author>
    <author>
      <name>Bastien Maureille</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IHMTEK</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06128v1</id>
    <updated>2019-04-12T09:41:33Z</updated>
    <published>2019-04-12T09:41:33Z</published>
    <title>Situationally-Induced Impairments and Disabilities Research</title>
    <summary>  Research has shown that various environmental factors impact smartphone
interaction and lead to Situationally-Induced Impairments and Disabilities. In
this work we discuss the importance of thoroughly understanding the effects of
these situational impairments on smartphone interaction. We argue that
systematic investigation of the effects of different situational impairments is
quintessential for conducting successful research in the field of SIIDs that
might lead to building appropriate sensing, modelling, and adapting techniques.
We also provide insights for future work identifying potential directions to
conduct research in SIIDs.
</summary>
    <author>
      <name>Zhanna Sarsenbayeva</name>
    </author>
    <author>
      <name>Vassilis Kostakos</name>
    </author>
    <author>
      <name>Jorge Goncalves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the CHI'19 Workshop: Addressing the Challenges of
  Situationally-Induced Impairments and Disabilities in Mobile Interaction,
  2019 (arXiv:1904.05382)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.06128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06131v1</id>
    <updated>2019-04-12T09:44:42Z</updated>
    <published>2019-04-12T09:44:42Z</published>
    <title>Situationally Induced Impairment in Navigation Support for Runners</title>
    <summary>  Mobile devices are ubiquitous and support us in a myriad of situations. In
this paper, we study the support that mobile devices provide for navigation. It
presents our findings on the Situational Induced Impairments and Disabilities
(SIID) during running. We define the context of runners and the factors
affecting the use of mobile devices for navigation during running. We discuss
design implications and introduce early concepts to address the uncovered SIID
issues. This work contributes to the growing body of research on SIID in using
mobile devices.
</summary>
    <author>
      <name>Shreepriya Shreepriya</name>
    </author>
    <author>
      <name>Danilo Gallo</name>
    </author>
    <author>
      <name>Sruthi Viswanathan</name>
    </author>
    <author>
      <name>Jutta Willamowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the CHI'19 Workshop: Addressing the Challenges of
  Situationally-Induced Impairments and Disabilities in Mobile Interaction,
  2019 (arXiv:1904.05382)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.06131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06132v1</id>
    <updated>2019-04-12T09:46:30Z</updated>
    <published>2019-04-12T09:46:30Z</published>
    <title>Looking At Situationally-Induced Impairments And Disabilities (SIIDs)
  With People With Cognitive Brain Injury</title>
    <summary>  In this document, we discuss our work into a speaker recognition to support
people with prosopagnosia and the limitations of alerting the user of whom they
are in discussion with. We will discuss how current research into Situationally
Induced Impairments Disabilities (SIIDs) can assist people with disabilities
and vice versa and how our work can support people who may find themselves in a
situation where they are impaired with facial recognition.
</summary>
    <author>
      <name>Osian Smith</name>
    </author>
    <author>
      <name>Stephen Lindsay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the CHI'19 Workshop: Addressing the Challenges of
  Situationally-Induced Impairments and Disabilities in Mobile Interaction,
  2019 (arXiv:1904.05382)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.06132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06134v1</id>
    <updated>2019-04-12T09:48:14Z</updated>
    <published>2019-04-12T09:48:14Z</published>
    <title>Universal Design and Adaptive Interfaces as a Strategy for Induced
  Disabilities</title>
    <summary>  There is great promise in creating effective technology experiences during
situationally-induced impairments and disabilities through the combination of
universal design and adaptive interfaces. We believe this combination is a
powerful approach for meeting the UX needs of people with disabilities,
including those which are temporary in nature. Research in each of these areas,
and the combination, illustrates this promise.
</summary>
    <author>
      <name>Aaron Steinfeld</name>
    </author>
    <author>
      <name>John Zimmerman</name>
    </author>
    <author>
      <name>Anthony Tomasic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the CHI'19 Workshop: Addressing the Challenges of
  Situationally-Induced Impairments and Disabilities in Mobile Interaction,
  2019 (arXiv:1904.05382)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.06134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.13333v1</id>
    <updated>2019-04-30T15:55:34Z</updated>
    <published>2019-04-30T15:55:34Z</published>
    <title>Coevo: a collaborative design platform with artificial agents</title>
    <summary>  We present Coevo, an online platform that allows both humans and artificial
agents to design shapes that solve different tasks. Our goal is to explore
common shared design tools that can be used by humans and artificial agents in
a context of creation. This approach can provide a better knowledge transfer
and interaction with artificial agents since a common language of design is
defined. In this paper, we outline the main components of this platform and
discuss the definition of a human-centered language to enhance human-AI
collaboration in co-creation scenarios.
</summary>
    <author>
      <name>Gerard Serra</name>
    </author>
    <author>
      <name>David Miralles</name>
    </author>
    <link href="http://arxiv.org/abs/1904.13333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.13333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.5.m; I.2; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.02812v1</id>
    <updated>2019-05-07T21:07:56Z</updated>
    <published>2019-05-07T21:07:56Z</published>
    <title>From GenderMag to InclusiveMag: An Inclusive Design Meta-Method</title>
    <summary>  How can software practitioners assess whether their software supports diverse
users? Although there are empirical processes that can be used to find
"inclusivity bugs" piecemeal, what is often needed is a systematic inspection
method to assess soft-ware's support for diverse populations. To help fill this
gap, this paper introduces InclusiveMag, a generalization of GenderMag that can
be used to generate systematic inclusiveness methods for a particular dimension
of diversity. We then present a multi-case study covering eight diversity
dimensions, of eight teams' experiences applying InclusiveMag to eight
under-served populations and their "mainstream" counterparts.
</summary>
    <author>
      <name>Christopher Mendez</name>
    </author>
    <author>
      <name>Lara Letaw</name>
    </author>
    <author>
      <name>Margaret Burnett</name>
    </author>
    <author>
      <name>Simone Stumpf</name>
    </author>
    <author>
      <name>Anita Sarma</name>
    </author>
    <author>
      <name>Claudia Hilderbrand</name>
    </author>
    <link href="http://arxiv.org/abs/1905.02812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05390v1</id>
    <updated>2019-05-14T04:42:21Z</updated>
    <published>2019-05-14T04:42:21Z</published>
    <title>WatchOut: A Road Safety Extension for Pedestrians on a Public Windshield
  Display</title>
    <summary>  We conducted a field study to investigate whether public windshield displays
are applicable as an additional interactive digital road safety warning sign.
We focused on investigating the acceptance and usability of our novel public
windshield display and its potential use for future applications. The study has
shown that users are open-minded to the idea of an extraverted windshield
display regardless the use case, whether it is used for safety purposes or
different content. Contrary to our hypothesis most people assumed they would
mistrust the system if it were as well established as traffic lights and
primarily rely on their own perception.
</summary>
    <author>
      <name>Matthias Geiger</name>
    </author>
    <author>
      <name>Changkun Ou</name>
    </author>
    <author>
      <name>Cedric Quintes</name>
    </author>
    <link href="http://arxiv.org/abs/1905.05390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06762v1</id>
    <updated>2019-05-04T02:49:38Z</updated>
    <published>2019-05-04T02:49:38Z</published>
    <title>Smartphone app with usage of AR technologies - SolAR System</title>
    <summary>  The article describes the AR mobile system for Sun system simulation. The
main characteristics of AR systems architecture are given. The differences
between tracking and without tracking technics are underlined. The architecture
of the system of use of complemented reality for the study of astronomy is
described. The features of the system and the principles of its work are
determined.
</summary>
    <author>
      <name>Glib Shchur</name>
    </author>
    <author>
      <name>Nataliya Shakhovska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONTECHMOD. AN INTERNATIONAL QUARTERLY JOURNAL - 2018, Vol. 07,
  No. 3, 63 - 68</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.06762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06777v1</id>
    <updated>2019-05-15T09:55:10Z</updated>
    <published>2019-05-15T09:55:10Z</published>
    <title>Towards Comparing Programming Paradigms</title>
    <summary>  Rapid technological progress in computer sciences finds solutions and at the
same time creates ever more complex requirements. Due to an evolving complexity
todays programming languages provide powerful frameworks which offer standard
solutions for recurring tasks to assist the programmer and to avoid the
re-invention of the wheel with so-called out-of-the-box-features. In this
paper, we propose a way of comparing different programming paradigms on a
theoretical, technical and practical level. Furthermore, the paper presents the
results of an initial comparison of two representative programming approaches,
both in the closed SAP environment.
</summary>
    <author>
      <name>Igor Ivkic</name>
    </author>
    <author>
      <name>Alexander Wöhrer</name>
    </author>
    <author>
      <name>Markus Tauber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/ICITST.2017.8356440</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/ICITST.2017.8356440" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2017 12th International Conference for Internet Technology and
  Secured Transactions (ICITST), Cambridge, UK</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.06777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06995v1</id>
    <updated>2019-05-14T14:57:20Z</updated>
    <published>2019-05-14T14:57:20Z</published>
    <title>Making ethical decisions for the immersive web</title>
    <summary>  Mixed reality (MR) ethics occupies a space that intersects with web ethics,
emerging tech ethics, healthcare ethics and product ethics (among others). This
paper focuses on how we can build an immersive web that encourages ethical
development and usage. The technology is beyond emerging (footnote: generally,
the ethics of emerging technologies are focused on ethical assessments of
research and innovation), but not quite entrenched. We're still in a position
to intervene in the development process, instead of attempting to retrofit
ethical decisions into an established design. While we have a wider range of
data to analyze than most emerging technologies, we're still in a much more
speculative state than entrenched technologies. This space is a challenge and
an opportunity.
</summary>
    <author>
      <name>Diane Hosfelt</name>
    </author>
    <link href="http://arxiv.org/abs/1905.06995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07054v1</id>
    <updated>2019-05-16T22:59:14Z</updated>
    <published>2019-05-16T22:59:14Z</published>
    <title>Are Automated Vehicles Safer than Manually Driven Cars?</title>
    <summary>  Are automated vehicles really safer than manually driven vehicles? If so, how
would we know? Answering this question has spurred a contentious debate.
Unfortunately, several issues make answering this question difficult for the
foreseeable future. First, how do we measure safety? Second, how can we keep
track of automated vehicle (AV) safety? Finally, how do we determine what is or
what is not an AV? Until these questions are addressed, it will continue to be
difficult to determine whether or when AVs might really be safer than manually
driven vehicles.
</summary>
    <author>
      <name>Lionel Peter Robert Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00146-019-00894-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00146-019-00894-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages. AI &amp; Society (2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07394v1</id>
    <updated>2019-05-17T17:42:56Z</updated>
    <published>2019-05-17T17:42:56Z</published>
    <title>MiSC: Mixed Strategies Crowdsourcing</title>
    <summary>  Popular crowdsourcing techniques mostly focus on evaluating workers' labeling
quality before adjusting their weights during label aggregation. Recently,
another cohort of models regard crowdsourced annotations as incomplete tensors
and recover unfilled labels by tensor completion. However, mixed strategies of
the two methodologies have never been comprehensively investigated, leaving
them as rather independent approaches. In this work, we propose $\textit{MiSC}$
($\textbf{Mi}$xed $\textbf{S}$trategies $\textbf{C}$rowdsourcing), a versatile
framework integrating arbitrary conventional crowdsourcing and tensor
completion techniques. In particular, we propose a novel iterative Tucker label
aggregation algorithm that outperforms state-of-the-art methods in extensive
experiments.
</summary>
    <author>
      <name>Ching-Yun Ko</name>
    </author>
    <author>
      <name>Rui Lin</name>
    </author>
    <author>
      <name>Shu Li</name>
    </author>
    <author>
      <name>Ngai Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, accepted to IJCAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08367v1</id>
    <updated>2019-05-20T22:28:11Z</updated>
    <published>2019-05-20T22:28:11Z</published>
    <title>SmartNight: Turning Off the Lights on Android</title>
    <summary>  Smartphone users benefit from content with dark color schemes: increasingly
common OLED displays are more power efficient the darker the display, and many
users prefer a dark display for night time use. Despite these benefits, many
applications and the majority of web content are drawn with white backgrounds.
There are many partial solutions to darken the displayed content, but none work
in all situations. Enter SmartNight, a content-aware solution to dynamically
darken content on Android. By trading off content fidelity, Android with
SmartNight displays content with nearly 90% lower average picture level. It is
implemented in the Android framework, and requires no external support. It
seamlessly incorporates existing solutions, making it a bridge between the
state-of-the-art and future solutions.
</summary>
    <author>
      <name>Andrew Banman</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09644v1</id>
    <updated>2019-05-22T09:37:11Z</updated>
    <published>2019-05-22T09:37:11Z</published>
    <title>Scientific Programs Imply Uncertainty. Results Expected and Unexpected</title>
    <summary>  Science and engineering have requests for a wide variety of programs, but I
think that all of them can be divided between two groups. Programs of the first
group deal with the well known situations and, by using well known equations,
give results for any combination of input parameters. Such programs are
specialized very powerful calculators. Another group of programs is needed to
analyse the situations with different levels of uncertainty. Programs are
developed at the best level of their authors, but scientists need to look at
the situations beyond the area of current knowledge, and they need programs to
do analysis in the areas of uncertainty. Is it possible do design programs
which allow to analyse the situations beyond the knowledge of developers?
</summary>
    <author>
      <name>Sergey Andreyev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.09644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00684v1</id>
    <updated>2019-07-01T12:10:25Z</updated>
    <published>2019-07-01T12:10:25Z</published>
    <title>Enabling Dialogue Management with Dynamically Created Dialogue Actions</title>
    <summary>  In order to take up the challenge of realising user-adaptive system
behaviour, we present an extension for the existing OwlSpeak Dialogue Manager
which enables the handling of dynamically created dialogue actions. This leads
to an increase in flexibility which can be used for adaptation tasks. After the
implementation of the modifications and the integration of the Dialogue Manager
into a full Spoken Dialogue System, an evaluation of the system has been
carried out. The results indicate that the participants were able to conduct
meaningful dialogues and that the system performs satisfactorily, showing that
the implementation of the Dialogue Manager was successful.
</summary>
    <author>
      <name>Juliana Miehle</name>
    </author>
    <author>
      <name>Louisa Pragst</name>
    </author>
    <author>
      <name>Wolfgang Minker</name>
    </author>
    <author>
      <name>Stefan Ultes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00998v1</id>
    <updated>2019-07-01T18:09:39Z</updated>
    <published>2019-07-01T18:09:39Z</published>
    <title>Geographical Security Questions for Fallback Authentication</title>
    <summary>  Fallback authentication is the backup authentication method used when the
primary authentication method (e.g., passwords, fingerprints, etc.) fails.
Currently, widely-deployed fallback authentication methods (e.g., security
questions, email resets, and SMS resets) suffer from documented security and
usability flaws that threaten the security of accounts. These flaws motivate us
to design and study Geographical Security Questions (GeoSQ), a system for
fallback authentication. GeoSQ is an Android application that utilizes
autobiographical location data for fallback authentication. We performed
security and usability analyses of GeoSQ through an in-person two-session lab
study (n=36,18 pairs). Our results indicate that GeoSQ exceeds the security of
its counterparts, while its usability (specifically login time) has room for
improvement.
</summary>
    <author>
      <name>Alaadin Addas</name>
    </author>
    <author>
      <name>Julie Thorpe</name>
    </author>
    <author>
      <name>Amirali Salehi-Abari</name>
    </author>
    <link href="http://arxiv.org/abs/1907.00998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01921v1</id>
    <updated>2019-07-03T13:10:10Z</updated>
    <published>2019-07-03T13:10:10Z</published>
    <title>Chatbots as Unwitting Actors</title>
    <summary>  Chatbots are popular for both task-oriented conversations and unstructured
conversations with web users. Several different approaches to creating comedy
and art exist across the field of computational creativity. Despite the
popularity and ease of use of chatbots, there have not been any attempts by
artists or comedians to use these systems for comedy performances. We present
two initial attempts to do so from our comedy podcast and call for future work
toward both designing chatbots for performance and for performing alongside
chatbots.
</summary>
    <author>
      <name>Allison Perrone</name>
    </author>
    <author>
      <name>Justin Edwards</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3342775.3342799</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3342775.3342799" rel="related"/>
    <link href="http://arxiv.org/abs/1907.01921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01923v1</id>
    <updated>2019-07-03T13:12:23Z</updated>
    <published>2019-07-03T13:12:23Z</published>
    <title>A Need for Trust in Conversational Interface Research</title>
    <summary>  Across several branches of conversational interaction research including
interactions with social robots, embodied agents, and conversational
assistants, users have identified trust as a critical part of those
interactions. Nevertheless, there is little agreement on what trust means
within these sort of interactions or how trust can be measured. In this paper,
we explore some of the dimensions of trust as it has been understood in
previous work and we outline some of the ways trust has been measured in the
hopes of furthering discussion of the concept across the field.
</summary>
    <author>
      <name>Justin Edwards</name>
    </author>
    <author>
      <name>Elaheh Sanoubari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3342775.3342809</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3342775.3342809" rel="related"/>
    <link href="http://arxiv.org/abs/1907.01923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02349v1</id>
    <updated>2019-07-04T12:03:03Z</updated>
    <published>2019-07-04T12:03:03Z</published>
    <title>Experience Management in Multi-player Games</title>
    <summary>  Experience Management studies AI systems that automatically adapt interactive
experiences such as games to tailor to specific players and to fulfill design
goals. Although it has been explored for several decades, existing work in
experience management has mostly focused on single-player experiences. This
paper is a first attempt at identifying the main challenges to expand EM to
multi-player/multi-user games or experiences. We also make connections to
related areas where solutions for similar problems have been proposed
(especially group recommender systems) and discusses the potential impact and
applications of multi-player EM.
</summary>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <author>
      <name>Santiago Ontañón</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04104v1</id>
    <updated>2019-07-09T11:59:54Z</updated>
    <published>2019-07-09T11:59:54Z</published>
    <title>User Guidance for Interactive Camera Calibration</title>
    <summary>  For building a Augmented Reality (AR) pipeline, the most crucial step is the
camera calibration as overall quality heavily depends on it. In turn camera
calibration itself is influenced most by the choice of camera-to-pattern poses
- yet currently there is only little research on guiding the user to a specific
pose. We build upon our novel camera calibration framework that is capable to
generate calibration poses in real-time and present a user study evaluating
different visualization methods to guide the user to a target pose. Using the
presented method even novel users are capable to perform a precise camera
calibration in about 2 minutes.
</summary>
    <author>
      <name>Pavel Rojtberg</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04390v1</id>
    <updated>2019-07-09T20:16:57Z</updated>
    <published>2019-07-09T20:16:57Z</published>
    <title>A Novel Contactless Human Machine Interface based on Machine Learning</title>
    <summary>  This paper describes a global framework that enables contactless human
machine interaction using computer vision and machine learning techniques. The
main originality of our framework is that only a very simple image acquisition
device, as a computer camera, is sufficient to establish a rich human machine
interaction as traditional devices such as mouse or keyboard. This framework is
based on well known computer vision techniques and efficient machine learning
techniques are used to detect and track user hand gestures so the end user can
control his computer using virtual interfaces with very simple gestures.
</summary>
    <author>
      <name>Frederic Magoules</name>
    </author>
    <author>
      <name>Qinmeng Zou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/dcabes.2017.37</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/dcabes.2017.37" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">16th International Symposium on Distributed Computing and
  Applications for Business Engineering and Science (DCABES), 2017, IEEE</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.04390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05220v1</id>
    <updated>2019-07-11T14:12:04Z</updated>
    <published>2019-07-11T14:12:04Z</published>
    <title>The Illusion of Animal Body Ownership and Its Potential for Virtual
  Reality Games</title>
    <summary>  Virtual reality offers the unique possibility to experience a virtual
representation as our own body. In contrast to previous research that
predominantly studied this phenomenon for humanoid avatars, our work focuses on
virtual animals. In this paper, we discuss different body tracking approaches
to control creatures such as spiders or bats and the respective virtual body
ownership effects. Our empirical results demonstrate that virtual body
ownership is also applicable for nonhumanoids and can even outperform
human-like avatars in certain cases. An additional survey confirms the general
interest of people in creating such experiences and allows us to initiate a
broad discussion regarding the applicability of animal embodiment for
educational and entertainment purposes.
</summary>
    <author>
      <name>Andrey Krekhov</name>
    </author>
    <author>
      <name>Sebastian Cmentowski</name>
    </author>
    <author>
      <name>Jens Krüger</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05800v1</id>
    <updated>2019-07-12T15:34:50Z</updated>
    <published>2019-07-12T15:34:50Z</published>
    <title>Find It: A Novel Way to Learn Through Play</title>
    <summary>  Autism Spectrum Disorder (ASD) is the area where many researches enduring
like Magnetic Resonance Imaging (MRI), called diffusion tensor imaging, Early
Start Denver Model (ESDM) to provide an easier life for the people diagnosed.
After years and years of combined funding sources from public and private
funding, these researches show great promises in recent years. In this paper,
we have tried to show a way how children with Down Syndrome Autism can learn
through game therapy. These game therapies have shown an immense number of
improvements among those children to learn alphabets along with developing
their motor skills and memory challenges.
</summary>
    <author>
      <name>Md. Tashfiqul Bari</name>
    </author>
    <author>
      <name>Tanvir Hassan</name>
    </author>
    <author>
      <name>Raisa Tabassum</name>
    </author>
    <author>
      <name>Zubaida Ahmed</name>
    </author>
    <author>
      <name>Swakkhar Shatabda</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conference on Computational Intelligence
  (IJCCI 2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.05800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07232v1</id>
    <updated>2019-07-16T19:42:18Z</updated>
    <published>2019-07-16T19:42:18Z</published>
    <title>A Novel Slip-Kalman Filter to Track the Progression of Reading Through
  Eye-Gaze Measurements</title>
    <summary>  In this paper, we propose an approach to track the progression of eye-gaze
while reading a block of text on computer screen. The proposed approach will
help to accurately quantify reading, e.g., identifying the lines of text that
were read/skipped and estimating the time spent on each line, based on
commercially available inexpensive eye-tracking devices. The proposed approach
is based on a novel Slip Kalman filter that is custom designed to track the
progression of reading. The performance of the proposed method is demonstrated
using 25 pages eye-tracking data collected using a commercial desk-mounted
eye-tracking device.
</summary>
    <author>
      <name>Stephen Bottos</name>
    </author>
    <author>
      <name>Balakumar Balasingam</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10380v1</id>
    <updated>2019-07-23T13:47:46Z</updated>
    <published>2019-07-23T13:47:46Z</published>
    <title>NONOTO: A Model-agnostic Web Interface for Interactive Music Composition
  by Inpainting</title>
    <summary>  Inpainting-based generative modeling allows for stimulating human-machine
interactions by letting users perform stylistically coherent local editions to
an object using a statistical model. We present NONOTO, a new interface for
interactive music generation based on inpainting models. It is aimed both at
researchers, by offering a simple and flexible API allowing them to connect
their own models with the interface, and at musicians by providing
industry-standard features such as audio playback, real-time MIDI output and
straightforward synchronization with DAWs using Ableton Link.
</summary>
    <author>
      <name>Théis Bazin</name>
    </author>
    <author>
      <name>Gaëtan Hadjeres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure. Published as a conference paper at the 10th
  International Conference on Computational Creativity (ICCC 2019), UNC
  Charlotte, North Carolina</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.10380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11743v1</id>
    <updated>2019-07-26T18:33:45Z</updated>
    <published>2019-07-26T18:33:45Z</published>
    <title>SCATTERSEARCH: Visual Querying of Scatterplot Visualizations</title>
    <summary>  Scatterplots are one of the simplest and most commonly-used visualizations
for understanding quantitative, multidimensional data. However, since
scatterplots only depict two attributes at a time, analysts often need to
manually generate and inspect large numbers of scatterplots to make sense of
large datasets with many attributes. We present a visual query system for
scatterplots, SCATTERSEARCH, that enables users to visually search and browse
through large collections of scatterplots. Users can query for other
visualizations based on a region of interest or find other scatterplots that
"look similar'' to a selected one. We present two demo scenarios, provide a
system overview of SCATTERSEARCH, and outline future directions.
</summary>
    <author>
      <name>Doris Jung-Lin Lee</name>
    </author>
    <author>
      <name>Jaewoo Kim</name>
    </author>
    <author>
      <name>Renxuan Wang</name>
    </author>
    <author>
      <name>Aditya Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.13320v1</id>
    <updated>2019-07-31T06:16:28Z</updated>
    <published>2019-07-31T06:16:28Z</published>
    <title>What-Why Analysis of Expert Interviews: Analysing
  Geographically-Embedded Flow Data</title>
    <summary>  In this paper, we present our analysis of five expert interviews, each from a
different application domain. Such analysis is crucial to understanding the
real-world scenarios of analysing geographically-embedded flow data. The
results of our analysis show that similar high-level tasks were conducted in
different domains. To better describe the targets of these tasks, we proposed
three flow-targets for analysing geographically-embedded flow data: single
flow, total flow and regional flow.
</summary>
    <author>
      <name>Yalong Yang</name>
    </author>
    <author>
      <name>Sarah Goodwin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/PacificVis.2019.00022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/PacificVis.2019.00022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at IEEE Pacific Visualization Symposium (PacificVis 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.13320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.13320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00662v2</id>
    <updated>2019-08-06T23:51:49Z</updated>
    <published>2019-08-01T23:57:09Z</published>
    <title>Visualising Geographically-Embedded Origin-Destination Flows: in 2D and
  immersive environments</title>
    <summary>  This thesis develops and evaluates effective techniques for visualisation of
flows (e.g. of people, trade, knowledge) between places on geographic maps.
This geographically-embedded flow data contains information about geographic
locations, and flows from origin locations to destination locations. We
explored the design space of OD flow visualisation in both 2D and immersive
environments. We do so by creating novel OD flow visualisations in both
environments, and then conducting controlled user studies to evaluate different
designs.
</summary>
    <author>
      <name>Yalong Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.26180/5c087e9980d8a</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.26180/5c087e9980d8a" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis, Monash University, Australia, December 2018. Update:
  corrected typos in arXiv comments</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.00662v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00662v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00744v1</id>
    <updated>2019-08-02T08:11:46Z</updated>
    <published>2019-08-02T08:11:46Z</published>
    <title>Towards Learning How to Properly Play UNO with the iCub Robot</title>
    <summary>  While interacting with another person, our reactions and behavior are much
affected by the emotional changes within the temporal context of the
interaction. Our intrinsic affective appraisal comprising perception,
self-assessment, and the affective memories with similar social experiences
will drive specific, and in most cases addressed as proper, reactions within
the interaction. This paper proposes the roadmap for the development of
multimodal research which aims to empower a robot with the capability to
provide proper social responses in a Human-Robot Interaction (HRI) scenario.
</summary>
    <author>
      <name>Pablo Barros</name>
    </author>
    <author>
      <name>Stefan Wermter</name>
    </author>
    <author>
      <name>Alessandra Sciutti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshops on Naturalistic Non-Verbal and Affective Human-Robot
  Interactions co-located with ICDL-EPIROB 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.00744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06877v1</id>
    <updated>2019-07-30T20:29:46Z</updated>
    <published>2019-07-30T20:29:46Z</published>
    <title>Decentralising power: how we are trying to keep CALLector ethical</title>
    <summary>  We present a brief overview of the CALLector project, and consider ethical
questions arising from its overall goal of creating a social network to support
creation and use of online CALL resources. We argue that these questions are
best addressed in a decentralised, pluralistic open source architecture.
</summary>
    <author>
      <name>Cathy Chua</name>
    </author>
    <author>
      <name>Hanieh Habibi</name>
    </author>
    <author>
      <name>Manny Rayner</name>
    </author>
    <author>
      <name>Nikos Tsourakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; based on talk presented at enetCollect WG3 &amp; WG5 Meeting,
  Leiden, Holland, 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CEUR Workshop proceedings vol 2390 http://ceur-ws.org/Vol-2390/
  2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.06877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07572v1</id>
    <updated>2019-08-20T19:10:21Z</updated>
    <published>2019-08-20T19:10:21Z</published>
    <title>Championing Research Through Design in HRI</title>
    <summary>  One of the challenges in conducting research on the intersection of the CHI
and Human-Robot Interaction (HRI) communities is in addressing the gap of
acceptable design research methods between the two. While HRI is focused on
interaction with robots and includes design research in its scope, the
community is not as accustomed to exploratory design methods as the CHI
community. This workshop paper argues for bringing exploratory design, and
specifically Research through Design (RtD) methods that have been established
in CHI for the past decade to the foreground of HRI. RtD can enable design
researchers in the field of HRI to conduct exploratory design work that asks
what is the right thing to design and share it within the community.
</summary>
    <author>
      <name>Michal Luria</name>
    </author>
    <author>
      <name>John Zimmerman</name>
    </author>
    <author>
      <name>Jodi Forlizzi</name>
    </author>
    <link href="http://arxiv.org/abs/1908.07572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07577v1</id>
    <updated>2019-08-20T19:23:34Z</updated>
    <published>2019-08-20T19:23:34Z</published>
    <title>Challenges of Designing HCI for Negative Emotions</title>
    <summary>  Emotions that are perceived as "negative" are inherent in the human
experience. Yet not much work in the field of HCI has looked into the role of
these emotions in interaction with technology. As technology is becoming more
social, personal and emotional by mediating our relationships and generating
new social entities (such as conversational agents and robots), it is valuable
to consider how it can support people's negative emotions and behaviors.
Research in Psychology shows that interacting with negative emotions correctly
can benefit well-being, yet the boundary between helpful and harmful is
delicate. This workshop paper looks at the opportunities of designing for
negative affect, and the challenge of "causing no harm" that arises in an
attempt to do so.
</summary>
    <author>
      <name>Michal Luria</name>
    </author>
    <author>
      <name>Amit Zoran</name>
    </author>
    <author>
      <name>Jodi Forlizzi</name>
    </author>
    <link href="http://arxiv.org/abs/1908.07577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08131v1</id>
    <updated>2019-08-21T22:36:37Z</updated>
    <published>2019-08-21T22:36:37Z</published>
    <title>A 'Canny' Approach to Spoken Language Interfaces</title>
    <summary>  Voice-enabled artefacts such as Amazon Echo are very popular, but there
appears to be a 'habitability gap' whereby users fail to engage with the full
capabilities of the device. This position paper draws a parallel with the
'uncanny valley' effect, thereby proposing a solution based on aligning the
visual, vocal, behavioural and cognitive affordances of future voice-enabled
devices.
</summary>
    <author>
      <name>Roger K. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the CHI 2019 Workshop on Mapping Theoretical and
  Methodological Perspectives for Understanding Speech Interface Interactions,
  4-9 May 2019, Glasgow, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10560v1</id>
    <updated>2019-08-28T06:01:42Z</updated>
    <published>2019-08-28T06:01:42Z</published>
    <title>Efficient Convolutional Neural Network for FMCW Radar Based Hand Gesture
  Recognition</title>
    <summary>  FMCW radar could detect object's range, speed and Angleof-Arrival, advantages
are robust to bad weather, good range resolution, and good speed resolution. In
this paper, we consider the FMCW radar as a novel interacting interface on
laptop. We merge sequences of object's range, speed, azimuth information into
single input, then feed to a convolution neural network to learn spatial and
temporal patterns. Our model achieved 96% accuracy on test set and real-time
test.
</summary>
    <author>
      <name>Xiaodong Cai</name>
    </author>
    <author>
      <name>Jingyi Ma</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Hemin Han</name>
    </author>
    <author>
      <name>Lili Ma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3341162.3343768</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3341162.3343768" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Poster in Ubicomp 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02368v1</id>
    <updated>2019-10-06T04:25:51Z</updated>
    <published>2019-10-06T04:25:51Z</published>
    <title>Computer-mediated Empathy</title>
    <summary>  While novel social networks and emerging technologies help us transcend the
spatial and temporal constraints inherent to in-person communication, the
trade-off is a loss of natural expressivity. While empathetic interaction is
already challenging in in-person communication, computer-mediated communication
makes such empathetically rich communication even more difficult. Are
technology and intelligent systems opportunities or threats to more empathic
interpersonal communication? Realizing empathy is suggested not only as a way
to communicate with others but also to design products for users and facilitate
creativity. In this position paper, I suggest a framework to breakdown empathy,
introduce each element, and show how computing, technologies, and algorithms
can support (or hinder) certain elements of the empathy framework.
</summary>
    <author>
      <name>Sang Won Lee</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Virginia Tech Workshop on the Future of Human-Computer
  Interaction, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.02368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04698v1</id>
    <updated>2019-10-02T21:10:19Z</updated>
    <published>2019-10-02T21:10:19Z</published>
    <title>Brown Ring Experiment in Virtual Reality</title>
    <summary>  Brown Ring Experiment is a very popular test to detect the presence of
Nitrate in salts commonly performed in chemical laboratories with supplies of
required chemicals. Our work clears out the need for a chemical laboratory and
chemicals in order to understand the experiment practically. We have used the
technology of Virtual Reality to fulfill this requirement. Our research work
can be extensively utilized to create virtual environments for conducting other
chemical processes in a virtual environment hence, eliminating the need for a
chemical laboratory. This can help students in remote areas with minimal
resources to fill in the void of practical experiments they have in their
learning process due to space constraints.
</summary>
    <author>
      <name>Prithaj Jana</name>
    </author>
    <author>
      <name>Emil Joswin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.04698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07381v1</id>
    <updated>2019-10-16T14:40:27Z</updated>
    <published>2019-10-16T14:40:27Z</published>
    <title>Using learning analytics to provide personalized recommendations for
  finding peers</title>
    <summary>  This work aims to propose a method to support students in finding appropriate
peers in collaborative and blended learning settings. The main goal of this
research is to bridge the gap between pedagogical theory and data driven
practice to provide personalized and adaptive guidance to students who engage
in computer supported learning activities. The research hypothesis is that we
can use Learning Analytics to model students' cognitive state and to assess
whether the student is in the Zone of Proximal Development. Based on this
assessment, we can plan how to provide scaffolding based on the principles of
Contingent Tutoring and how to form study groups based on the principles of the
Zone of Proximal Development.
</summary>
    <author>
      <name>Irene-Angelica Chounta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.36616.78081</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.36616.78081" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, conference, CollabTech</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08006v1</id>
    <updated>2019-10-15T17:09:03Z</updated>
    <published>2019-10-15T17:09:03Z</published>
    <title>Body as controller</title>
    <summary>  In the process of developing a new digital music interface, the author faced
three questions that have attracted little to no attention in the literature.
By tracking body joints, a performer can use body parts to directly control a
digital music instrument. An immediate question that follows asks which limb(s)
is more effective for the instrument. The next question asks that movement
should be measured relative to a particular reference point. And the last
question asks about the mathematical form of the mapping function from the
movement feature to the sound parameters. This paper attempts to discuss why
finding an answer to these questions is worthwhile and to provide possible
solutions that require further investigation.
</summary>
    <author>
      <name>Lilac Atassi</name>
    </author>
    <link href="http://arxiv.org/abs/1910.08006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08865v1</id>
    <updated>2019-10-20T01:01:42Z</updated>
    <published>2019-10-20T01:01:42Z</published>
    <title>Deck.gl: Large-scale Web-based Visual Analytics Made Easy</title>
    <summary>  In this paper, we demonstrate how deck.gl, an open-source project born out of
data-heavy visual analytics applications, has grown into the robust
visualization framework it is today. We begin by explaining why we built
another data visualization framework in the first place. Then, we summarize our
design goals (distilled from our interactions with users) and discuss how they
guided the development of the framework's main features. We use two real-world
applications of deck.gl to showcase how it can be applied to simplify the
creation of data-heavy visualizations. We also discuss our lessons learned as
we continue to improve the framework for the larger visualization community.
</summary>
    <author>
      <name>Yang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The IEEE Workshop on Visualization in Practice, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.08865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.09382v1</id>
    <updated>2019-10-21T13:56:08Z</updated>
    <published>2019-10-21T13:56:08Z</published>
    <title>Danse-doigts, a Fine Motor Game</title>
    <summary>  This paper describes the design, implementation and testing of
"Danse-doigts", an edutainment therapeutic application for hemiplegic children.
The objective of this program is twofold. Firstly, to allow them to train their
fine motor skills on tablet. Secondly, to study the effect of this training on
their numerical performance (counting, calculation...). The target population
and the objective of evaluating numerical skills influenced the design. The
software was developed using standard web technologies but is based on a new
parallel programming library written in JavaScript. Applications and libraries
are free of charge and easy to install on most tablets.
</summary>
    <author>
      <name>Jean-Ferdy Susini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEDRIC</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Pons</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEDRIC</arxiv:affiliation>
    </author>
    <author>
      <name>Nolwenn Guedin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">FPSE</arxiv:affiliation>
    </author>
    <author>
      <name>Catherine Thevenot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNIL</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Modelling, measurement and control C, AMSE, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.09382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.09382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00665v1</id>
    <updated>2019-11-02T06:44:32Z</updated>
    <published>2019-11-02T06:44:32Z</published>
    <title>Chat-Bot-Kit: A web-based tool to simulate text-based interactions
  between humans and with computers</title>
    <summary>  In this paper, we describe Chat-Bot-Kit, a web-based tool for text-based
chats that we designed for research purposes in computer-mediated communication
(CMC). Chat-Bot-Kit enables to carry out language studies on text-based
real-time chats for the purpose of research: The generated messages are
structured with language performance data such as pause and speed of
keyboard-handling and the movement of the mouse. The tool provides two modes of
chat communications - quasi-synchron and synchron modes - and various typing
indicators. The tool is also designed to be used in wizard-of-oz studies in
Human-Computer Interaction (HCI) and for the evaluation of chatbots (dialogue
systems) in Natural Language Processing (NLP).
</summary>
    <author>
      <name>Kyoko Sugisaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01318v1</id>
    <updated>2019-11-04T16:30:39Z</updated>
    <published>2019-11-04T16:30:39Z</published>
    <title>Sequential/Spatial, a Survey of Interactive Information Retrieval
  Methods for Controlled Experimentation and Evaluation</title>
    <summary>  This survey presents studies that investigated non-spatial (sequential) and
spatial information retrieval systems in parallel during a battery of
information-seeking tasks with respect to user navigational behaviors,
incidental learning, retrieval performance, cognitive abilities &amp; load, direct
manipulation of 2D &amp; 3D interfaces, and satisfaction. I consider how
information theory has contributed to the concepts of foraging, sense-making,
exploration, and how the applied areas of interactive information retrieval
(IIR) and cognitive/behavioral psychology have implemented these concepts into
architecture, interface design, experimental design, user study, and evaluation
methodology.
</summary>
    <author>
      <name>Michael Segundo Ortz</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03287v1</id>
    <updated>2019-11-08T14:36:44Z</updated>
    <published>2019-11-08T14:36:44Z</published>
    <title>Accessible tables in digital documents</title>
    <summary>  Accessibility of tables on websites for Visually Impaired Persons (VIP) is
not optimal with screen readers which are not always effective for the recovery
of visual information (2D). Actual Web/Multimedia technologies are not taking
in account the difference of the visual perception with respect to the vocal
perception which is linear. This paper analyses the difficulties for accessing
to spatial information with the existing recommendations for the conception of
accessible (for all) websites. Different solutions to facilitate accessible
table creation are presented.
</summary>
    <author>
      <name>Katerine Romeo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>E Pissaloux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNIROUEN</arxiv:affiliation>
    </author>
    <author>
      <name>F Serin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CNRIUT'2019 Congr{\`e}s National de la Recherche des IUT, IUT de
  Toulon, Jun 2019, Toulon, France. pp.138-140</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.03287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05564v1</id>
    <updated>2019-06-18T00:06:15Z</updated>
    <published>2019-06-18T00:06:15Z</published>
    <title>Interaction with Ubiquitous Robots and Autonomous IoT</title>
    <summary>  Robotics have been slowly permeating Internet of Things (IoT) where the
previously ubiquitous but static sensors are now given the power to actively
navigate the environment and even interact with users. Emergence of these
ubiquitous swarms of robots not only opens up the range of possible
applications, but also increases the number of elements to study and design
for. We do not yet understand how, when, and where these robots should move,
manipulate, and touch around people. Through user-centered studies, we aim to
better understand how to best design for interaction with Autonomous IoT or a
swarm of ubiquitous robots.
</summary>
    <author>
      <name>Lawrence H. Kim</name>
    </author>
    <author>
      <name>Sean Follmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the CHI 2019 Workshop on New Directions for the IoT:
  Automate, Share, Build, and Care, (arXiv:1906.06089)</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.05564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06727v1</id>
    <updated>2019-11-15T16:23:46Z</updated>
    <published>2019-11-15T16:23:46Z</published>
    <title>Accessibility to textual and visual information on websites for visually
  impaired persons</title>
    <summary>  Access to textual and visual information for visually impaired persons
becomes very difficult with screen readers which are not adapted to different
websites.This paper analyses the use of different technologies for access
digital content and to establish some ameliorations to the existing
recommendations to accessible website conception for all. The preliminary
evaluation results with visually impaired people of our website ACCESSPACE
which is constructed with the existing recommendations, confirm the project's
relevance.
</summary>
    <author>
      <name>Katerine Romeo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Edwige Pissaloux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Serin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. Handicap 2018, Jun 2018, Paris, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.06727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07447v1</id>
    <updated>2019-11-18T06:27:48Z</updated>
    <published>2019-11-18T06:27:48Z</published>
    <title>Subspace Shapes: Enhancing High-Dimensional Subspace Structures via
  Ambient Occlusion Shading</title>
    <summary>  We test the hypothesis whether transforming a data matrix into a 3D shaded
surface or even a volumetric display can be more appealing to humans than a
scatterplot since it makes direct use of the innate 3D scene understanding
capabilities of the human visual system. We also test whether 3D shaded
displays can add a significant amount of information to the visualization of
high-dimensional data, especially when enhanced with proper tools to navigate
the various 3D subspaces. Our experiments suggest that mainstream users prefer
shaded displays over scatterplots for visual cluster analysis tasks after
receiving training for both. Our experiments also provide evidence that 3D
displays can better communicate spatial relationships, size, and shape of
clusters.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Klaus Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10629v1</id>
    <updated>2019-11-24T22:36:15Z</updated>
    <published>2019-11-24T22:36:15Z</published>
    <title>Fatigue Detection</title>
    <summary>  Nowadays, there are many fatigue detection methods and the majority of them
are tracking eye in real-time using one or two cameras to detect the physical
responses in eyes. It is indicated that the responses in eyes have high
relativity with driver fatigue. As part of this project, We will propose a
fatigue detection system based on pose estimation. Using pose estimation, We
plan to mark the body joints in the upper body for shoulders and neck. Then, we
plan to compare the location of the joints of the current posture with the
ideal posture.
</summary>
    <author>
      <name>Ashish Verma</name>
    </author>
    <author>
      <name>Ankush Goyal</name>
    </author>
    <author>
      <name>Davinderjit Kaur</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00369v1</id>
    <updated>2019-12-01T09:42:50Z</updated>
    <published>2019-12-01T09:42:50Z</published>
    <title>Talking with Robots: Opportunities and Challenges</title>
    <summary>  Notwithstanding the tremendous progress that is taking place in spoken
language technology, effective speech-based human-robot interaction still
raises a number of important challenges. Not only do the fields of robotics and
spoken language technology present their own special problems, but their
combination raises an additional set of issues. In particular, there is a large
gap between the formulaic speech that typifies contemporary spoken dialogue
systems and the flexible nature of human-human conversation. It is pointed out
that grounded and situated speech-based human-robot interaction may lead to
deeper insights into the pragmatics of language usage, thereby overcoming the
current `habitability gap'.
</summary>
    <author>
      <name>Roger K. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for presentation at the UNESCO International Conference
  Language Technologies for All (LT4All), Paris, 4-6 December 2019
  (https://en.unesco.org/LT4All)</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00669v1</id>
    <updated>2019-12-02T10:21:34Z</updated>
    <published>2019-12-02T10:21:34Z</published>
    <title>KRM-based Dialogue Management</title>
    <summary>  A KRM-based dialogue management (DM) is proposed using to implement
human-computer dialogue system in complex scenarios. KRM-based DM has a well
description ability and it can ensure the logic of the dialogue process. Then a
complex application scenario in the Internet of Things (IOT) industry and a
dialogue system implemented based on the KRM-based DM will be introduced, where
the system allows enterprise customers to customize topics and adapts
corresponding topics in the interaction process with users. The experimental
results show that the system can complete the interactive tasks well, and can
effectively solve the problems of topic switching, information inheritance
between topics, change of dominance.
</summary>
    <author>
      <name>Wenwu Qu</name>
    </author>
    <author>
      <name>Xiaoyu Chi</name>
    </author>
    <author>
      <name>Wei Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures,</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02744v2</id>
    <updated>2019-12-06T10:03:46Z</updated>
    <published>2019-11-19T10:51:31Z</published>
    <title>Measurement and analysis of visitors' trajectories in crowded museums</title>
    <summary>  We tackle the issue of measuring and analyzing the visitors' dynamics in
crowded museums. We propose an IoT-based system -- supported by artificial
intelligence models -- to reconstruct the visitors' trajectories throughout the
museum spaces. Thanks to this tool, we are able to gather wide ensembles of
visitors' trajectories, allowing useful insights for the facility management
and the preservation of the art pieces. Our contribution comes with one
successful use case: the Galleria Borghese in Rome, Italy.
</summary>
    <author>
      <name>Pietro Centorrino</name>
    </author>
    <author>
      <name>Alessandro Corbetta</name>
    </author>
    <author>
      <name>Emiliano Cristiani</name>
    </author>
    <author>
      <name>Elia Onofri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures, International Conference on Metrology for
  Archaeology and Cultural Heritage</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 IMEKO TC-4 International Conference on Metrology for
  Archaeology and Cultural Heritage, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.02744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 68U20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03125v1</id>
    <updated>2019-11-08T14:39:23Z</updated>
    <published>2019-11-08T14:39:23Z</published>
    <title>Accessibility of websites for visually impaired persons</title>
    <summary>  Accessibility of websites for visually impaired persons is mishandled by
screen readers which are not always adapted to interactivity needed by actual
web/multimedia technologies. This paper analyses the difficulties to access to
information with the use of different technologies and the existing
recommendations for the conception of accessible websites for all. Preliminary
results with visually impaired persons on our ACCESSPACE project website are
presented.
</summary>
    <author>
      <name>Katerine Romeo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Edwige Pissaloux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISIR</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Serin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. arXiv admin note: text overlap with arXiv:1911.06727</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CNRIUT 2018, Institut Universitaire de Technologie, Aix Marseille
  Universit{\'e}, Jun 2018, Aix-en-Provence, France. pp.163-165</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.03125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06972v2</id>
    <updated>2019-12-27T11:40:06Z</updated>
    <published>2019-12-15T04:51:15Z</published>
    <title>Utilizing Players' Playtime Records for Churn Prediction: Mining
  Playtime Regularity</title>
    <summary>  In the free online game industry, churn prediction is an important research
topic. Reducing the churn rate of a game significantly helps with the success
of the game. Churn prediction helps a game operator identify possible churning
players and keep them engaged in the game via appropriate operational
strategies, marketing strategies, and/or incentives. Playtime related features
are some of the widely used universal features for most churn prediction
models. In this paper, we consider developing new universal features for churn
predictions for long-term players based on players' playtime.
</summary>
    <author>
      <name>Wanshan Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eugene</arxiv:affiliation>
    </author>
    <author>
      <name>Ting Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eugene</arxiv:affiliation>
    </author>
    <author>
      <name>Junlin Zeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eugene</arxiv:affiliation>
    </author>
    <author>
      <name>Lijun Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eugene</arxiv:affiliation>
    </author>
    <author>
      <name>Shivakant Mishra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eugene</arxiv:affiliation>
    </author>
    <author>
      <name> Youjian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eugene</arxiv:affiliation>
    </author>
    <author>
      <name> Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1912.06972v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06972v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06979v1</id>
    <updated>2019-12-15T05:34:45Z</updated>
    <published>2019-12-15T05:34:45Z</published>
    <title>Breaking Speech Recognizers to Imagine Lyrics</title>
    <summary>  We introduce a new method for generating text, and in particular song lyrics,
based on the speech-like acoustic qualities of a given audio file. We repurpose
a vocal source separation algorithm and an acoustic model trained to recognize
isolated speech, instead inputting instrumental music or environmental sounds.
Feeding the "mistakes" of the vocal separator into the recognizer, we obtain a
transcription of words \emph{imagined} to be spoken in the input audio. We
describe the key components of our approach, present initial analysis, and
discuss the potential of the method for machine-in-the-loop collaboration in
creative applications.
</summary>
    <author>
      <name>Jon Gillick</name>
    </author>
    <author>
      <name>David Bamman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2019 Workshop on Machine Learning for Creativity and
  Design</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.06979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08809v1</id>
    <updated>2019-12-17T22:55:06Z</updated>
    <published>2019-12-17T22:55:06Z</published>
    <title>Field Label Prediction for Autofill in Web Browsers</title>
    <summary>  Automatic form fill is an important productivity related feature present in
major web browsers, which predicts the field labels of a web form and
automatically fills values in a new form based on the values previously filled
for the same field in other forms. This feature increases the convenience and
efficiency of users who have to fill similar information in fields in multiple
forms. In this paper we describe a machine learning solution for predicting the
form field labels, implemented as a web service using Azure ML Studio.
</summary>
    <author>
      <name>Joy Bose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.08809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11729v2</id>
    <updated>2019-12-30T01:35:59Z</updated>
    <published>2019-12-26T00:04:45Z</published>
    <title>Discussion of Intelligent Electric Wheelchairs for Caregivers and Care
  Recipients</title>
    <summary>  In order to reduce the burden on caregivers, we developed an intelligent
electric wheelchair. We held workshops with caregivers, asked then regarding
the problems in caregiving, and developed problem-solving methods. In the
workshop, caregivers' physical fitness and psychology of the older adults were
found to be problems and a solution was proposed. We implemented a cooperative
operation function for multiple electric wheelchairs based on the workshop and
demonstrated it at a nursing home. By listening to older adults, we obtained
feedback on the automatic driving electric wheelchair. From the results of this
study, we discovered the issues and solutions to be applied to the intelligent
electric wheelchair.
</summary>
    <author>
      <name>Satoshi Hashizume</name>
    </author>
    <author>
      <name>Ippei Suzuki</name>
    </author>
    <author>
      <name>Kazuki Takazawa</name>
    </author>
    <author>
      <name>Yoichi Ochiai</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11729v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11729v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12652v1</id>
    <updated>2019-12-29T13:52:38Z</updated>
    <published>2019-12-29T13:52:38Z</published>
    <title>An assistive HCI system based on block scanning objects using eye blinks</title>
    <summary>  Human-Computer Interaction (HCI) provides a new communication channel between
human and the computer. We develop an assistive system based on block scanning
techniques using eye blinks that presents a hands-free interface between human
and computer for people with motor impairments. The developed system has been
tested by 12 users who performed 10 common in computer tasks using eye blinks
with scanning time 1.0 second. The performance of the proposed system has been
evaluated by selection time, selection accuracy, false alarm rate and average
success rate. The success rate has found 98.1%.
</summary>
    <author>
      <name>Supriya Sarker</name>
    </author>
    <author>
      <name>Md. Shahraduan Mazumder</name>
    </author>
    <author>
      <name>Md. Sajedur Rahman</name>
    </author>
    <author>
      <name>Md. Anayt Rabbi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.12652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.13273v1</id>
    <updated>2019-12-31T11:30:04Z</updated>
    <published>2019-12-31T11:30:04Z</published>
    <title>From Ideation to Implications: Directions for the Internet of Things in
  the Home</title>
    <summary>  In this paper we give a brief overview of our approaches and ongoing work for
future directions of the Internet of Things (IoT) with a focus on the IoT in
the home. We highlight some of our activities including tools and methods for
an ideation-driven approach as well as for an implications-driven approach. We
point to some findings of workshops and empirical field-studies. We show
examples for new classes of idiosyncratic IoT devices, how implications emerge
by (mis)using sensor data and how users interacted with IoT systems in shared
spaces.
</summary>
    <author>
      <name>Albrecht Kurze</name>
    </author>
    <author>
      <name>Arne Berger</name>
    </author>
    <author>
      <name>Teresa Denefleh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the CHI 2019 Workshop on New Directions for the IoT:
  Automate, Share, Build, and Care, (arXiv:1906.06089)</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.13273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.13273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05409v1</id>
    <updated>2020-02-13T09:50:34Z</updated>
    <published>2020-02-13T09:50:34Z</published>
    <title>A User-centered Approach for Optimizing Information Visualizations</title>
    <summary>  The optimization of information visualizations is time consuming and
expensive. To reduce this we propose an improvement of existing optimization
approaches based on user-centered design, focusing on readability,
comprehensibility, and user satisfaction as optimization goals. The changes
comprise (1) a separate optimization of user interface and representation, (2)
a fully automated evaluation of the representation, and (3) qualitative user
studies for simultaneously creating and evaluating interface variants. On the
basis of these results we are able to find a local optimum of an information
visualization in an efficient way.
</summary>
    <author>
      <name>David Baum</name>
    </author>
    <author>
      <name>Pascal Kovacs</name>
    </author>
    <author>
      <name>Ulrich Eisenecker</name>
    </author>
    <author>
      <name>Richard Müller</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WSCG2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.05409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06280v1</id>
    <updated>2020-02-14T22:57:35Z</updated>
    <published>2020-02-14T22:57:35Z</published>
    <title>I-nteract: A cyber-physical system for real-time interaction with
  physical and virtual objects using mixed reality technologies for additive
  manufacturing</title>
    <summary>  This paper presents I-nteract, a cyber-physical system that enables real-time
interaction with real and virtual objects in a mixed augmented reality
environment to design 3D models for additive manufacturing. The system has been
developed using mixed reality technologies such as HoloLens, for augmenting
visual feedback, and haptic gloves, for augmenting haptic force feedback. The
efficacy of the system has been demonstrated by generating 3D model using a
novel scanning method to 3D print a customized orthopedic cast for human arm,
by estimating spring rates of compression springs, and by simulating
interaction with a virtual spring using hand.
</summary>
    <author>
      <name>Ammar Malik</name>
    </author>
    <author>
      <name>Hugo Lhachemi</name>
    </author>
    <author>
      <name>Robert Shorten</name>
    </author>
    <link href="http://arxiv.org/abs/2002.06280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10860v1</id>
    <updated>2020-02-23T09:52:05Z</updated>
    <published>2020-02-23T09:52:05Z</published>
    <title>Toward dynamical crowd control to prevent hazardous situations</title>
    <summary>  It is common for large crowds to gather to attend games, exhibitions,
political rallies, and other events. Thus, careful designs and operational
plans are made to ensure the safe, secure, and efficient movement of people in
these crowded environments. However, the congestion created by large crowds has
resulted in hazardous incidents across the world. Developments in information
technology can provide new means to disseminate public information, thus
changing human behavior in situations of danger and duress. In this paper, we
propose a crowd control and evacuation guidance management system using digital
promotional signage to demonstrate the effects of crowd control via
simulations.
</summary>
    <author>
      <name>Tomoichi Takahashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3pages, 7 figures, submitted to PED2018</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02307v1</id>
    <updated>2020-03-04T19:51:30Z</updated>
    <published>2020-03-04T19:51:30Z</published>
    <title>What is affordance theory and how can it be used in communication
  research?</title>
    <summary>  Affordance theory proposes that the use of an object is intrinsically
determined by its physical shape. However, when translated to digital objects,
affordance theory loses explanatory power, as the same physical affordances,
for example, screens, can have many socially constructed meanings and can be
used in many ways. Furthermore, the affordance theory core idea that physical
affordances have intrinsic, pre-cognitive meaning cannot be sustained for the
highly symbolic nature of digital affordances, which gain meaning through
social learning and use. A possible way to solve this issue is to think about
on-screen affordances as symbols and affordance research as a semiotic and
linguistic enterprise.
</summary>
    <author>
      <name>Sorin Adam Matei</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02312v1</id>
    <updated>2020-03-04T20:11:26Z</updated>
    <published>2020-03-04T20:11:26Z</published>
    <title>Re-Imagining HCI: New Materialist Philosophy and Figurations as Tool for
  Design</title>
    <summary>  In this paper we interrogate the practices of imagining in human-computer
interaction (HCI), particularly in scenario building (SBE) and persona
construction. We discuss the philosophical premises of HCI imaginings in
rationalism, cognitivism and phenomenology, and we propose (feminist) new
materialist philosophy as an enriching perspective that helps generate a
holistic, relational perspective of users, imaginaries and technologies. In the
end we explore the method of figurations as a potential tool for HCI design.
</summary>
    <author>
      <name>Goda Klumbyte</name>
    </author>
    <author>
      <name>Claude Draude</name>
    </author>
    <author>
      <name>Loren Britton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, paper was presented at the workshop 'Standing on
  the Shoulders of Giants: Exploring the Intersection of Philosophy and HCI',
  ACM CHI Conference on Human Factors in Computing Systems, 4-9 May 2019,
  Glasgow, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.02312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.5.3; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02622v1</id>
    <updated>2020-03-05T14:12:19Z</updated>
    <published>2020-03-05T14:12:19Z</published>
    <title>Towards Effective Human-AI Collaboration in GUI-Based Interactive Task
  Learning Agents</title>
    <summary>  We argue that a key challenge in enabling usable and useful interactive task
learning for intelligent agents is to facilitate effective Human-AI
collaboration. We reflect on our past 5 years of efforts on designing,
developing and studying the SUGILITE system, discuss the issues on
incorporating recent advances in AI with HCI principles in mixed-initiative
interactions and multi-modal interactions, and summarize the lessons we
learned. Lastly, we identify several challenges and opportunities, and describe
our ongoing work
</summary>
    <author>
      <name>Toby Jia-Jun Li</name>
    </author>
    <author>
      <name>Jingya Chen</name>
    </author>
    <author>
      <name>Tom M. Mitchell</name>
    </author>
    <author>
      <name>Brad A. Myers</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2020 Workshop on Artificial Intelligence for HCI: A Modern
  Approach (AI4HCI)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.02622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02662v1</id>
    <updated>2020-03-05T14:39:33Z</updated>
    <published>2020-03-05T14:39:33Z</published>
    <title>Implementation of a Natural User Interface to Command a Drone</title>
    <summary>  In this work, we propose the use of a Natural User Interface (NUI) through
body gestures using the open source library OpenPose, looking for a more
dynamic and intuitive way to control a drone. For the implementation, we use
the Robotic Operative System (ROS) to control and manage the different
components of the project. Wrapped inside ROS, OpenPose (OP) processes the
video obtained in real-time by a commercial drone, allowing to obtain the
user's pose. Finally, the keypoints from OpenPose are obtained and translated,
using geometric constraints, to specify high-level commands to the drone.
Real-time experiments validate the full strategy.
</summary>
    <author>
      <name>Brandon Yam-Viramontes</name>
    </author>
    <author>
      <name>Diego Mercado-Ravell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICUAS48674.2020.9213995</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICUAS48674.2020.9213995" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2020 International Conference on Unmanned Aircraft Systems (ICUAS),
  Athens, Greece, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.02662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.04661v1</id>
    <updated>2020-03-10T12:29:40Z</updated>
    <published>2020-03-10T12:29:40Z</published>
    <title>Further Exploring Communal Technology Use in Smart Homes: Social
  Expectations</title>
    <summary>  Device use in smart homes is becoming increasingly communal, requiring
cohabitants to navigate a complex social and technological context. In this
paper, we report findings from an exploratory survey grounded in our prior work
on communal technology use in the home [4]. The findings highlight the
importance of considering qualities of social relationships and technology in
understanding expectations and intentions of communal technology use. We
propose a design perspective of social expectations, and we suggest existing
designs can be expanded using already available information such as location,
and considering additional information, such as levels of trust and
reliability.
</summary>
    <author>
      <name>Martin J. Kraemer</name>
    </author>
    <author>
      <name>Ulrik Lyngs</name>
    </author>
    <author>
      <name>Helena Webb</name>
    </author>
    <author>
      <name>Ivan Flechais</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3334480.3382972</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3334480.3382972" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in CHI '20 Extended Abstracts, April 25--30, 2020,
  Honolulu, HI, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.04661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11875v1</id>
    <updated>2020-03-26T13:00:10Z</updated>
    <published>2020-03-26T13:00:10Z</published>
    <title>Creating Personas with Disabilities</title>
    <summary>  Personas can help raise awareness among stakeholders about users' needs.
While personas are made-up people, they are based on facts gathered from user
research. Personas can also be used to raise awareness of universal design and
accessibility needs of people with disabilities. We review the current state of
the art of the personas and review some research and industry projects that use
them. We outline techniques that can be used to create personas with
disabilities. This includes advice on how to get more information about
assistive technology and how to better include people with disabilities in the
persona creation process. We also describe our use of personas with
disabilities in several projects and discuss how it has helped to find
accessibility issues.
</summary>
    <author>
      <name>Trenton Schulz</name>
    </author>
    <author>
      <name>Kristin Skeide Fuglerud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-31534-3_22</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-31534-3_22" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12282v1</id>
    <updated>2020-03-27T08:53:09Z</updated>
    <published>2020-03-27T08:53:09Z</published>
    <title>SpatialRugs: Enhancing Spatial Awareness of Movement in Dense Pixel
  Visualizations</title>
    <summary>  Compact visual summaries of spatio-temporal movement data often strive to
express accurate positions of movers. We present SpatialRugs, a technique to
enhance the spatial awareness of movements in dense pixel visualizations.
SpatialRugs apply 2D colormaps to visualize location mapped to a juxtaposed
display. We explore the effect of various colormaps discussing perceptual
limitations and introduce a custom color-smoothing method to mitigate distorted
patterns of collective movement behavior.
</summary>
    <author>
      <name>Juri F. Buchmüller</name>
    </author>
    <author>
      <name>Udo Schlegel</name>
    </author>
    <author>
      <name>Eren Cakmak</name>
    </author>
    <author>
      <name>Evanthia Dimara</name>
    </author>
    <author>
      <name>Daniel A. Keim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.12282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12496v1</id>
    <updated>2020-03-27T15:58:35Z</updated>
    <published>2020-03-27T15:58:35Z</published>
    <title>Maneuver-based Driving for Intervention in Autonomous Cars</title>
    <summary>  The way we communicate with autonomous cars will fundamentally change as soon
as manual input is no longer required as back-up for the autonomous system.
Maneuver-based driving is a potential way to allow still the user to intervene
with the autonomous car to communicate requests such as stopping at the next
parking lot. In this work, we highlight different research questions that still
need to be explored to gain insights into how such control can be realized in
the future.
</summary>
    <author>
      <name>Henrik Detjen</name>
    </author>
    <author>
      <name>Stefan Geisler</name>
    </author>
    <author>
      <name>Stefan Schneegass</name>
    </author>
    <link href="http://arxiv.org/abs/2003.12496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13044v1</id>
    <updated>2020-03-29T14:53:11Z</updated>
    <published>2020-03-29T14:53:11Z</published>
    <title>Implicit Cooperation: Emotion Detection for Validation and Adaptation of
  Automated Vehicles' Driving Behavior</title>
    <summary>  Human emotion detection in automated vehicles helps to improve comfort and
safety. Research in the automotive domain focuses a lot on sensing drivers'
drowsiness and aggression. We present a new form of implicit driver-vehicle
cooperation, where emotion detection is integrated into an automated vehicle's
decision-making process. Constant evaluation of the driver's reaction to
vehicle behavior allows us to revise decisions and helps to increase the safety
of future automated vehicles.
</summary>
    <author>
      <name>Henrik Detjen</name>
    </author>
    <author>
      <name>Stefan Geisler</name>
    </author>
    <author>
      <name>Stefan Schneegass</name>
    </author>
    <link href="http://arxiv.org/abs/2003.13044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13934v1</id>
    <updated>2020-03-31T03:19:29Z</updated>
    <published>2020-03-31T03:19:29Z</published>
    <title>Vibrotactile Feedback for Vertical 2D Space Exploration</title>
    <summary>  Visually impaired people encounter many challenges in their everyday life,
especially when it comes to navigating and representing space. The issue of
shopping is addressed mostly on the level of navigation and product detection,
but conveying clues about the object position to the user is rarely
implemented. This work presents a prototype of vibrotactile wristband using
spatiotemporal patterns to help visually impaired users reach an object in the
2D plane in front of them. A pilot study on twelve blindfolded sighted subjects
showed that discretizing space in a seven by seven targets matrix and conveying
clues with a discrete pattern on the vertical axis and a continuous pattern on
the horizontal axis is an intuitive and effective design.
</summary>
    <author>
      <name>Lancelot Dupont</name>
    </author>
    <author>
      <name>Christophe Jouffrais</name>
    </author>
    <author>
      <name>Simon T. Perrault</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3399715.3399834</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3399715.3399834" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4+1 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.13934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08030v1</id>
    <updated>2020-04-17T01:59:23Z</updated>
    <published>2020-04-17T01:59:23Z</published>
    <title>Smartphone camera based pointer</title>
    <summary>  Large screen displays are omnipresent today as a part of infrastructure for
presentations and entertainment. Also powerful smartphones with integrated
camera(s) are ubiquitous. However, there are not many ways in which smartphones
and screens can interact besides casting the video from a smartphone. In this
paper, we present a novel idea that turns a smartphone into a direct virtual
pointer on the screen using the phone's camera. The idea and its implementation
are simple, robust, efficient and fun to use. Besides the mathematical concepts
of the idea we accompany the paper with a small javascript project
(www.mobiletvgames.com) which demonstrates the possibility of the new
interaction technique presented as a massive multiplayer game in the HTML5
framework.
</summary>
    <author>
      <name>Predrag Lazic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.08030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08382v1</id>
    <updated>2020-04-16T20:23:55Z</updated>
    <published>2020-04-16T20:23:55Z</published>
    <title>From Horseback Riding to Changing the World: UX Competence as a Journey</title>
    <summary>  In this paper, we explore the notion of competence in UX based on the
perspective of practitioners. As a result of this exploration, we observed four
domains through which we conceptualize a plan of sources of competence that
describes the ways a UX practitioner develop competence. Based on this plane,
we present the idea of competence as a journey. A journey whose furthest stage
implies an urge towards transforming society and UX practice.
</summary>
    <author>
      <name>Omar Sosa-Tzec</name>
    </author>
    <author>
      <name>Erik Stolterman Bergqvist</name>
    </author>
    <author>
      <name>Marty A. Siegel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.08382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09204v1</id>
    <updated>2020-04-20T11:12:30Z</updated>
    <published>2020-04-20T11:12:30Z</published>
    <title>Supporting Creative Work with Crowd Feedback Systems</title>
    <summary>  Crowd feedback systems have the potential to support creative workers with
feedback from the crowd. In this position paper for the Workshop on Designing
Crowd-powered Creativity Support Systems (DC2S2) at CHI '19, we present three
creativity support tools in which we explore how creative workers can be
assisted with crowdsourced formative and summative feedback. For each of the
three crowd feedback systems, we provide one idea for future research.
</summary>
    <author>
      <name>Jonas Oppenlaender</name>
    </author>
    <author>
      <name>Simo Hosio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, Workshop on Designing Crowd-powered Creativity
  Support Systems (DC2S2)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.09204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.1.2; I.3.6; H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12217v1</id>
    <updated>2020-04-25T19:39:35Z</updated>
    <published>2020-04-25T19:39:35Z</published>
    <title>Gesture controlled environment using sixth sense technology and its
  implementation in IoT</title>
    <summary>  This paper proposes an idea of building an interface to merge the existing
technologies like Image processing, Internet of Things, Sixth sense, etc. at
one place to reduce the hardware restrictions imposed on a user and improve the
responsiveness of the system. The wearable device comprises of a camera, a
projector, and its own gesture-controlled environment having smart tools based
on trending techniques like gesture recognition, color marker detection, and
speech recognition. The interface is trained using machine learning. It is also
interfaced with an IoT based lab to access the lab controls remotely, enhance
the security, and to connect devices present in the lab.
</summary>
    <author>
      <name>Shubhankar Mohan</name>
    </author>
    <author>
      <name>Aditi Chaudhary</name>
    </author>
    <author>
      <name>Prachie Gupta</name>
    </author>
    <author>
      <name>Ritu Tiwari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.13864v1</id>
    <updated>2020-04-28T21:47:02Z</updated>
    <published>2020-04-28T21:47:02Z</published>
    <title>A Review of Surface Haptics:Enabling Tactile Effects on Touch Surfaces</title>
    <summary>  We review the current technology underlying surface haptics that converts
passive touch surfaces to active ones (machine haptics), our perception of
tactile stimuli displayed through active touch surfaces (human haptics), their
potential applications (human-machine interaction), and finally the challenges
ahead of us in making them available through commercial systems. This review
primarily covers the tactile interactions of human fingers or hands with
surface-haptics displays by focusing on the three most popular actuation
methods: vibrotactile, electrostatic, and ultrasonic.
</summary>
    <author>
      <name>Cagatay Basdogan</name>
    </author>
    <author>
      <name>Frederic Giraud</name>
    </author>
    <author>
      <name>Vincent Levesque</name>
    </author>
    <author>
      <name>Seungmoon Choi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TOH.2020.2990712</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TOH.2020.2990712" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 177 references, review paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.13864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.03011v1</id>
    <updated>2020-05-06T11:51:31Z</updated>
    <published>2020-05-06T11:51:31Z</published>
    <title>Overview of Surgical Simulation</title>
    <summary>  Motivated by the current demand of clinical governance, surgical simulation
is now a well-established modality for basic skills training and assessment.
The practical deployment of the technique is a multi-disciplinary venture
encompassing areas in engineering, medicine and psychology. This paper provides
an overview of the key topics involved in surgical simulation and associated
technical challenges. The paper discusses the clinical motivation for surgical
simulation, the use of virtual environments for surgical training, model
acquisition and simplification, deformable models, collision detection, tissue
property measurement, haptic rendering and image synthesis. Additional topics
include surgical skill training and assessment metrics as well as challenges
facing the incorporation of surgical simulation into medical education
curricula.
</summary>
    <author>
      <name>Mohamed A. ElHelw</name>
    </author>
    <link href="http://arxiv.org/abs/2005.03011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.03011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05019v1</id>
    <updated>2020-05-08T09:17:19Z</updated>
    <published>2020-05-08T09:17:19Z</published>
    <title>Post-human interaction design, yes, but cautiously</title>
    <summary>  Post-human design runs the risk of obscuring the fact that AI technology
actually imports a Cartesian humanist logic, which subsequently influences how
we design and conceive of so-called smart or intelligent objects. This leads to
unwanted metaphorical attributions of human qualities to smart objects.
Instead, starting from an embodied sensemaking perspective, designers should
demand of engineers to radically transform the very structure of AI technology,
in order to truly support critical posthuman values of collectivity,
relationality and community building.
</summary>
    <author>
      <name>Jelle van Dijk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A "provocation" contribution to the acm Designing Interactive Systems
  2020 Conference, Eindhoven, July 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05026v1</id>
    <updated>2020-04-24T14:48:26Z</updated>
    <published>2020-04-24T14:48:26Z</published>
    <title>Delightful Companions: Supporting Well-Being Through Design Delight</title>
    <summary>  This paper presents three design products referred to as delightful
companions that are intended to help people engage in well-being practices. It
also introduces the approach utilized to guide the design decisions during
their creation. Design delight is the name of this approach, which comprises
six experiential qualities that are regarded as antecedents of delight. The
objective of this paper is to introduce the approach and the companions and
state the two paths that have defined the future steps of this research.
</summary>
    <author>
      <name>Omar Sosa-Tzec</name>
    </author>
    <author>
      <name>Gowri Balasubramaniam</name>
    </author>
    <author>
      <name>Sylvia Sinsabaugh</name>
    </author>
    <author>
      <name>Evan Sobetski</name>
    </author>
    <author>
      <name>Rogerio Pinto</name>
    </author>
    <author>
      <name>Shervin Assari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05445v1</id>
    <updated>2020-05-11T21:29:55Z</updated>
    <published>2020-05-11T21:29:55Z</published>
    <title>Polyrhythmic Bimanual Coordination Training using Haptic Force Feedback</title>
    <summary>  It is challenging to develop two thoughts at the same time or perform two
uncorrelated motions simultaneously. This work looks specifically towards
training humans to perform a 2:3 polyrhythmic bimanual ratio using haptic force
feedback devices (SensAble Phantom OMNI). We implemented an interactive
training session to help participants learn to decouple their hand motions
quickly. Three subjects (2 Females, 1 Male) were tested and have successfully
increased their scores after adaptive training durations of under five minutes.
</summary>
    <author>
      <name>Ramy Mounir</name>
    </author>
    <author>
      <name>Kyle Reed</name>
    </author>
    <link href="http://arxiv.org/abs/2005.05445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.06295v1</id>
    <updated>2020-06-11T10:01:59Z</updated>
    <published>2020-06-11T10:01:59Z</published>
    <title>Transparency in Language Generation: Levels of Automation</title>
    <summary>  Language models and conversational systems are growing increasingly advanced,
creating outputs that may be mistaken for humans. Consumers may thus be misled
by advertising, media reports, or vagueness regarding the role of automation in
the production of language. We propose a taxonomy of language automation, based
on the SAE levels of driving automation, to establish a shared set of terms for
describing automated language. It is our hope that the proposed taxonomy can
increase transparency in this rapidly advancing field.
</summary>
    <author>
      <name>Justin Edwards</name>
    </author>
    <author>
      <name>Allison Perrone</name>
    </author>
    <author>
      <name>Philip R. Doyle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3405755.3406136</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3405755.3406136" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at CUI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.06295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.06295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09645v1</id>
    <updated>2020-06-17T04:07:13Z</updated>
    <published>2020-06-17T04:07:13Z</published>
    <title>ExSampling: a system for the real-time ensemble performance of
  field-recorded environmental sounds</title>
    <summary>  We propose ExSampling: an integrated system of recording application and Deep
Learning environment for a real-time music performance of environmental sounds
sampled by field recording. Automated sound mapping to Ableton Live tracks by
Deep Learning enables field recording to be applied to real-time performance,
and create interactions among sound recorders, composers and performers.
</summary>
    <author>
      <name>Atsuya Kobayashi</name>
    </author>
    <author>
      <name>Reo Anzai</name>
    </author>
    <author>
      <name>Nao Tokui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The International Conference on New Interfaces for Musical Expression
  2020 poster presentation. 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.09645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.13898v1</id>
    <updated>2020-06-24T17:34:12Z</updated>
    <published>2020-06-24T17:34:12Z</published>
    <title>Order of Control and Perceived Control over Personal Information</title>
    <summary>  Focusing on personal information disclosure, we apply control theory and the
notion of the Order of Control to study people's understanding of the
implications of information disclosure and their tendency to consent to
disclosure. We analyzed the relevant literature and conducted a preliminary
online study (N = 220) to explore the relationship between the Order of Control
and perceived control over personal information. Our analysis of existing
research suggests that the notion of the Order of Control can help us
understand people's decisions regarding the control over their personal
information. We discuss limitations and future directions for research
regarding the application of the idea of the Order of Control to online
privacy.
</summary>
    <author>
      <name>Yefim Shulman</name>
    </author>
    <author>
      <name>Thao Ngo</name>
    </author>
    <author>
      <name>Joachim Meyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-42504-3_23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-42504-3_23" rel="related"/>
    <link href="http://arxiv.org/abs/2006.13898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.14654v1</id>
    <updated>2020-06-25T18:31:51Z</updated>
    <published>2020-06-25T18:31:51Z</published>
    <title>Exploratory Study of Young Children's Social Media Needs and
  Requirements</title>
    <summary>  As social media are becoming increasingly popular among young children, it is
important to explore this population's needs and requirements from these
platforms. As a first step to this, we conducted an exploratory design workshop
with children aged between ten and eleven years to find out about their social
media needs and requirements. Through an analysis of the paper prototypes
solicited from the workshop, here we discuss the social media features that are
the most desired by this population.
</summary>
    <author>
      <name>Di "Chelsea" Sun</name>
    </author>
    <author>
      <name>Vaishnavi Melkote</name>
    </author>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3397617.3397836</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3397617.3397836" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Abstracts of the 19th ACM International Conference on
  Interaction Design and Children (IDC 2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.14654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04495v2</id>
    <updated>2020-11-23T21:47:25Z</updated>
    <published>2020-07-09T01:14:25Z</published>
    <title>Hack.VR: A Programming Game in Virtual Reality</title>
    <summary>  In this article we describe Hack.VR, an object-oriented programming game in
virtual reality. Hack.VR uses a VR programming language in which nodes
represent functions and node connections represent data flow. Using this
programming framework, players reprogram VR objects such as elevators, robots,
and switches. Hack.VR has been designed to be highly interactable both
physically and semantically.
</summary>
    <author>
      <name>Dominic Kao</name>
    </author>
    <author>
      <name>Christos Mousas</name>
    </author>
    <author>
      <name>Alejandra J. Magana</name>
    </author>
    <author>
      <name>D. Fox Harrell</name>
    </author>
    <author>
      <name>Rabindra Ratan</name>
    </author>
    <author>
      <name>Edward F. Melcer</name>
    </author>
    <author>
      <name>Brett Sherrick</name>
    </author>
    <author>
      <name>Paul Parsons</name>
    </author>
    <author>
      <name>Dmitri A. Gusev</name>
    </author>
    <link href="http://arxiv.org/abs/2007.04495v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04495v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04882v1</id>
    <updated>2020-07-09T15:34:22Z</updated>
    <published>2020-07-09T15:34:22Z</published>
    <title>A Neuro-inspired Theory of Joint Human-Swarm Interaction</title>
    <summary>  Human-swarm interaction (HSI) is an active research challenge in the realms
of swarm robotics and human-factors engineering. Here we apply a cognitive
systems engineering perspective and introduce a neuro-inspired joint systems
theory of HSI. The mindset defines predictions for adaptive, robust and
scalable HSI dynamics and therefore has the potential to inform human-swarm
loop design.
</summary>
    <author>
      <name>Jonas D. Hasbach</name>
    </author>
    <author>
      <name>Maren Bennewitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICRA Workshop on Human-Swarm Interaction 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.04882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.05286v1</id>
    <updated>2020-07-10T10:13:34Z</updated>
    <published>2020-07-10T10:13:34Z</published>
    <title>TangToys: Smart Toys that can Communicate and Improve Children's
  Wellbeing</title>
    <summary>  Children can find it challenging to communicate their emotions especially
when experiencing mental health challenges. Technological solutions may help
children communicate digitally and receive support from one another as advances
in networking and sensors enable the real-time transmission of physical
interactions. In this work, we pursue the design of multiple tangible user
interfaces designed for children containing multiple sensors and feedback
actuators. Bluetooth is used to provide communication between Tangible Toys
(TangToys) enabling peer to peer support groups to be developed and allowing
feedback to be issued whenever other children are nearby. TangToys can provide
a non-intrusive means for children to communicate their wellbeing through play.
</summary>
    <author>
      <name>Kieran Woodward</name>
    </author>
    <author>
      <name>Eiman Kanjo</name>
    </author>
    <author>
      <name>David J Brown</name>
    </author>
    <author>
      <name>Becky Inkster</name>
    </author>
    <link href="http://arxiv.org/abs/2007.05286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09177v2</id>
    <updated>2021-01-15T17:31:00Z</updated>
    <published>2020-07-17T18:25:10Z</published>
    <title>iNNk: A Multi-Player Game to Deceive a Neural Network</title>
    <summary>  This paper presents iNNK, a multiplayer drawing game where human players team
up against an NN. The players need to successfully communicate a secret code
word to each other through drawings, without being deciphered by the NN. With
this game, we aim to foster a playful environment where players can, in a small
way, go from passive consumers of NN applications to creative thinkers and
critical challengers.
</summary>
    <author>
      <name>Jennifer Villareale</name>
    </author>
    <author>
      <name>Ana Acosta-Ruiz</name>
    </author>
    <author>
      <name>Samuel Arcaro</name>
    </author>
    <author>
      <name>Thomas Fox</name>
    </author>
    <author>
      <name>Evan Freed</name>
    </author>
    <author>
      <name>Robert Gray</name>
    </author>
    <author>
      <name>Mathias Löwe</name>
    </author>
    <author>
      <name>Panote Nuchprayoon</name>
    </author>
    <author>
      <name>Aleksanteri Sladek</name>
    </author>
    <author>
      <name>Rush Weigelt</name>
    </author>
    <author>
      <name>Yifu Li</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2007.09177v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09177v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09884v1</id>
    <updated>2020-07-20T04:39:57Z</updated>
    <published>2020-07-20T04:39:57Z</published>
    <title>Parallel Oculomotor Plant Mathematical Model for Large Scale Eye
  Movement Simulation</title>
    <summary>  The usage of eye tracking sensors is expected to grow in virtual (VR) and
augmented reality (AR) platforms. Provided that users of these platforms
consent to employing captured eye movement signals for authentication and
health assessment, it becomes important to estimate oculomotor plant and brain
function characteristics in real time. This paper shows a path toward that goal
by presenting a parallel processing architecture capable of estimating
oculomotor plant characteristics and comparing its performance to a
single-threaded implementation. Results show that the parallel implementation
improves the speed, accuracy, and throughput of oculomotor plant characteristic
estimation versus the original serial version for both large-scale and
real-time simulation.
</summary>
    <author>
      <name>Alex Karpov</name>
    </author>
    <author>
      <name>Jacob Liberman</name>
    </author>
    <author>
      <name>Dillon Lohr</name>
    </author>
    <author>
      <name>Oleg Komogortsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.09884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10235v1</id>
    <updated>2020-07-20T16:25:35Z</updated>
    <published>2020-07-20T16:25:35Z</published>
    <title>Privacy Implications of Eye Tracking in Mixed Reality</title>
    <summary>  Mixed Reality (MR) devices require a world with always-on sensors and
real-time processing applied to their outputs. We have grappled with some of
the ethical concerns presented by this scenario, such as bystander privacy
issues with smartphones and cameras. However, MR technologies demand that we
define and defend privacy in this new paradigm. This paper focuses on the
challenges presented by eye tracking and gaze tracking, techniques that have
commonly been deployed in the HCI community for years but are now being
integrated into MR devices by default.
</summary>
    <author>
      <name>Diane Hosfelt</name>
    </author>
    <author>
      <name>Nicole Shadowen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CHI Workshop on Exploring Potentially Abusive Ethical
  Social and Political Implications of Mixed Reality Research in HCI
  (https://chi2020.acm.org/accepted-workshops/#W37)</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11686v1</id>
    <updated>2020-07-07T09:19:58Z</updated>
    <published>2020-07-07T09:19:58Z</published>
    <title>A report on the first virtual PLDI conference</title>
    <summary>  This is a report on the PLDI 2020 conference, for which I was General Chair,
which was held virtually for the first time as a result of the COVID-19
pandemic. The report contains: my personal reflections on the positive and
negative aspects of the event; a description of the format of the event and
associated logistical details; and data (with some analysis) on attendees'
views on the conference format, the extent to which attendees engaged with the
conference, attendees' views on virtual vs. physical conferences (with a focus
on PLDI specifically) and the diversity of conference registrants. I hope that
the report will be a useful resource for organizers of upcoming virtual
conferences, and generally interesting to the Programming Languages community
and beyond.
</summary>
    <author>
      <name>Alastair F. Donaldson</name>
    </author>
    <link href="http://arxiv.org/abs/2007.11686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11742v1</id>
    <updated>2020-07-23T01:32:00Z</updated>
    <published>2020-07-23T01:32:00Z</published>
    <title>Engineering Reliable Interactions in the Reality-Artificiality Continuum</title>
    <summary>  Milgram's reality-virtuality continuum applies to interaction in the physical
space dimension, going from real to virtual. However, interaction has a social
dimension as well, that can go from real to artificial depending on the
companion with whom the user interacts. In this paper we present our vision of
the Reality-Artificiality bidimensional Continuum (RAC), we identify some
challenges in its design and development and we discuss how reliable
interactions might be supported inside RAC.
</summary>
    <author>
      <name>Davide Ancona</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Genova, DIBRIS</arxiv:affiliation>
    </author>
    <author>
      <name>Chiara Bassano</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Genova, DIBRIS</arxiv:affiliation>
    </author>
    <author>
      <name>Manuela Chessa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Genova, DIBRIS</arxiv:affiliation>
    </author>
    <author>
      <name>Viviana Mascardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Genova, DIBRIS</arxiv:affiliation>
    </author>
    <author>
      <name>Fabio Solari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Genova, DIBRIS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.319.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.319.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings AREA 2020, arXiv:2007.11260</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 319, 2020, pp. 69-80</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.11742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12238v1</id>
    <updated>2020-07-10T17:58:22Z</updated>
    <published>2020-07-10T17:58:22Z</published>
    <title>MiniConf -- A Virtual Conference Framework</title>
    <summary>  MiniConf is a framework for hosting virtual academic conferences motivated by
the sudden inability for these events to be hosted globally. The framework is
designed to be global and asynchronous, interactive, and to promote browsing
and discovery. We developed the system to be sustainable and maintainable, in
particular ensuring that it is open-source, easy to setup, and scalable on
minimal hardware. In this technical report, we discuss design decisions,
provide technical detail, and show examples of a case study deployment.
</summary>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <author>
      <name>Hendrik Strobelt</name>
    </author>
    <link href="http://arxiv.org/abs/2007.12238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.14461v1</id>
    <updated>2020-07-28T20:09:09Z</updated>
    <published>2020-07-28T20:09:09Z</published>
    <title>Modeling Behaviour to Predict User State: Self-Reports as Ground Truth</title>
    <summary>  Methods that detect user states such as emotions are useful for interactive
systems. In this position paper, we argue for model-based approaches that are
trained on user behaviour and self-reported user state as ground truths. In an
application context, they record behaviour, extract relevant features, and use
the models to predict user states. We describe how this approach can be
implemented and discuss its benefits in comparison to solely self-reports in an
application and to models of behaviour without the selfreport ground truths.
Finally, we discuss shortcomings of this approach by considering its drawbacks
and limitations.
</summary>
    <author>
      <name>Julian Frommel</name>
    </author>
    <author>
      <name>Regan L Mandryk</name>
    </author>
    <link href="http://arxiv.org/abs/2007.14461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.00249v1</id>
    <updated>2020-12-01T04:09:39Z</updated>
    <published>2020-12-01T04:09:39Z</published>
    <title>Cross-artform performance using networked interfaces: Last Man to Die's
  Vital LMTD</title>
    <summary>  In 2009 the cross artform group, Last Man to Die, presented a series of
performances using new interfaces and networked performance to integrate the
three artforms of its members (actor, Hanna Cormick, visual artist, Benjamin
Forster and percussionist, Charles Martin). This paper explains our artistic
motivations and design for a computer vision surface and networked heartbeat
sensor as well as the experience of mounting our first major work, Vital LMTD.
</summary>
    <author>
      <name>Charles Martin</name>
    </author>
    <author>
      <name>Benjamin Forster</name>
    </author>
    <author>
      <name>Hanna Cormick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1177843</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1177843" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression (2010) pp. 204-207</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.00249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.00296v1</id>
    <updated>2020-12-01T06:36:26Z</updated>
    <published>2020-12-01T06:36:26Z</published>
    <title>Tracking Ensemble Performance on Touch-Screens with Gesture
  Classification and Transition Matrices</title>
    <summary>  We present and evaluate a novel interface for tracking ensemble performances
on touch-screens. The system uses a Random Forest classifier to extract
touch-screen gestures and transition matrix statistics. It analyses the
resulting gesture-state sequences across an ensemble of performers. A series of
specially designed iPad apps respond to this real-time analysis of free-form
gestural performances with calculated modifications to their musical
interfaces. We describe our system and evaluate it through cross-validation and
profiling as well as concert experience.
</summary>
    <author>
      <name>Charles Martin</name>
    </author>
    <author>
      <name>Henry Gardner</name>
    </author>
    <author>
      <name>Ben Swift</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1179130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1179130" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2015, pp. 359-364</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.00296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02404v1</id>
    <updated>2020-12-04T05:03:58Z</updated>
    <published>2020-12-04T05:03:58Z</published>
    <title>Composing an Ensemble Standstill Work for Myo and Bela</title>
    <summary>  This paper describes the process of developing a standstill performance work
using the Myo gesture control armband and the Bela embedded computing platform.
The combination of Myo and Bela allows a portable and extensible version of the
standstill performance concept while introducing muscle tension as an
additional control parameter. We describe the technical details of our setup
and introduce Myo-to-Bela and Myo-to-OSC software bridges that assist with
prototyping compositions using the Myo controller.
</summary>
    <author>
      <name>Charles Patrick Martin</name>
    </author>
    <author>
      <name>Alexander Refsum Jensenius</name>
    </author>
    <author>
      <name>Jim Torresen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1302543</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1302543" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2018, pp. 196-197</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.02404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04411v1</id>
    <updated>2020-12-08T12:57:37Z</updated>
    <published>2020-12-08T12:57:37Z</published>
    <title>An Enhanced MA Plot with R-Shiny to Ease Exploratory Analysis of
  Transcriptomic Data</title>
    <summary>  MA plots are used to analyze the genome-wide differences in gene expression
between two distinct biological conditions. An MA plot is usually rendered as a
static scatter plot. Our interview with 3 experts in genomics showed that we
could improve the usability of this plot by adding interactive analytic
features. In this work we present the design study of the enhanced MA plot.
</summary>
    <author>
      <name>Ali Sheharyar</name>
    </author>
    <author>
      <name>Talar Boghos Yacoubian</name>
    </author>
    <author>
      <name>Dina Aljogol</name>
    </author>
    <author>
      <name>Borbala Mifsud</name>
    </author>
    <author>
      <name>Dena Al Thani</name>
    </author>
    <author>
      <name>Michael Aupetit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at BioVis 2020 Redesign Challenge @ IEEE VIS.
  http://biovis.net/2020/program_ieee/</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.04411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05637v2</id>
    <updated>2023-07-28T08:14:15Z</updated>
    <published>2020-12-10T12:43:10Z</published>
    <title>Simplify Node-RED For End User Development in SeismoCloud</title>
    <summary>  Networks of IoT devices often require configuration and definition of
behavior by the final user. Node-RED is a flow-based programming platform
commonly used for End User Development, but it requires networking and
protocols skills in order to be efficiently used. We add a level of abstraction
to Node-RED nodes in order to allow non-skilled users to configure and control
networks of IoT devices and online services. We applied such abstractions to
the SeismoCloud application for earthquake monitoring.
</summary>
    <author>
      <name>Enrico Bassetti</name>
    </author>
    <author>
      <name>Emanuele Panizzi</name>
    </author>
    <author>
      <name>Edoardo Ottavianelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.05637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06708v1</id>
    <updated>2020-12-12T03:00:54Z</updated>
    <published>2020-12-12T03:00:54Z</published>
    <title>Enabling Input on Tiny/Headless Systems Using Morse Code</title>
    <summary>  This paper presents results of a pilot study that explored the potential of
Morse code as a method for text entry on mobile devices. In the study,
participants without prior experience with Morse code reached 6.7 wpm with a
Morse code keyboard in three short sessions. Learning was observed both in
terms of text entry speed and accuracy, which suggests that the overall
performance of the keyboard is likely to improve with practice.
</summary>
    <author>
      <name>Anna-Maria Gueorguieva</name>
    </author>
    <author>
      <name>Gulnar Rakhmetulla</name>
    </author>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Poster at the 2nd Annual Center for Cellular and Biomolecular
  Machines Open House, October 22, 2018, University of California, Merced, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.06708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09130v1</id>
    <updated>2020-12-15T18:31:28Z</updated>
    <published>2020-12-15T18:31:28Z</published>
    <title>Empathic Chatbot: Emotional Intelligence for Empathic Chatbot: Emotional
  Intelligence for Mental Health Well-being</title>
    <summary>  Conversational chatbots are Artificial Intelligence (AI)-powered applications
that assist users with various tasks by responding in natural language and are
prevalent across different industries. Most of the chatbots that we encounter
on websites and digital assistants such as Alexa, Siri does not express empathy
towards the user, and their ability to empathise remains immature. Lack of
empathy towards the user is not critical for a transactional or interactive
chatbot, but the bots designed to support mental healthcare patients need to
understand the emotional state of the user and tailor the conversations. This
research explains the different types of emotional intelligence methodologies
adopted in the development of an empathic chatbot and how far they have been
adopted and succeeded.
</summary>
    <author>
      <name>Sarada Devaram</name>
    </author>
    <link href="http://arxiv.org/abs/2012.09130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.10422v1</id>
    <updated>2020-12-18T18:25:58Z</updated>
    <published>2020-12-18T18:25:58Z</published>
    <title>Smart Refrigerator using Internet of Things and Android</title>
    <summary>  The kitchen is regarded as the central unit of the traditional as well as
modern homes. It is where people cook meals and where our families sit together
to eat food. The refrigerator is the pivotal of all that, and hence it plays an
important part in our regular lives. The idea of this project is to improvise
the normal refrigerator into a smart one by making it to place order for food
items and to create an virtual interactive environment between it and the user.
</summary>
    <author>
      <name>Abhishek Das</name>
    </author>
    <author>
      <name>Vivek Dhuri</name>
    </author>
    <author>
      <name>Ranjushree Pal</name>
    </author>
    <link href="http://arxiv.org/abs/2012.10422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13961v1</id>
    <updated>2020-12-27T15:23:06Z</updated>
    <published>2020-12-27T15:23:06Z</published>
    <title>Mastering Music Instruments through Technology in Solo Learning Sessions</title>
    <summary>  Mastering a musical instrument requires time-consuming practice even if
students are guided by an expert. In the overwhelming majority of the time, the
students practice by themselves and traditional teaching materials, such as
videos or textbooks, lack interaction and guidance possibilities. Adequate
feedback, however, is highly important to prevent the acquirement of wrong
motions and to avoid potential health problems. In this paper, we envision
musical instruments as smart objects to enhance solo learning sessions. We give
an overview of existing approaches and setups and discuss them. Finally, we
conclude with recommendations for designing smart and augmented musical
instruments for learning purposes.
</summary>
    <author>
      <name>Karola Marky</name>
    </author>
    <author>
      <name>Andreas Weiß</name>
    </author>
    <author>
      <name>Julien Gedeon</name>
    </author>
    <author>
      <name>Sebastian Günther</name>
    </author>
    <link href="http://arxiv.org/abs/2012.13961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.01126v1</id>
    <updated>2020-12-28T06:45:47Z</updated>
    <published>2020-12-28T06:45:47Z</published>
    <title>Methodology for design of templates of text communication messages for
  software marketing</title>
    <summary>  A methodology is proposed for design of templates of text communication
messages that are based on best practices of experts in software marketing,
ideas of marketing, communication theory, copywriting, media linguistics,
semiotics. Description of the subject area is based on conceptual modeling and
production systems. For the purposes of testing, the methodology was used as
the basis of a software product. Decision support recommender system for design
of communication messages for software marketing.
</summary>
    <author>
      <name>E. K. Malakhovskaya</name>
    </author>
    <author>
      <name>Y. P. Ekhlakov</name>
    </author>
    <author>
      <name>P. V. Senchenko</name>
    </author>
    <author>
      <name>A. A. Sidorov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/1862/1/012014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/1862/1/012014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 p</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.01126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02565v1</id>
    <updated>2021-01-07T14:43:51Z</updated>
    <published>2021-01-07T14:43:51Z</published>
    <title>Augmentix -- An Augmented Reality System for asymmetric Teleteaching</title>
    <summary>  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system "Augmentix" is presented, which includes a differentiation between the
two user roles "teacher" and "student" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
</summary>
    <author>
      <name>Nico Feld</name>
    </author>
    <link href="http://arxiv.org/abs/2101.02565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06133v1</id>
    <updated>2021-01-15T14:26:12Z</updated>
    <published>2021-01-15T14:26:12Z</published>
    <title>Teaming up with information agents</title>
    <summary>  Despite the intricacies involved in designing a computer as a teampartner, we
can observe patterns in team behavior which allow us to describe at a general
level how AI systems are to collaborate with humans. Whereas most work on
human-machine teaming has focused on physical agents (e.g. robotic systems),
our aim is to study how humans can collaborate with information agents. We
propose some appropriate team design patterns, and test them using our
Collaborative Intelligence Analysis (CIA) tool.
</summary>
    <author>
      <name>Jurriaan van Diggelen</name>
    </author>
    <author>
      <name>Wiard Jorritsma</name>
    </author>
    <author>
      <name>Bob van der Vecht</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07708v1</id>
    <updated>2021-01-19T16:18:48Z</updated>
    <published>2021-01-19T16:18:48Z</published>
    <title>Leveraging Peer Review in Visualization Education: A Proposal for a New
  Model</title>
    <summary>  In visualization education, both science and humanities, the literature is
often divided into two parts: the design aspect and the analysis of the
visualization. However, we find limited discussion on how to motivate and
engage visualization students in the classroom. In the field of Writing
Studies, researchers develop tools and frameworks for student peer review of
writing. Based on the literature review from the field of Writing Studies, this
paper proposes a new framework to implement visualization peer review in the
classroom to engage today's students. This framework can be customized for
incremental and double-blind review to inspire students and reinforce critical
thinking about visualization.
</summary>
    <author>
      <name>Alon Friedman</name>
    </author>
    <author>
      <name>Paul Rosen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pedagogy Data Visualization Workshop @ IEEE VIS, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.07708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09138v1</id>
    <updated>2021-01-22T14:59:45Z</updated>
    <published>2021-01-22T14:59:45Z</published>
    <title>LonelyText: A Short Messaging Based Classification of Loneliness</title>
    <summary>  Loneliness does not only have emotional implications on a person but also on
his/her well-being. The study of loneliness has been challenging and largely
inconclusive in findings because of the several factors that might correlate to
the phenomenon. We present one approach to predicting this event by discovering
patterns of language associated with loneliness. Our results show insights and
promising directions for mining text from instant messaging to predict
loneliness.
</summary>
    <author>
      <name>Mawulolo K. Ameko</name>
    </author>
    <author>
      <name>Sonia Baee</name>
    </author>
    <author>
      <name>Laura E. Barnes</name>
    </author>
    <link href="http://arxiv.org/abs/2101.09138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09999v3</id>
    <updated>2021-06-21T10:43:33Z</updated>
    <published>2021-01-25T10:29:17Z</published>
    <title>Democratizing information visualization. A study to map the value of
  graphic design to easier knowledge transfer of scientific research</title>
    <summary>  Visual representations are becoming important in science communication and
education. This explorative study investigates the perception of STEM
researchers, without any specific visual design background, and the value of
visual representations as tools to support the communication of technical and
scientific knowledge among academics and a wider non-technical community. Early
findings show that visual representations can positively support scientists to
share research outcomes in a more compelling, visually clear, and impactful
manner, reaching a wider audience across different disciplines.
</summary>
    <author>
      <name>Matteo Zallio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 images</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.09999v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09999v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11102v1</id>
    <updated>2021-01-05T07:36:44Z</updated>
    <published>2021-01-05T07:36:44Z</published>
    <title>Data driven Decision Support on Students Behavior using Fuzzy Based
  Approach</title>
    <summary>  Monitoring of students behavior in school needs further consideration in
order to lessen the number of casualties in every term. The study designs a
data driven decision support on students behavior utilizing Fuzzy Based
Approach. The study successfully produces common behavioral problems of the
student and able to give interventions for the improvement of students
behavior. Student behavioral problems identified were absenteeism, tardiness
and poor academic performance.
</summary>
    <author>
      <name>Jerry M. Lumasag</name>
    </author>
    <author>
      <name>Hidear Talirongan</name>
    </author>
    <author>
      <name>Florence Jean B. Talirongan</name>
    </author>
    <author>
      <name>Charies L. Labanza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, 6 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Middle East Journal of Applied Science &amp; Technology 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.11102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.12038v1</id>
    <updated>2021-01-27T10:42:20Z</updated>
    <published>2021-01-27T10:42:20Z</published>
    <title>From pixels to notes: a computational implementation of synaesthesia for
  cultural artefacts</title>
    <summary>  Synaesthesia is a condition that enables people to sense information in the
form of several senses at once. This work describes a Python implementation of
a simulation of synaesthesia between listening to music and viewing a painting.
Based on Scriabin's definition, we developed a deterministic process to produce
a melody after processing a painting, mimicking the production of notes from
colours in the field of view of persons experiencing synaesthesia.
</summary>
    <author>
      <name>Dimitris Kritikos</name>
    </author>
    <author>
      <name>Kostas Karpouzis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3399715.3400869</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3399715.3400869" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AVI2CH 2020: Workshop on Advanced Visual Interfaces and Interactions
  in Cultural Heritage, Ischia, Italy - Sept 28-October 2, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.12038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.05984v1</id>
    <updated>2021-03-10T10:47:10Z</updated>
    <published>2021-03-10T10:47:10Z</published>
    <title>Mixed Reality Interaction Techniques</title>
    <summary>  This chapter gives an overview of interaction techniques for mixed reality
including augmented and virtual reality (AR/VR). Various modalities for input
and output are discussed. Specifically, techniques for tangible and
surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and
mouse-based, as well as haptic interaction are discussed. Furthermore, the
combination of multiple modalities in multisensory and multimodal interaction,
as well as interaction using multiple physical or virtual displays, are
presented. Finally, interaction with intelligent virtual agents is considered.
</summary>
    <author>
      <name>Jens Grubert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Springer Handbook of Augmented Reality</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.05984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.05984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.08483v2</id>
    <updated>2021-03-16T13:25:33Z</updated>
    <published>2021-03-15T16:02:16Z</published>
    <title>All in one stroke? Intervention Spaces for Dark Patterns</title>
    <summary>  This position paper draws from the complexity of dark patterns to develop
arguments for differentiated interventions. We propose a matrix of
interventions with a \textit{measure axis} (from user-directed to
environment-directed) and a \textit{scope axis} (from general to specific). We
furthermore discuss a set of interventions situated in different fields of the
intervention spaces. The discussions at the 2021 CHI workshop "What can CHI do
about dark patterns?" should help hone the matrix structure and fill its fields
with specific intervention proposals.
</summary>
    <author>
      <name>Arianna Rossi</name>
    </author>
    <author>
      <name>Kerstin Bongard-Blanchy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position Paper at the Workshop "What Can CHI Do About Dark Patterns?"
  at the CHI Conference on Human Factors in Computing Systems (CHI'21), May
  8--13, 2021, Online Virtual Conference (originally Yokohama, Japan)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.08483v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.08483v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12645v1</id>
    <updated>2021-03-23T15:56:27Z</updated>
    <published>2021-03-23T15:56:27Z</published>
    <title>FoamFactor: Hydrogel-Foam Composite with Tunable Stiffness and
  Compressibility</title>
    <summary>  This paper presents FoamFactor, a novel material with tunable stiffness and
compressibility between hydration states, and a tailored pipeline to design and
fabricate artifacts consisting of it. This technique compounds hydrogel with
open-cell foams via additive manufacturing to produce a water-responsive
composite material. Enabled by the large volumetric changes of hydrogel
dispersions, the material is soft and compressible when dehydrated and becomes
stiffer and rather incompressible when hydrated. Leveraging this material
property transition, we explore its design space in various aspects pertaining
to the transition of hydration states, including multi-functional shoes,
amphibious cars, mechanical transmission systems, and self-deploying robotic
grippers.
</summary>
    <author>
      <name>Humphrey Yang</name>
    </author>
    <author>
      <name>Zeyu Yan</name>
    </author>
    <author>
      <name>Danli Luo</name>
    </author>
    <author>
      <name>Lining Yao</name>
    </author>
    <link href="http://arxiv.org/abs/2103.12645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.12645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15787v1</id>
    <updated>2021-03-29T17:31:07Z</updated>
    <published>2021-03-29T17:31:07Z</published>
    <title>Meeting in the notebook: a notebook-based environment for
  micro-submissions in data science collaborations</title>
    <summary>  Developers in data science and other domains frequently use computational
notebooks to create exploratory analyses and prototype models. However, they
often struggle to incorporate existing software engineering tooling into these
notebook-based workflows, leading to fragile development processes. We
introduce Assembl\'{e}, a new development environment for collaborative data
science projects, in which promising code fragments of data science pipelines
can be contributed as pull requests to an upstream repository entirely from
within JupyterLab, abstracting away low-level version control tool usage. We
describe the design and implementation of Assembl\'{e} and report on a user
study of 23 data scientists.
</summary>
    <author>
      <name>Micah J. Smith</name>
    </author>
    <author>
      <name>Jürgen Cito</name>
    </author>
    <author>
      <name>Kalyan Veeramachaneni</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01022v1</id>
    <updated>2021-04-02T12:45:03Z</updated>
    <published>2021-04-02T12:45:03Z</published>
    <title>Human Biases Preventing The Widespread Adoption Of Self-Driving Cars</title>
    <summary>  Self-driving cars offer a plethora of safety advantages over our accustomed
human-driven ones, yet many individuals feel uneasy sharing the road with these
machines and entrusting their lives to their driving capabilities. Thus,
bringing about a widespread adoption of autonomous cars requires overcoming
these compulsions through careful planning and forethought. Here we break down
the three primary psychological barriers that may hamstring or even wholly
prevent their widespread adoption as well as how to tackle them.
</summary>
    <author>
      <name>Benjamin Kahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.01022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04311v1</id>
    <updated>2021-04-09T11:33:02Z</updated>
    <published>2021-04-09T11:33:02Z</published>
    <title>Helping People Deal With Disinformation -- A Socio-Technical Perspective</title>
    <summary>  At the latest since the advent of the Internet, disinformation and conspiracy
theories have become ubiquitous. Recent examples like QAnon and Pizzagate prove
that false information can lead to real violence. In this motivation statement
for the Workshop on Human Aspects of Misinformation at CHI 2021, I explain my
research agenda focused on 1. why people believe in disinformation, 2. how
people can be best supported in recognizing disinformation, and 3. what the
potentials and risks of different tools designed to fight disinformation are.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be presented at the Workshop on Human Aspects of
  Misinformation at CHI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04415v1</id>
    <updated>2021-04-09T15:07:11Z</updated>
    <published>2021-04-09T15:07:11Z</published>
    <title>Automatic Knowledge Extraction with Human Interface</title>
    <summary>  OrbWeaver, an automatic knowledge extraction system paired with a human
interface, streamlines the use of unintuitive natural language processing
software for modeling systems from their documentation. OrbWeaver enables the
indirect transfer of knowledge about legacy systems by leveraging open source
tools in document understanding and processing as well as using web based user
interface constructs. By design, OrbWeaver is scalable, extensible, and usable;
we demonstrate its utility by evaluating its performance in processing a corpus
of documents related to advanced persistent threats in the cyber domain. The
results indicate better knowledge extraction by revealing hidden relationships,
linking co-related entities, and gathering evidence.
</summary>
    <author>
      <name>Steve Schmidt</name>
    </author>
    <author>
      <name>Denley Lam</name>
    </author>
    <author>
      <name>Patrick Hayden</name>
    </author>
    <link href="http://arxiv.org/abs/2104.04415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04912v1</id>
    <updated>2021-04-11T04:21:54Z</updated>
    <published>2021-04-11T04:21:54Z</published>
    <title>Visualization Improvisation</title>
    <summary>  Teaching visualization design involve making students familiar and make them
work with visualization models, framework and perspectives. Visualization
research accommodates a plethora of perspectives emerging from researchers of
varied backgrounds. These diverse range of perspectives give rise to multiples
models, frameworks and perspectives to teach visualization design. In this
paper, we look at an approach to visualization teaching by using
improvisational techniques. The basic idea is to design a visualization without
using an existing predefined model. Since improvisation, by definition, is not
a model or a framework, this work presents a reflection on how improvisation
can be a way of teaching visualization design.
</summary>
    <author>
      <name>Swaroop Panda</name>
    </author>
    <author>
      <name>Shatarupa Thakurta Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been peer-reviewed and accepted to VisActivities: IEEE
  VIS Workshop on Data Vis Activities held in conjunction with IEEE VIS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04922v1</id>
    <updated>2021-04-11T05:41:48Z</updated>
    <published>2021-04-11T05:41:48Z</published>
    <title>A Preliminary Model for the Design of Music Visualizations</title>
    <summary>  Music Visualization is basically the transformation of data from the aural to
the visual space. There are a variety of music visualizations, across
applications, present on the web. Models of Visualization include conceptual
frameworks helpful for designing, understanding and making sense of
visualizations. In this paper, we propose a preliminary model for Music
Visualization. We build the model by using two conceptual pivots, Visualization
Stimulus and Data Property. To demonstrate the utility of the model we
deconstruct and design visualizations with toy examples using the model and
finally conclude by proposing further applications of and future work on our
proposed model.
</summary>
    <author>
      <name>Swaroop Panda</name>
    </author>
    <author>
      <name>Shatarupa Thakurta Roy</name>
    </author>
    <link href="http://arxiv.org/abs/2104.04922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05340v1</id>
    <updated>2021-04-12T10:39:25Z</updated>
    <published>2021-04-12T10:39:25Z</published>
    <title>Speaking of Trust -- Speech as a Measure of Trust</title>
    <summary>  Since trust measures in human-robot interaction are often subjective or not
possible to implement real-time, we propose to use speech cues (on what, when
and how the user talks) as an objective real-time measure of trust. This could
be implemented in the robot to calibrate towards appropriate trust. However, we
would like to open the discussion on how to deal with the ethical implications
surrounding this trust measure.
</summary>
    <author>
      <name>Ella Velner</name>
    </author>
    <author>
      <name>Khiet P. Truong</name>
    </author>
    <author>
      <name>Vanessa Evers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in TRAITS Workshop Proceedings (arXiv:2103.12679) held in conjunction
  with Companion of the 2021 ACM/IEEE International Conference on Human-Robot
  Interaction, March 2021, Pages 709-711</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05356v1</id>
    <updated>2021-04-12T11:18:35Z</updated>
    <published>2021-04-12T11:18:35Z</published>
    <title>What We Measure in Mixed Reality Experiments</title>
    <summary>  There are many potential measures that one might use when evaluating
mixed-reality experiences. In this position paper I will argue that there are
various stances to take for evaluation, depending on the framing of the
experience within a larger body of work. I will draw upon various types of work
that my team has been involved with in order to illustrate these different
stances. I will then sketch out some directions for developing more robust
measures that can help the field move forward.
</summary>
    <author>
      <name>Anthony Steed</name>
    </author>
    <link href="http://arxiv.org/abs/2104.05356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05366v1</id>
    <updated>2021-04-12T11:34:14Z</updated>
    <published>2021-04-12T11:34:14Z</published>
    <title>Effectiveness of Social Virtual Reality</title>
    <summary>  A lot of work in social virtual reality, including our own group's, has
focused on effectiveness of specific social behaviours such as eye-gaze, turn
taking, gestures and other verbal and non-verbal cues. We have built upon these
to look at emergent phenomena such as co-presence, leadership and trust. These
give us good information about the usability issues of specific social VR
systems, but they don't give us much information about the requirements for
such systems going forward. In this short paper we discuss how we are
broadening the scope of our work on social systems, to move out of the
laboratory to more ecologically valid situations and to study groups using
social VR for longer periods of time.
</summary>
    <author>
      <name>Lisa Izzouzi</name>
    </author>
    <author>
      <name>Anthony Steed</name>
    </author>
    <link href="http://arxiv.org/abs/2104.05366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06221v1</id>
    <updated>2021-04-13T14:15:16Z</updated>
    <published>2021-04-13T14:15:16Z</published>
    <title>Questionnaires and Qualitative Feedback Methods to Measure User
  Experience in Mixed Reality</title>
    <summary>  Evaluating the user experience of a software system is an essential final
step of every research. Several concepts such as flow, affective state,
presences, or immersion exist to measure user experience. Typical measurement
techniques analyze physiological data, gameplay data, and questionnaires.
Qualitative feedback methods are another approach to collect detailed user
insights. In this position paper, we will discuss how we used questionnaires
and qualitative feedback methods in previous mixed reality work to measure user
experience. We will present several measurement examples, discuss their current
limitations, and provide guideline propositions to support comparable mixed
reality user experience research in the future.
</summary>
    <author>
      <name>Tobias Drey</name>
    </author>
    <author>
      <name>Michael Rietzler</name>
    </author>
    <author>
      <name>Enrico Rukzio</name>
    </author>
    <link href="http://arxiv.org/abs/2104.06221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07086v1</id>
    <updated>2021-04-14T19:24:10Z</updated>
    <published>2021-04-14T19:24:10Z</published>
    <title>Game Design for Blockchain Learning</title>
    <summary>  Blockchain is a new technological approach that has gained popularity on the
market due to its application in several areas such as education, health,
security, and smart cities, among others. However, understanding how blockchain
works is not easy at first, especially for non-technical people, because it
relies on a non-trivial computational process. We have developed a game board -
called Blocktrain - whose game mechanics are based on the blockchain processing
model. This game gives people the opportunity to learn key blockchain concepts
while playing. In this paper, we describe the game design process and
assessment of the game as pedagogical instrument.
</summary>
    <author>
      <name>Diogo Cortiz</name>
    </author>
    <author>
      <name>Newton Calegari</name>
    </author>
    <author>
      <name>Fabiana Oliveira</name>
    </author>
    <author>
      <name>Daniel Couto Gatti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper published in the II International Conference on Game, Game Art
  and Gamification</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.07086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07598v2</id>
    <updated>2022-05-02T10:19:24Z</updated>
    <published>2021-04-15T16:57:47Z</published>
    <title>Can Artificial Intelligence Make Art?</title>
    <summary>  In two experiments (total N=693) we explored whether people are willing to
consider paintings made by AI-driven robots as art, and robots as artists.
Across the two experiments, we manipulated three factors: (i) agent type
(AI-driven robot v. human agent), (ii) behavior type (intentional creation of a
painting v. accidental creation), and (iii) object type (abstract v.
representational painting). We found that people judge robot paintings and
human painting as art to roughly the same extent. However, people are much less
willing to consider robots as artists than humans, which is partially explained
by the fact that they are less disposed to attribute artistic intentions to
robots.
</summary>
    <author>
      <name>Elzė Sigutė Mikalonytė</name>
    </author>
    <author>
      <name>Markus Kneer</name>
    </author>
    <link href="http://arxiv.org/abs/2104.07598v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07598v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11214v1</id>
    <updated>2021-04-22T17:49:36Z</updated>
    <published>2021-04-22T17:49:36Z</published>
    <title>Topological Simplifications of Hypergraphs</title>
    <summary>  We study hypergraph visualization via its topological simplification. We
explore both vertex simplification and hyperedge simplification of hypergraphs
using tools from topological data analysis. In particular, we transform a
hypergraph to its graph representations known as the line graph and clique
expansion. A topological simplification of such a graph representation induces
a simplification of the hypergraph. In simplifying a hypergraph, we allow
vertices to be combined if they belong to almost the same set of hyperedges,
and hyperedges to be merged if they share almost the same set of vertices. Our
proposed approaches are general, mathematically justifiable, and they put
vertex simplification and hyperedge simplification in a unifying framework.
</summary>
    <author>
      <name>Youjia Zhou</name>
    </author>
    <author>
      <name>Archit Rathore</name>
    </author>
    <author>
      <name>Emilie Purvine</name>
    </author>
    <author>
      <name>Bei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11386v1</id>
    <updated>2021-04-23T02:46:00Z</updated>
    <published>2021-04-23T02:46:00Z</published>
    <title>Recording Reusable and Guided Analytics From Interaction Histories</title>
    <summary>  The use of visual analytics tools has gained popularity in various domains,
helping users discover meaningful information from complex and large data sets.
Users often face difficulty in disseminating the knowledge discovered without
clear recall of their exploration paths and analysis processes. We introduce a
visual analysis tool that allows analysts to record reusable and guided
analytics from their interaction logs. To capture the analysis process, we use
a decision tree whose node embeds visualizations and guide to define a visual
analysis task. The tool enables analysts to formalize analysis strategies,
build best practices, and guide novices through systematic workflows.
</summary>
    <author>
      <name>Nam Wook Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.11386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12020v1</id>
    <updated>2021-04-24T20:47:06Z</updated>
    <published>2021-04-24T20:47:06Z</published>
    <title>Towards Low-burden Responses to Open Questions in VR</title>
    <summary>  Subjective self-reports in VR user studies is a burdening and often tedious
task for the participants. To minimize the disruption with the ongoing
experience VR research has started to administer the surveying directly inside
the virtual environments. However, due to the tedious nature of text-entry in
VR, most VR surveying tools focus on closed questions with predetermined
responses, while open questions with free-text responses remain unexplored.
This neglects a crucial part of UX research. To provide guidance on suitable
self-reporting methods for open questions in VR user studies, this position
paper presents a comparative study with three text-entry methods in VR and
outlines future directions towards low-burden qualitative responding.
</summary>
    <author>
      <name>Dmitry Alexandrovsky</name>
    </author>
    <author>
      <name>Susanne Putze</name>
    </author>
    <author>
      <name>Alexander Schülke</name>
    </author>
    <author>
      <name>Rainer Malaka</name>
    </author>
    <link href="http://arxiv.org/abs/2104.12020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12154v1</id>
    <updated>2021-04-25T13:22:32Z</updated>
    <published>2021-04-25T13:22:32Z</published>
    <title>Case Study on Using Colours in Constructing Emotions by Interactive
  Digital Narratives</title>
    <summary>  This article addresses the possibility of supporting the construction of
emotions in the participants of Interactive Digital Narratives (IDN) by means
of colours. The article uses goal models for expressing protostories. The core
of the article consists of the case study where two colour synes-thetes were
asked to choose colours for eight emotions. Thereafter the same synesthetes
were asked to choose colours for the emotions that support the attainment of
goals in the Cinderella narrative, which serves as an example protostory. The
article also discusses the perspectives for applying the method proposed by us
to using colours in constructing emotions by IDNs.
</summary>
    <author>
      <name>Kuldar Taveter</name>
    </author>
    <author>
      <name>Eliise Marie Taveter</name>
    </author>
    <link href="http://arxiv.org/abs/2104.12154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12816v1</id>
    <updated>2021-04-12T18:16:28Z</updated>
    <published>2021-04-12T18:16:28Z</published>
    <title>Survey: Vitals Screening Techniques for a Safer Environment</title>
    <summary>  With COVID-19 disrupting operations across various sectors of the workforce
(e.g., offices, airports, libraries, schools), preventative measures enabling
resumption of work are quickly becoming a necessity. In this paper, we present
the need for Vitals Screening Techniques (VIST) where more than one vital is
screened to ensure the safety of the population (e.g. temperature, heart rate,
and blood oxygen levels). VIST can be deployed in crowded environments to
provide the new necessary layer of safety. We provide extensive coverage of
state-of-art technology that can assist in tackling this emerging problem, and
evaluate one of the existing products on the market that employ VIST.
</summary>
    <author>
      <name>Sarah Baig</name>
    </author>
    <author>
      <name>Mohammed Elbadry</name>
    </author>
    <link href="http://arxiv.org/abs/2104.12816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14699v1</id>
    <updated>2021-04-29T23:47:13Z</updated>
    <published>2021-04-29T23:47:13Z</published>
    <title>Why should we care about register? Reflections on chatbot language
  design</title>
    <summary>  This position paper discusses the relevance of register as a theoretical
framework for chatbot language design. We present the concept of register and
discuss how using register-specific language influence the user's perceptions
of the interaction with chatbots. Additionally, we point several research
opportunities that are important to pursue to establish register as a
foundation for advancing chatbot's communication skills.
</summary>
    <author>
      <name>Ana Paula Chaves</name>
    </author>
    <author>
      <name>Marco Aurelio Gerosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2101.11089</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14811v2</id>
    <updated>2022-06-10T08:11:07Z</updated>
    <published>2021-04-30T07:55:18Z</published>
    <title>Why scholars are diagramming neural network models</title>
    <summary>  Complex models, such as neural networks (NNs), are comprised of many
interrelated components. In order to represent these models, eliciting and
characterising the relations between components is essential. Perhaps because
of this, diagrams, as "icons of relation", are a prevalent medium for
signifying complex models. Diagrams used to communicate NN architectures are
currently extremely varied. The diversity in diagrammatic choices provides an
opportunity to gain insight into the aspects which are being prioritised for
communication. In this philosophical exploration of NN diagrams, we integrate
theories of conceptual models, communication theory, and semiotics.
</summary>
    <author>
      <name>Guy Clarke Marshall</name>
    </author>
    <author>
      <name>Caroline Jay</name>
    </author>
    <author>
      <name>Andre Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01488v2</id>
    <updated>2021-05-07T03:06:52Z</updated>
    <published>2021-05-03T02:27:00Z</published>
    <title>Accessibility Across Borders</title>
    <summary>  Since prior work has identified that cultural differences influence user
design preferences and interaction methods, as well as emphasizing the need to
reflect on the appropriateness of popular HCI principles, we believe that it is
equally important to apply this inquiry to digital accessibility and how
accessibility fits within the design process around the world. Our long-term
plan is to build upon work in this area by investigating how digital designers
in different parts of the world consider accessibility and whether current
accessibility resources (often developed in the west) meet or conflict with
their approach to design.
</summary>
    <author>
      <name>Garreth W. Tigwell</name>
    </author>
    <author>
      <name>Kristen Shinohara</name>
    </author>
    <author>
      <name>Laleh Nourian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as part of the CHI '21 Workshop: Decolonizing HCI Across
  Borders</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.01488v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01488v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05306v1</id>
    <updated>2021-05-11T19:04:21Z</updated>
    <published>2021-05-11T19:04:21Z</published>
    <title>Intelligent interactive technologies for mental health and well-being</title>
    <summary>  Mental healthcare has seen numerous benefits from interactive technologies
and artificial intelligence. Various interventions have successfully used
intelligent technologies to automate the assessment and evaluation of
psychological treatments and mental well-being and functioning. These
technologies include different types of robots, video games, and conversational
agents. The paper critically analyzes existing solutions with the outlooks for
their future. In particular, we: i)give an overview of the technology for
mental health, ii) critically analyze the technology against the proposed
criteria, and iii) provide the design outlooks for these technologies.
</summary>
    <author>
      <name>Mladjan Jovanovic</name>
    </author>
    <author>
      <name>Aleksandar Jevremovic</name>
    </author>
    <author>
      <name>Milica Pejovic-Milovancevic</name>
    </author>
    <link href="http://arxiv.org/abs/2105.05306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05322v1</id>
    <updated>2021-05-11T19:36:18Z</updated>
    <published>2021-05-11T19:36:18Z</published>
    <title>Diplomat: A conversational agent framework for goal-oriented group
  discussion</title>
    <summary>  Recent work in human-computer interaction has explored the use of
conversational agents as facilitators for group goal-oriented discussions.
Inspired by this work and by the apparent lack of tooling available to support
it, we created Diplomat, a Python-based framework for building conversational
agent facilitators. Diplomat is designed to support simple specification of
agent functionality as well as customizable integration with online chat
services. We document a preliminary user study we conducted to help inform the
design of Diplomat. We also describe the architecture, capabilities, and
limitations of our tool, which we have shared on GitHub.
</summary>
    <author>
      <name>Kevin Hogan</name>
    </author>
    <author>
      <name>Annabelle Baer</name>
    </author>
    <author>
      <name>James Purtilo</name>
    </author>
    <link href="http://arxiv.org/abs/2105.05322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06730v1</id>
    <updated>2021-05-14T09:31:43Z</updated>
    <published>2021-05-14T09:31:43Z</published>
    <title>Simulating Social Acceptability With Agent-based Modeling</title>
    <summary>  Social acceptability is an important consideration for HCI designers who
develop technologies for social contexts. However, the current theoretical
foundations of social acceptability research do not account for the complex
interactions among the actors in social situations and the specific role of
technology. In order to improve the understanding of how context shapes and is
shaped by situated technology interactions, we suggest to reframe the social
space as a dynamic bundle of social practices and explore it with simulation
studies using agent-based modeling. We outline possible research directions
that focus on specific interactions among practices as well as regularities in
emerging patterns.
</summary>
    <author>
      <name>Alarith Uhde</name>
    </author>
    <author>
      <name>Marc Hassenzahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.06730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10153v1</id>
    <updated>2021-05-21T06:35:16Z</updated>
    <published>2021-05-21T06:35:16Z</published>
    <title>How Can I Swing Like Pro?: Golf Swing Analysis Tool for Self Training</title>
    <summary>  In this work, we present an analysis tool to help golf beginners compare
their swing motion with experts' swing motion. The proposed application
synchronizes videos with different swing phase timings using the latent
features extracted by a neural network-based encoder and detects key frames
where discrepant motions occur. We visualize synchronized image frames and 3D
poses that help users recognize the difference and the key factors that can be
important for their swing skill improvement.
</summary>
    <author>
      <name>Chen-Chieh Liao</name>
    </author>
    <author>
      <name>Dong-Hyun Hwang</name>
    </author>
    <author>
      <name>Hideki Koike</name>
    </author>
    <link href="http://arxiv.org/abs/2105.10153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10186v1</id>
    <updated>2021-05-21T07:53:40Z</updated>
    <published>2021-05-21T07:53:40Z</published>
    <title>Experiencing Utopia. A Positive Approach to Design Fiction</title>
    <summary>  Design Fiction is known for its provocative and often dystopian speculations
about the future. In this paper, we present an alternative approach that
focuses primarily on the positive. We propose to imagine, enact, and evaluate
utopia with participants. By doing so, we react to four main critiques
concerning Design Fiction: (1) its negativity, (2) its contextlessness, (3) its
elitist authorship, and (4) its missing evaluation methods.
</summary>
    <author>
      <name>Judith Dörrenbächer</name>
    </author>
    <author>
      <name>Matthias Laschke</name>
    </author>
    <author>
      <name>Diana Löffler</name>
    </author>
    <author>
      <name>Ronda Ringfort</name>
    </author>
    <author>
      <name>Sabrina Großkopp</name>
    </author>
    <author>
      <name>Marc Hassenzahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.10186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10366v1</id>
    <updated>2021-05-21T14:07:57Z</updated>
    <published>2021-05-21T14:07:57Z</published>
    <title>Going Beyond Second Screens: Applications for the Multi-display
  Intelligent Living Room</title>
    <summary>  This work aims to investigate how the amenities offered by Intelligent
Environments can be used to shape new types of useful, exciting and fulfilling
experiences while watching sports or movies. Towards this direction, two
ambient media players were developed aspiring to offer live access to secondary
information via the available displays of an Intelligent Living Room, and to
appropriately exploit the technological equipment so as to support natural
interaction. Expert-based evaluation experiments revealed some factors that can
influence the overall experience significantly, without hindering the viewers'
immersion to the main media.
</summary>
    <author>
      <name>Asterios Leonidis</name>
    </author>
    <author>
      <name>Maria Korozi</name>
    </author>
    <author>
      <name>Vassilis Kouroumalis</name>
    </author>
    <author>
      <name>Emmanouil Adamakis</name>
    </author>
    <author>
      <name>Dimitris Milathianakis</name>
    </author>
    <author>
      <name>Constantine Stephanidis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3452918.3465486</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3452918.3465486" rel="related"/>
    <link href="http://arxiv.org/abs/2105.10366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11037v1</id>
    <updated>2021-05-23T22:57:17Z</updated>
    <published>2021-05-23T22:57:17Z</published>
    <title>Visualization -- a vital decision driving tool for enterprises</title>
    <summary>  This report documents the results found through surveys and interviews on how
visualizations help the employees in their workspace. The objectives of this
study were to get in-depth knowledge on what prepares an employee to have the
right skill set in constructing an informative visualization as well as the
tools and techniques that they use on their daily basis for analysis and
visualization purposes. Using the results gathered, we sorted the information
in many different ways for analysis and came to conclusions ranging from
corporation-based strategies to individualized employee and position
preferences.
</summary>
    <author>
      <name>Rajath Chikkatur Srinivasa</name>
    </author>
    <author>
      <name>Supriya Arun</name>
    </author>
    <author>
      <name>Lauren James</name>
    </author>
    <author>
      <name>Ying Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12453v1</id>
    <updated>2021-05-26T10:30:50Z</updated>
    <published>2021-05-26T10:30:50Z</published>
    <title>How Do Users Interact with an Error-Prone In-Air Gesture Recognizer?</title>
    <summary>  We present results of two pilot studies that investigated human error
behaviours with an error prone in-air gesture recognizer. During the studies,
users performed a small set of simple in-air gestures. In the first study,
these gestures were abstract. The second study associated concrete tasks with
each gesture. Interestingly, the error patterns observed in the two studies
were substantially different.
</summary>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <author>
      <name>Wolfgang Stuerzlinger</name>
    </author>
    <author>
      <name>Euclides Jose de Mendonca Filho</name>
    </author>
    <author>
      <name>Alec Gordynski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In CHI 2014 Workshop on Gesture-based Interaction Design:
  Communication and Cognition (April 26, 2014). Toronto, Ontario, Canada, 69-72</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.12453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12944v1</id>
    <updated>2021-05-27T05:30:23Z</updated>
    <published>2021-05-27T05:30:23Z</published>
    <title>MarioMix: Creating Aligned Playstyles for Bots with Interactive
  Reinforcement Learning</title>
    <summary>  In this paper, we propose a generic framework that enables game developers
without knowledge of machine learning to create bot behaviors with playstyles
that align with their preferences. Our framework is based on interactive
reinforcement learning (RL), and we used it to create a behavior authoring tool
called MarioMix. This tool enables non-experts to create bots with varied
playstyles for the game titled Super Mario Bros. The main interaction procedure
of MarioMix consists of presenting short clips of gameplay displaying
precomputed bots with different playstyles to end-users. Then, end-users can
select the bot with the playstyle that behaves as intended. We evaluated
MarioMix by incorporating input from game designers working in the industry.
</summary>
    <author>
      <name>Christian Arzate Cruz</name>
    </author>
    <author>
      <name>Takeo Igarashi</name>
    </author>
    <link href="http://arxiv.org/abs/2105.12944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12949v1</id>
    <updated>2021-05-27T05:46:43Z</updated>
    <published>2021-05-27T05:46:43Z</published>
    <title>A Survey on Interactive Reinforcement Learning: Design Principles and
  Open Challenges</title>
    <summary>  Interactive reinforcement learning (RL) has been successfully used in various
applications in different fields, which has also motivated HCI researchers to
contribute in this area. In this paper, we survey interactive RL to empower
human-computer interaction (HCI) researchers with the technical background in
RL needed to design new interaction techniques and propose new applications. We
elucidate the roles played by HCI researchers in interactive RL, identifying
ideas and promising research directions. Furthermore, we propose generic design
principles that will provide researchers with a guide to effectively implement
interactive RL applications.
</summary>
    <author>
      <name>Christian Arzate Cruz</name>
    </author>
    <author>
      <name>Takeo Igarashi</name>
    </author>
    <link href="http://arxiv.org/abs/2105.12949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.14465v1</id>
    <updated>2021-05-30T08:37:35Z</updated>
    <published>2021-05-30T08:37:35Z</published>
    <title>A Brief Survey on Interactive Automotive UI</title>
    <summary>  Automotive User Interface (AutoUI) is relatively a new discipline in the
context of both Transportation Engineering and Human Machine Interaction (HMI).
It covers various HMI aspects both inside and outside vehicle ranging from
operating the vehicle itself, undertaking various secondary tasks, driver
behaviour analysis, cognitive load estimation and so on. This review paper
discusses various interactive HMI inside a vehicle used for undertaking
secondary tasks. We divided recent HMIs through four sections on virtual touch
interfaces, wearable devices, speech recognition and non-visual interfaces and
eye gaze controlled systems. Finally, we summarized advantages and
disadvantages of various technologies.
</summary>
    <author>
      <name>Gowdham Prabhakar</name>
    </author>
    <author>
      <name>Pradipta Biswas</name>
    </author>
    <link href="http://arxiv.org/abs/2105.14465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06255v1</id>
    <updated>2021-06-11T09:14:59Z</updated>
    <published>2021-06-11T09:14:59Z</published>
    <title>Improving Take-over Situation by Active Communication</title>
    <summary>  In this short paper an idea is sketched, how to support drivers of an
autonomous vehicle in taking back control of the vehicle after a longer section
of autonomous cruising. The hypothesis is that a clear communication about the
location and behavior of relevant objects in the environment will help the
driver to quickly grasp the situational context and thus support drivers in
safely handling the ongoing driving situation manually after take-over. Based
on this hypothesis, a research concept is sketched, which entails the necessary
components as well as the disciplines involved.
</summary>
    <author>
      <name>Monika Sester</name>
    </author>
    <author>
      <name>Mark Vollrath</name>
    </author>
    <author>
      <name>Hao Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2106.06255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09763v1</id>
    <updated>2021-06-17T18:45:44Z</updated>
    <published>2021-06-17T18:45:44Z</published>
    <title>Sensory Modality Mapping for Game Adaptation and Design</title>
    <summary>  In this paper we examine methods for taking game-related information provided
in one sensory modality and transforming it to another sensor modality in order
to more effectively accommodate sensory-constrained players. We then consider
methods for the adaptation and design of games for which gameplay interactions
are constrained to a subset of sensory modalities in ways that preserve a
common level of novelty-of-experience for players with different sensory
capabilities. It is hoped that improved shared experiences can promote
interactions among a more diverse spectrum of players.
</summary>
    <author>
      <name>Jeffrey Uhlmann</name>
    </author>
    <link href="http://arxiv.org/abs/2106.09763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12857v1</id>
    <updated>2021-06-24T09:43:15Z</updated>
    <published>2021-06-24T09:43:15Z</published>
    <title>Pattern-based Visualization of Knowledge Graphs</title>
    <summary>  We present a novel approach to knowledge graph visualization based on
ontology design patterns. This approach relies on OPLa (Ontology Pattern
Language) annotations and on a catalogue of visual frames, which are associated
with foundational ontology design patterns. We demonstrate that this approach
significantly reduces the cognitive load required to users for visualizing and
interpreting a knowledge graph and guides the user in exploring it through
meaningful thematic paths provided by ontology patterns.
</summary>
    <author>
      <name>Luigi Asprino</name>
    </author>
    <author>
      <name>Christian Colonna</name>
    </author>
    <author>
      <name>Misael Mongiovì</name>
    </author>
    <author>
      <name>Margherita Porena</name>
    </author>
    <author>
      <name>Valentina Presutti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.12857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13388v1</id>
    <updated>2021-06-25T02:10:03Z</updated>
    <published>2021-06-25T02:10:03Z</published>
    <title>Influences on Drivers' Understandings of Systems by Presenting Image
  Recognition Results</title>
    <summary>  It is essential to help drivers have appropriate understandings of level 2
automated driving systems for keeping driving safety. A human machine interface
(HMI) was proposed to present real time results of image recognition by the
automated driving systems to drivers. It was expected that drivers could better
understand the capabilities of the systems by observing the proposed HMI.
Driving simulator experiments with 18 participants were preformed to evaluate
the effectiveness of the proposed system. Experimental results indicated that
the proposed HMI could effectively inform drivers of potential risks
continuously and help drivers better understand the level 2 automated driving
systems.
</summary>
    <author>
      <name>Bo Yang</name>
    </author>
    <author>
      <name>Koichiro Inoue</name>
    </author>
    <author>
      <name>Satoshi Kitazaki</name>
    </author>
    <author>
      <name>Kimihiko Nakano</name>
    </author>
    <link href="http://arxiv.org/abs/2106.13388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13675v1</id>
    <updated>2021-05-30T09:26:53Z</updated>
    <published>2021-05-30T09:26:53Z</published>
    <title>Creating and Implementing a Smart Speaker</title>
    <summary>  We have seen significant advancements in Artificial Intelligence and Machine
Learning in the 21st century. It has enabled a new technology where we can have
a human-like conversation with the machines. The most significant use of this
speech recognition and contextual understanding technology exists in the form
of a Smart Speaker. We have a wide variety of Smart Speaker products available
to us. This paper aims to decode its creation and explain the technology that
makes these Speakers, "Smart."
</summary>
    <author>
      <name>Sanskar Jethi</name>
    </author>
    <author>
      <name>Avinash Kumar Choudhary</name>
    </author>
    <author>
      <name>Yash Gupta</name>
    </author>
    <author>
      <name>Abhishek Chaudhary</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IT in Industry, Vol. 9, No.3, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.13675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.16106v1</id>
    <updated>2021-06-30T14:54:25Z</updated>
    <published>2021-06-30T14:54:25Z</published>
    <title>How can design help enhance trust calibration in public autonomous
  vehicles?</title>
    <summary>  Trust is a multilayered concept with critical relevance when it comes to
introducing new technologies. Understanding how humans will interact with
complex vehicle systems and preparing for the functional, societal and
psychological aspects of autonomous vehicles' entry into our cities is a
pressing concern. Design tools can help calibrate the adequate and affordable
level of trust needed for a safe and positive experience. This study focuses on
passenger interactions capable of enhancing the system trustworthiness and data
accuracy in future shared public transportation.
</summary>
    <author>
      <name>Yuri Klebanov</name>
    </author>
    <author>
      <name>Romi Mikulinsky</name>
    </author>
    <author>
      <name>Tom Reznikov</name>
    </author>
    <author>
      <name>Miles Pennington</name>
    </author>
    <author>
      <name>Yoshihiro Suda</name>
    </author>
    <author>
      <name>Toshihiro Hiraoka</name>
    </author>
    <author>
      <name>Shoichi Kanzaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, IV 2021 Nagoya, Trust Calibration Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.16106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.16106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.16122v2</id>
    <updated>2021-11-02T12:27:45Z</updated>
    <published>2021-06-30T15:19:20Z</published>
    <title>Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical
  Decisions</title>
    <summary>  Departing from the claim that AI needs to be trustworthy, we find that
ethical advice from an AI-powered algorithm is trusted even when its users know
nothing about its training data and when they learn information about it that
warrants distrust. We conducted online experiments where the subjects took the
role of decision-makers who received advice from an algorithm on how to deal
with an ethical dilemma. We manipulated the information about the algorithm and
studied its influence. Our findings suggest that AI is overtrusted rather than
distrusted. We suggest digital literacy as a potential remedy to ensure the
responsible use of AI.
</summary>
    <author>
      <name>Sebastian Krügel</name>
    </author>
    <author>
      <name>Andreas Ostermaier</name>
    </author>
    <author>
      <name>Matthias Uhl</name>
    </author>
    <link href="http://arxiv.org/abs/2106.16122v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.16122v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04770v1</id>
    <updated>2021-08-10T16:24:30Z</updated>
    <published>2021-08-10T16:24:30Z</published>
    <title>Examining correlation between trust and transparency with explainable
  artificial intelligence</title>
    <summary>  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
</summary>
    <author>
      <name>Arnav Kartikeya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05960v1</id>
    <updated>2021-08-01T23:36:01Z</updated>
    <published>2021-08-01T23:36:01Z</published>
    <title>Motivations and Expectations for Virtual Gift-Giving in Douyin Live
  Streams</title>
    <summary>  Social live streaming services (SLSS) (e.g. Twitch, YouTube Live) combine
real-time presentation of user-generated content with social networking
features that allow streamers and viewers to interact in a co-present social
online setting. Mobile live streaming is now an integrated feature in many
social media apps (e.g. Facebook, Instagram, Snapchat) and has recently
advanced as a popular form of content creation on entertainment-based
short-video apps, especially in the Chinese market and foremost Douyin, the
Chinese version of TikTok.
</summary>
    <author>
      <name>Huilian Sophie Qiu</name>
    </author>
    <author>
      <name>Daniel Klug</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5210/spir.v2021i0.12194</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5210/spir.v2021i0.12194" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 22nd Annual Conference of the Association of Internet
  Researchers, Virtual Event, 13-16 Oct 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AoIR Selected Papers of Internet Research, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.05960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.09277v1</id>
    <updated>2021-08-18T17:33:17Z</updated>
    <published>2021-08-18T17:33:17Z</published>
    <title>MHealth: An Artificial Intelligence Oriented Mobile Application for
  Personal Healthcare Support</title>
    <summary>  Main objective of this study is to introduce an expert system-based mHealth
application that takes Artificial Intelligence support by considering
previously introduced solutions from the literature and employing possible
requirements for a better solution. Thanks to that research study, a mobile
software system having Artificial Intelligence support and providing dynamic
support against the common health problems in daily life was designed-developed
and it was evaluated via survey and diagnosis-based evaluation tasks.
Evaluation tasks indicated positive outcomes for the mHealth system.
</summary>
    <author>
      <name>Ismail Ali Afrah</name>
    </author>
    <author>
      <name>Utku Kose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Multidisciplinary Developments, 5(1), 2020, 14-30</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.09277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.09277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.00133v1</id>
    <updated>2021-09-01T00:56:53Z</updated>
    <published>2021-09-01T00:56:53Z</published>
    <title>AugLimb: Compact Robotic Limb for Human Augmentation</title>
    <summary>  This work proposes a compact robotic limb, AugLimb, that can augment our body
functions and support the daily activities. AugLimb adopts the double-layer
scissor unit for the extendable mechanism which can achieve 2.5 times longer
than the forearm length. The proposed device can be mounted on the user's upper
arm, and transform into compact state without obstruction to wearers. The
proposed device is lightweight with low burden exerted on the wearer. We
developed the prototype of AugLimb to demonstrate the proposed mechanisms. We
believe that the design methodology of AugLimb can facilitate human
augmentation research for practical use. see
http://www.jaist.ac.jp/~xie/auglimb.html
</summary>
    <author>
      <name>Zeyu Ding</name>
    </author>
    <author>
      <name>Shogo Yoshida</name>
    </author>
    <author>
      <name>Toby Chong</name>
    </author>
    <author>
      <name>Tsukasa Fukusato</name>
    </author>
    <author>
      <name>Takuma Torii</name>
    </author>
    <author>
      <name>Haoran Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.00133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.00133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02386v1</id>
    <updated>2021-08-07T17:27:13Z</updated>
    <published>2021-08-07T17:27:13Z</published>
    <title>Augmented Reality for Education: A Review</title>
    <summary>  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
</summary>
    <author>
      <name>Carlo H. Godoy Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.38124/IJISRT20JUN256</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.38124/IJISRT20JUN256" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. arXiv admin note: text overlap with arXiv:2101.03683</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Volume 5 Issue 6 June 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.02386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91Axx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.m; D.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03669v1</id>
    <updated>2021-09-08T14:15:32Z</updated>
    <published>2021-09-08T14:15:32Z</published>
    <title>A Mixed-Initiative Visual Analytics Approach for Qualitative Causal
  Modeling</title>
    <summary>  Modeling complex systems is a time-consuming, difficult and fragmented task,
often requiring the analyst to work with disparate data, a variety of models,
and expert knowledge across a diverse set of domains. Applying a user-centered
design process, we developed a mixed-initiative visual analytics approach, a
subset of the Causemos platform, that allows analysts to rapidly assemble
qualitative causal models of complex socio-natural systems. Our approach
facilitates the construction, exploration, and curation of qualitative models
bringing together data across disparate domains. Referencing a recent user
evaluation, we demonstrate our approach's ability to interactively enrich user
mental models and accelerate qualitative model building.
</summary>
    <author>
      <name>Fahd Husain</name>
    </author>
    <author>
      <name>Pascale Proulx</name>
    </author>
    <author>
      <name>Meng-Wei Chang</name>
    </author>
    <author>
      <name>Rosa Romero-Gomez</name>
    </author>
    <author>
      <name>Holland Vasquez</name>
    </author>
    <link href="http://arxiv.org/abs/2109.03669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04023v1</id>
    <updated>2021-09-09T03:52:04Z</updated>
    <published>2021-09-09T03:52:04Z</published>
    <title>Rethinking Immersive Virtual Reality and Empathy</title>
    <summary>  In this position paper, we aim to spark more discussions surrounding the use
of empathy as the intended outcome of many studies on immersive virtual reality
experiences. As a construct, empathy has many significant flaws that may lead
to unintended and negative outcomes, going against our original goal of
employing these technologies for the betterment of society. We highlight the
possible advantages of designing for rational compassion instead, and propose
alternative research directions and outcome measurements for immersive virtual
reality that urgently warrant our attention.
</summary>
    <author>
      <name>Ken Jen Lee</name>
    </author>
    <author>
      <name>Edith Law</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, ACM CSCW 2021 workshop, arttech: Performance and Embodiment
  in Technology for Resilience and Mental Health</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.04023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; J.4; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04719v1</id>
    <updated>2021-09-10T07:54:07Z</updated>
    <published>2021-09-10T07:54:07Z</published>
    <title>NaviChoker: Augmenting Pressure Sensation via Pneumatic Actuator</title>
    <summary>  Many technologies have been developed in recent years to present audiovisual
information in new ways, but developing an information presentation interface
to convey tactile information is still a challenge. We propose a tactile device
using wearable technology that is an all-around pressure presentation system
using pneumatic actuators. Specifically, we develop a system in which a choker
equipped with a pneumatic actuator is worn around the neck, that applies
pressure in any direction to indicate to the user the direction in which to
walk and also when to start and stop walking. In this paper, we describe the
construction of the device, evaluation experiments, our assessment of the
prototype, and future plans for the device.
</summary>
    <author>
      <name>Shogo Yoshida</name>
    </author>
    <author>
      <name>Haoran Xie</name>
    </author>
    <author>
      <name>Kazunori Miyata</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3460881.3460937</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3460881.3460937" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of AH 2021. 4 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.04719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11365v1</id>
    <updated>2021-09-23T13:22:14Z</updated>
    <published>2021-09-23T13:22:14Z</published>
    <title>Tumera: Tutor of Photography Beginners</title>
    <summary>  With the popularity of photographic equipment, more and more people are
starting to learn photography by themselves. Although they have easy access to
photographic materials, it is uneasy to obtain professional feedback or
guidance that can help them improve their photography skills. Therefore, we
develop an intelligently interactive system, Tumera, that provides aesthetics
guidance for photography beginners. When shooting, Tumera gives timely feedback
on the pictures in the view port. After shooting, scores evaluating the
aesthetic quality of different aspects of the photos and corresponding
improvement suggestions are given. Tumera allows users to share, rank, discuss,
and learn from their works and interaction with the system based on the scores
and suggestions. In the experiment, Tumera showed good accuracy, real-time
computing ability, and effective guiding performance.
</summary>
    <author>
      <name>Xiaoran Wu</name>
    </author>
    <author>
      <name>Jia Jia</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HCI International 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.11365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11408v2</id>
    <updated>2024-01-05T08:53:06Z</updated>
    <published>2021-09-23T14:42:15Z</published>
    <title>Reinforced Natural Language Interfaces via Entropy Decomposition</title>
    <summary>  In this paper, we study the technical problem of developing conversational
agents that can quickly adapt to unseen tasks, learn task-specific
communication tactics, and help listeners finish complex, temporally extended
tasks. We find that the uncertainty of language learning can be decomposed to
an entropy term and a mutual information term, corresponding to the structural
and functional aspect of language, respectively. Combined with reinforcement
learning, our method automatically requests human samples for training when
adapting to new tasks and learns communication protocols that are succinct and
helpful for task completion. Human and simulation test results on a referential
game and a 3D navigation game prove the effectiveness of the proposed method.
</summary>
    <author>
      <name>Xiaoran Wu</name>
    </author>
    <author>
      <name>Yipeng Kang</name>
    </author>
    <link href="http://arxiv.org/abs/2109.11408v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11408v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.15224v1</id>
    <updated>2021-09-30T15:50:44Z</updated>
    <published>2021-09-30T15:50:44Z</published>
    <title>The Explanatory Gap in Algorithmic News Curation</title>
    <summary>  Considering the large amount of available content, social media platforms
increasingly employ machine learning (ML) systems to curate news. This paper
examines how well different explanations help expert users understand why
certain news stories are recommended to them. The expert users were
journalists, who are trained to judge the relevance of news. Surprisingly, none
of the explanations are perceived as helpful. Our investigation provides a
first indication of a gap between what is available to explain ML-based
curation systems and what users need to understand such systems. We call this
the Explanatory Gap in Machine Learning-based Curation Systems.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-87031-7_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-87031-7_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be presented at the Third Multidisciplinary
  International Symposium, MISDOOM 2021, held in Oxford, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.15224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.15224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02768v2</id>
    <updated>2021-10-07T16:37:57Z</updated>
    <published>2021-10-04T12:11:36Z</published>
    <title>Posture Recognition in the Critical Care Settings using Wearable Devices</title>
    <summary>  Low physical activity levels in the intensive care units (ICU) patients have
been linked to adverse clinical outcomes. Therefore, there is a need for
continuous and objective measurement of physical activity in the ICU to
quantify the association between physical activity and patient outcomes. This
measurement would also help clinicians evaluate the efficacy of proposed
rehabilitation and physical therapy regimens in improving physical activity. In
this study, we examined the feasibility of posture recognition in an ICU
population using data from wearable sensors.
</summary>
    <author>
      <name>Anis Davoudi</name>
    </author>
    <author>
      <name>Patrick J. Tighe</name>
    </author>
    <author>
      <name>Azra Bihorac</name>
    </author>
    <author>
      <name>Parisa Rashidi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.02768v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.02768v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02930v1</id>
    <updated>2021-10-06T17:24:30Z</updated>
    <published>2021-10-06T17:24:30Z</published>
    <title>"What Artists Want": Elicitation of Artist Requirements to Feed the
  Design on a New Collaboration Platform for Creative Work</title>
    <summary>  Aiming at designing a decentralized platform to support grassroot initiatives
for self-organized creative work, the present work solicited feedback from a
group of visual artists regarding their work processes and concerns. The paper
presents the qualitative methodology followed for collecting requirements from
the target audience of the envisioned software solution. The data gathered from
the focus group is analyzed and we conclude with a set of important
requirements that the future platform needs to fulfill.
</summary>
    <author>
      <name>Angeliki Antoniou</name>
    </author>
    <author>
      <name>Ioanna Lykourentzou</name>
    </author>
    <author>
      <name>Antonios Liapis</name>
    </author>
    <author>
      <name>Dimitra Nikolou</name>
    </author>
    <author>
      <name>Marily Konstantinopoulou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.02930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.02930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.03379v1</id>
    <updated>2021-10-07T12:16:54Z</updated>
    <published>2021-10-07T12:16:54Z</published>
    <title>Hafnia Hands: A Multi-Skin Hand Texture Resource for Virtual Reality
  Research</title>
    <summary>  We created a hand texture resource (with different skin tone versions as well
as non-human hands) for use in virtual reality studies. This makes it easier to
run lab and remote studies where the hand representation is matched to the
participant's own skin tone. We validate that the virtual hands with our
textures align with participants view of their own real hands and allow to
create VR applications where participants have an increased sense of body
ownership. These properties are critical for a range of VR studies, such as of
immersion.
</summary>
    <author>
      <name>Henning Pohl</name>
    </author>
    <author>
      <name>Aske Mottelson</name>
    </author>
    <link href="http://arxiv.org/abs/2110.03379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04129v1</id>
    <updated>2021-10-07T07:49:55Z</updated>
    <published>2021-10-07T07:49:55Z</published>
    <title>Morphological Matrices as a Tool for Crowdsourced Ideation</title>
    <summary>  Designing a novel product is a difficult task not well suited for non-expert
crowd workers. In this work-in-progress paper, we first motivate why the design
of persuasive products is an interesting context for studying creativity and
the creative leap. We then present a pilot study on the crowdsourced design of
persuasive products. The pilot study motivated our subsequent feasibility study
on the use of morphological matrices as a tool for crowdsourced ideation and
product design. Given the morphological matrix, workers were able to come up
with valid and significantly more relevant ideas for novel persuasive products.
</summary>
    <author>
      <name>Jonas Oppenlaender</name>
    </author>
    <link href="http://arxiv.org/abs/2110.04129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04890v1</id>
    <updated>2021-10-10T20:07:33Z</updated>
    <published>2021-10-10T20:07:33Z</published>
    <title>Perceptions and attitudes of Children and Young People to Artificial
  Intelligence in Medicine</title>
    <summary>  There is increasing interest in Artificial Intelligence and its application
to medicine. Perceptions are less well-known, notably amongst children and
young people. 21 members of a Young Persons Advisory Group for research,
recommend creating an enabling environment with children and young people,
through educational workshops with practical examples that use Artificial
Intelligence to help, but not replace humans, address issues, build trust, and
effectively communicate about potential opportunities.
</summary>
    <author>
      <name>Sheena Visram</name>
    </author>
    <author>
      <name>Deirdre Leyden</name>
    </author>
    <author>
      <name>Oceiah Annesley</name>
    </author>
    <author>
      <name>Dauda Bappa</name>
    </author>
    <author>
      <name>Neil J Sebire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.04890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09431v1</id>
    <updated>2021-10-18T15:59:13Z</updated>
    <published>2021-10-18T15:59:13Z</published>
    <title>Comparing Deep Neural Nets with UMAP Tour</title>
    <summary>  Neural networks should be interpretable to humans. In particular, there is a
growing interest in concepts learned in a layer and similarity between layers.
In this work, a tool, UMAP Tour, is built to visually inspect and compare
internal behavior of real-world neural network models using well-aligned,
instance-level representations. The method used in the visualization also
implies a new similarity measure between neural network layers. Using the
visual tool and the similarity measure, we find concepts learned in
state-of-the-art models and dissimilarities between them, such as GoogLeNet and
ResNet.
</summary>
    <author>
      <name>Mingwei Li</name>
    </author>
    <author>
      <name>Carlos Scheidegger</name>
    </author>
    <link href="http://arxiv.org/abs/2110.09431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10131v1</id>
    <updated>2021-10-19T17:43:34Z</updated>
    <published>2021-10-19T17:43:34Z</published>
    <title>Personal Health Knowledge Graph for Clinically Relevant Diet
  Recommendations</title>
    <summary>  We propose a knowledge model for capturing dietary preferences and personal
context to provide personalized dietary recommendations. We develop a knowledge
model called the Personal Health Ontology, which is grounded in semantic
technologies, and represents a patient's combined medical information, social
determinants of health, and observations of daily living elicited from
interviews with diabetic patients. We then generate a personal health knowledge
graph that captures temporal patterns from synthetic food logs, annotated with
concepts from the Personal Health Ontology. We further discuss how lifestyle
guidelines grounded in semantic technologies can be reasoned with the generated
personal health knowledge graph to provide appropriate dietary recommendations
that satisfy the user's medical and other lifestyle needs.
</summary>
    <author>
      <name>Oshani Seneviratne</name>
    </author>
    <author>
      <name>Jonathan Harris</name>
    </author>
    <author>
      <name>Ching-Hua Chen</name>
    </author>
    <author>
      <name>Deborah L. McGuinness</name>
    </author>
    <link href="http://arxiv.org/abs/2110.10131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11986v1</id>
    <updated>2021-10-22T18:11:44Z</updated>
    <published>2021-10-22T18:11:44Z</published>
    <title>Local, Interactive, and Actionable: a Pandemic Behavioral Nudge</title>
    <summary>  The informational environment surrounding the Covid-19 pandemic has been
widely recognized as fragmented, politicized, and complex [1]. This has
resulted in polarized public views regarding the veracity of scientific
communication, the severity of the threat posed by the virus, and the necessity
of nonpharmaceutical interventions (NPIs) which can slow the spread of
infections [2]. This paper describes CovidCommitment.org, an effort toward
enhancing NPI adoption through the combination of a social behavioral
commitment device and interactive map-based visualizations of localized
infection data as tabulated via a 1-hourdrive-time isochrone. This paper
describes the system design and presents a preliminary analysis of user
behavior within the system.
</summary>
    <author>
      <name>Alex Rich</name>
    </author>
    <author>
      <name>Cameron Yick</name>
    </author>
    <author>
      <name>David Gotz</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.15387v1</id>
    <updated>2021-10-15T20:55:33Z</updated>
    <published>2021-10-15T20:55:33Z</published>
    <title>Anticipation-driven Adaptive Architecture for Assisted Living</title>
    <summary>  Anticipatory expression underlies human performance. Medical conditions and,
especially, aging result in diminished anticipatory action. In order to
mitigate the loss, means for engaging still available resources (capabilities)
can be provided. In particular, anticipation-driven adaptive environments could
be beneficial in medical care, as well as in assisted living for those seeking
such assistance. These adaptive environments are conceived to be individualized
and individualizable, in order to stimulate independent action instead of
creating dependencies.
</summary>
    <author>
      <name>Mihai Nadin</name>
    </author>
    <author>
      <name>Asma Naz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.15387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.15387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01210v1</id>
    <updated>2021-11-01T19:05:17Z</updated>
    <published>2021-11-01T19:05:17Z</published>
    <title>Understanding the Use of Voice Assistants by Older Adults</title>
    <summary>  Older adults are using voice-based technologies in a variety of different
contexts and are uniquely positioned to benefit from smart speakers' handsfree,
voice-based interface. In order to better understand the ways in which older
adults engage with and learn how to use smart speakers, we conducted
qualitative, semi-structured interviews with four older adults who own smart
speakers. Emerging findings indicate that older adults benefit from smart
speakers as both an assistive and a social technology. Findings also suggest
that when older adults learn new technologies in a formal, communal environment
there is successful adoption.
</summary>
    <author>
      <name>Margot Hanley</name>
    </author>
    <author>
      <name>Shiri Azenkot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CSCW '18: Accessible Voice Interface Workshop, Companion of the 2018
  ACM Conference on Computer Supported Cooperative Work and Social Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.01210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.03656v1</id>
    <updated>2021-10-28T23:27:45Z</updated>
    <published>2021-10-28T23:27:45Z</published>
    <title>ironbci. Open source. Brain-computer interface with the embedded board
  to monitor the physiological subject's condition and environmental parameters</title>
    <summary>  This manuscript presented brain-computer interface (STM32 and ADS1299) with
the embedded board with sensors to monitor the subject's state and environment.
To reduce power consumption and device size, we used sensors made in
Micro-Electro-Mechanical Systems technology (MEMS) - a gyroscope,
accelerometer, and environmental monitoring sensors: CO2, temperature,
humidity, ambient sound, and pulse and blood oxygen saturation. Data from the
device is transmitted using TCP-PI (UART by Bluetooth) protocol to a computer
or mobile device. Open-source https://github.com/Ildaron/ironbci
</summary>
    <author>
      <name>Ildar Rakhmatulin</name>
    </author>
    <link href="http://arxiv.org/abs/2111.03656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.03718v2</id>
    <updated>2021-11-25T10:06:32Z</updated>
    <published>2021-11-05T20:25:11Z</published>
    <title>Quadrupedal Robotic Guide Dog with Vocal Human-Robot Interaction</title>
    <summary>  Guide dogs play a critical role in the lives of many, however training them
is a time- and labor-intensive process. We are developing a method to allow an
autonomous robot to physically guide humans using direct human-robot
communication. The proposed algorithm will be deployed on a Unitree A1
quadrupedal robot and will autonomously navigate the person to their
destination while communicating with the person using a speech interface
compatible with the robot. This speech interface utilizes cloud based services
such as Amazon Polly and Google Cloud to serve as the text-to-speech and
speech-to-text engines.
</summary>
    <author>
      <name>Kavan Mehrizi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Hopper Dean &amp; NSF REU: Transfer-to-Excellence Research Experiences
  for Undergraduates (TTE REU), University of California, Berkeley</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.03718v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03718v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.03942v1</id>
    <updated>2021-11-06T19:20:35Z</updated>
    <published>2021-11-06T19:20:35Z</published>
    <title>Extended Reality for Knowledge Work in Everyday Environments</title>
    <summary>  Virtual and Augmented Reality have the potential to change information work.
The ability to modify the workers senses can transform everyday environments
into a productive office, using portable head-mounted displays combined with
conventional interaction devices, such as keyboards and tablets. While a stream
of better, cheaper and lighter HMDs have been introduced for consumers in
recent years, there are still many challenges to be addressed to allow this
vision to become reality. This chapter summarizes the state of the art in the
field of extended reality for knowledge work in everyday environments and
proposes steps to address the open challenges.
</summary>
    <author>
      <name>Verena Biener</name>
    </author>
    <author>
      <name>Eyal Ofek</name>
    </author>
    <author>
      <name>Michel Pahud</name>
    </author>
    <author>
      <name>Per Ola Kristensson</name>
    </author>
    <author>
      <name>Jens Grubert</name>
    </author>
    <link href="http://arxiv.org/abs/2111.03942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06326v1</id>
    <updated>2021-11-11T17:34:11Z</updated>
    <published>2021-11-11T17:34:11Z</published>
    <title>Integrating psychotherapy practices and gamified elements in novel game
  mechanics for stress relief</title>
    <summary>  We explore novel game mechanics and techniques in the domain of gamified and
game-based mobile mental health applications. By combining modern game design
elements with techniques applied by practitioners (e.g., therapists) and known
mechanics used in relevant games, we developed an integrated mobile game.
Playtesting with a group of individuals showed a positive response towards the
study's claims and a promising direction for further research.
</summary>
    <author>
      <name>Styliani Zygotegou</name>
    </author>
    <author>
      <name>Georgios Anastassakis</name>
    </author>
    <author>
      <name>Georgios Tsatiris</name>
    </author>
    <author>
      <name>Kostas Karpouzis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 3rd International Conference Digital Culture &amp;
  AudioVisual Challenges, Interdisciplinary Creativity in Arts and Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.07209v2</id>
    <updated>2022-05-24T00:42:52Z</updated>
    <published>2021-11-14T00:10:24Z</published>
    <title>An Assessment of the Eye Tracking Signal Quality Captured in the
  HoloLens 2</title>
    <summary>  We present an analysis of the eye tracking signal quality of the HoloLens 2s
integrated eye tracker. Signal quality was measured from eye movement data
captured during a random saccades task from a new eye movement dataset
collected on 30 healthy adults. We characterize the eye tracking signal quality
of the device in terms of spatial accuracy, spatial precision, temporal
precision, linearity, and crosstalk. Most notably, our evaluation of spatial
accuracy reveals that the eye movement data in our dataset appears to be
uncalibrated. Recalibrating the data using a subset of our dataset task
produces notably better eye tracking signal quality.
</summary>
    <author>
      <name>Samantha D. Aziz</name>
    </author>
    <author>
      <name>Oleg V. Komogortsev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3517031.3529626</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3517031.3529626" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.07209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08501v1</id>
    <updated>2021-11-15T04:17:57Z</updated>
    <published>2021-11-15T04:17:57Z</published>
    <title>A Survey on Task Assignment in Crowdsourcing</title>
    <summary>  Quality improvement methods are essential to gathering high-quality
crowdsourced data, both for research and industry applications. A popular and
broadly applicable method is task assignment that dynamically adjusts crowd
workflow parameters. In this survey, we review task assignment methods that
address: heterogeneous task assignment, question assignment, and plurality
problems in crowdsourcing. We discuss and contrast how these methods estimate
worker performance, and highlight potential challenges in their implementation.
Finally, we discuss future research directions for task assignment methods, and
how crowdsourcing platforms and other stakeholders can benefit from them.
</summary>
    <author>
      <name>Danula Hettiachchi</name>
    </author>
    <author>
      <name>Vassilis Kostakos</name>
    </author>
    <author>
      <name>Jorge Goncalves</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3494522</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3494522" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, To appear in ACM Computing Surveys</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08830v1</id>
    <updated>2021-11-16T23:24:31Z</updated>
    <published>2021-11-16T23:24:31Z</published>
    <title>How Mock Model Training Enhances User Perceptions of AI Systems</title>
    <summary>  Artificial Intelligence (AI) is an integral part of our daily technology use
and will likely be a critical component of emerging technologies. However,
negative user preconceptions may hinder adoption of AI-based decision making.
Prior work has highlighted the potential of factors such as transparency and
explainability in improving user perceptions of AI. We further contribute to
work on improving user perceptions of AI by demonstrating that bringing the
user in the loop through mock model training can improve their perceptions of
an AI agent's capability and their comfort with the possibility of using
technology employing the AI agent.
</summary>
    <author>
      <name>Amama Mahmood</name>
    </author>
    <author>
      <name>Gopika Ajaykumar</name>
    </author>
    <author>
      <name>Chien-Ming Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Human Centered AI (HCAI) workshop at NeurIPS (2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00107v1</id>
    <updated>2021-11-30T21:16:51Z</updated>
    <published>2021-11-30T21:16:51Z</published>
    <title>LGBTQ Privacy Concerns on Social Media</title>
    <summary>  We conducted semi-structured interviews with members of the LGBTQ community
about their privacy practices and concerns on social networking sites.
Participants used different social media sites for different needs and adapted
to not being completely out on each site. We would value the opportunity to
discuss the unique privacy and security needs of this population with workshop
participants and learn more about the privacy needs of other marginalized user
groups from researchers who have worked in those communities.
</summary>
    <author>
      <name>Christine Geeng</name>
    </author>
    <author>
      <name>Alexis Hiniker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop at 2018 CHI conference on human factors in computing
  systems: Exploring Individual Differences in Privacy</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00875v1</id>
    <updated>2021-12-01T22:56:36Z</updated>
    <published>2021-12-01T22:56:36Z</published>
    <title>Secure and Safety Mobile Network System for Visually Impaired People</title>
    <summary>  The proposed system aims to be a techno-friend of visually impaired people to
assist them in orientation and mobility both indoor and outdoor. Moving through
an unknown environment becomes a real challenge for most of them, although they
rely on their other senses. An age old mechanism used for assistance for the
blind people is a white cane commonly known as walking cane a simple and purely
mechanical device to detect the ground, uneven surfaces, holes and steps using
simple Tactile-force feedback.
</summary>
    <author>
      <name>Shyama Kumari Arunachalam</name>
    </author>
    <author>
      <name>Roopa V</name>
    </author>
    <author>
      <name>Meena H B</name>
    </author>
    <author>
      <name> Vijayalakshmi</name>
    </author>
    <author>
      <name>T Malavika</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, Accepted at 2012 IEEE ICECE, Bangalore</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01146v1</id>
    <updated>2021-12-02T11:51:13Z</updated>
    <published>2021-12-02T11:51:13Z</published>
    <title>Conversational Agents in Therapeutic Interventions for
  Neurodevelopmental Disorders: A Survey</title>
    <summary>  Neurodevelopmental Disorders (NDD) are a group of conditions with onset in
the developmental period characterized by deficits in the cognitive and social
areas. Conversational agents have been increasingly explored to support
therapeutic interventions for people with NDD. This survey provides a
structured view of the crucial design features of these systems, the types of
therapeutic goals they address, and the empirical methods adopted for their
evaluation. From this analysis, we elaborate a set of recommendations and
highlight the gaps left unsolved in the state of the art, upon which we ground
a research agenda on conversational agents for NDD.
</summary>
    <author>
      <name>Fabio Catania</name>
    </author>
    <author>
      <name>Micol Spitale</name>
    </author>
    <author>
      <name>Franca Garzotto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3564269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3564269" rel="related"/>
    <link href="http://arxiv.org/abs/2112.01146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01584v1</id>
    <updated>2021-12-02T19:56:57Z</updated>
    <published>2021-12-02T19:56:57Z</published>
    <title>Wearable Affective Memory Augmentation</title>
    <summary>  Human memory prioritizes the storage and recall of information that is
emotionally-arousing and/or important in a process known as value-directed
memory. When experiencing a stream of information (e.g. conversation, book,
lecture, etc.), the individual makes conscious and subconscious value
assessments of the incoming information and uses this as a metric to determine
what to remember. In order to improve automatic recall of memory, previous
memory augmentation systems have sensed users' physiological state to determine
which sensory media should be prioritized. Here, we propose to prioritize
memories using the affective state of individuals that the user is interacting
with. Thereby, the proposed wearable Affective Memory Augmentation system uses
affective information from the user's social companions in order to facilitate
value-directed memory.
</summary>
    <author>
      <name>Cayden Pierce</name>
    </author>
    <author>
      <name>Steve Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, LaTex</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.05118v1</id>
    <updated>2021-12-09T18:53:18Z</updated>
    <published>2021-12-09T18:53:18Z</published>
    <title>Web Platform for Visualisation of Kinematic Data captured from a Motor
  Tele-rehabilitation System</title>
    <summary>  Stroke can have a severe impact on an individual's quality of life, leading
to consequences such as motor loss and communication problems, especially among
the elderly. Studies have shown that early and easy access to stroke
rehabilitation can improve an elderly individual's quality of life, and that
telerehabilitation is a solution that facilitates this improvement. In this
work, we visualize movement to music during rehabilitation exercises captured
by the Kinect motion sensor, using a dedicated Serious Game called `Move to the
Music'(MoMu). Our system provides a quantitative view of progress made by
patients during a motor rehabilitation regime for healthcare professionals to
track remotely (tele-rehab).
</summary>
    <author>
      <name>Praveena Satkunarajah</name>
    </author>
    <author>
      <name>Kat Agres</name>
    </author>
    <link href="http://arxiv.org/abs/2112.05118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11232v1</id>
    <updated>2021-12-20T14:17:19Z</updated>
    <published>2021-12-20T14:17:19Z</published>
    <title>Human Activity Recognition (HAR) in Smart Homes</title>
    <summary>  Generally, Human Activity Recognition (HAR) consists of monitoring and
analyzing the behavior of one or more persons in order to deduce their
activity. In a smart home context, the HAR consists of monitoring daily
activities of the residents. Thanks to this monitoring, a smart home can offer
home assistance services to improve quality of life, autonomy and health of
their residents, especially for elderly and dependent people.
</summary>
    <author>
      <name>Damien Bouchabou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT Atlantique - INFO, Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Christophe Lohr</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT Atlantique - INFO, Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Ioannis Kanellos</name>
    </author>
    <author>
      <name>Sao Mai Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2111.04418</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01375v1</id>
    <updated>2022-02-23T12:51:19Z</updated>
    <published>2022-02-23T12:51:19Z</published>
    <title>From Digital Media to Empathic Reality: A Systematic Review of Empathy
  Research in Extended Reality Environments</title>
    <summary>  Recent advances in extended reality (XR) technologies have enabled new and
increasingly realistic empathy tools and experiences. In XR, all interactions
take place in different spatial contexts, all with different features,
affordances, and constraints. We present a systematic literature survey of
recent work on empathy in XR. As a result, we contribute a research roadmap
with three future opportunities in XR-enabled empathy research across both
physical and virtual spaces.
</summary>
    <author>
      <name>Ville Paananen</name>
    </author>
    <author>
      <name>Mohammad Sina Kiarostami</name>
    </author>
    <author>
      <name>Lik-Hang Lee</name>
    </author>
    <author>
      <name>Tristan Braud</name>
    </author>
    <author>
      <name>Simo Hosio</name>
    </author>
    <link href="http://arxiv.org/abs/2203.01375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02241v1</id>
    <updated>2022-03-04T11:17:17Z</updated>
    <published>2022-03-04T11:17:17Z</published>
    <title>Undoing Seamlessness: Exploring Seams for Critical Visualization</title>
    <summary>  While seamful design has been part of discourses and work within HCI contexts
for some time, it has not yet been fully explored in data visualization design.
At the same time, critics of visualization have been arguing that the
representation of data as contextual, contingent, relational, partial,
heterogeneous, and situated is currently lacking in visualization. Seamful
visualization promises a fresh perspective on visualization design as we seek
to find more expressive encodings and novel approaches to representing data
that acknowledge their wider qualities and limitations. By consulting seams in
other realms and exploring existing seams and seamfulness in visualization,
this paper offers a foundation for conceptualizing seamful visualization,
points towards the value of seams and seamfulness in critical visualization,
and proposes principles for engaging with seamful visualization in practice and
research.
</summary>
    <author>
      <name>Nicole Hengesbach</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3491101.3519703</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3491101.3519703" rel="related"/>
    <link href="http://arxiv.org/abs/2203.02241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.09577v1</id>
    <updated>2022-03-17T19:35:42Z</updated>
    <published>2022-03-17T19:35:42Z</published>
    <title>MolecuSense: Using Force-Feedback Gloves for Creating and Interacting
  with Ball-and-Stick Molecules in VR</title>
    <summary>  We contribute MolecuSense, a virtual version of a physical molecule
construction kit, based on visualization in Virtual Reality (VR) and
interaction with force-feedback gloves. Targeting at chemistry education, our
goal is to make virtual molecule structures more tangible. Results of an
initial user study indicate that the VR molecular construction kit was
positively received. Compared to a physical construction kit, the VR molecular
construction kit is on the same level in terms of natural interaction. Besides,
it fosters the typical digital advantages though, such as saving, exporting,
and sharing of molecules. Feedback from the study participants has also
revealed potential future avenues for tangible molecule visualizations.
</summary>
    <author>
      <name>Patrick Gebhardt</name>
    </author>
    <author>
      <name>Xingyao Yu</name>
    </author>
    <author>
      <name>Andreas Köhn</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3554944.3554956</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3554944.3554956" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.09577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.09577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.10264v1</id>
    <updated>2022-03-19T07:47:45Z</updated>
    <published>2022-03-19T07:47:45Z</published>
    <title>Assessing Gender Bias in Predictive Algorithms using eXplainable AI</title>
    <summary>  Predictive algorithms have a powerful potential to offer benefits in areas as
varied as medicine or education. However, these algorithms and the data they
use are built by humans, consequently, they can inherit the bias and prejudices
present in humans. The outcomes can systematically repeat errors that create
unfair results, which can even lead to situations of discrimination (e.g.
gender, social or racial). In order to illustrate how important is to count
with a diverse training dataset to avoid bias, we manipulate a well-known
facial expression recognition dataset to explore gender bias and discuss its
implications.
</summary>
    <author>
      <name>Cristina Manresa-Yee</name>
    </author>
    <author>
      <name>Silvia Ramis</name>
    </author>
    <link href="http://arxiv.org/abs/2203.10264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12262v1</id>
    <updated>2022-03-23T08:17:25Z</updated>
    <published>2022-03-23T08:17:25Z</published>
    <title>Multi-Mosaics: Corpus Summarizing and Exploration using multiple
  Concordance Mosaic Visualisations</title>
    <summary>  Researchers working in areas such as lexicography, translation studies, and
computational linguistics, use a combination of automated and semi-automated
tools to analyze the content of text corpora. Keywords, named entities, and
events are often extracted automatically as the first step in the analysis.
Concordancing -- or the arranging of passages of a textual corpus in
alphabetical order according to user-defined keywords -- is one of the oldest
and still most widely used forms of text analysis. This paper describes
Multi-Mosaics, a tool for corpus analysis using multiple implicitly linked
Concordance Mosaic visualisations. Multi-Mosaics supports examining linguistic
relationships within the context windows surrounding extracted keywords.
</summary>
    <author>
      <name>Shane Sheehan</name>
    </author>
    <author>
      <name>Saturnino Luz</name>
    </author>
    <author>
      <name>Masood Masoodian</name>
    </author>
    <link href="http://arxiv.org/abs/2203.12262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13320v1</id>
    <updated>2022-03-24T19:59:53Z</updated>
    <published>2022-03-24T19:59:53Z</published>
    <title>Data-Driven Visual Reflection on Music Instrument Practice</title>
    <summary>  We propose a data-driven approach to music instrument practice that allows
studying patterns and long-term trends through visualization. Inspired by life
logging and fitness tracking, we imagine musicians to record their practice
sessions over the span of months or years. The resulting data in the form of
MIDI or audio recordings can then be analyzed sporadically to track progress
and guide decisions. Toward this vision, we started exploring various
visualization designs together with a group of nine guitarists, who provided us
with data and feedback over the course of three months.
</summary>
    <author>
      <name>Frank Heyen</name>
    </author>
    <author>
      <name>Quynh Quang Ngo</name>
    </author>
    <author>
      <name>Kuno Kurzhals</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2022 IMI Workshop https://teamdarmstadt.de/imi/</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.14372v1</id>
    <updated>2022-03-27T19:13:29Z</updated>
    <published>2022-03-27T19:13:29Z</published>
    <title>Algorithmic support of a personal virtual assistant for automating the
  processing of client requests</title>
    <summary>  This article describes creating algorithmic support for the functioning of a
personal virtual assistant, which allows automating the processing of customer
requests. The study aims to reduce errors and processing time for a client
request in business systems - text chats or voice channels using a text
transcription system. The results of the development of algorithmic support and
an assessment of the quality of work on synthetic data presented.
</summary>
    <author>
      <name>Konstantin Dobratulin</name>
    </author>
    <author>
      <name>Marina Nezhurina</name>
    </author>
    <link href="http://arxiv.org/abs/2203.14372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.14372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15834v1</id>
    <updated>2022-03-10T13:58:34Z</updated>
    <published>2022-03-10T13:58:34Z</published>
    <title>Collaborative Learning and Patterns of Practice</title>
    <summary>  In this article, an overview of the background, the research approaches and
the patterns of practice in the field of collaborative learning are provided. A
definition of collaborative learning and an overview of fundamental aspects
that shape research and practice in this field are included. Pedagogies and
learning theories that are used as foundations of the field alongside goals and
objectives of collaborative learning approaches are discussed. Popular patterns
of practice, exploring their application in classrooms and elaborating on the
state of the art around those practices in research are outlined. A discussion
about important topics, open questions and future directions are provided in
conclusion.
</summary>
    <author>
      <name>Irene-Angelia Chounta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-10576-1_83</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-10576-1_83" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, entry for the Encyclopedia of Education and Information
  Technologies, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.15834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.16882v1</id>
    <updated>2022-03-31T08:22:06Z</updated>
    <published>2022-03-31T08:22:06Z</published>
    <title>A Contextual Framework for Adaptive User Interfaces: Modelling the
  Interaction Environment</title>
    <summary>  The interaction context (or environment) is key to any HCI task and
especially to adaptive user interfaces (AUIs), since it represents the
conditions under which users interact with computers. Unfortunately, there are
currently no formal representations to model said interaction context. In order
to address this gap, we propose a contextual framework for AUIs and illustrate
a practical applica- tion using learning management systems as a case study. We
also discuss limitations of our framework and offer discussion points about the
realisation of truly context-aware AUIs.
</summary>
    <author>
      <name>Mateusz Dubiel</name>
    </author>
    <author>
      <name>Bereket Abera Yilma</name>
    </author>
    <author>
      <name>Kayhan Latifzadeh</name>
    </author>
    <author>
      <name>Luis A. Leiva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.16882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.16882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03731v1</id>
    <updated>2022-04-07T20:50:12Z</updated>
    <published>2022-04-07T20:50:12Z</published>
    <title>GreaseVision: Rewriting the Rules of the Interface</title>
    <summary>  Digital harms can manifest across any interface. Key problems in addressing
these harms include the high individuality of harms and the fast-changing
nature of digital systems. As a result, we still lack a systematic approach to
study harms and produce interventions for end-users. We put forward
GreaseVision, a new framework that enables end-users to collaboratively develop
interventions against harms in software using a no-code approach and recent
advances in few-shot machine learning. The contribution of the framework and
tool allow individual end-users to study their usage history and create
personalized interventions. Our contribution also enables researchers to study
the distribution of harms and interventions at scale.
</summary>
    <author>
      <name>Siddhartha Datta</name>
    </author>
    <author>
      <name>Konrad Kollnig</name>
    </author>
    <author>
      <name>Nigel Shadbolt</name>
    </author>
    <link href="http://arxiv.org/abs/2204.03731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06119v2</id>
    <updated>2022-07-11T17:45:38Z</updated>
    <published>2022-04-13T00:02:08Z</published>
    <title>Do you know "saudade"? The importance of a cultural and language-based
  emotion approach for HCI</title>
    <summary>  Today most technologies and interfaces are designed to be global. We argue
that if emotional aspects are incorporated during the design phase of
technologies and interfaces - or in technologies for recognizing users'
emotions such as affective computing - culture and language should be taken as
relevant criteria.
</summary>
    <author>
      <name>Diogo Cortiz</name>
    </author>
    <author>
      <name>Paulo Boggio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position Paper accepted to The Future of Emotion in Human-Computer
  Interaction Workshop at ACM CHI 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06821v1</id>
    <updated>2022-04-14T08:45:06Z</updated>
    <published>2022-04-14T08:45:06Z</published>
    <title>Justice in interaction design: preventing manipulation in interfaces</title>
    <summary>  Designers incorporate values in the design process that raise risks for
vulnerable groups. Persuasion in user interfaces can quickly turn into
manipulation and become potentially harmful for those groups in the realm of
intellectual disabilities, class, or health, requiring proactive responsibility
approaches in design. Here we introduce the Capability Sensitive Design
Approach and explain how it can be used proactively to inform designers'
decisions when it comes to evaluating justice in their designs preventing the
risk of manipulation.
</summary>
    <author>
      <name>Lorena Sanchez Chamorro</name>
    </author>
    <author>
      <name>Kerstin Bongard-Blanchy</name>
    </author>
    <author>
      <name>Vincent Koenig</name>
    </author>
    <link href="http://arxiv.org/abs/2204.06821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07900v1</id>
    <updated>2022-04-17T01:45:02Z</updated>
    <published>2022-04-17T01:45:02Z</published>
    <title>Using HCI to Tackle Race and Gender Bias in ADHD Diagnosis</title>
    <summary>  Attention Deficit Hyperactivity Disorder (ADHD) is a behavioral disorder that
impacts an individual's education, relationships, career, and ability to
acquire fair and just police interrogations. Yet, traditional methods used to
diagnose ADHD in children and adults are known to have racial and gender bias.
In recent years, diagnostic technology has been studied by both HCI and ML
researchers. However, these studies fail to take into consideration racial and
gender stereotypes that may impact the accuracy of their results. We highlight
the importance of taking race and gender into consideration when creating
diagnostic technology for ADHD and provide HCI researchers with suggestions for
future studies.
</summary>
    <author>
      <name>Naba Rizvi</name>
    </author>
    <author>
      <name>Khalil Mrini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, CHI 2020 workshop submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.07900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.08156v1</id>
    <updated>2022-04-18T04:04:21Z</updated>
    <published>2022-04-18T04:04:21Z</published>
    <title>Interaction Design of Dwell Selection Toward Gaze-based AR/VR
  Interaction</title>
    <summary>  In this paper, we first position the current dwell selection among gaze-based
interactions and its advantages against head-gaze selection, which is the
mainstream interface for HMDs. Next, we show how dwell selection and head-gaze
selection are used in an actual interaction situation. By comparing these two
selection methods, we describe the potential of dwell selection as an essential
AR/VR interaction.
</summary>
    <author>
      <name>Toshiya Isomoto</name>
    </author>
    <author>
      <name>Shota Yamanaka</name>
    </author>
    <author>
      <name>Buntarou Shizuki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3517031.3531628</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3517031.3531628" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2022 ACM Symposium on Eye Tracking Research &amp;
  Applications (ETRA 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.08156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.08156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09745v2</id>
    <updated>2022-05-04T21:44:03Z</updated>
    <published>2022-04-20T19:00:27Z</published>
    <title>ColorCode: A Bayesian Approach to Augmentative and Alternative
  Communication with Two Buttons</title>
    <summary>  Many people with severely limited muscle control can only communicate through
augmentative and alternative communication (AAC) systems with a small number of
buttons. In this paper, we present the design for ColorCode, which is an AAC
system with two buttons that uses Bayesian inference to determine what the user
wishes to communicate. Our information-theoretic analysis of ColorCode
simulations shows that it is efficient in extracting information from the user,
even in the presence of errors, achieving nearly optimal error correction.
ColorCode is provided as open source software
(https://github.com/mrdaly/ColorCode).
</summary>
    <author>
      <name>Matthew Daly</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/2022.slpat-1.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/2022.slpat-1.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, SLPAT Workshop - ACL 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09745v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09745v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10854v1</id>
    <updated>2022-04-14T16:18:14Z</updated>
    <published>2022-04-14T16:18:14Z</published>
    <title>Delivering data differently</title>
    <summary>  Human-computer interaction relies on mouse/touchpad, keyboard, and screen,
but tools have recently been developed that engage sound, smell, touch,
muscular resistance, voice dialogue, balance, and multiple senses at once. How
might these improvements impact upon the practice of statistics and data
science? People with low vision may be better able to grasp and explore data.
More generally, methods developed to enable this have the potential to allow
sighted people to use more senses and become better analysts. We would like to
adapt some of the wide range of available computer and sensory input/output
technologies to transform data science workflows. Here is a vision of what this
synthesis might accomplish.
</summary>
    <author>
      <name>Gwynn Sturdevant</name>
    </author>
    <author>
      <name>A. Jonathan R. Godfrey</name>
    </author>
    <author>
      <name>Andrew Gelman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10949v1</id>
    <updated>2022-04-22T22:30:29Z</updated>
    <published>2022-04-22T22:30:29Z</published>
    <title>The Dark Souls of Archaeology: Recording Elden Ring</title>
    <summary>  Archaeology can be broadly defined as the study and interpretation of the
past through material remains. Videogame worlds, though immaterial in nature,
can also afford opportunities to study the people who existed within them based
on what they leave behind. In this paper we present the first formal
archaeological survey of a predominantly single-player game, by examining the
player-generated content that is asynchronously distributed to players in the
videogame Elden Ring.
</summary>
    <author>
      <name>Florence Smith Nicholls</name>
    </author>
    <author>
      <name>Michael Cook</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.11367v1</id>
    <updated>2022-04-24T21:59:47Z</updated>
    <published>2022-04-24T21:59:47Z</published>
    <title>Ordered-logit pedestrian stress model for traffic flow with automated
  vehicles</title>
    <summary>  An ordered-logit model is developed to study the effects of Automated
Vehicles (AVs) in the traffic mix on the average stress level of a pedestrian
when crossing an urban street at mid-block. Information collected from a
galvanic skin resistance sensor and virtual reality experiments are transformed
into a dataset with interpretable average stress levels (low, medium, and high)
and geometric, traffic, and environmental conditions. Modelling results
indicate a decrease in average stress level with the increase in the percentage
of AVs in the traffic mix.
</summary>
    <author>
      <name>Kimia Kamal</name>
    </author>
    <author>
      <name>Bilal Farooq</name>
    </author>
    <author>
      <name>Mahwish Mudassar</name>
    </author>
    <author>
      <name>Arash Kalatian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IV51971.2022.9827316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IV51971.2022.9827316" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In: IEEE Intelligent Vehicles Symposium Workshops (XXIV Workshops),
  2022, Aachen, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.11367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13546v1</id>
    <updated>2022-04-28T14:55:50Z</updated>
    <published>2022-04-28T14:55:50Z</published>
    <title>DMINR: A Tool to Support Journalists Information Verification and
  Exploration</title>
    <summary>  Journalists are key information workers who have specific requirements from
information systems to support the verification and exploration of information.
We overview the DMINR tool that has been designed and developed to meet the
needs of journalists through the examination of journalists information
behaviour in a newsroom. We outline our co-design process as well as the
design, implementation and deployment of the tool. We report a usability test
on the tool and conclude with details of how to develop the tool further
</summary>
    <author>
      <name>Andrew MacFarlane</name>
    </author>
    <author>
      <name>Marisela Gutierrez-Lopez</name>
    </author>
    <author>
      <name>Stephann Makri</name>
    </author>
    <author>
      <name>Tim Atwell</name>
    </author>
    <author>
      <name>Sondess Missaoui</name>
    </author>
    <author>
      <name>Colin Porlezza</name>
    </author>
    <author>
      <name>Glenda Cooper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0165v1</id>
    <updated>2008-03-03T01:34:30Z</updated>
    <published>2008-03-03T01:34:30Z</published>
    <title>Documenting Spreadsheets</title>
    <summary>  This paper discusses spreadsheets documentation and new means to achieve this
end by using Excel's built-in "Comment" function. By structuring comments, they
can be used as an essential tool to fully explain spreadsheet. This will
greatly facilitate spreadsheet change control, risk management and auditing. It
will fill a crucial gap in corporate governance by adding essential information
that can be managed in order to satisfy internal controls and accountability
standards.
</summary>
    <author>
      <name>Raymond Payette</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 15 screen shots</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 163-173
  ISBN:1-905617-08-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.2.5; H.4.1; K.6.4; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1862v1</id>
    <updated>2008-03-12T22:09:59Z</updated>
    <published>2008-03-12T22:09:59Z</published>
    <title>Exploring Human Factors in Spreadsheet Development</title>
    <summary>  In this paper we consider human factors and their impact on spreadsheet
development in strategic decision-making. This paper brings forward research
from many disciplines both directly related to spreadsheets and a broader
spectrum from psychology to industrial processing. We investigate how human
factors affect a simplified development cycle and what the potential
consequences are.
</summary>
    <author>
      <name>Simon Thorne</name>
    </author>
    <author>
      <name>David Ball</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2005 161-172
  ISBN:1-902724-16-X</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.1862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.2.5; H.4.1; K.6.4; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3186v1</id>
    <updated>2008-03-21T15:38:25Z</updated>
    <published>2008-03-21T15:38:25Z</published>
    <title>Towards a human eye behavior model by applying Data Mining Techniques on
  Gaze Information from IEC</title>
    <summary>  In this paper, we firstly present what is Interactive Evolutionary
Computation (IEC) and rapidly how we have combined this artificial intelligence
technique with an eye-tracker for visual optimization. Next, in order to
correctly parameterize our application, we present results from applying data
mining techniques on gaze information coming from experiments conducted on
about 80 human individuals.
</summary>
    <author>
      <name>Denis Pallez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Brisson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Thierry Baccino</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPEQ</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Proceedings of the Third International Conference on Human
  Centered Processes - Human Centered Processes, Delft : Pays-Bas (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.3186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1293v1</id>
    <updated>2011-05-06T15:04:27Z</updated>
    <published>2011-05-06T15:04:27Z</published>
    <title>Eigengestures for natural human computer interface</title>
    <summary>  We present the application of Principal Component Analysis for data acquired
during the design of a natural gesture interface. We investigate the concept of
an eigengesture for motion capture hand gesture data and present the
visualisation of principal components obtained in the course of conducted
experiments. We also show the influence of dimensionality reduction on
reconstructed gesture data quality.
</summary>
    <author>
      <name>Piotr Gawron</name>
    </author>
    <author>
      <name>Przemysław Głomb</name>
    </author>
    <author>
      <name>Jarosław Adam Miszczak</name>
    </author>
    <author>
      <name>Zbigniew Puchała</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-23169-8_6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-23169-8_6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Intelligent and Soft Computing, 2011, Volume 103/2011,
  49-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.1293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2890v1</id>
    <updated>2011-05-14T11:43:51Z</updated>
    <published>2011-05-14T11:43:51Z</published>
    <title>Improving Usability of Interactive Graphics Specification and
  Implementation with Picking Views and Inverse Transformations</title>
    <summary>  Specifying and programming graphical interactions are difficult tasks,
notably because designers have difficulties to express the dynamics of the
interaction. This paper shows how the MDPC architecture improves the usability
of the specification and the implementation of graphical interaction. The
architecture is based on the use of picking views and inverse transforms from
the graphics to the data. With three examples of graphical interaction, we show
how to express them with the architecture, how to implement them, and how this
improves programming usability. Moreover, we show that it enables implementing
graphical interaction without a scene graph. This kind of code prevents from
errors due to cache consistency management.
</summary>
    <author>
      <name>Stéphane Conversy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1105.2890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.1742v1</id>
    <updated>2011-12-08T00:45:06Z</updated>
    <published>2011-12-08T00:45:06Z</published>
    <title>HandsInAir: A Wearable System for Remote Collaboration</title>
    <summary>  We present HandsInAir, a real-time collaborative wearable system for remote
collaboration. The system is developed to support real world scenarios in which
a remote mobile helper guides a local mobile worker performing a physical task.
HandsInAir implements a novel approach to support mobility of remote
collaborators. This approach allows the helper to perform gestures without
having to touch tangible objects, requiring little environment support. The
system consists of two parts: the helper part and the worker part. The two
parts are connected via a wireless network and the collaboration partners
communicate with each other via audio and visual links. In this paper, we
review related work, describe technical implementation of the system and
envision future work for further improvements.
</summary>
    <author>
      <name>Weidong Huang</name>
    </author>
    <author>
      <name>Leila Alem</name>
    </author>
    <author>
      <name>Jalal Albasri</name>
    </author>
    <link href="http://arxiv.org/abs/1112.1742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.1742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.4190v1</id>
    <updated>2011-12-18T21:17:38Z</updated>
    <published>2011-12-18T21:17:38Z</published>
    <title>An Empirical Study on End-users Productivity Using Model-based
  Spreadsheets</title>
    <summary>  Spreadsheets are widely used, and studies have shown that most end-user
spreadsheets contain nontrivial errors. To improve end-users productivity,
recent research proposes the use of a model-driven engineering approach to
spreadsheets. In this paper we conduct the first systematic empirical study to
assess the effectiveness and efficiency of this approach. A set of spreadsheet
end users worked with two different model-based spreadsheets, and we present
and analyze here the results achieved.
</summary>
    <author>
      <name>Laura Beckwith</name>
    </author>
    <author>
      <name>Jácome Cunha</name>
    </author>
    <author>
      <name>João Paulo Fernandes</name>
    </author>
    <author>
      <name>João Saraiva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, 9 Colour Figures, 2 Tables; Proc. European Spreadsheet
  Risks Int. Grp. (EuSpRIG) 2011, ISBN 978-0-9566256-9-4</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.4190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.4190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.2959v1</id>
    <updated>2012-10-10T15:42:23Z</updated>
    <published>2012-10-10T15:42:23Z</published>
    <title>Psychophysical Responses Comparison in Spatial Visual, Audiovisual, and
  Auditory BCI-Spelling Paradigms</title>
    <summary>  The paper presents a pilot study conducted with spatial visual, audiovisual
and auditory brain-computer-interface (BCI) based speller paradigms. The
psychophysical experiments are conducted with healthy subjects in order to
evaluate a difficulty and a possible response accuracy variability. We also
present preliminary EEG results in offline BCI mode. The obtained results
validate a thesis, that spatial auditory only paradigm performs as good as the
traditional visual and audiovisual speller BCI tasks.
</summary>
    <author>
      <name>Moonjeong Chang</name>
    </author>
    <author>
      <name>Nozomu Nishikawa</name>
    </author>
    <author>
      <name>Zhenyu Cai</name>
    </author>
    <author>
      <name>Shoji Makino</name>
    </author>
    <author>
      <name>Tomasz M. Rutkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 6th International Conference on Soft Computing and Intelligent
  Systems and The 13th International Symposium on Advanced Intelligent Systems,
  2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.2959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.2959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1746v1</id>
    <updated>2013-06-06T16:07:23Z</updated>
    <published>2013-06-06T16:07:23Z</published>
    <title>Condition Driven Adaptive Music Generation for Computer Games</title>
    <summary>  The video game industry has grown to a multi-billion dollar, worldwide
industry. The background music tends adaptively in reference to the specific
game content during the game length of the play. Adaptive music should be
further explored by looking at the particular condition in the game; such
condition is driven by generating a specific music in the background which best
fits in with the active game content throughout the length of the gameplay.
This research paper outlines the use of condition driven adaptive music
generation for audio and video to dynamically incorporate adaptively.
</summary>
    <author>
      <name>Alamgir Naushad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/10652-5416</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/10652-5416" rel="related"/>
    <link href="http://arxiv.org/abs/1306.1746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03400v2</id>
    <updated>2015-08-30T13:33:54Z</updated>
    <published>2015-03-11T16:14:16Z</published>
    <title>Get 'em Moles! : Learning Spelling and Pronunciation through an
  Educational Game</title>
    <summary>  Get 'em Moles! is a single-player educational game inspired by the classic
arcade game Whac-A-Mole. Primarily designed for touchscreen devices, Get 'em
Moles! aims to teach English spelling and pronunciation through engaging game
play. This paper describes the game, design decisions in the form of elements
that support learning, preliminary play-testing results, and future work.
</summary>
    <author>
      <name>Dhruv Chand</name>
    </author>
    <author>
      <name>Karthik Gopalakrishnan</name>
    </author>
    <author>
      <name>Nisha KK</name>
    </author>
    <author>
      <name>Mudit Sinha</name>
    </author>
    <author>
      <name>Shreya Sriram</name>
    </author>
    <link href="http://arxiv.org/abs/1503.03400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04375v2</id>
    <updated>2015-03-17T01:05:42Z</updated>
    <published>2015-03-15T02:48:08Z</published>
    <title>Optimization of Switch Keyboards</title>
    <summary>  Patients with motor control difficulties often "type" on a computer using a
switch keyboard to guide a scanning cursor to text elements. We show how to
optimize some parts of the design of switch keyboards by casting the design
problem as mixed integer programming. A new algorithm to find an optimized
design solution is approximately 3600 times faster than a previous algorithm,
which was also susceptible to finding a non-optimal solution. The optimization
requires a model of the probability of an entry error, and we show how to build
such a model from experimental data. Example optimized keyboards are
demonstrated.
</summary>
    <author>
      <name>Xiao Zhang</name>
    </author>
    <author>
      <name>Kan Fang</name>
    </author>
    <author>
      <name>Gregory Francis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2513383.2513394</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2513383.2513394" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 15th International ACM SIGACCESS Conference on
  Computers and Accessibility 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04375v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04375v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08866v1</id>
    <updated>2015-03-30T22:35:03Z</updated>
    <published>2015-03-30T22:35:03Z</published>
    <title>Towards Data-Driven Hierarchical Surgical Skill Analysis</title>
    <summary>  This paper evaluates methods of hierarchical skill analysis developed in
aerospace to the problem of surgical skill assessment and modeling. The
analysis employs tool motion data of Fundamental of Laparoscopic Skills (FLS)
tasks collected from clinicians of various skill levels at three different
clinical teaching hospitals in the United States. Outcomes are evaluated based
on their ability to provide relevant information about the underlying processes
across the entire system hierarchy including control, guidance and planning.
</summary>
    <author>
      <name>Bin Li</name>
    </author>
    <author>
      <name>Berenice Mettler</name>
    </author>
    <author>
      <name>Timonthy M. Kowalewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">M2CAI 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.08866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00714v1</id>
    <updated>2017-10-31T18:24:33Z</updated>
    <published>2017-10-31T18:24:33Z</published>
    <title>Doris: A tool for interactive exploration of historic corpora (Extended
  Version)</title>
    <summary>  Insights into social phenomenon can be gleaned from trends and patterns in
corpora of documents associated with that phenomenon. Recent years have
witnessed the use of computational techniques, mostly based on keywords, to
analyze large corpora for these purposes. In this paper, we extend these
techniques to incorporate semantic features. We introduce Doris, an interactive
exploration tool that combines semantic features with information retrieval
techniques to enable exploration of document corpora corresponding to the
social phenomenon. We discuss the semantic techniques and describe an
implementation on a corpus of United States (US) presidential speeches. We
illustrate, with examples, how the ability to combine syntactic and semantic
features in a visualization helps researchers more easily gain insights into
the underlying phenomenon.
</summary>
    <author>
      <name>Sreya Guha</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04513v1</id>
    <updated>2017-11-13T10:51:28Z</updated>
    <published>2017-11-13T10:51:28Z</published>
    <title>COMBINE: a novel drug discovery platform designed to capture insight and
  experience of users</title>
    <summary>  The insight and experience gained by a researcher are often lost because the
current productive and analytics software are inherently data-centric,
disconnected, and scattered. The connected nature of insight and experience can
be captured if the applications themselves are connected. How connected
applications concept is implemented in COnstruct cheMical and BIological
NEtwork (COMBINE), a novel user-centric drug discovery platform, is described.
Using publicly available data, how COMBINE users capture insight and experience
is explained, and how COMBINE users perform data organization, data sharing,
data analysis, and data visualization is illustrated.
</summary>
    <author>
      <name>Sung Jin Cho</name>
    </author>
    <link href="http://arxiv.org/abs/1711.04513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06134v1</id>
    <updated>2017-11-14T01:34:12Z</updated>
    <published>2017-11-14T01:34:12Z</published>
    <title>"Making you happy makes me happy" -- Measuring Individual Mood with
  Smartwatches</title>
    <summary>  We introduce a system to measure individual happiness based on interpreting
body sensors on smartwatches. In our prototype system we use a Pebble
smartwatch to track activity, heartrate, light level, and GPS coordinates, and
extend it with external information such as weather data, humidity, and day of
the week. Training our machine learning-based mood prediction system using
random forests with data manually entered into the smartwatch, we achieve
prediction accuracy of up to 94%. We find that besides body signals, the
weather data exerts a strong influence on mood. In addition our system also
allows us to identify friends who are indicators of our positive or negative
mood.
</summary>
    <author>
      <name>Pascal Budner</name>
    </author>
    <author>
      <name>Joscha Eirich</name>
    </author>
    <author>
      <name>Peter A. Gloor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06553v1</id>
    <updated>2017-11-17T14:33:59Z</updated>
    <published>2017-11-17T14:33:59Z</published>
    <title>Understanding Graph and Understanding Map and their Potential
  Applications</title>
    <summary>  Based on the previously proposed concept Understanding Tree, this paper
introduces two concepts: Understanding Graph and Understanding Map, and
explores their potential applications. Understanding Graph and Understanding
Map can be deemed as special cases of mind map, semantic network, or concept
map. The two main differences are: Firstly, the data sources for constructing
Understanding Map and Understanding Graph are distinctive and simple. Secondly,
the relations between concepts in Understanding Graph and Understanding Map are
monotonous. Based on their characteristics, applications of them include
quantitatively measuring a concept's complexity degree, quantitatively
measuring a concept's importance degree in a domain, and computing an optimized
learning sequence for comprehending a concept etc. Further study involves
evaluating their performances in these applications.
</summary>
    <author>
      <name>Gangli Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1711.06553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07574v1</id>
    <updated>2017-11-20T23:11:17Z</updated>
    <published>2017-11-20T23:11:17Z</published>
    <title>Data Capture &amp; Analysis to Assess Impact of Carbon Credit Schemes</title>
    <summary>  Data enables Non-Governmental Organisations (NGOs) to quantify the impact of
their initiatives to themselves and to others. The increasing amount of data
stored today can be seen as a direct consequence of the falling costs in
obtaining it. Cheap data acquisition harnesses existing communications networks
to collect information. Globally, more people are connected by the mobile phone
network than by the Internet. We worked with Vita, a development organisation
implementing green initiatives to develop an SMS-based data collection
application to collect social data surrounding the impacts of their
initiatives. We present our system design and lessons learned from
on-the-ground testing.
</summary>
    <author>
      <name>Matilda Rhode</name>
    </author>
    <author>
      <name>Omer Rana</name>
    </author>
    <author>
      <name>Tim Edwards</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11074v1</id>
    <updated>2017-11-29T19:36:10Z</updated>
    <published>2017-11-29T19:36:10Z</published>
    <title>Towards Cross-Surface Immersion Using Low Cost Multi-Sensory Output Cues
  to Support Proxemics and Kinesics Across Heterogeneous Systems</title>
    <summary>  Collaboration in immersive systems can be achieved by using an immersive
display system (i.e. CAVE and Head-Mounted Display), but how do we communicate
immersion cross-surface for low immersive displays, such as desktops, tablets,
and smartphones? In this paper, we present a discussion of proxemics and
kinesics to support based on observation of physical collaboration. We present
our research agenda to investigate low-cost multi-sensory output cues to
communicate proxemics and kinesics aspects cross-surface. Doing so may increase
the level of presence, co-presence, and immersion, and improve the
effectiveness of collaboration cross-surface.
</summary>
    <author>
      <name>Rajiv Khadka</name>
    </author>
    <author>
      <name>James Money</name>
    </author>
    <author>
      <name>Amy Banic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, ISS Cross-Surface: Challenges and Opportunities
  of Spatial and Proxemic Interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11319v2</id>
    <updated>2019-07-23T20:49:58Z</updated>
    <published>2017-11-30T11:08:34Z</published>
    <title>Creative Autonomy Through Salience and Multidominance in Interactive
  Music Systems: Evaluating an Implementation</title>
    <summary>  Interactive music systems always exhibit some autonomy in the creative
process. The capacity to generate novel material while retaining mutuality to
the interaction is proposed here as the bare minimum for creative autonomy in
such systems. Video Interactive VST Orchestra is a system incorporating an
adaptive technique based both on the concept of salience as a means for
retaining mutuality to the interplay and on multidominance in the adaptive
generation process as a means for introducing novelty. We call this property
reflexive multidominance. A case study providing evidence of such creative
autonomy in VIVO is presented.
</summary>
    <author>
      <name>Fabio Paolizzo</name>
    </author>
    <author>
      <name>Colin G. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures, 2 tables, 2 supplement material (audio/video
  links)</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11319v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11319v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02711v1</id>
    <updated>2018-05-07T19:30:44Z</updated>
    <published>2018-05-07T19:30:44Z</published>
    <title>Evaluation of Visualization by Demonstration and Manual View
  Specification</title>
    <summary>  We present an exploratory study comparing the visualization construction and
data exploration processes of people using two visualization tools, each
implementing a different interaction paradigm. One of the visualization tools
implements the manual view specification paradigm (Polestar) and another
implements the visualization by demonstration paradigm (VisExemplar). Findings
of our study indicate that the interaction paradigms implemented in these tools
influence: 1) approaches used for constructing visualizations, 2) how users
form goals, 3) how many visualization alternatives are considered and created,
and 4) the feeling of control during the visualization construction process.
</summary>
    <author>
      <name>Bahador Saket</name>
    </author>
    <author>
      <name>Alex Endert</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07064v2</id>
    <updated>2018-05-22T08:26:23Z</updated>
    <published>2018-05-18T06:35:21Z</published>
    <title>Evaluation of a congruent auditory feedback for Motor Imagery BCI</title>
    <summary>  Designing a feedback that helps participants to achieve higher performances
is an important concern in brain-computer interface (BCI) research. In a pilot
study, we demonstrate how a congruent auditory feedback could improve
classification in a electroencephalography (EEG) motor imagery BCI. This is a
promising result for creating alternate feedback modality.
</summary>
    <author>
      <name>Emmanuel Christophe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Kronland-Martinet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Arthur Micoulaud-Franchi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Jelena Mladenović</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Gaëlle Mougin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PG, HCP</arxiv:affiliation>
    </author>
    <author>
      <name>Jean Vion-Dury</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Solvi Ystad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Mitsuko Aramaki</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International BCI meeting, May 2018, Asilomar, United States.
  http://bcisociety.org/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.07064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08418v2</id>
    <updated>2018-08-05T07:17:12Z</updated>
    <published>2018-05-22T06:21:09Z</published>
    <title>Task Allocation in Mobile Crowd Sensing: State of the Art and Future
  Opportunities</title>
    <summary>  Mobile Crowd Sensing (MCS) is the special case of crowdsourcing, which
leverages the smartphones with various embedded sensors and user's mobility to
sense diverse phenomenon in a city. Task allocation is a fundamental research
issue in MCS, which is crucial for the efficiency and effectiveness of MCS
applications. In this article, we specifically focus on the task allocation in
MCS systems. We first present the unique features of MCS allocation compared to
generic crowdsourcing, and then provide a comprehensive review for diversifying
problem formulation and allocation algorithms together with future research
opportunities.
</summary>
    <author>
      <name>Jiangtao Wang</name>
    </author>
    <author>
      <name>Leye Wang</name>
    </author>
    <author>
      <name>Yasha Wang</name>
    </author>
    <author>
      <name>Daqing Zhang</name>
    </author>
    <author>
      <name>Linghe Kong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08418v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08418v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09109v1</id>
    <updated>2018-05-22T08:20:26Z</updated>
    <published>2018-05-22T08:20:26Z</published>
    <title>Active Inference for Adaptive BCI: application to the P300 Speller</title>
    <summary>  Adaptive Brain-Computer interfaces (BCIs) have shown to improve performance,
however a general and flexible framework to implement adaptive features is
still lacking. We appeal to a generic Bayesian approach, called Active
Inference (AI), to infer user's intentions or states and act in a way that
optimizes performance. In realistic P300-speller simulations, AI outperforms
traditional algorithms with an increase in bit rate between 18% and 59%, while
offering a possibility of unifying various adaptive implementations within one
generic framework.
</summary>
    <author>
      <name>Jelena Mladenović</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc, CRNL</arxiv:affiliation>
    </author>
    <author>
      <name>Jérémy Frey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRNL</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Maby</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRNL</arxiv:affiliation>
    </author>
    <author>
      <name>Mateus Joffily</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GATE Lyon Saint-Étienne</arxiv:affiliation>
    </author>
    <author>
      <name>Fabien Lotte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremie Mattout</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRNL</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International BCI meeting, May 2018, Asilomar, United States.
  2018, http://bcisociety.org/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.09109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09322v1</id>
    <updated>2018-05-23T14:27:05Z</updated>
    <published>2018-05-23T14:27:05Z</published>
    <title>Reworked Second Order Blind Identification and Support Vector Machine
  technique towards imagery movement identification from EEG signals</title>
    <summary>  During imagery motor movements tasks, the so called mu and beta event related
desynchronization (ERD) and synchronization (ERS) are taking place, allowing us
to determine human patient imagery movement. However, initial recordings of
electroencephalography (EEG) signals contain system and environmental noise as
well as interference that must be ejected in order to separate the ERS/ERD
events from the rest of the signal. This paper presents a new technique based
on a reworked Second Order Blind Identification (SOBI) algorithm for noise
removal while imagery movement classification is implemented using Support
Vector Machine (SVM) technique.
</summary>
    <author>
      <name>Kalogiannis Gregory</name>
    </author>
    <author>
      <name>Hassapis George</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 page, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12376v1</id>
    <updated>2018-05-31T08:32:37Z</updated>
    <published>2018-05-31T08:32:37Z</published>
    <title>CrowdRev: A platform for Crowd-based Screening of Literature Reviews</title>
    <summary>  In this paper and demo we present a crowd and crowd+AI based system, called
CrowdRev, supporting the screening phase of literature reviews and achieving
the same quality as author classification at a fraction of the cost, and
near-instantly. CrowdRev makes it easy for authors to leverage the crowd, and
ensures that no money is wasted even in the face of difficult papers or
criteria: if the system detects that the task is too hard for the crowd, it
just gives up trying (for that paper, or for that criteria, or altogether),
without wasting money and never compromising on quality.
</summary>
    <author>
      <name>Jorge Ramirez</name>
    </author>
    <author>
      <name>Evgeny Krivosheev</name>
    </author>
    <author>
      <name>Marcos Baez</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <author>
      <name>Boualem Benatallah</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00929v1</id>
    <updated>2018-08-08T23:01:43Z</updated>
    <published>2018-08-08T23:01:43Z</published>
    <title>EEG-Based Driver Drowsiness Estimation Using Convolutional Neural
  Networks</title>
    <summary>  Deep learning, including convolutional neural networks (CNNs), has started
finding applications in brain-computer interfaces (BCIs). However, so far most
such approaches focused on BCI classification problems. This paper extends
EEGNet, a 3-layer CNN model for BCI classification, to BCI regression, and also
utilizes a novel spectral meta-learner for regression (SMLR) approach to
aggregate multiple EEGNets for improved performance. Our model uses the power
spectral density (PSD) of EEG signals as the input. Compared with raw EEG
inputs, the PSD inputs can reduce the computational cost significantly, yet
achieve much better regression performance. Experiments on driver drowsiness
estimation from EEG signals demonstrate the outstanding performance of our
approach.
</summary>
    <author>
      <name>Yuqi Cui</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03650v2</id>
    <updated>2018-09-12T01:14:00Z</updated>
    <published>2018-09-11T01:51:24Z</published>
    <title>Evaluation of Preference of Multimedia Content using Deep Neural
  Networks for Electroencephalography</title>
    <summary>  Evaluation of quality of experience (QoE) based on electroencephalography
(EEG) has received great attention due to its capability of real-time QoE
monitoring of users. However, it still suffers from rather low recognition
accuracy. In this paper, we propose a novel method using deep neural networks
toward improved modeling of EEG and thereby improved recognition accuracy. In
particular, we aim to model spatio-temporal characteristics relevant for QoE
analysis within learning models. The results demonstrate the effectiveness of
the proposed method.
</summary>
    <author>
      <name>Seong-Eun Moon</name>
    </author>
    <author>
      <name>Soobeom Jang</name>
    </author>
    <author>
      <name>Jong-Seok Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the 10th International Conference on Quality of
  Multimedia Experience (QoMEX 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.03650v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03650v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03920v1</id>
    <updated>2018-09-11T14:22:25Z</updated>
    <published>2018-09-11T14:22:25Z</published>
    <title>Multidevice mobile sessions: A first look</title>
    <summary>  The increasing number of users with multiple mobile devices underscores the
importance of understanding how users interact, often simultaneously, with
these multiple devices. However, most device based monitoring studies have
focused only on a single device type. In contrast, we study the multidevice
usage of a US-based panel through device based monitoring on panelist's
smartphone and tablet devices. We present a broad range of results from
characterizing individual multidevice sessions to estimating device usage
substitution. For example, we find that for panelists, 50% of all device
interaction time can be considered multidevice usage.
</summary>
    <author>
      <name>Benjamin Finley</name>
    </author>
    <author>
      <name>Tapio Soikkeli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.pmcj.2016.11.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.pmcj.2016.11.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted Manuscript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">B. Finley, T. Soikkeli, Multidevice mobile sessions: A first look,
  Pervasive and Mobile Computing, Volume 39, 2017, Pages 267-283</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.03920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04507v1</id>
    <updated>2018-09-12T15:13:44Z</updated>
    <published>2018-09-12T15:13:44Z</published>
    <title>Investigating the generalizability of EEG-based Cognitive Load
  Estimation Across Visualizations</title>
    <summary>  We examine if EEG-based cognitive load (CL) estimation is generalizable
across the character, spatial pattern, bar graph and pie chart-based
visualizations for the nback~task. CL is estimated via two recent approaches:
(a) Deep convolutional neural network, and (b) Proximal support vector
machines. Experiments reveal that CL estimation suffers across visualizations
motivating the need for effective machine learning techniques to benchmark
visual interface usability for a given analytic task.
</summary>
    <author>
      <name>Viral Parekh</name>
    </author>
    <author>
      <name>Maneesh Bilalpur</name>
    </author>
    <author>
      <name>Sharavan Kumar</name>
    </author>
    <author>
      <name>Stefan Winkler</name>
    </author>
    <author>
      <name>C V Jawahar</name>
    </author>
    <author>
      <name>Ramanathan Subramanian</name>
    </author>
    <link href="http://arxiv.org/abs/1809.04507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05352v1</id>
    <updated>2018-09-14T11:05:21Z</updated>
    <published>2018-09-14T11:05:21Z</published>
    <title>Put a Ring on It: Text Entry Performance on a Grip Ring Attached
  Smartphone</title>
    <summary>  This paper presents results of a study investing effects of grip rings on
text entry. Results revealed that grip rings do not affect text entry
performance in terms of speed, accuracy, or keystrokes per character. It then
reflects on future research directions based on the results and observations
from the study. The purpose of this work is to stress the necessity of
classifying and evaluating low-cost mobile phone accessories.
</summary>
    <author>
      <name>Monwen Shen</name>
    </author>
    <author>
      <name>Gulnar Rakhmetulla</name>
    </author>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MobileHCI 2018 Workshop on Socio-Technical Aspects of Text Entry
  (September 3, 2018). Barcelona, Spain, CEUR-WS.org/Vol-2183, 6-10</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.05352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05635v1</id>
    <updated>2018-09-15T02:34:58Z</updated>
    <published>2018-09-15T02:34:58Z</published>
    <title>Hierarchical Graphical Models for Context-Aware Hybrid Brain-Machine
  Interfaces</title>
    <summary>  We present a novel hierarchical graphical model based context-aware hybrid
brain-machine interface (hBMI) using probabilistic fusion of
electroencephalographic (EEG) and electromyographic (EMG) activities. Based on
experimental data collected during stationary executions and subsequent
imageries of five different hand gestures with both limbs, we demonstrate
feasibility of the proposed hBMI system through within session and online
across sessions classification analyses. Furthermore, we investigate the
context-aware extent of the model by a simulated probabilistic approach and
highlight potential implications of our work in the field of
neurophysiologically-driven robotic hand prosthetics.
</summary>
    <author>
      <name>Ozan Ozdenizci</name>
    </author>
    <author>
      <name>Sezen Yagmur Gunay</name>
    </author>
    <author>
      <name>Fernando Quivira</name>
    </author>
    <author>
      <name>Deniz Erdogmus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40th International Engineering in Medicine and Biology Conference
  (EMBC 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.05635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08817v1</id>
    <updated>2018-09-24T09:45:33Z</updated>
    <published>2018-09-24T09:45:33Z</published>
    <title>The Struggle is Real: Analyzing Ground Truth Data of TLS
  (Mis-)Configurations</title>
    <summary>  As of today, TLS is the most commonly used protocol to protect communication
content. To provide good security, it is of central importance, that
administrators know how to configure their services correctly. For this
purpose, services like, e.g., Qualys SSL Server Test can be leveraged to test
the correctness of a given web server configuration. We analyzed the
utilization of this service over a period of 2.5 months and found two major
usage-patterns. In addition, there is a relation between the number of
test-runs and the resulting quality (i.e., security) of a TLS configuration.
</summary>
    <author>
      <name>Christian Tiefenau</name>
    </author>
    <author>
      <name>Emanuel von Zezschwitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Poster presented at the 14th Symposium on Usable Privacy and Security
  (SOUPS 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08884v1</id>
    <updated>2018-09-21T09:59:03Z</updated>
    <published>2018-09-21T09:59:03Z</published>
    <title>Taking Informed Action on Student Activity in MOOCs</title>
    <summary>  This paper presents a novel approach to understand specific student behavior
in MOOCs. Instructors currently perceive participants only as one homogeneous
group. In order to improve learning outcomes, they encourage students to get
active in the discussion forum and remind them of approaching deadlines. While
these actions are most likely helpful, their actual impact is often not
measured. Additionally, it is uncertain whether such generic approaches
sometimes cause the opposite effect, as some participants are bothered with
irrelevant information. On the basis of fine granular events emitted by our
learning platform, we derive metrics and enable teachers to employ clustering,
in order to divide the vast field of participants into meaningful subgroups to
be addressed individually.
</summary>
    <author>
      <name>Ralf Teusner</name>
    </author>
    <author>
      <name>Kai-Adrian Rollmann</name>
    </author>
    <author>
      <name>Jan Renz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3051457.3053971</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3051457.3053971" rel="related"/>
    <link href="http://arxiv.org/abs/1809.08884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08893v1</id>
    <updated>2018-09-20T18:04:32Z</updated>
    <published>2018-09-20T18:04:32Z</published>
    <title>SPOT: Open Source framework for scientific data repository and
  interactive visualization</title>
    <summary>  SPOT is an open source and free visual data analytics tool for
multi-dimensional data-sets. Its web-based interface allows a quick analysis of
complex data interactively. The operations on data such as aggregation and
filtering are implemented. The generated charts are responsive and OpenGL
supported. It follows FAIR principles to allow reuse and comparison of the
published data-sets. The software also support PostgreSQL database for
scalability.
</summary>
    <author>
      <name>Faruk Diblen</name>
    </author>
    <author>
      <name>Jisk Attema</name>
    </author>
    <author>
      <name>Rena Bakhshi</name>
    </author>
    <author>
      <name>Sascha Caron</name>
    </author>
    <author>
      <name>Luc Hendriks</name>
    </author>
    <author>
      <name>Bob Stienen</name>
    </author>
    <link href="http://arxiv.org/abs/1809.08893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.09417v1</id>
    <updated>2018-09-25T11:44:27Z</updated>
    <published>2018-09-25T11:44:27Z</published>
    <title>Reflection On Reflection In Design Study</title>
    <summary>  Visualization design study research methodologies emphasize the need for
reflection to generate knowledge. And yet, there is very little guidance in the
literature specifying what reflection in the context of design studies actually
involves. We initiated a community discussion on this topic through a panel at
the 2017 IEEE VIS Conference - this report documents the panel discussion. We
analyze the panel content through the lense of our own reflective experiences
and propose several priorities for ongoing thinking on reflection in applied
visualization research.
</summary>
    <author>
      <name>Jason Dykes</name>
    </author>
    <author>
      <name>Miriah Meyer</name>
    </author>
    <link href="http://arxiv.org/abs/1809.09417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.09417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.02197v1</id>
    <updated>2018-12-05T19:35:40Z</updated>
    <published>2018-12-05T19:35:40Z</published>
    <title>The Office of the Future: Virtual, Portable and Global</title>
    <summary>  Virtual Reality has the potential to change the way we work. We envision the
future office worker to be able to work productively everywhere solely using
portable standard input devices and immersive head-mounted displays. Virtual
Reality has the potential to enable this, by allowing users to create working
environments of their choice and by relieving them from physical world
limitations such as constrained space or noisy environments. In this article,
we investigate opportunities and challenges for realizing this vision and
discuss implications from recent findings of text entry in virtual reality as a
core office task.
</summary>
    <author>
      <name>Jens Grubert</name>
    </author>
    <author>
      <name>Eyal Ofek</name>
    </author>
    <author>
      <name>Michel Pahud</name>
    </author>
    <author>
      <name>Per Ola Kristensson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCG.2018.2875609</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCG.2018.2875609" rel="related"/>
    <link href="http://arxiv.org/abs/1812.02197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03200v4</id>
    <updated>2019-08-18T08:16:54Z</updated>
    <published>2018-12-07T20:09:57Z</published>
    <title>Esports Athletes and Players: a Comparative Study</title>
    <summary>  We present a comparative study of the players' and professional players'
(athletes') performance in Counter Strike: Global Offensive (CS:GO) discipline.
Our study is based on ubiquitous sensing helping identify the biometric
features significantly contributing to the classification of particular skills
of the players. The research provides better understanding why the athletes
demonstrate superior performance as compared to other players.
</summary>
    <author>
      <name>Nikita Khromov</name>
    </author>
    <author>
      <name>Alexander Korotin</name>
    </author>
    <author>
      <name>Andrey Lange</name>
    </author>
    <author>
      <name>Anton Stepanov</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <author>
      <name>Andrey Somov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.03200v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03200v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03219v1</id>
    <updated>2018-12-07T21:37:49Z</updated>
    <published>2018-12-07T21:37:49Z</published>
    <title>Dice in the Black Box: User Experiences with an Inscrutable Algorithm</title>
    <summary>  We demonstrate that users may be prone to place an inordinate amount of trust
in black box algorithms that are framed as intelligent. We deploy an algorithm
that purportedly assesses the positivity and negativity of a users' writing
emotional writing. In actuality, the algorithm responds in a random fashion. We
qualitatively examine the paths to trust that users followed while testing the
system. In light of the ease with which users may trust systems exhibiting
"intelligent behavior" we recommend corrective approaches.
</summary>
    <author>
      <name>Aaron Springer</name>
    </author>
    <author>
      <name>Victoria Hollis</name>
    </author>
    <author>
      <name>Steve Whittaker</name>
    </author>
    <link href="http://arxiv.org/abs/1812.03219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03220v1</id>
    <updated>2018-12-07T21:38:05Z</updated>
    <published>2018-12-07T21:38:05Z</published>
    <title>What Are You Hiding? Algorithmic Transparency and User Perceptions</title>
    <summary>  Extensive recent media focus has been directed towards the dark side of
intelligent systems, how algorithms can influence society negatively. Often,
transparency is proposed as a solution or step in the right direction.
Unfortunately, research is mixed on the impact of transparency on the user
experience. We examine transparency in the context an interactive system that
predicts positive/negative emotion from a users' written text. We unify
seemingly this contradictory research under a single model. We show that
transparency can negatively affect accuracy perceptions for users whose
expectations were not violated by the system's prediction; however,
transparency also limits the damage done when users' expectations are violated
by system predictions.
</summary>
    <author>
      <name>Aaron Springer</name>
    </author>
    <author>
      <name>Steve Whittaker</name>
    </author>
    <link href="http://arxiv.org/abs/1812.03220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07653v1</id>
    <updated>2018-11-29T20:31:45Z</updated>
    <published>2018-11-29T20:31:45Z</published>
    <title>Using Pupil Diameter to Measure Cognitive Load</title>
    <summary>  In this paper, we will present a method for measuring cognitive load and
online real-time feedback using the Tobii Pro 2 eye-tracking glasses. The
system is envisaged to be capable of estimating high cognitive load states and
situations, and adjust human-machine interfaces to the user's needs. The system
is using well-known metrics such as average pupillary size over time. Our
system can provide cognitive load feedback at 17-18 Hz. We will elaborate on
our results of a HRI study using this tool to show it's functionality.
</summary>
    <author>
      <name>Georgios Minadakis</name>
    </author>
    <author>
      <name>Katrin Lohan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AI-HRI AAAI-FSS, 2018 (arXiv:1809.06606)</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.07653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.11090v1</id>
    <updated>2018-12-28T16:21:40Z</updated>
    <published>2018-12-28T16:21:40Z</published>
    <title>Enhanced Touchable Projector-depth System with Deep Hand Pose Estimation</title>
    <summary>  Touchable projection with structured light range cameras is a prolific medium
for large interaction surfaces, affording multiple simultaneous users and
simple, cheap setup. However robust touch detection in such projector-depth
systems is difficult to achieve due to measurement noise. We propose a novel
combination of surface touch detection and a deep network for hand pose
estimation, which aids in detecting both on- and above-surface hand gestures,
disambiguating multiple touch fingers, as well as recovering fingertip
positions in face of noisy input. We present the details of our GPU-accelerated
system and an evaluation of its performance, as well as applications such as an
enhanced virtual keyboard that utilizes the added features.
</summary>
    <author>
      <name>Zhi Chai</name>
    </author>
    <author>
      <name>Roy Shilkrot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.11090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02836v1</id>
    <updated>2019-06-06T22:15:05Z</updated>
    <published>2019-06-06T22:15:05Z</published>
    <title>Black-boxing the user: internet protocol over xylophone players (IPoXP)</title>
    <summary>  We introduce IP over Xylophone Players (IPoXP), a novel Internet protocol
between two computers using xylophone-based Arduino interfaces. In our
implementation, human operators are situated within the lowest layer of the
network, transmitting data between computers by striking designated keys. We
discuss how IPoXP inverts the traditional mode of human-computer interaction,
with a computer using the human as an interface to communicate with another
computer.
</summary>
    <author>
      <name>R. Stuart Geiger</name>
    </author>
    <author>
      <name>Yoon Jung Jeong</name>
    </author>
    <author>
      <name>Emily Manders</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2212776.2212785</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2212776.2212785" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In CHI 2012 Extended Abstracts on Human Factors in Computing
  Systems (CHI EA 2012, alt.chi). ACM, New York, NY, USA, p. 71-80. DOI:
  https://doi.org/10.1145/2212776.2212785</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.02836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04435v1</id>
    <updated>2019-06-11T08:13:15Z</updated>
    <published>2019-06-11T08:13:15Z</published>
    <title>Enhancing Battle Maps through Flow Graphs</title>
    <summary>  So-called battle maps are an appropriate way to visually summarize the flow
of battles as they happen in many team-based combat games. Such maps can be a
valuable tool for retrospective analysis of battles for the purpose of training
or for providing a summary representation for spectators. In this paper an
extension to the battle map algorithm previously proposed by the author and
which addresses a shortcoming in the depiction of troop movements is described.
The extension does not require alteration of the original algorithm and can
easily be added as an intermediate step before rendering. The extension is
illustrated using gameplay data from the team-based multiplayer game World of
Tanks.
</summary>
    <author>
      <name>Günter Wallner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE Conference on Games 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04837v1</id>
    <updated>2019-06-11T21:46:24Z</updated>
    <published>2019-06-11T21:46:24Z</published>
    <title>Toward Best Practices for Explainable B2B Machine Learning</title>
    <summary>  To design tools and data pipelines for explainable B2B machine learning (ML)
systems, we need to recognize not only the immediate audience of such tools and
data, but also (1) their organizational context and (2) secondary audiences.
Our learnings are based on building custom ML-based chatbots for recruitment.
We believe that in the B2B context, "explainable" ML means not only a system
that can "explain itself" through tools and data pipelines, but also enables
its domain-expert users to explain it to other stakeholders.
</summary>
    <author>
      <name>Kit Kuksenok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure; position paper for INTERACT 2019 workshop on
  Humans in the Loop: Bridging AI and HCI</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05094v1</id>
    <updated>2019-06-11T15:24:45Z</updated>
    <published>2019-06-11T15:24:45Z</published>
    <title>Organic Building Generation in Minecraft</title>
    <summary>  This paper presents a method for generating floor plans for structures in
Minecraft (Mojang 2009). Given a 3D space, it will auto-generate a building to
fill that space using a combination of constrained growth and cellular
automata. The result is a series of organic-looking buildings complete with
rooms, windows, and doors connecting them. The method is applied to the
Generative Design in Minecraft (GDMC) competition to auto-generate buildings in
Minecraft, and the results are discussed.
</summary>
    <author>
      <name>Michael Cerny Green</name>
    </author>
    <author>
      <name>Christoph Salge</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures, published at PCG workshop at the Foundations of
  Digital Games Conference 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06089v1</id>
    <updated>2019-06-14T09:27:25Z</updated>
    <published>2019-06-14T09:27:25Z</published>
    <title>Proceedings of the CHI 2019 Workshop on New Directions for the IoT:
  Automate, Share, Build, and Care</title>
    <summary>  This volume represents the proceedings of the CHI 2019 Workshop on New
Directions for the IoT: Automate, Share, Build, and Care.
</summary>
    <author>
      <name>Carolina Fuentes</name>
    </author>
    <author>
      <name>Martin Porcheron</name>
    </author>
    <author>
      <name>Joel Fischer</name>
    </author>
    <author>
      <name>Nervo Verdezoto</name>
    </author>
    <author>
      <name>Oren Zuckerman</name>
    </author>
    <author>
      <name>Enrico Constanza</name>
    </author>
    <author>
      <name>Valeria Herskovic</name>
    </author>
    <author>
      <name>Leila Takayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Participant contributions to the CHI 2019 Workshop on New Directions
  for the IoT: Automate, Share, Build, and Care, held at the CHI 2019
  conference on Human Factors in Computing Systems, Glasgow, Scotland, 5 May
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.06089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07864v1</id>
    <updated>2019-06-19T01:01:31Z</updated>
    <published>2019-06-19T01:01:31Z</published>
    <title>Predicting Personality Traits from Physical Activity Intensity</title>
    <summary>  Call and messaging logs from mobile devices have been used to predict human
personality traits successfully in recent years. However, the widely available
accelerometer data is not yet utilized for this purpose. In this research, we
explored some important features describing human physical activity intensity,
used for the very first time to predict human personality traits through raw
accelerometer data. Using a set of newly introduced metrics, we combined
physical activity intensity features with traditional phone activity features
for personality prediction. The experiment results show that the predicted
personality scores are closer to the ground truth, with observable reduction of
errors in predicting the Big-5 personality traits across male and female.
</summary>
    <author>
      <name>Nan Gao</name>
    </author>
    <author>
      <name>Wei Shao</name>
    </author>
    <author>
      <name>Flora D Salim</name>
    </author>
    <link href="http://arxiv.org/abs/1906.07864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10557v1</id>
    <updated>2019-06-25T14:15:37Z</updated>
    <published>2019-06-25T14:15:37Z</published>
    <title>Multi-Modal Measurements of Mental Load</title>
    <summary>  This position paper describes an experiment conducted to understand the
relationships between different physiological measures including pupil
Diameter, Blinking Rate, Heart Rate, and Heart Rate Variability in order to
develop an estimation of users' mental load in real-time (see Sidebar 1). Our
experiment involved performing a task to spot a correct or an incorrect word or
sentence with different difficulties in order to induce mental load. We briefly
present the analysis of task performance and response time for the items of the
experiment task.
</summary>
    <author>
      <name>Ingo Keller</name>
    </author>
    <author>
      <name>Muneeb Imtiaz Ahmad</name>
    </author>
    <author>
      <name>Katrin Lohan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI Conference-W12, April 2019, Glasgow, United Kingdom</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.12051v1</id>
    <updated>2019-06-28T05:56:57Z</updated>
    <published>2019-06-28T05:56:57Z</published>
    <title>A Taxonomy for Virtual and Augmented Reality in Education</title>
    <summary>  In this paper, a taxonomy for VR/AR in education is presented that can help
differentiate and categorise education experiences and provide indication as to
why some applications of fail whereas others succeed. Examples will be
presented to illustrate the taxonomy, including its use in developing and
planning two current VR projects in our laboratory. The first project is a VR
application for the training of Chemical Engineering students (and potentially
industrial operators) on the use of a physical pilot plant facility. The second
project involves the use of VR cinematography for enacting ethics scenarios
(and thus ethical awareness and development) pertinent to engineering work
situations.
</summary>
    <author>
      <name>Jiri Motejlek</name>
    </author>
    <author>
      <name>Esat Alpay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Society for Engineering Education Conference 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.12051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.12051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00608v1</id>
    <updated>2019-09-02T09:01:49Z</updated>
    <published>2019-09-02T09:01:49Z</published>
    <title>Collecting and Structuring Information in the Information Collage</title>
    <summary>  Knowledge workers, such as scientists, journalists, or consultants,
adaptively seek, gather, and consume information. These processes are often
inefficient as existing user interfaces provide limited possibilities to
combine information from various sources and different formats into a common
knowledge representation. In this paper, we present the concept of an
information collage (IC) -- a web browser extension combining manual spatial
organization of gathered information fragments and automatic text analysis for
interactive content exploration and expressive visual summaries. We used IC for
case studies with knowledge workers from different domains and longer-term
field studies over a period of one month. We identified three different ways
how users collect and structure information and provide design recommendations
how to support these observed usage strategies.
</summary>
    <author>
      <name>Sebastian Sippl</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <author>
      <name>Manuela Waldner</name>
    </author>
    <link href="http://arxiv.org/abs/1909.00608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02043v1</id>
    <updated>2019-09-04T18:37:58Z</updated>
    <published>2019-09-04T18:37:58Z</published>
    <title>PARQR: Augmenting the Piazza Online Forum to Better Support Degree
  Seeking Online Masters Students</title>
    <summary>  We introduce PARQR, a tool for online education forums that reduces duplicate
posts by 40\% in a degree seeking online masters program at a top university.
Instead of performing a standard keyword search, PARQR monitors questions as
students compose them and continuously suggests relevant posts. In testing,
PARQR correctly recommends a relevant post, if one exists, 73.5\% of the time.
We discuss PARQR's design, initial experimental results comparing different
semesters with and without PARQR, and interviews we conducted with teaching
instructors regarding their experience with PARQR.
</summary>
    <author>
      <name>Noah Bilgrien</name>
    </author>
    <author>
      <name>Roy Finkelberg</name>
    </author>
    <author>
      <name>Chirag Tailor</name>
    </author>
    <author>
      <name>India Irish</name>
    </author>
    <author>
      <name>Girish Murali</name>
    </author>
    <author>
      <name>Abhishek Mangal</name>
    </author>
    <author>
      <name>Niklas Gustafsson</name>
    </author>
    <author>
      <name>Sumedha Raman</name>
    </author>
    <author>
      <name>Thad Starner</name>
    </author>
    <author>
      <name>Rosa Arriaga</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02507v1</id>
    <updated>2019-09-05T16:21:37Z</updated>
    <published>2019-09-05T16:21:37Z</published>
    <title>A Generalized Web Component for Domain-Independent Smart Assistants</title>
    <summary>  This article introduces an open-source web component, Instant Expert, which
allows robust and efficient integration of a natural language question
answering system to web-based platforms in any domain. Web Components are a set
of web technologies to allow the creation of reusable, customizable, and
encapsulated HTML elements. The Instant Expert web component consists of the
user input (i.e. text, voice, multi-selection), question processing, and user
interface modules. Two use cases are developed to demonstrate the component's
features, benefits, and usage. The goal of this project is to pave the way for
next-generation information systems by mitigating the challenges of developing
voice-enabled and domain-informed smart assistants for communicating knowledge
in any domain.
</summary>
    <author>
      <name>Yusuf Sermet</name>
    </author>
    <author>
      <name>Ibrahim Demir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02800v2</id>
    <updated>2019-09-10T10:40:14Z</updated>
    <published>2019-09-06T10:07:20Z</published>
    <title>CrowdHub: Extending crowdsourcing platforms for the controlled
  evaluation of tasks designs</title>
    <summary>  We present CrowdHub, a tool for running systematic evaluations of task
designs on top of crowdsourcing platforms. The goal is to support the
evaluation process, avoiding potential experimental biases that, according to
our empirical studies, can amount to 38% loss in the utility of the collected
dataset in uncontrolled settings. Using CrowdHub, researchers can map their
experimental design and automate the complex process of managing task execution
over time while controlling for returning workers and crowd demographics, thus
reducing bias, increasing utility of collected data, and making more efficient
use of a limited pool of subjects.
</summary>
    <author>
      <name>Jorge Ramírez</name>
    </author>
    <author>
      <name>Simone Degiacomi</name>
    </author>
    <author>
      <name>Davide Zanella</name>
    </author>
    <author>
      <name>Marcos Baez</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <author>
      <name>Boualem Benatallah</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03054v1</id>
    <updated>2019-09-06T11:10:34Z</updated>
    <published>2019-09-06T11:10:34Z</published>
    <title>Calibrating Wayfinding Decisions in Pedestrian Simulation Models: The
  Entropy Map</title>
    <summary>  This paper presents entropy maps, an approach to describing and visualising
uncertainty among alternative potential movement intentions in pedestrian
simulation models. In particular, entropy maps show the instantaneous level of
randomness in decisions of a pedestrian agent situated in a specific point of
the simulated environment with an heatmap approach. Experimental results
highlighting the relevance of this tool supporting modelers are provided and
discussed.
</summary>
    <author>
      <name>Luca Crociani</name>
    </author>
    <author>
      <name>Giuseppe Vizzari</name>
    </author>
    <author>
      <name>Stefania Bandini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pre-print of paper presented at the The 16th International Conference
  on Modeling Decisions for Artificial Intelligence, Milan, Italy September 4 -
  6, 2019. arXiv admin note: substantial text overlap with arXiv:1610.07901</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.4; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04387v1</id>
    <updated>2019-09-10T10:12:59Z</updated>
    <published>2019-09-10T10:12:59Z</published>
    <title>A Crowd-based Evaluation of Abuse Response Strategies in Conversational
  Agents</title>
    <summary>  How should conversational agents respond to verbal abuse through the user? To
answer this question, we conduct a large-scale crowd-sourced evaluation of
abuse response strategies employed by current state-of-the-art systems. Our
results show that some strategies, such as "polite refusal" score highly across
the board, while for other strategies demographic factors, such as age, as well
as the severity of the preceding abuse influence the user's perception of which
response is appropriate. In addition, we find that most data-driven models lag
behind rule-based or commercial systems in terms of their perceived
appropriateness.
</summary>
    <author>
      <name>Amanda Cercas Curry</name>
    </author>
    <author>
      <name>Verena Rieser</name>
    </author>
    <link href="http://arxiv.org/abs/1909.04387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05833v1</id>
    <updated>2019-09-12T17:39:50Z</updated>
    <published>2019-09-12T17:39:50Z</published>
    <title>Compact 3 DOF Driving Simulator using Immersive Virtual Reality</title>
    <summary>  A driving simulator was created using commercially available 3 degree of
freedom motion platform (DOFreality H3) and a virtual reality head-mounted
display (Oculus CV1). Using virtual reality headset as the visual simulation
system with low-cost moving base platform allowed us to create a high-fidelity
driving simulator with minimal cost and space. A custom motion cueing algorithm
was used to minimize visuo-vestibular conflict, and simulator sickness
questionnaire (SSQ) was used to measure progression of simulator sickness over
time while driving on a highway environment.
</summary>
    <author>
      <name>Jungsu Pak</name>
    </author>
    <author>
      <name>Uri Maoz</name>
    </author>
    <link href="http://arxiv.org/abs/1909.05833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06959v1</id>
    <updated>2019-09-13T10:38:55Z</updated>
    <published>2019-09-13T10:38:55Z</published>
    <title>Feeling Anxious? Perceiving Anxiety in Tweets using Machine Learning</title>
    <summary>  This study provides a predictive measurement tool to examine perceived
anxiety from a longitudinal perspective, using a non-intrusive machine learning
approach to scale human rating of anxiety in microblogs. Results suggest that
our chosen machine learning approach depicts perceived user state-anxiety
fluctuations over time, as well as mean trait anxiety. We further find a
reverse relationship between perceived anxiety and outcomes such as social
engagement and popularity. Implications on the individual, organizational, and
societal levels are discussed.
</summary>
    <author>
      <name>Dritjon Gruda</name>
    </author>
    <author>
      <name>Souleiman Hasan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.chb.2019.04.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.chb.2019.04.020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers in Human Behavior, 98, 2019, 245-255</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.06959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08052v1</id>
    <updated>2019-09-13T14:51:05Z</updated>
    <published>2019-09-13T14:51:05Z</published>
    <title>Towards an Adaptive Robot for Sports and Rehabilitation Coaching</title>
    <summary>  The work presented in this paper aims to explore how, and to what extent, an
adaptive robotic coach has the potential to provide extra motivation to adhere
to long-term rehabilitation and help fill the coaching gap which occurs during
repetitive solo practice in high performance sport. Adapting the behavior of a
social robot to a specific user, using reinforcement learning (RL), could be a
way of increasing adherence to an exercise routine in both domains. The
requirements gathering phase is underway and is presented in this paper along
with the rationale of using RL in this context.
</summary>
    <author>
      <name>Martin K. Ross</name>
    </author>
    <author>
      <name>Frank Broz</name>
    </author>
    <author>
      <name>Lynne Baillie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI-HRI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.08052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12281v1</id>
    <updated>2019-09-26T17:44:06Z</updated>
    <published>2019-09-26T17:44:06Z</published>
    <title>Human-Centric Program Synthesis</title>
    <summary>  Program synthesis techniques offer significant new capabilities in searching
for programs that satisfy high-level specifications. While synthesis has been
thoroughly explored for input/output pair specifications
(programming-by-example), this paper asks: what does program synthesis look
like beyond examples? What actual issues in day-to-day development would stand
to benefit the most from synthesis? How can a human-centric perspective inform
the exploration of alternative specification languages for synthesis? I sketch
a human-centric vision for program synthesis where programmers explore and
learn languages and APIs aided by a synthesis tool.
</summary>
    <author>
      <name>Will Crichton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at PLATEAU'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.12281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13443v1</id>
    <updated>2019-09-30T03:40:59Z</updated>
    <published>2019-09-30T03:40:59Z</published>
    <title>Using Conversational Agents To Support Learning By Teaching</title>
    <summary>  Conversational agents are becoming increasingly popular for supporting and
facilitating learning. Conventional pedagogical agents are designed to play the
role of human teachers by giving instructions to the students. In this paper,
we investigate the use of conversational agents to support the
'learning-by-teaching' paradigm where the agent receives instructions from
students. In particular, we introduce Curiosity Notebook: an educational
application that harvests conversational interventions to facilitate students'
learning. Recognizing such interventions can not only help in engaging students
within learning interactions, but also provide a deeper insight into the
intricacies involved in designing conversational agents for educational
purposes.
</summary>
    <author>
      <name>Nalin Chhibber</name>
    </author>
    <author>
      <name>Edith Law</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, Presented at Conversational Agents Workshop, CHI
  2019 (Glasgow, UK)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00892v1</id>
    <updated>2020-01-03T17:06:58Z</updated>
    <published>2020-01-03T17:06:58Z</published>
    <title>Exploration of Interaction Techniques for Graph-based Modelling in
  Virtual Reality</title>
    <summary>  Editing and manipulating graph-based models within immersive environments is
largely unexplored and certain design activities could benefit from using those
technologies. For example, in the case study of architectural modelling, the 3D
context of Virtual Reality naturally matches the intended output product, i.e.
a 3D architectural geometry. Since both the state of the art and the state of
the practice are lacking, we explore the field of VR-based interactive
modelling, and provide insights as to how to implement proper interactions in
that context, with broadly available devices. We consequently produce several
open-source software prototypes for manipulating graph-based models in VR.
</summary>
    <author>
      <name>Adrien Coppens</name>
    </author>
    <author>
      <name>Berat Bicer</name>
    </author>
    <author>
      <name>Naz Yilmaz</name>
    </author>
    <author>
      <name>Serhat Aras</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01868v1</id>
    <updated>2020-01-07T03:04:26Z</updated>
    <published>2020-01-07T03:04:26Z</published>
    <title>Closed loop application of electroadhesion for increased precision in
  texture rendering</title>
    <summary>  Tactile displays based on friction modulation offer wide-bandwidth forces
rendered directly on the fingertip. However, due to a number of touch
conditions (e.g., normal force, skin hydration) that result in variations in
the friction force and the strength of modulation effect, the precision of the
force rendering remains limited. In this paper we demonstrate a closed-loop
electroadhesion method for precise playback of friction force profiles on a
human finger and we apply this method to the tactile rendering of several
textiles encountered in everyday life.
</summary>
    <author>
      <name>Roman V. Grigorii</name>
    </author>
    <author>
      <name>J. Edward Colgate</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TOH.2020.2972350</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TOH.2020.2972350" rel="related"/>
    <link href="http://arxiv.org/abs/2001.01868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.06578v1</id>
    <updated>2020-01-18T02:22:45Z</updated>
    <published>2020-01-18T02:22:45Z</published>
    <title>City Planning with Augmented Reality</title>
    <summary>  We present an early study designed to analyze how city planning and the
health of senior citizens can benefit from the use of augmented reality (AR)
using Microsoft's HoloLens. We also explore whether AR and VR can be used to
help city planners receive real-time feedback from citizens, such as the
elderly, on virtual plans, allowing for informed decisions to be made before
any construction begins.
</summary>
    <author>
      <name>Catherine Angelini</name>
    </author>
    <author>
      <name>Adam S. Williams</name>
    </author>
    <author>
      <name>Mathew Kress</name>
    </author>
    <author>
      <name>Edgar Ramos Vieira</name>
    </author>
    <author>
      <name>Newton D'Souza</name>
    </author>
    <author>
      <name>Naphtali D. Rishe</name>
    </author>
    <author>
      <name>Joseph Medina</name>
    </author>
    <author>
      <name>Francisco R. Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This was accepted by Graphics Interface 2019 (GI '2019) HCI track as
  a poster paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.06578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.06579v1</id>
    <updated>2020-01-18T02:31:30Z</updated>
    <published>2020-01-18T02:31:30Z</published>
    <title>Towards a Virtual Reality Home IoT Network Visualizer</title>
    <summary>  We present an IoT home network visualizer that utilizes virtual reality (VR).
This prototype demonstrates the potential that VR has to aid in the
understanding of home IoT networks. This is particularly important due the
increased number of household devices now connected to the Internet. This
prototype is able to function in a standard display or a VR headset. A
prototype was developed to aid in the understanding of home IoT networks for
homeowners.
</summary>
    <author>
      <name>Drew Johnston</name>
    </author>
    <author>
      <name>Jarret Flack</name>
    </author>
    <author>
      <name>Indrakshi Ray</name>
    </author>
    <author>
      <name>Francisco R. Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted at the Graphics Interface 2019 (GI' 19) HCI
  Track for a poster paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.06579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07546v1</id>
    <updated>2020-01-15T03:27:36Z</updated>
    <published>2020-01-15T03:27:36Z</published>
    <title>Exploring an Application of Virtual Reality for Early Detection of
  Dementia</title>
    <summary>  Facing the severe global dementia problem, an exploration was conducted
adopting the technology of virtual reality (VR). This report lays a technical
foundation for further research project "Early Detection of Dementia Using
Testing Tools in VR Environment", which illustrates the process of developing a
VR application using Unity 3D software on Oculus Go. This preliminary
exploration is composed of three steps, including 3D virtual scene
construction, VR interaction design and monitoring. The exploration was
recorded to provide basic technical guidance and detailed method for subsequent
research.
</summary>
    <author>
      <name>Yiming Zhong</name>
    </author>
    <author>
      <name>Yuan Tian</name>
    </author>
    <author>
      <name>Mira Park</name>
    </author>
    <author>
      <name>Soonja Yeom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 tables, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.07546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08791v1</id>
    <updated>2020-01-23T20:18:44Z</updated>
    <published>2020-01-23T20:18:44Z</published>
    <title>Machine learning based co-creative design framework</title>
    <summary>  We propose a flexible, co-creative framework bringing together multiple
machine learning techniques to assist human users to efficiently produce
effective creative designs. We demonstrate its potential with a perfume bottle
design case study, including human evaluation and quantitative and qualitative
analyses.
</summary>
    <author>
      <name>Brian Quanz</name>
    </author>
    <author>
      <name>Wei Sun</name>
    </author>
    <author>
      <name>Ajay Deshpande</name>
    </author>
    <author>
      <name>Dhruv Shah</name>
    </author>
    <author>
      <name>Jae-eun Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thirty-third Conference on Neural Information Processing Systems
  (NeurIPS) 2019 Workshop on Machine Learning for Creativity and Design,
  December 14th, 2019, Vancouver, Canada
  (https://neurips2019creativity.github.io/)</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.1; D.2.2; I.5.4; I.3.6; H.1.2; H.5.0; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09551v1</id>
    <updated>2020-01-27T01:08:11Z</updated>
    <published>2020-01-27T01:08:11Z</published>
    <title>Machine Learning for a Music Glove Instrument</title>
    <summary>  A music glove instrument equipped with force sensitive, flex and IMU sensors
is trained on an electric piano to learn note sequences based on a time series
of sensor inputs. Once trained, the glove is used on any surface to generate
the sequence of notes most closely related to the hand motion. The data is
collected manually by a performer wearing the glove and playing on an electric
keyboard. The feature space is designed to account for the key hand motion,
such as the thumb-under movement. Logistic regression along with bayesian
belief networks are used learn the transition probabilities from one note to
another. This work demonstrates a data-driven approach for digital musical
instruments in general.
</summary>
    <author>
      <name>Joseph Bakarji</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.02016v1</id>
    <updated>2020-08-05T09:29:32Z</updated>
    <published>2020-08-05T09:29:32Z</published>
    <title>Activity and mood-based routing for autonomous vehicles</title>
    <summary>  A significant amount of our daily lives is dedicated to driving, leading to
an unavoidable exposure to driving-related stress. The rise of autonomous
vehicles will likely lessen the extent of this stress and enhance the routine
traveling experience. Yet, no matter how diverse they may be, current routing
criteria are limited to considering only the passive preferences of a vehicle's
users. Thus, to enhance the overall driving experience in autonomous vehicles,
we advocate here for the diversification of routing criteria, by additionally
emphasizing activity- and mood-based requirements.
</summary>
    <author>
      <name>Ankit Kariryaa</name>
    </author>
    <author>
      <name>Tony Veale</name>
    </author>
    <author>
      <name>Johannes Schöning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.02016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.02234v1</id>
    <updated>2020-08-05T17:02:22Z</updated>
    <published>2020-08-05T17:02:22Z</published>
    <title>An Augmented Reality Interaction Interface for Autonomous Drone</title>
    <summary>  Human drone interaction in autonomous navigation incorporates spatial
interaction tasks, including reconstructed 3D map from the drone and human
desired target position. Augmented Reality (AR) devices can be powerful
interactive tools for handling these spatial interactions. In this work, we
build an AR interface that displays the reconstructed 3D map from the drone on
physical surfaces in front of the operator. Spatial target positions can be
further set on the 3D map by intuitive head gaze and hand gesture. The AR
interface is deployed to interact with an autonomous drone to explore an
unknown environment. A user study is further conducted to evaluate the overall
interaction performance.
</summary>
    <author>
      <name>Chuhao Liu</name>
    </author>
    <author>
      <name>Shaojie Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, IROS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.02234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03298v1</id>
    <updated>2020-08-08T09:54:21Z</updated>
    <published>2020-08-08T09:54:21Z</published>
    <title>FitsGeo -- Python package for PHITS geometry development and
  visualization</title>
    <summary>  An easy way to define and visualize geometry for PHITS input files
introduced. Suggested FitsGeo Python package helps to define surfaces as Python
objects and manipulate them conveniently. VPython assists to view defined
geometry interactively which boosts geometry development and helps with
complicated cases. Every class that sets the surface object has methods with
some extra properties. As well as geometry generation for PHITS input,
additional modules developed for material and cell definition. Any user with a
very basic knowledge of Python can define the geometry in a convenient way and
use it in further research related to particle transport.
</summary>
    <author>
      <name>Ivan Gordeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.03298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03982v1</id>
    <updated>2020-08-10T09:32:38Z</updated>
    <published>2020-08-10T09:32:38Z</published>
    <title>Social Interactions Clustering MOOC Students: An Exploratory Study</title>
    <summary>  An exploratory study on social interactions of MOOC students in FutureLearn
was conducted, to answer "how can we cluster students based on their social
interactions?" Comments were categorized based on how students interacted with
them, e.g., how a student's comment received replies from peers. Statistical
modelling and machine learning were used to analyze comment categorization,
resulting in 3 strong and stable clusters.
</summary>
    <author>
      <name>Lei Shi</name>
    </author>
    <author>
      <name>Alexandra Cristea</name>
    </author>
    <author>
      <name>Ahmad Alamri</name>
    </author>
    <author>
      <name>Armando M. Toda</name>
    </author>
    <author>
      <name>Wilk Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 20th IEEE International Conference on Advanced Learning
  Technologies (ICALT2020), page 172-174</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.03982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.06030v2</id>
    <updated>2020-09-03T09:51:05Z</updated>
    <published>2020-08-13T17:40:48Z</published>
    <title>On the design of text editors</title>
    <summary>  Text editors are written by and for developers. They come with a large set of
default and implicit choices in terms of layout, typography, colorization and
interaction that hardly change from one editor to the other. It is not clear if
these implicit choices derive from the ignorance of alternatives or if they
derive from developers' habits, reproducing what they are used to. The goal of
this article is to characterize these implicit choices and to illustrate what
are some alternatives without prescribing one or the other.
</summary>
    <author>
      <name>Nicolas P. Rougier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.06030v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.06030v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11310v3</id>
    <updated>2021-02-12T21:06:28Z</updated>
    <published>2020-08-25T23:51:09Z</published>
    <title>Why Shouldn't All Charts Be Scatter Plots? Beyond Precision-Driven
  Visualizations</title>
    <summary>  A central concept in information visualization research and practice is the
notion of visual variable effectiveness, or the perceptual precision at which
values are decoded given visual channels of encoding. Formative work from
Cleveland &amp; McGill has shown that position along a common axis is the most
effective visual variable for comparing individual values. One natural
conclusion is that any chart that is not a dot plot or scatterplot is deficient
and should be avoided. In this paper we refute a caricature of this
"scatterplots only" argument as a way to call for new perspectives on how
information visualization is researched, taught, and evaluated.
</summary>
    <author>
      <name>Enrico Bertini</name>
    </author>
    <author>
      <name>Michael Correll</name>
    </author>
    <author>
      <name>Steven Franconeri</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11310v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11310v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11785v1</id>
    <updated>2020-08-26T20:06:30Z</updated>
    <published>2020-08-26T20:06:30Z</published>
    <title>Understanding scholarly Natural Language Processing system diagrams
  through application of the Richards-Engelhardt framework</title>
    <summary>  We utilise Richards-Engelhardt framework as a tool for understanding Natural
Language Processing systems diagrams. Through four examples from scholarly
proceedings, we find that the application of the framework to this ecological
and complex domain is effective for reflecting on these diagrams. We argue for
vocabulary to describe multiple-codings, semiotic variability, and
inconsistency or misuse of visual encoding principles in diagrams. Further, for
application to scholarly Natural Language Processing systems, and perhaps
systems diagrams more broadly, we propose the addition of "Grouping by Object"
as a new visual encoding principle, and "Emphasising" as a new visual encoding
type.
</summary>
    <author>
      <name>Guy Clarke Marshall</name>
    </author>
    <author>
      <name>Caroline Jay</name>
    </author>
    <author>
      <name>André Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, pre-print</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11834v1</id>
    <updated>2020-08-26T21:46:45Z</updated>
    <published>2020-08-26T21:46:45Z</published>
    <title>Conversations On Multimodal Input Design With Older Adults</title>
    <summary>  Multimodal input systems can help bridge the wide range of physical abilities
found in older generations. After conducting a survey/interview session with a
group of older adults at an assisted living community we believe that gesture
and speech should be the main inputs for that system. Additionally,
collaborative design of new systems was found to be useful for facilitating
conversations around input design with this demographic.
</summary>
    <author>
      <name>Adam S. Williams</name>
    </author>
    <author>
      <name>Sarah Coler</name>
    </author>
    <author>
      <name>Francisco Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the CHI 2020 Designing Interactions for the Ageing
  Populations Addressing Global Challenges Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.13057v3</id>
    <updated>2020-10-04T08:23:23Z</updated>
    <published>2020-08-29T21:25:16Z</published>
    <title>What are Data Insights to Professional Visualization Users?</title>
    <summary>  While many visualization researchers have attempted to define data insights,
little is known about how visualization users perceive them. We interviewed 23
professional users of end-user visualization platforms (e.g., Tableau and Power
BI) about their experiences with data insights. We report on seven
characteristics of data insights based on interviewees' descriptions. Grounded
in these characteristics, we propose practical implications for creating tools
that aim to automatically communicate data insights to users.
</summary>
    <author>
      <name>Po-Ming Law</name>
    </author>
    <author>
      <name>Alex Endert</name>
    </author>
    <author>
      <name>John Stasko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as IEEE VIS 2020 short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.13057v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13057v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.13060v2</id>
    <updated>2020-09-04T19:46:36Z</updated>
    <published>2020-08-29T21:28:16Z</published>
    <title>Characterizing Automated Data Insights</title>
    <summary>  Many researchers have explored tools that aim to recommend data insights to
users. These tools automatically communicate a rich diversity of data insights
and offer such insights for many different purposes. However, there is a lack
of structured understanding concerning what researchers of these tools mean by
"insight" and what tasks in the analysis workflow these tools aim to support.
We conducted a systematic review of existing systems that seek to recommend
data insights. Grounded in the review, we propose 12 types of automated
insights and four purposes of automating insights. We further discuss the
design opportunities emerged from our analysis.
</summary>
    <author>
      <name>Po-Ming Law</name>
    </author>
    <author>
      <name>Alex Endert</name>
    </author>
    <author>
      <name>John Stasko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as IEEE VIS 2020 short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.13060v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13060v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00722v1</id>
    <updated>2020-09-01T21:41:01Z</updated>
    <published>2020-09-01T21:41:01Z</published>
    <title>Encodable: Configurable Grammar for Visualization Components</title>
    <summary>  There are so many libraries of visualization components nowadays with their
APIs often different from one another. Could these components be more similar,
both in terms of the APIs and common functionalities? For someone who is
developing a new visualization component, how should the API look like? This
work drew inspiration from visualization grammar, decoupled the grammar from
its rendering engine and adapted it into a configurable grammar for individual
components called Encodable. Encodable helps component authors define grammar
for their components, and parse encoding specifications from users into utility
functions for the implementation. This paper explains the grammar design and
demonstrates how to build components with it.
</summary>
    <author>
      <name>Krist Wongsuphasawat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will be published at IEEE VIS (InfoVis/VAST/SciVis) 2020. ACM 2012
  CCS - Human-centered computing, Visualization, Visualization design and
  evaluation methods</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.01785v1</id>
    <updated>2020-09-03T16:50:25Z</updated>
    <published>2020-09-03T16:50:25Z</published>
    <title>Data-First Visualization Design Studies</title>
    <summary>  We introduce the notion of a data-first design study which is triggered by
the acquisition of real-world data instead of specific stakeholder analysis
questions. We propose an adaptation of the design study methodology framework
to provide practical guidance and to aid transferability to other data-first
design processes. We discuss opportunities and risks by reflecting on two of
our own data-first design studies. We review 64 previous design studies and
identify 16 of them as edge cases with characteristics that may indicate a
data-first design process in action.
</summary>
    <author>
      <name>Michael Oppermann</name>
    </author>
    <author>
      <name>Tamara Munzner</name>
    </author>
    <link href="http://arxiv.org/abs/2009.01785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02063v1</id>
    <updated>2020-09-04T08:33:38Z</updated>
    <published>2020-09-04T08:33:38Z</published>
    <title>ViS-Á-ViS : Detecting Similar Patterns in Annotated Literary Text</title>
    <summary>  We present a web-based system called ViS-\'A-ViS aiming to assist literary
scholars in detecting repetitive patterns in an annotated textual corpus.
Pattern detection is made possible using distant reading visualizations that
highlight potentially interesting patterns. In addition, the system uses
time-series alignment algorithms, and in particular, dynamic time warping
(DTW), to detect patterns automatically. We present a case-study where an
ancient Hebrew poetry corpus was manually annotated with figurative language
devices as metaphors and similes and then loaded into the system. Preliminary
results confirm the effectiveness of the system in analyzing the annotated data
and in detecting literary patterns and similarities.
</summary>
    <author>
      <name>Moshe Schorr</name>
    </author>
    <author>
      <name>Oren Mishali</name>
    </author>
    <author>
      <name>Benny Kimelfeld</name>
    </author>
    <author>
      <name>Ophir Münz-Manor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.02063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02094v2</id>
    <updated>2020-09-20T21:38:55Z</updated>
    <published>2020-09-04T10:21:23Z</published>
    <title>GlassViz: Visualizing Automatically-Extracted Entry Points for Exploring
  Scientific Corpora in Problem-Driven Visualization Research</title>
    <summary>  In this paper, we report the development of a model and a proof-of-concept
visual text analytics (VTA) tool to enhance documentdiscovery in a
problem-driven visualization research (PDVR) con-text. The proposed model
captures the cognitive model followed bydomain and visualization experts by
analyzing the interdisciplinarycommunication channel as represented by keywords
found in twodisjoint collections of research papers. High distributional
inter-collection similarities are employed to build informative
keywordassociations that serve as entry points to drive the exploration of
alarge document corpus. Our approach is demonstrated in the contextof research
on visualization for the digital humanities.
</summary>
    <author>
      <name>Alejandro Benito-Santos</name>
    </author>
    <author>
      <name>Roberto Therón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEEVIS 2020 short papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.02094v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02094v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02538v1</id>
    <updated>2020-09-05T14:22:31Z</updated>
    <published>2020-09-05T14:22:31Z</published>
    <title>A Visual Analytics Approach to Scheduling Customized Shuttle Buses via
  Perceiving Passengers' Travel Demands</title>
    <summary>  Shuttle buses have been a popular means to move commuters sharing similar
origins and destinations during periods of high travel demand. However,
planning and deploying reasonable, customized service bus systems becomes
challenging when the commute demand is rather dynamic. It is difficult, if not
impossible to form a reliable, unbiased estimation of user needs in such a case
using traditional modeling methods. We propose a visual analytics approach to
facilitating assessment of actual, varying travel demands and planning of night
customized shuttle systems. A preliminary case study verifies the efficacy of
our approach.
</summary>
    <author>
      <name>Qiangqiang Liu</name>
    </author>
    <author>
      <name>Quan Li</name>
    </author>
    <author>
      <name>Chunfeng Tang</name>
    </author>
    <author>
      <name>Huanbin Lin</name>
    </author>
    <author>
      <name>Xiaojuan Ma</name>
    </author>
    <author>
      <name>Tianjian Chen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VIS 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.02538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05233v1</id>
    <updated>2020-09-11T04:56:07Z</updated>
    <published>2020-09-11T04:56:07Z</published>
    <title>Narrative Transitions in Data Videos</title>
    <summary>  Transitions are widely used in data videos to seamlessly connect data-driven
charts or connect visualizations and non-data-driven motion graphics. To inform
the transition designs in data videos, we conduct a content analysis based on
more than 3500 clips extracted from 284 data videos. We annotate visualization
types and transition designs on these segments, and examine how these
transitions help make connections between contexts. We propose a taxonomy of
transitions in data videos, where two transition categories are defined in
building fluent narratives by using visual variables.
</summary>
    <author>
      <name>Junxiu Tang</name>
    </author>
    <author>
      <name>Lingyun Yu</name>
    </author>
    <author>
      <name>Tan Tang</name>
    </author>
    <author>
      <name>Xinhuan Shu</name>
    </author>
    <author>
      <name>Lu Ying</name>
    </author>
    <author>
      <name>Yuhua Zhou</name>
    </author>
    <author>
      <name>Peiran Ren</name>
    </author>
    <author>
      <name>Yingcai Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the Proceedings of IEEE VIS 2020 Short Papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.05233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05349v1</id>
    <updated>2020-09-11T11:28:53Z</updated>
    <published>2020-09-11T11:28:53Z</published>
    <title>Alfie: An Interactive Robot with a Moral Compass</title>
    <summary>  This work introduces Alfie, an interactive robot that is capable of answering
moral (deontological) questions of a user. The interaction of Alfie is designed
in a way in which the user can offer an alternative answer when the user
disagrees with the given answer so that Alfie can learn from its interactions.
Alfie's answers are based on a sentence embedding model that uses
state-of-the-art language models, e.g. Universal Sentence Encoder and BERT.
Alfie is implemented on a Furhat Robot, which provides a customizable user
interface to design a social robot.
</summary>
    <author>
      <name>Cigdem Turan</name>
    </author>
    <author>
      <name>Patrick Schramowski</name>
    </author>
    <author>
      <name>Constantin Rothkopf</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3382507.3421163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3382507.3421163" rel="related"/>
    <link href="http://arxiv.org/abs/2009.05349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05802v3</id>
    <updated>2020-11-20T03:49:57Z</updated>
    <published>2020-09-12T14:32:00Z</published>
    <title>Learning Daily Calorie Intake Standard using a Mobile Game</title>
    <summary>  Mobile games can contribute to learning at greater success. In this paper, we
have developed and evaluated a novel educational game, named FoodCalorie, to
learn the food calorie intake standards. Our game is aimed to learn the calorie
values of various traditional Bangladeshi foods and the calorie intake standard
that varies with age and gender. Our study confirms the finding of existing
studies that game-based learning can enhance the learning experience.
</summary>
    <author>
      <name>Anik Das</name>
    </author>
    <author>
      <name>Sumaiya Amin</name>
    </author>
    <author>
      <name>Muhammad Ashad Kabir</name>
    </author>
    <author>
      <name>Md. Sabir Hossain</name>
    </author>
    <author>
      <name>Mohammad Mainul Islam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4018/IJGBL.2021040104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4018/IJGBL.2021040104" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Game-Based Learning, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.05802v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05802v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.06272v1</id>
    <updated>2020-09-14T08:56:55Z</updated>
    <published>2020-09-14T08:56:55Z</published>
    <title>Implications of ageing for the design of cognitive interaction systems</title>
    <summary>  We are living longer in times of the biggest technological revolution
humanity had ever seen before. Trying to understand how these two facts
interact with each other, or more specifically, trying to maximise the benefits
that new developments could potentially offer for the enhancement of the
quality of life of older adults, is a task on which we have already begun to
work. In particular, the rapid growth of cognitive interaction systems (CISs),
technologies that learn and interact with humans to extend what human and
machine could do on their own, offers a promising landscape of possibilities.
</summary>
    <author>
      <name>L. Morillo-Mendez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ageing in a changing society: Interdisciplinary popular science
  contributions from the Newbreed research school, E. Kristoffersson and T.
  Strandberg (eds)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.06272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.06272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.06494v1</id>
    <updated>2020-09-14T14:54:30Z</updated>
    <published>2020-09-14T14:54:30Z</published>
    <title>Play Music An HCI Oriented Evaluation of Googles Default Music Player
  Interface</title>
    <summary>  The work embodied in this paper attempts to suggest a few improvements to the
playlist creation task interface of the Google Play Music Android application
based on recommended practices encountered in the Human-Computer Interaction
discipline. The improvements are largely centered on intuitive navigation and
selection actions, in order to facilitate a smoother experience in creating,
ordering, and adding to music playlists. The work records the efforts in
need-finding, design brainstorming, and prototype design and evaluation. The
work was undertaken over a single design life cycle, and is an attempt at
applying recommended practices in HCI to a widely used real world application.
</summary>
    <author>
      <name>Venkatesh Vijaykumar</name>
    </author>
    <link href="http://arxiv.org/abs/2009.06494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.06494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.06781v2</id>
    <updated>2021-01-18T21:42:23Z</updated>
    <published>2020-09-14T22:56:47Z</published>
    <title>Pilot: Winner of the Human-Agent Negotiation Challenge at IJCAI 2020</title>
    <summary>  This document describes our agent Pilot, winner of the Human-Agent
Negotiation Challenge at ANAC, IJCAI 2020. Pilot is a virtual human that
participates in a sequence of three negotiations with a human partner. Our
system is based on the Interactive Arbitration Guide Online (IAGO) negotiation
framework. We leverage prior Affective Computing and Psychology research in
negotiations to guide various key principles that define the behavior and
personality of our agent.
</summary>
    <author>
      <name>Kushal Chawla</name>
    </author>
    <author>
      <name>Gale Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Winner at ANAC, IJCAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.06781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.06781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.07342v1</id>
    <updated>2020-09-15T20:22:16Z</updated>
    <published>2020-09-15T20:22:16Z</published>
    <title>Scatterplot Selection Applying a Graph Coloring Problem</title>
    <summary>  Scatterplot selection is an effective approach to represent essential
portions of multidimensional data in a limited display space. Various metrics
for evaluation of scatterplots such as scagnostics have been presented and
applied to scatterplot selection. This paper presents a new scatterplot
selection technique that applies multiple metrics. The technique firstly
calculates scores of scatterplots with multiple metrics and then constructs a
graph by connecting similar scatterplots. The technique applies a graph
coloring problem so that different colors are assigned to similar scatterplots.
We can extract a set of various scatterplots by selecting them that the
specific same color is assigned. This paper introduces visualization examples
with a retail dataset containing multidimensional climate and sales values.
</summary>
    <author>
      <name>Takayuki Itoh</name>
    </author>
    <author>
      <name>Asuka Nakabayashi</name>
    </author>
    <author>
      <name>Mariko Hagita</name>
    </author>
    <link href="http://arxiv.org/abs/2009.07342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.07342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10261v2</id>
    <updated>2020-09-26T01:48:17Z</updated>
    <published>2020-09-22T01:15:51Z</published>
    <title>Influences of Temporal Factors on GPS-based Human Mobility Lifestyle</title>
    <summary>  Analysis of human mobility from GPS trajectories becomes crucial in many
aspects such as policy planning for urban citizens, location-based service
recommendation/prediction, and especially mitigating the spread of biological
and mobile viruses. In this paper, we propose a method to find temporal factors
affecting the human mobility lifestyle. We collected GPS data from 100
smartphone users in Japan. We designed a model that consists of 13 temporal
patterns. We then applied a multiple linear regression and found that people
tend to keep their mobility habits on Thursday and the days in the second week
of a month but tend to lose their habits on Friday. We also explained some
reasons behind these findings.
</summary>
    <author>
      <name>Tran Phuong Thao</name>
    </author>
    <link href="http://arxiv.org/abs/2009.10261v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10261v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11404v1</id>
    <updated>2020-09-23T22:29:04Z</updated>
    <published>2020-09-23T22:29:04Z</published>
    <title>Balancing simulation and gameplay -- applying game user research to
  LeukemiaSIM</title>
    <summary>  A bioinformatics researcher and a game design researcher walk into a lab...
This paper shares two case-studies of a collaboration between a bioinformatics
researcher who is developing a set of educational VR simulations for youth and
a consultative game design researcher with a background in games User Research
(GUR) techniques who assesses and iteratively improves the player experience in
the simulations. By introducing games-based player engagement strategies, the
two researchers improve the (re)playability of these VR simulations to
encourage greater player engagement and retention.
</summary>
    <author>
      <name>Erin Brintnell</name>
    </author>
    <author>
      <name>Owen Brierley</name>
    </author>
    <author>
      <name>Neil Christensen</name>
    </author>
    <author>
      <name>Christian Jacob</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.12152v1</id>
    <updated>2020-09-25T12:02:15Z</updated>
    <published>2020-09-25T12:02:15Z</published>
    <title>Ethical conceptual replication of visualization research considering
  sources of methodological bias and practical significance</title>
    <summary>  General design principles for visualization have been relatively
well-established based on a combination of cognitive and perceptual theory and
empirical evaluations over the past 20 years. To determine how these principles
hold up across use contexts and end-users, I argue that we should emphasize
conceptual replication focused on determining practical significance and
reducing methodological biases. This shift in thinking aims to determine how
design principles interact with methodological approaches, laying the
groundwork for visualization meta-science.
</summary>
    <author>
      <name>Ian T. Ruginski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to contribute to the discussion at VisPsych2020, a workshop
  at the IEEE Visualization Conference 2020 Salt Lake City. For associated
  code, see https://osf.io/ebwx9/</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.12152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.12152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13219v1</id>
    <updated>2020-09-28T11:15:09Z</updated>
    <published>2020-09-28T11:15:09Z</published>
    <title>Metrics for Multi-Touch Input Technologies</title>
    <summary>  Multi-touch input technologies are becoming popular with the increased
interest in touchscreen- and touchpad-based devices. A great deal of work has
been done on different multi-touch technologies, and researchers and
practitioners are frequently coming up with new ones. However, it is almost
impossible to compare such technologies due to the absence of multi-touch
performance metrics. Designers usually use their own methods to report their
techniques' performances. Moreover, multi-touch interaction was never modeled.
That makes it impossible for designers to predict the performance of a new
technology before developing it, costing them valuable time, effort, and money.
This article discusses the necessity of having dedicated performance metrics
and prediction model for multi-touch technologies, and ways of approaching
that.
</summary>
    <author>
      <name>Ahmed Sabbir Arif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2010</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13646v1</id>
    <updated>2020-09-28T21:45:36Z</updated>
    <published>2020-09-28T21:45:36Z</published>
    <title>A Psychology of Visualization or (External) Representation?</title>
    <summary>  What is a visualization? There is limited utility in trifling with
definitions, except insofar as one serves as a tool for communicating and
conceptualizing our subject matter; a statement of identity for a community. To
establish Visualization Psychology as a viable inter-disciplinary research
programme, we must first define the object(s) of our collective inquiry. I
propose that while we might refer to the study of "visualization" for the
term's colloquial accessibility and pragmatic alignment with other fields, we
should consider for exploration a class of artifacts and corresponding
processes more expansive and profound: external representations. What follows
is an argument for the study of external representation as the foundation for a
new interdisciplinary endeavor, and approach to mapping the corresponding
problem space.
</summary>
    <author>
      <name>Amy Rae Fox</name>
    </author>
    <link href="http://arxiv.org/abs/2009.13646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00488v1</id>
    <updated>2020-10-01T15:29:37Z</updated>
    <published>2020-10-01T15:29:37Z</published>
    <title>Developing Effective Community Network Analysis Tools According to
  Visualization Psychology</title>
    <summary>  Visualization is a useful technology in health science, and especially for
community network analysis. Because visualization applications in healthcare
are typically risk-averse, health psychologists can play a significant role in
ensuring appropriate and effective uses of visualization techniques in
healthcare. In this paper, we examine the role of health psychologists in the
triangle of "health science", "visualization technology", and "visualization
psychology". We conclude that health psychologists can use visualization to aid
data intelligence workflows in healthcare and health psychology, while
researching into visualization psychology to aid the improvement and
optimization of data visualization processes.
</summary>
    <author>
      <name>Darren J. Edwards</name>
    </author>
    <author>
      <name>Min Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first author gives a presentation at IEEE VIS2020 Workshop on
  Visualization Psychology based on this arXiv report</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01316v1</id>
    <updated>2020-10-03T10:05:58Z</updated>
    <published>2020-10-03T10:05:58Z</published>
    <title>Accounts, Accountability and Agency for Safe and Ethical AI</title>
    <summary>  We examine the problem of explainable AI (xAI) and explore what delivering
xAI means in practice, particularly in contexts that involve formal or informal
and ad-hoc collaboration where agency and accountability in decision-making are
achieved and sustained interactionally. We use an example from an earlier study
of collaborative decision-making in screening mammography and the difficulties
users faced when trying to interpret the behavior of an AI tool to illustrate
the challenges of delivering usable and effective xAI. We conclude by setting
out a study programme for future research to help advance our understanding of
xAI requirements for safe and ethical AI.
</summary>
    <author>
      <name>Rob Procter</name>
    </author>
    <author>
      <name>Mark Rouncefield</name>
    </author>
    <author>
      <name>Peter Tolmie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01323v1</id>
    <updated>2020-10-03T11:08:14Z</updated>
    <published>2020-10-03T11:08:14Z</published>
    <title>The Design of Tangible Digital Musical Instruments</title>
    <summary>  Here we present guidelines that highlight the impact of haptic feedback upon
the experiences of computer musicians using Digital Musical Instruments (DMIs).
In this context, haptic feedback offers a tangible, bi-directional exchange
between a musician and a DMI. We propose that by adhering to and exploring
these guidelines the application of haptic feedback can enhance and augment the
physical and affective experiences of a musician in interactions with these
devices. It has been previously indicated that in the design of haptic DMIs,
the experiences and expectations of a musician must be considered for the
creation of tangible DMIs and that haptic feedback can be used to address the
physical-digital divide that currently exists between users of such
instruments.
</summary>
    <author>
      <name>Gareth W. Young</name>
    </author>
    <author>
      <name>Katie Crowley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MusTWork 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01570v1</id>
    <updated>2020-10-04T12:55:43Z</updated>
    <published>2020-10-04T12:55:43Z</published>
    <title>Problems and Prospects for Intimate Musical Control of Computers</title>
    <summary>  In this paper we describe our efforts towards the development of live
performance computer-based musical instrumentation. Our design criteria include
initial ease of use coupled with a long term potential for virtuosity, minimal
and low variance latency, and clear and simple strategies for programming the
relationship between gesture and musical result. We present custom controllers
and unique adaptations of standard gestural interfaces, a programmable
connectivity processor, a communications protocol called Open Sound Control
(OSC), and a variety of metaphors for musical control. We further describe
applications of our technology to a variety of real musical performances and
directions for future research.
</summary>
    <author>
      <name>David Wessel</name>
    </author>
    <author>
      <name>Matthew Wright</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176382</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176382" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01571v1</id>
    <updated>2020-10-04T12:56:14Z</updated>
    <published>2020-10-04T12:56:14Z</published>
    <title>Input Devices for Musical Expression: Borrowing Tools from HCI</title>
    <summary>  This paper reviews the existing literature on input device evaluation and
design in human-computer interaction (HCI) and discusses possible applications
of this knowledge to the design and evaluation of new interfaces for musical
expression. Specifically, a set of musical tasks is suggested to allow the
evaluation of different existing controllers.
</summary>
    <author>
      <name>Nicola Orio</name>
    </author>
    <author>
      <name>Norbert Schnell</name>
    </author>
    <author>
      <name>Marcelo M. Wanderley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176370</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176370" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01574v1</id>
    <updated>2020-10-04T12:57:08Z</updated>
    <published>2020-10-04T12:57:08Z</published>
    <title>The Accordiatron: A MIDI Controller For Interactive Music</title>
    <summary>  The Accordiatron is a new MIDI controller for real-time performance based on
the paradigm of a conventional squeeze box or concertina. It translates the
gestures of a performer to the standard communication protocol of MIDI,
allowing for flexible mappings of performance data to sonic parameters. When
used in conjunction with a realtime signal processing environment, the
Accordiatron becomes an expressive, versatile musical instrument. A combination
of sensory outputs providing both discrete and continuous data gives the subtle
expressiveness and control necessary for interactive music.
</summary>
    <author>
      <name>Michael Gurevich</name>
    </author>
    <author>
      <name>Stephan von Muehlen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176364</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176364" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01576v1</id>
    <updated>2020-10-04T12:57:58Z</updated>
    <published>2020-10-04T12:57:58Z</published>
    <title>Body, Clothes, Water, and Toys: Media Towards Natural Music Expressions
  with Digital Sounds</title>
    <summary>  In this paper, we introduce our research challenges for creating new musical
instruments using everyday-life media with intimate interfaces, such as the
self-body, clothes, water and stuffed toys. Various sensor technologies
including image processing and general touch sensitive devices are employed to
exploit these interaction media. The focus of our effort is to provide
user-friendly and enjoyable experiences for new music and sound performances.
Multimodality of musical instruments is explored in each attempt. The degree of
controllability in the performance and the richness of expressions are also
discussed for each installation.
</summary>
    <author>
      <name>Kenji Mase</name>
    </author>
    <author>
      <name>Tomoko Yonezawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176368</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176368" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01578v1</id>
    <updated>2020-10-04T12:58:51Z</updated>
    <published>2020-10-04T12:58:51Z</published>
    <title>Creating Contexts of Creativity: Musical Composition with Modular
  Components</title>
    <summary>  This paper describes a series of projects that explore the possibilities of
musical expression through the combination of pre-composed, interlocking,
modular components. In particular, this paper presents a modular soundtrack
recently composed by the author for "Currents of Creativity," a permanent
interactive video wall installation at the Pope John Paul II Cultural Center
which is slated to open Easter 2001 in Washington, DC.
</summary>
    <author>
      <name>Gideon D'Arcangelo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176360</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176360" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01579v1</id>
    <updated>2020-10-04T12:59:20Z</updated>
    <published>2020-10-04T12:59:20Z</published>
    <title>New Musical Interfaces and New Music-making Paradigms</title>
    <summary>  The conception and design of new musical interfaces is a multidisciplinary
area that tightly relates technology and artistic creation. In this paper, the
author first exposes some of the questions he has posed himself during more
than a decade experience as a performer, composer, interface and software
designer, and educator. Finally, he illustrates these topics with some examples
of his work.
</summary>
    <author>
      <name>Sergi Jordà</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176366</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176366" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01944v1</id>
    <updated>2020-10-05T12:14:37Z</updated>
    <published>2020-10-05T12:14:37Z</published>
    <title>Actors in VR storytelling</title>
    <summary>  Virtual Reality (VR) storytelling enhances the immersion of users into
virtual environments (VE). Its use in virtual cultural heritage presentations
helps the revival of the genius loci (the spirit of the place) of cultural
monuments. This paper aims to show that the use of actors in VR storytelling
adds to the quality of user experience and improves the edutainment value of
virtual cultural heritage applications. We will describe the Baiae dry visit
application which takes us to a time travel in the city considered by the Roman
elite as "Little Rome (Pusilla Roma)" and presently is only partially preserved
under the sea.
</summary>
    <author>
      <name>Selma Rizvic</name>
    </author>
    <author>
      <name>Dusanka Boskovic</name>
    </author>
    <author>
      <name>Fabio Bruno</name>
    </author>
    <author>
      <name>Barbara Davidde Petriaggi</name>
    </author>
    <author>
      <name>Sanda Sljivo</name>
    </author>
    <author>
      <name>Marco Cozza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VS-Games.2019.8864520</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VS-Games.2019.8864520" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print version</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.02207v1</id>
    <updated>2020-10-04T13:00:04Z</updated>
    <published>2020-10-04T13:00:04Z</published>
    <title>The Interactive Dance Club: Avoiding Chaos In A Multi Participant
  Environment</title>
    <summary>  In 1998 we designed enabling technology and a venue concept that allowed
several participants to influence a shared musical and visual experience. Our
primary goal was to deliver musically coherent and visually satisfying results
from several participants' input. The result, the Interactive Dance Club, ran
for four nights at the ACM SIGGRAPH 98 convention in Orlando, Florida. In this
paper we will briefly describe the Interactive Dance Club, our "10 Commandments
of Interactivity," and what we learned from its premiere at SIGGRAPH 98.
</summary>
    <author>
      <name>Ryan Ulyate</name>
    </author>
    <author>
      <name>David Bianciardi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176378</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176378" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.02207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.02207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.02401v1</id>
    <updated>2020-10-06T00:12:21Z</updated>
    <published>2020-10-06T00:12:21Z</published>
    <title>Tactical Patterns for Grassroots Urban Repair</title>
    <summary>  The process of revitalizing cities in the United States suffers from balky
and unresponsive processes---de jure egalitarian but de facto controlled and
mediated by city officials and powerful interests, not residents. We argue
that, instead, our goal should be to put city planning in the hands of the
people, and to that end, give ordinary residents pattern-based planning tools
to help them redesign (and repair) their urban surrounds. Through this,
residents can explore many disparate ideas, try them, and, if successful,
replicate them, enabling bottom-up city planning through direct action. We
describe a prototype for such a tool that leverages classic patterns to enable
city planning by residents, using case studies from Los Angeles as guides for
both the problem and potential solution.
</summary>
    <author>
      <name>Sarah Cooney</name>
    </author>
    <author>
      <name>Barath Raghavan</name>
    </author>
    <link href="http://arxiv.org/abs/2010.02401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.02401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03213v1</id>
    <updated>2020-10-07T06:47:42Z</updated>
    <published>2020-10-07T06:47:42Z</published>
    <title>Designing, Playing, and Performing with a Vision-based Mouth Interface</title>
    <summary>  The role of the face and mouth in speech production as well asnon-verbal
communication suggests the use of facial action tocontrol musical sound. Here
we document work on theMouthesizer, a system which uses a headworn
miniaturecamera and computer vision algorithm to extract shapeparameters from
the mouth opening and output these as MIDIcontrol changes. We report our
experience with variousgesture-to-sound mappings and musical applications,
anddescribe a live performance which used the Mouthesizerinterface.
</summary>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <author>
      <name>Michael Haehnel</name>
    </author>
    <author>
      <name>Nobuji Tetsutani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176529</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176529" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.03213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03223v1</id>
    <updated>2020-10-07T07:04:07Z</updated>
    <published>2020-10-07T07:04:07Z</published>
    <title>Sonification of Facial Actions for Musical Expression</title>
    <summary>  The central role of the face in social interaction and non-verbal
communication suggests we explore facial action as a means of musical
expression. This paper presents the design, implementation, and preliminary
studies of a novel system utilizing face detection and optic flow algorithms to
associate facial movements with sound synthesis in a topographically specific
fashion. We report on our experience with various gesture-to-sound mappings and
applications, and describe our preliminary experiments at musical performance
using the system.
</summary>
    <author>
      <name>Mathias Funk</name>
    </author>
    <author>
      <name>Kazuhiro Kuwabara</name>
    </author>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176749</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176749" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2005 (NIME-05)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.03223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03265v1</id>
    <updated>2020-10-07T08:36:43Z</updated>
    <published>2020-10-07T08:36:43Z</published>
    <title>A Novel Face-tracking Mouth Controller and its Application to
  Interacting with Bioacoustic Models</title>
    <summary>  We describe a simple, computationally light, real-time system for tracking
the lower face and extracting information about the shape of the open mouth
from a video sequence. The system allows unencumbered control of audio
synthesis modules by the action of the mouth. We report work in progress to use
the mouth controller to interact with a physical model of sound production by
the avian syrinx.
</summary>
    <author>
      <name>Gamhewage C. de Silva</name>
    </author>
    <author>
      <name>Tamara Smyth</name>
    </author>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176666</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176666" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2004 (NIME-04)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.03265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04096v1</id>
    <updated>2020-10-08T16:25:02Z</updated>
    <published>2020-10-08T16:25:02Z</published>
    <title>Did You Get The Gist Of It? Understanding How Visualization Impacts
  Decision-Making</title>
    <summary>  As visualization researchers evaluate the impact of visualization design on
decision-making, they often hold a one-dimensional perspective on the cognitive
processes behind making a decision. Several psychological and economical
researchers have shown that to make decisions, people rely on quantitative
reasoning as well as gist-based intuition -- two systems that operate in
parallel. In this position paper, we discuss decision theories and provide
suggestions to bridge the gap between the evaluation of decision-making in
visualization and psychology research. The goal is to question the limits of
our knowledge and to advocate for a more nuanced understanding of
decision-making with visualization.
</summary>
    <author>
      <name>Melanie Bancilhon</name>
    </author>
    <author>
      <name>Alvitta Ottley</name>
    </author>
    <link href="http://arxiv.org/abs/2010.04096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04101v1</id>
    <updated>2020-10-08T16:37:30Z</updated>
    <published>2020-10-08T16:37:30Z</published>
    <title>Human-Drone Interactions with Semi-Autonomous Cohorts of Collaborating
  Drones</title>
    <summary>  Research in human-drone interactions has primarily focused on cases in which
a person interacts with a single drone as an active controller, recipient of
information, or a social companion; or cases in which an individual, or a team
of operators interacts with a swarm of drones as they perform some coordinated
flight patterns. In this position paper we explore a third scenario in which
multiple humans and drones collaborate in an emergency response scenario. We
discuss different types of interactions, and draw examples from current
DroneResponse project.
</summary>
    <author>
      <name>Jane Cleland-Huang</name>
    </author>
    <author>
      <name>Ankit Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Interdisciplinary Workshop on Human-Drone
  Interaction co-located with the 2020 ACM CHI Conference on Human Factors in
  Computing Systems (CHI 2020) - http://ceur-ws.org/Vol-2617/</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.04101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04276v1</id>
    <updated>2020-10-08T21:57:52Z</updated>
    <published>2020-10-08T21:57:52Z</published>
    <title>interface : Electronic Chamber Ensemble</title>
    <summary>  This paper presents the interface developments and music of the duo
"interface," formed by Curtis Bahn and Dan Trueman. We describe gestural
instrument design, interactive performance interfaces for improvisational
music, spherical speakers (multi-channel, outward-radiating geodesic speaker
arrays) and Sensor-Speaker-Arrays (SenSAs: combinations of various sensor
devices with spherical speaker arrays). We discuss the concept, design and
construction of these systems, and, give examples from several newly published
CDs of work by Bahn and Trueman.
</summary>
    <author>
      <name>Curtis Bahn</name>
    </author>
    <author>
      <name>Dan Trueman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176355</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176355" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.04276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.06279v1</id>
    <updated>2020-10-13T10:45:16Z</updated>
    <published>2020-10-13T10:45:16Z</published>
    <title>An Immersive Virtual Environment for Collaborative Geovisualization</title>
    <summary>  This paper presents an immersive virtual reality environment that can be used
to develop collaborative educational applications. Multiple users can
collaborate within the virtual shared space and communicate with each other
through voice. To asses the feasibility of the collaborative environment a
novel case-study concerned the education of a geography was developed and
evaluated. The geovisualization experiment scenario explores the possibility of
learning geography in a collaborative virtual environment. A user-study with 30
participants was performed. Participants evaluated and commented on the
usability and interaction methods used within the virtual environment.
</summary>
    <author>
      <name>Milan Dolezal</name>
    </author>
    <author>
      <name>Jiri Chmelik</name>
    </author>
    <author>
      <name>Fotis Liarokapis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VS-GAMES.2017.8056613</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VS-GAMES.2017.8056613" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.06279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12324v2</id>
    <updated>2020-12-28T16:31:36Z</updated>
    <published>2020-10-15T12:09:53Z</published>
    <title>The power of pictures: using ML assisted image generation to engage the
  crowd in complex socioscientific problems</title>
    <summary>  Human-computer image generation using Generative Adversarial Networks (GANs)
is becoming a well-established methodology for casual entertainment and open
artistic exploration. Here, we take the interaction a step further by weaving
in carefully structured design elements to transform the activity of
ML-assisted imaged generation into a catalyst for large-scale popular dialogue
on complex socioscientific problems such as the United Nations Sustainable
Development Goals (SDGs) and as a gateway for public participation in research.
</summary>
    <author>
      <name>Janet Rafner</name>
    </author>
    <author>
      <name>Lotte Philipsen</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Joel Simon</name>
    </author>
    <author>
      <name>Jacob Sherson</name>
    </author>
    <link href="http://arxiv.org/abs/2010.12324v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12324v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13035v1</id>
    <updated>2020-10-25T04:48:47Z</updated>
    <published>2020-10-25T04:48:47Z</published>
    <title>Enactive Mandala: Audio-visualizing Brain Waves</title>
    <summary>  We are exploring the design and implementation of artificial expressions,
kinetic audio-visual representations of real-time physiological data that
reflect emotional and cognitive state. In this work, we demonstrate a
prototype, the Enactive Mandala, which maps real-time EEG signals to modulate
ambient music and animated visual music. Transparent real-time audio-visual
feedback of brainwave qualities supports intuitive insight into the connection
between thoughts and physiological states.
</summary>
    <author>
      <name>Tomohiro Tokunaga</name>
    </author>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1178678</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1178678" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.13035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13111v1</id>
    <updated>2020-10-25T13:08:42Z</updated>
    <published>2020-10-25T13:08:42Z</published>
    <title>Develop Health Monitoring and Management System to Track Health
  Condition and Nutrient Balance for School Students</title>
    <summary>  Health Monitoring and Management System (HMMS) is an emerging technology for
decades. Researchers are working on this field to track health conditions for
different users. Researchers emphasize tracking health conditions from an early
stage to the human body. Therefore, different research works have been
conducted to establish HMMS in schools. Researchers propose different
frameworks and technologies for their HMMS to check student's health condition.
In this paper, we introduce a complete and scalable HMMS to track health
conditions and nutrient balance for students from primary school. We define
procedures step by step to establish a robust HMMS where big data methodologies
can be used for further prediction for diseases.
</summary>
    <author>
      <name>Mohammad Ali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 3 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.13111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13197v1</id>
    <updated>2020-10-25T19:13:01Z</updated>
    <published>2020-10-25T19:13:01Z</published>
    <title>Gestop : Customizable Gesture Control of Computer Systems</title>
    <summary>  The established way of interfacing with most computer systems is a mouse and
keyboard. Hand gestures are an intuitive and effective touchless way to
interact with computer systems. However, hand gesture based systems have seen
low adoption among end-users primarily due to numerous technical hurdles in
detecting in-air gestures accurately. This paper presents Gestop, a framework
developed to bridge this gap. The framework learns to detect gestures from
demonstrations, is customizable by end-users and enables users to interact in
real-time with computers having only RGB cameras, using gestures.
</summary>
    <author>
      <name>Sriram Krishna</name>
    </author>
    <author>
      <name>Nishant Sinha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3430984.3430993</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3430984.3430993" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, to appear in the proceedings of the 8th ACM IKDD
  CODS and 26th COMAD (CODS-COMAD '21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.13197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13352v1</id>
    <updated>2020-10-26T05:37:18Z</updated>
    <published>2020-10-26T05:37:18Z</published>
    <title>The Age-related Differences in Web Information Search Process</title>
    <summary>  Older adults' need for quality health information has never been more
critical as during the COVID-19 pandemic. Yet, they are susceptible to the
wide-spread misinformation disseminated through search engines and social
media. To build a search-related behavioral profile of older adults, this
article surveys the empirical research on age-related differences in query
formulation, search strategies, information evaluation, and susceptibility to
misinformation effects. It also decomposes the mechanisms (i.e., cognitive
changes, development goal shift) and moderators (i.e., search task and
interface design) of such differences. To inform the design of information
systems to improve older adults' information search experience, we discuss
opportunities for future research.
</summary>
    <author>
      <name>Zhaopeng Xing</name>
    </author>
    <author>
      <name>Xiaojun Yuan</name>
    </author>
    <author>
      <name>Lisa Vizer</name>
    </author>
    <link href="http://arxiv.org/abs/2010.13352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.03969v1</id>
    <updated>2020-11-08T12:35:52Z</updated>
    <published>2020-11-08T12:35:52Z</published>
    <title>Chatbots as conversational healthcare services</title>
    <summary>  Chatbots are emerging as a promising platform for accessing and delivering
healthcare services. The evidence is in the growing number of publicly
available chatbots aiming at taking an active role in the provision of
prevention, diagnosis, and treatment services. This article takes a closer look
at how these emerging chatbots address design aspects relevant to healthcare
service provision, emphasizing the Human-AI interaction aspects and the
transparency in AI automation and decision making.
</summary>
    <author>
      <name>Mlađan Jovanović</name>
    </author>
    <author>
      <name>Marcos Baez</name>
    </author>
    <author>
      <name>Fabio Casati</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MIC.2020.3037151</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MIC.2020.3037151" rel="related"/>
    <link href="http://arxiv.org/abs/2011.03969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.03969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05039v1</id>
    <updated>2020-11-10T11:07:33Z</updated>
    <published>2020-11-10T11:07:33Z</published>
    <title>OnionBot: A System for Collaborative Computational Cooking</title>
    <summary>  An unsolved challenge in cooking automation is designing for shared kitchen
workspaces. In particular, robots struggle with dexterity in the unstructured
and dynamic kitchen environment. We propose that human-machine collaboration
can be achieved without robotic manipulation. We describe a novel system design
using computer vision to inform intelligent cooking interventions. This
human-centered approach does not require actuators and promotes dynamic,
natural collaboration. We show that automation that assists user-led actions
can offer meaningful cooking assistance and can generate the image databases
needed for fully autonomous robotic systems of the future. We provide an open
source implementation of our work and encourage the research community to build
upon it.
</summary>
    <author>
      <name>Bennet Cobley</name>
    </author>
    <author>
      <name>David Boyle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures. Open source repository is available at
  https://github.com/onionbot</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07510v1</id>
    <updated>2020-11-15T12:18:47Z</updated>
    <published>2020-11-15T12:18:47Z</published>
    <title>Model-Driven Synthesis for Programming Tutors</title>
    <summary>  When giving automated feedback to a student working on a beginner's exercise,
many programming tutors run into a completeness problem. On the one hand, we
want a student to experiment freely. On the other hand, we want a student to
write her program in such a way that we can provide constructive feedback. We
propose to investigate how we can overcome this problem by using program
synthesis, which we use to generate correct solutions that closely match a
student program, and give feedback based on the results.
</summary>
    <author>
      <name>Niek Mulleners</name>
    </author>
    <author>
      <name>Johan Jeuring</name>
    </author>
    <author>
      <name>Bastiaan Heeren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented at HATRA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.07510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.08324v1</id>
    <updated>2020-11-16T22:55:49Z</updated>
    <published>2020-11-16T22:55:49Z</published>
    <title>Examining the Feasibility of Off-the-Shelf Algorithms for Masking
  Directly Identifiable Information in Social Media Data</title>
    <summary>  The identification and removal/replacement of protected information from
social media data is an understudied problem, despite being desirable from an
ethical and legal perspective. This paper identifies types of potentially
directly identifiable information (inspired by protected health information in
clinical texts) contained in tweets that may be readily removed using
off-the-shelf algorithms, introduces an English dataset of tweets annotated for
identifiable information, and compiles these off-the-shelf algorithms into a
tool (Nightjar) to evaluate the feasibility of using Nightjar to remove
directly identifiable information from the tweets. Nightjar as well as the
annotated data can be retrieved from https://bitbucket.org/mdredze/nightjar.
</summary>
    <author>
      <name>Rachel Dorn</name>
    </author>
    <author>
      <name>Alicia L. Nobles</name>
    </author>
    <author>
      <name>Masoud Rouhizadeh</name>
    </author>
    <author>
      <name>Mark Dredze</name>
    </author>
    <link href="http://arxiv.org/abs/2011.08324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09088v1</id>
    <updated>2020-11-18T05:02:20Z</updated>
    <published>2020-11-18T05:02:20Z</published>
    <title>Three Patterns to Support Empathy in Computer-Mediated Human Interaction</title>
    <summary>  We present three patterns for computer-mediated interaction which we
discovered during the design and development of a platform for remote teaching
and learning of kanji, the Chinese characters used in written Japanese. Our aim
in developing this system was to provide a basis for embodiment in remote
interaction, and in particular to support the experience of empathy by both
teacher and student. From this study, the essential elements are abstracted and
suggested as design patterns for other computer-mediated interaction systems.
</summary>
    <author>
      <name>Michael J. Lyons</name>
    </author>
    <author>
      <name>Daniel Kluender</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.4278447</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.4278447" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM CHI'04 Workshop Human-Computer-Human Interaction Patterns:
  Workshop on the Human Role in HCI Patterns Vienna, Austria, April 25-26, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.09088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.13779v1</id>
    <updated>2020-11-27T15:27:36Z</updated>
    <published>2020-11-27T15:27:36Z</published>
    <title>An Integrated Approach Towards the Construction of an HCI Methodological
  Framework</title>
    <summary>  We present a methodological framework aiming at the support of HCI
practitioners and researchers in selecting and applying the most appropriate
combination of HCI methods for particular problems. We highlight the need for a
clear and effective overview of methods and provide further discussion on
possible extensions that can support recent trends and needs, such as the focus
on specific application domains.
</summary>
    <author>
      <name>Tasos Spiliotopoulos</name>
    </author>
    <author>
      <name>Ian Oakley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 1st European Workshop on HCI Design and
  Evaluation, Limassol, Cyprus</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.13779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.13779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.02510v1</id>
    <updated>2021-02-04T09:40:41Z</updated>
    <published>2021-02-04T09:40:41Z</published>
    <title>User Interface Factors of Mobile UX: A Study with an Incident Reporting
  Application</title>
    <summary>  Smartphones are now ubiquitous, yet our understanding of user interface
factors that maximize mobile user experience (UX), is still limited. This work
presents a controlled experiment, which investigated factors that affect the
usability and UX of a mobile incident reporting app. The results indicate that
sequence of user interface elements matters while striving to increase UX, and
that there is no difference between tab and scrolling as navigation modalities
in short forms. These findings can serve as building blocks for
empirically-derived guidelines for mobile incident reporting.
</summary>
    <author>
      <name>Lasse Einfeldt</name>
    </author>
    <author>
      <name>Auriol Degbelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted for presentation at the 5th International Conference
  on Human Computer Interaction Theory and Applications (HUCAPP@VISIGRAPP 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.02510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.02510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.1.2; H.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03955v1</id>
    <updated>2021-02-08T00:35:59Z</updated>
    <published>2021-02-08T00:35:59Z</published>
    <title>A Probabilistic Interpretation of Motion Correlation Selection
  Techniques</title>
    <summary>  Motion correlation interfaces are those that present targets moving in
different patterns, which the user can select by matching their motion. In this
paper, we re-formulate the task of target selection as a probabilistic
inference problem. We demonstrate that previous interaction techniques can be
modelled using a Bayesian approach and that how modelling the selection task as
transmission of information can help us make explicit the assumptions behind
similarity measures. We propose ways of incorporating uncertainty into the
decision-making process and demonstrate how the concept of entropy can
illuminate the measurement of the quality of a design. We apply these
techniques in a case study and suggest guidelines for future work.
</summary>
    <author>
      <name>Eduardo Velloso</name>
    </author>
    <author>
      <name>Carlos Hitoshi Morimoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411764.3445184</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411764.3445184" rel="related"/>
    <link href="http://arxiv.org/abs/2102.03955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06391v2</id>
    <updated>2021-02-15T05:25:35Z</updated>
    <published>2021-02-12T08:28:28Z</published>
    <title>Multiversal views on language models</title>
    <summary>  The virtuosity of language models like GPT-3 opens a new world of possibility
for human-AI collaboration in writing. In this paper, we present a framework in
which generative language models are conceptualized as multiverse generators.
This framework also applies to human imagination and is core to how we read and
write fiction. We call for exploration into this commonality through new forms
of interfaces which allow humans to couple their imagination to AI to write,
explore, and understand non-linear fiction. We discuss the early insights we
have gained from actively pursuing this approach by developing and testing a
novel multiversal GPT-3-assisted writing interface.
</summary>
    <author>
      <name>Laria Reynolds</name>
    </author>
    <author>
      <name>Kyle McDonell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.06391v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06391v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10387v1</id>
    <updated>2021-02-20T16:56:24Z</updated>
    <published>2021-02-20T16:56:24Z</published>
    <title>Towards Teachable Conversational Agents</title>
    <summary>  The traditional process of building interactive machine learning systems can
be viewed as a teacher-learner interaction scenario where the machine-learners
are trained by one or more human-teachers. In this work, we explore the idea of
using a conversational interface to investigate the interaction between
human-teachers and interactive machine-learners. Specifically, we examine
whether teachable AI agents can reliably learn from human-teachers through
conversational interactions, and how this learning compare with traditional
supervised learning algorithms. Results validate the concept of teachable
conversational agents and highlight the factors relevant for the development of
machine learning systems that intend to learn from conversational interactions.
</summary>
    <author>
      <name>Nalin Chhibber</name>
    </author>
    <author>
      <name>Edith Law</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 3 Figures, 2 Tables, Presented at NeurIPS 2020: Human in the
  Loop Dialogue Systems Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.10387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10678v1</id>
    <updated>2021-02-21T20:38:20Z</updated>
    <published>2021-02-21T20:38:20Z</published>
    <title>Towards Immersive Virtual Reality Simulations of Bionic Vision</title>
    <summary>  Bionic vision is a rapidly advancing field aimed at developing visual
neuroprostheses ('bionic eyes') to restore useful vision to people who are
blind. However, a major outstanding challenge is predicting what people 'see'
when they use their devices. The limited field of view of current devices
necessitates head movements to scan the scene, which is difficult to simulate
on a computer screen. In addition, many computational models of bionic vision
lack biological realism. To address these challenges, we propose to embed
biologically realistic models of simulated prosthetic vision (SPV) in immersive
virtual reality (VR) so that sighted subjects can act as 'virtual patients' in
real-world tasks.
</summary>
    <author>
      <name>Justin Kasowski</name>
    </author>
    <author>
      <name>Nathan Wu</name>
    </author>
    <author>
      <name>Michael Beyeler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, to be presented at Augmented Humans</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.10678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10685v1</id>
    <updated>2021-02-21T21:04:16Z</updated>
    <published>2021-02-21T21:04:16Z</published>
    <title>EvoK: Connecting loved ones through Heart Rate sharing</title>
    <summary>  In this work, we present EvoK, a new way of sharing one's heart rate with
feedback from their close contacts to alleviate social isolation and
loneliness. EvoK consists of a pair of wearable prototype devices (i.e., sender
and receiver). The sender is designed as a headband enabling continuous sensing
of heart rate with aesthetic designs to maximize social acceptance. The
receiver is designed as a wristwatch enabling unobtrusive receiving of the
loved one's continuous heart rate with multi-modal notification systems.
</summary>
    <author>
      <name>Esha Shandilya</name>
    </author>
    <author>
      <name>Yiwen Wang</name>
    </author>
    <author>
      <name>Xuan Zhao</name>
    </author>
    <author>
      <name>Mingming Fan</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10787v1</id>
    <updated>2021-02-22T05:49:02Z</updated>
    <published>2021-02-22T05:49:02Z</published>
    <title>Fair and Responsible AI: A Focus on the Ability to Contest</title>
    <summary>  As the use of artificial intelligence (AI) in high-stakes decision-making
increases, the ability to contest such decisions is being recognised in AI
ethics guidelines as an important safeguard for individuals. Yet, there is
little guidance on how AI systems can be designed to support contestation. In
this paper we explain that the design of a contestation process is important
due to its impact on perceptions of fairness and satisfaction. We also consider
design challenges, including a lack of transparency as well as the numerous
design options that decision-making entities will be faced with. We argue for a
human-centred approach to designing for contestability to ensure that the needs
of decision subjects, and the community, are met.
</summary>
    <author>
      <name>Henrietta Lyons</name>
    </author>
    <author>
      <name>Eduardo Velloso</name>
    </author>
    <author>
      <name>Tim Miller</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.11819v1</id>
    <updated>2021-02-23T17:44:36Z</updated>
    <published>2021-02-23T17:44:36Z</published>
    <title>I Want My App That Way: Reclaiming Sovereignty Over Personal Devices</title>
    <summary>  Dark patterns in mobile apps take advantage of cognitive biases of end-users
and can have detrimental effects on people's lives. Despite growing research in
identifying remedies for dark patterns and established solutions for desktop
browsers, there exists no established methodology to reduce dark patterns in
mobile apps. Our work introduces GreaseDroid, a community-driven app
modification framework enabling non-expert users to disable dark patterns in
apps selectively.
</summary>
    <author>
      <name>Konrad Kollnig</name>
    </author>
    <author>
      <name>Siddhartha Datta</name>
    </author>
    <author>
      <name>Max Van Kleek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411763.3451632</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411763.3451632" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as Late-Breaking Work (LBW) for CHI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.11819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03833v2</id>
    <updated>2021-07-12T05:29:05Z</updated>
    <published>2021-07-08T13:22:20Z</published>
    <title>Towards Collaborative Photorealistic VR Meeting Rooms</title>
    <summary>  When designing 3D applications it is necessary to find a compromise between
cost (e.g. money, time) and achievable realism of the virtual environment.
Reusing existing assets has an impact on the uniqueness of the application and
creating high quality 3D assets is very time consuming and expensive. We aim
for a low cost, high quality and minimal time effort solution to create virtual
environments. This paper's main contribution is a novel way of creating a
virtual meeting application by utilizing augmented spherical images for photo
realistic virtual environments.
</summary>
    <author>
      <name>Alexander Schäfer</name>
    </author>
    <author>
      <name>Gerd Reis</name>
    </author>
    <author>
      <name>Didier Stricker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3340764.3344466</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3340764.3344466" rel="related"/>
    <link href="http://arxiv.org/abs/2107.03833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08573v2</id>
    <updated>2021-08-02T23:15:21Z</updated>
    <published>2021-07-19T01:28:50Z</published>
    <title>AffectiveTDA: Using Topological Data Analysis to Improve Analysis and
  Explainability in Affective Computing</title>
    <summary>  We present an approach utilizing Topological Data Analysis to study the
structure of face poses used in affective computing, i.e., the process of
recognizing human emotion. The approach uses a conditional comparison of
different emotions, both respective and irrespective of time, with multiple
topological distance metrics, dimension reduction techniques, and face
subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our
topology-based approach captures known patterns, distinctions between emotions,
and distinctions between individuals, which is an important step towards more
robust and explainable emotion recognition by machines.
</summary>
    <author>
      <name>Hamza Elhamdadi</name>
    </author>
    <author>
      <name>Shaun Canavan</name>
    </author>
    <author>
      <name>Paul Rosen</name>
    </author>
    <link href="http://arxiv.org/abs/2107.08573v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08573v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09917v1</id>
    <updated>2021-07-21T07:36:01Z</updated>
    <published>2021-07-21T07:36:01Z</published>
    <title>Audit, Don't Explain -- Recommendations Based on a Socio-Technical
  Understanding of ML-Based Systems</title>
    <summary>  In this position paper, I provide a socio-technical perspective on machine
learning-based systems. I also explain why systematic audits may be preferable
to explainable AI systems. I make concrete recommendations for how institutions
governed by public law akin to the German T\"UV and Stiftung Warentest can
ensure that ML systems operate in the interest of the public.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18420/muc2021-mci-ws02-232</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18420/muc2021-mci-ws02-232" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be presented at the Workshop on User-Centered
  Artificial Intelligence (UCAI '21) at Mensch und Computer 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.11172v1</id>
    <updated>2021-07-22T16:50:59Z</updated>
    <published>2021-07-22T16:50:59Z</published>
    <title>Towards an understanding of how humans perceive stiffness during
  bimanual exploration</title>
    <summary>  In this paper, an experimental testbed and associated psychophysical paradigm
are presented for understanding how people discriminate torsional stiffness
using wrist rotation about their forearm. Featured in the testbed are two 1-DoF
rotary kinesthetic haptic devices. An adaptive staircase was used to evaluate
JNDs for a stiffness discrimination task where participants explored virtual
torsion springs by rotating their forearms. The JNDs were evaluated across
seven different conditions, under four different exploration modes: bimanual,
unimanual, bimanual feedback for unimanual displacement, and unimanual feedback
for bimanual displacement. The discrimination results will inform future
investigation into understanding how stiffness percepts vary.
</summary>
    <author>
      <name>Mohit Singhala</name>
    </author>
    <author>
      <name>Jacob Carducci</name>
    </author>
    <author>
      <name>Jeremy D. Brown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as Works-in-Progress paper at Haptics Symposium 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.11172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.11172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12704v1</id>
    <updated>2021-07-27T10:02:57Z</updated>
    <published>2021-07-27T10:02:57Z</published>
    <title>The cyclotactor: towards a tactile platform for musical interaction</title>
    <summary>  This paper reports on work in progress on a finger-based tactile I/O device
for musical interaction. Central to the device is the ability to set up
cyclical relationships between tactile input and output. A direct practical
application of this to musical interaction is given, using the idea to
multiplex two degrees of freedom on a single tactile loop.
</summary>
    <author>
      <name>Staas de Jong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1179571</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1179571" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.12704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12709v1</id>
    <updated>2021-07-27T10:10:25Z</updated>
    <published>2021-07-27T10:10:25Z</published>
    <title>Developing the cyclotactor</title>
    <summary>  This paper presents developments in the technology underlying the
cyclotactor, a finger-based tactile I/O device for musical interaction. These
include significant improvements both in the basic characteristics of tactile
interaction and in the related (vibro)tactile sample rates, latencies, and
timing precision. After presenting the new prototype's tactile output force
landscape, some of the new possibilities for interaction are discussed,
especially those for musical interaction with zero audio/tactile latency.
</summary>
    <author>
      <name>Staas de Jong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1177591</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1177591" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.12709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.00336v1</id>
    <updated>2022-01-02T11:15:29Z</updated>
    <published>2022-01-02T11:15:29Z</published>
    <title>Visilence: An Interactive Visualization Tool for Error Resilience
  Analysis</title>
    <summary>  Soft errors have become one of the major concerns for HPC applications, as
those errors can result in seriously corrupted outcomes, such as silent data
corruptions (SDCs). Prior studies on error resilience have studied the
robustness of HPC applications. However, it is still difficult for program
developers to identify potential vulnerability to soft errors. In this paper,
we present Visilence, a novel visualization tool to visually analyze error
vulnerability based on the control-flow graph generated from HPC applications.
Visilence efficiently visualizes the affected program states under injected
errors and presents the visual analysis of the most vulnerable parts of an
application. We demonstrate the effectiveness of Visilence through a case
study.
</summary>
    <author>
      <name>Shaolun Ruan</name>
    </author>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Qiang Guan</name>
    </author>
    <link href="http://arxiv.org/abs/2201.00336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04130v1</id>
    <updated>2022-01-11T18:52:36Z</updated>
    <published>2022-01-11T18:52:36Z</published>
    <title>Multi-physics integration platform MuPIF: Application for composite
  material design</title>
    <summary>  This paper presents the design of the MuPIF distributed, multi-physics
simulation platform and its performance in the context of the H2020
Composelector project. The description of MuPIF's model and data interfaces
provides implementation and operational details that illustrate how MuPIF is a
highly versatile and robust tool for various engineering applications. Its
distributed design and the integration of models in a workflow allows MuPIF to
simulate real industrial tasks. The design of a composite airplane frame for
the Composelector project, focusing on innovative design of composite materials
and structures, demonstrates MuPIF's capabilities and ability to be integrated
into Business Decision Support Systems.
</summary>
    <author>
      <name>B. Patzák</name>
    </author>
    <author>
      <name>V. Šmilauer</name>
    </author>
    <author>
      <name>M. Horák</name>
    </author>
    <author>
      <name>S. Šulc</name>
    </author>
    <author>
      <name>E. Dvořáková</name>
    </author>
    <link href="http://arxiv.org/abs/2201.04130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05980v3</id>
    <updated>2023-03-06T06:26:21Z</updated>
    <published>2022-01-16T06:18:04Z</published>
    <title>User-Centered Design (VIII): A New Framework of Intelligent
  Sociotechnical Systems and Prospects for Future Human Factors Research</title>
    <summary>  Traditional sociotechnical systems (STS) theory has been widely used, but
there are many new characteristics in the STS environment as we enter the
intelligence era, resulting in the limitations of traditional STS. Based on the
"user-centered design" philosophy, this paper proposes a new framework of
intelligent sociotechnical systems (iSTS) and outlines the new characteristics
of iSTS as well as its implications for the development of intelligent systems.
Future research of iSTS requires interdisciplinary collaboration, including
human factors engineering, this paper finally proposes suggestions from two
aspects of human factors engineering methodology and approaches.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Chinese language</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.05980v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05980v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08684v3</id>
    <updated>2023-04-14T09:53:01Z</updated>
    <published>2022-01-21T13:19:30Z</published>
    <title>VisQualdex -- the comprehensive guide to good data visualization</title>
    <summary>  The rapid influx of low-quality data visualisations is one of the main
challenges in today's communication. Misleading, unreadable, or confusing
visualisations spread misinformation, failing to fulfill their purpose. The
lack of proper tooling further heightens the problem of the quality assessment
process. Therefore, we propose VisQualdex, a systematic set of guidelines
isnpired by the Grammar of Graphics for evaluating the quality of data
visualisations. To increase the practical impact of VisQualdex, we make these
guidelines available in the form of the web server, visqual.info.
</summary>
    <author>
      <name>Jan Sawicki</name>
    </author>
    <author>
      <name>Michał Burdukiewicz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.26583/sv.15.1.11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.26583/sv.15.1.11" rel="related"/>
    <link href="http://arxiv.org/abs/2201.08684v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08684v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10470v1</id>
    <updated>2022-01-25T17:16:29Z</updated>
    <published>2022-01-25T17:16:29Z</published>
    <title>Wicked Implications for Human Interaction with IoT Sensor Data</title>
    <summary>  Human data interaction with sensor data from smart homes can cause some
implications when it comes to human sensemaking of this data. With our
data-driven method Guess the Data for individual and collective data work we
revealed in previous work a number of potential pitfalls when interacting with
this type of data. We introduce some of the identified, often wicked
implications for further discussion.
</summary>
    <author>
      <name>Albrecht Kurze</name>
    </author>
    <author>
      <name>Andreas Bischof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop Human-Data Interaction through Design at Conference on Human
  Factors in Computing Systems (CHI21). May 9, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.10470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10484v1</id>
    <updated>2022-01-25T17:26:16Z</updated>
    <published>2022-01-25T17:26:16Z</published>
    <title>Scented Dice: New interaction qualities for ideating connected devices</title>
    <summary>  Much research has been done around creating multisensory ideation and
prototyping tools. We relate our own multisensory tool, the Loaded Dice, to
this domain. We briefly explain what sensing and actuating possibilities the
Loaded Dice already have (including thermal sensations), and how they are
methodically embedded in workshops using an extended interaction vocabulary to
characterize and ideate multisensory experiences. We briefly ponder simple
technical ways to add smell as an output modality, and discuss why and how
smell capabilities will enrich ideation workshops in co-design.
</summary>
    <author>
      <name>Albrecht Kurze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop Smell, Taste, and Temperature Interfaces at Conference on
  Human Factors in Computing Systems (CHI 21). May 6-8, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.10484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10491v1</id>
    <updated>2022-01-25T17:35:52Z</updated>
    <published>2022-01-25T17:35:52Z</published>
    <title>Playing The Ethics Card: Ethical Aspects In Design Tools For Inspiration
  And Education</title>
    <summary>  This paper relates findings of own research in the domain of co-design tools
in terms of ethical aspects and their opportunities for inspiration and in HCI
education. We overview a number of selected general-purpose HCI/design tools as
well as domain specific tools for the Internet of Things. These tools are often
card-based, not only suitable for workshops with co-designers but also for
internal workshops with students to include these aspects in the built-up of
their expertise, sometimes even in a playful way.
</summary>
    <author>
      <name>Albrecht Kurze</name>
    </author>
    <author>
      <name>Arne Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop Co-designing Resources for Ethics Education in HCI at
  Conference on Human Factors in Computing Systems (CHI 21). May 9, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.10491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.12026v1</id>
    <updated>2022-01-28T10:24:47Z</updated>
    <published>2022-01-28T10:24:47Z</published>
    <title>Dynamic pricing and discounts by means of interactive presentation
  systems in stationary point of sales</title>
    <summary>  The main purpose of this article was to create a model and simulate the
profitability conditions of an interactive presentation system (IPS) with the
recommender system (RS) used in the kiosk. 90 million simulations have been run
in Python with SymPy to address the problem of discount recommendation offered
to the clients according to their usage of the IPS.
</summary>
    <author>
      <name>Marcin Lewicki</name>
    </author>
    <author>
      <name>Tomasz Kajdanowicz</name>
    </author>
    <author>
      <name>Piotr Bródka</name>
    </author>
    <author>
      <name>Janusz Sobecki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-77970-2_46</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-77970-2_46" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: ICCS 2021. Lecture Notes in Computer Science, vol 12745.
  Springer, Cham (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.12026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.12026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.12200v1</id>
    <updated>2022-01-28T15:57:56Z</updated>
    <published>2022-01-28T15:57:56Z</published>
    <title>Blue Ceramics: Co-designing Morphing Ceramics for Seagrass Meadow
  Restoration</title>
    <summary>  Seagrass meadows are twice as efficient as forests at capturing and storing
carbon, but over the last two decades they have been disappearing due to human
activities. We take a nature-centered design approach using contextual inquiry
and iterative participatory designs methods to consolidate knowledge from the
marine and material sciences to industrial design. The sketches and renders
documented evolved into the design and fabrication guidelines. This pictorial
documents a dialogue between designers and scientists to design an ecological
intervention using digital fabrication to manufacture morphing ceramics for
seagrass meadow restoration.
</summary>
    <author>
      <name>Rachel Arredondo</name>
    </author>
    <author>
      <name>Ofri Dar</name>
    </author>
    <author>
      <name>Kylon Chiang</name>
    </author>
    <author>
      <name>Arielle Blonder</name>
    </author>
    <author>
      <name>Linning Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages with 32 figures, ACM C&amp;C Pictorial</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.12200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.12200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.12344v1</id>
    <updated>2022-01-26T08:03:09Z</updated>
    <published>2022-01-26T08:03:09Z</published>
    <title>Socioergonomics: A few clarifications on the
  Technology-Organizations-People Tryptic</title>
    <summary>  This position paper introduces and coins the term socioergonomics, considered
as a sociological, ontological, and methodological support to human systems
integration (HSI). It describes the evolution of ergonomics from early
physiological to psychological to contemporary social sciences approaches
supporting Industry 4.0 sociotechnical systems engineering. It presents a
Technology Readiness Levels (TRLs) extension to Organizational Readiness Levels
(ORLs) and a departure toward a socioergonomics approach that includes systemic
properties such as flexibility, separability, and emergent social facts.
</summary>
    <author>
      <name>Guy Andre Boy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INCOSE HSI2021 International Conference Proceedings, 2021, San Diego,
  California, USA (virtual), United States</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.12344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.12344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02479v1</id>
    <updated>2022-02-05T03:23:47Z</updated>
    <published>2022-02-05T03:23:47Z</published>
    <title>Algorithmic nudge to make better choices: Evaluating effectiveness of
  XAI frameworks to reveal biases in algorithmic decision making to users</title>
    <summary>  In this position paper, we propose the use of existing XAI frameworks to
design interventions in scenarios where algorithms expose users to problematic
content (e.g. anti vaccine videos). Our intervention design includes facts (to
indicate algorithmic justification of what happened) accompanied with either
fore warnings or counterfactual explanations. While fore warnings indicate
potential risks of an action to users, the counterfactual explanations will
indicate what actions user should perform to change the algorithmic outcome. We
envision the use of such interventions as `decision aids' to users which will
help them make informed choices.
</summary>
    <author>
      <name>Prerna Juneja</name>
    </author>
    <author>
      <name>Tanushree Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/2202.02479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10564v1</id>
    <updated>2022-02-21T22:45:59Z</updated>
    <published>2022-02-21T22:45:59Z</published>
    <title>Human-in-the-loop Machine Learning: A Macro-Micro Perspective</title>
    <summary>  Though technical advance of artificial intelligence and machine learning has
enabled many promising intelligent systems, many computing tasks are still not
able to be fully accomplished by machine intelligence. Motivated by the
complementary nature of human and machine intelligence, an emerging trend is to
involve humans in the loop of machine learning and decision-making. In this
paper, we provide a macro-micro review of human-in-the-loop machine learning.
We first describe major machine learning challenges which can be addressed by
human intervention in the loop. Then we examine closely the latest research and
findings of introducing humans into each step of the lifecycle of machine
learning. Finally, we analyze current research gaps and point out future
research directions.
</summary>
    <author>
      <name>Jiangtao Wang</name>
    </author>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Liming Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10964v1</id>
    <updated>2022-02-22T15:12:06Z</updated>
    <published>2022-02-22T15:12:06Z</published>
    <title>Comparing Controller With the Hand Gestures Pinch and Grab for Picking
  Up and Placing Virtual Objects</title>
    <summary>  Grabbing virtual objects is one of the essential tasks for Augmented,
Virtual, and Mixed Reality applications. Modern applications usually use a
simple pinch gesture for grabbing and moving objects. However, picking up
objects by pinching has disadvantages. It can be an unnatural gesture to pick
up objects and prevents the implementation of other gestures which would be
performed with thumb and index. Therefore it is not the optimal choice for many
applications. In this work, different implementations for grabbing and placing
virtual objects are proposed and compared. Performance and accuracy of the
proposed techniques are measured and compared.
</summary>
    <author>
      <name>Alexander Schäfer</name>
    </author>
    <author>
      <name>Gerd Reis</name>
    </author>
    <author>
      <name>Didier Stricker</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01756v1</id>
    <updated>2022-05-03T19:59:16Z</updated>
    <published>2022-05-03T19:59:16Z</published>
    <title>Social Practice Cards: Research material to study social contexts as
  interwoven practice constellations</title>
    <summary>  Studying how social contexts shape technology interactions and how we
experience them is hard. One challenge is that social contexts are very dynamic
and shaped by the situated practices of everyone involved. As a result, the
same human-technology interaction can be experienced quite differently
depending on what other people around us do. As a first step to study
interpersonal and interpractice dynamics, we collected a broad range of visual
representations of practices, such as "riding a bike" or "skipping the rope".
This material can be used to further explore how different, co-located
practices relate to each other.
</summary>
    <author>
      <name>Alarith Uhde</name>
    </author>
    <author>
      <name>Mena Mesenhöller</name>
    </author>
    <author>
      <name>Marc Hassenzahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2022 Workshop InContext: Futuring User-Experience Design Tools</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.01756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04802v1</id>
    <updated>2022-05-10T11:02:29Z</updated>
    <published>2022-05-10T11:02:29Z</published>
    <title>Vibration-based communication for deafblind people</title>
    <summary>  Deafblind people have both hearing and visual impairments, which makes
communication with other people often dependent on expensive technologies e.g.,
Braille displays, or on caregivers acting as interpreters. This paper presents
Morse I/O (MIO), a vibrotactile interface for Android, evaluated through
experiments and interviews with deafblind participants. MIO was shown to enable
consistent text entry and recognition after only a few hours of practice. The
participants were willing to continue using the interface, although there were
perceived difficulties in learning to use it. Overall, MIO is a cost-effective,
portable interface for deafblind people without access to Braille displays or
similar.
</summary>
    <author>
      <name>David C. Kutner</name>
    </author>
    <author>
      <name>Sunčica Hadžidedić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures Accepted at the IEEE Haptics Symposium 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04831v1</id>
    <updated>2022-05-09T17:52:47Z</updated>
    <published>2022-05-09T17:52:47Z</published>
    <title>An Engineer's Nightmare: 102 Years of Critical Robotics</title>
    <summary>  A critical and re-configured HRI might look to the arts, where another
history of robots has been unfolding since the Czech artist Karel Capek's
critical robotic labor parable of 1921, in which the word robot was coined in
its modern usage. This paper explores several vectors by which artist-created
robots, both physical and imaginary, have offered pronounced contrasts to
robots-as-usual, and offers directions as to how these more emancipated cousins
might be useful to the field of HRI.
</summary>
    <author>
      <name>Christopher Csíkszentmihályi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the "Re-Configuring Human-Robot Interaction" workshop,
  HRI'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06890v1</id>
    <updated>2022-05-13T21:03:09Z</updated>
    <published>2022-05-13T21:03:09Z</published>
    <title>AI and Citizen Science for Serendipity</title>
    <summary>  It has been argued that introducing AI to creative practices destroys
spontaneity, intuition and serendipity. However, the design of systems that
leverage complex interactions between citizen scientists (members of the public
engaged in research tasks) and computational AI methods have the potential to
facilitate creative exploration and chance encounters. Drawing from theories
and literature about serendipity and computation, this article points to three
interrelated aspects that support the emergence of serendipity in hybrid
citizen science systems: the task environment; the characteristics of citizen
scientists; and anomalies and errors.
</summary>
    <author>
      <name>Marisa Ponti</name>
    </author>
    <author>
      <name>Anastasia Skarpeti</name>
    </author>
    <author>
      <name>Bruno Kestemont</name>
    </author>
    <link href="http://arxiv.org/abs/2205.06890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06919v1</id>
    <updated>2022-05-13T23:01:57Z</updated>
    <published>2022-05-13T23:01:57Z</published>
    <title>Grounding Explainability Within the Context of Global South in XAI</title>
    <summary>  In this position paper, we propose building a broader and deeper
understanding around Explainability in AI by 'grounding' it in social contexts,
the socio-technical systems operate in. We situate our understanding of
grounded explainability in the 'Global South' in general and India in
particular and express the need for more research within the global south
context when it comes to explainability and AI.
</summary>
    <author>
      <name>Deepa Singh</name>
    </author>
    <author>
      <name>Michal Slupczynski</name>
    </author>
    <author>
      <name>Ajit G. Pillai</name>
    </author>
    <author>
      <name>Vinoth Pandian Sermuga Pandian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Presented at CHI 2022 Workshop on Human-Centered Explainable
  AI (HCXAI): Beyond Opening the Black-Box of AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.06919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.08783v1</id>
    <updated>2022-05-18T08:07:23Z</updated>
    <published>2022-05-18T08:07:23Z</published>
    <title>Improving Pedestrian Priority via Grouping and Virtual Lanes</title>
    <summary>  The shared space design is applied in urban streets to support barrier-free
movement and integrate traffic participants (such as pedestrians, cyclists and
vehicles) into a common road space. Regardless of the low-speed environment,
sharing space with motor vehicles can make vulnerable road users feel uneasy.
Yet, walking in groups increases their confidence as well as influence the
yielding behavior of drivers. Therefore, we propose an innovative approach to
support the crossing of pedestrians via grouping and project the virtual lanes
in shared spaces. This paper presents the important components of the crowd
steering system, discusses the enablers and gaps in the current approach, and
illustrates the proposed idea with concept diagrams.
</summary>
    <author>
      <name>Yao Li</name>
    </author>
    <author>
      <name>Vinu Kamalasanan</name>
    </author>
    <author>
      <name>Mariana Batista</name>
    </author>
    <author>
      <name>Monika Sester</name>
    </author>
    <link href="http://arxiv.org/abs/2205.08783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.08783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.15163v1</id>
    <updated>2022-05-30T15:10:12Z</updated>
    <published>2022-05-30T15:10:12Z</published>
    <title>You Have Earned a Trophy: Characterize In-Game Achievements and Their
  Completions</title>
    <summary>  Achievement systems have been actively adopted in gaming platforms to
maintain players' interests. Among them, trophies in PlayStation games are one
of the most successful achievement systems. While the importance of trophy
design has been casually discussed in many game developers' forums, there has
been no systematic study of the historical dataset of trophies yet. In this
work, we construct a complete dataset of PlayStation games and their trophies
and investigate them from both the developers' and players' perspectives.
</summary>
    <author>
      <name>Haewoon Kwak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of the paper accepted at the 14th International ACM
  Conference on Web Science in 2022 (WebSci'22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.15163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.10212v1</id>
    <updated>2022-06-21T09:29:55Z</updated>
    <published>2022-06-21T09:29:55Z</published>
    <title>A Context Model for Personal Data Streams</title>
    <summary>  We propose a model of the situational context of a person and show how it can
be used to organize and, consequently, reason about massive streams of sensor
data and annotations, as they can be collected from mobile devices, e.g.
smartphones, smartwatches or fitness trackers. The proposed model is validated
on a very large dataset about the everyday life of one hundred and fifty-eight
people over four weeks, twenty-four hours a day.
</summary>
    <author>
      <name>Fausto Giunchiglia</name>
    </author>
    <author>
      <name>Xiaoyue Li</name>
    </author>
    <author>
      <name>Matteo Busso</name>
    </author>
    <author>
      <name>Marcelo Rodas-Britez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, APWeb WAIM Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.10212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.10212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.14568v1</id>
    <updated>2022-06-14T20:39:02Z</updated>
    <published>2022-06-14T20:39:02Z</published>
    <title>ADARP: A Multi Modal Dataset for Stress and Alcohol Relapse
  Quantification in Real Life Setting</title>
    <summary>  Stress detection and classification from wearable sensor data is an emerging
area of research with significant implications for individuals' physical and
mental health. In this work, we introduce a new dataset, ADARP, which contains
physiological data and self-report outcomes collected in real-world ambulatory
settings involving individuals diagnosed with alcohol use disorders. We
describe the user study, present details of the dataset, establish the
significant correlation between physiological data and self-reported outcomes,
demonstrate stress classification, and make our dataset public to facilitate
research.
</summary>
    <author>
      <name>Ramesh Kumar Sah</name>
    </author>
    <author>
      <name>Michael McDonell</name>
    </author>
    <author>
      <name>Patricia Pendry</name>
    </author>
    <author>
      <name>Sara Parent</name>
    </author>
    <author>
      <name>Hassan Ghasemzadeh</name>
    </author>
    <author>
      <name>Michael J Cleveland</name>
    </author>
    <link href="http://arxiv.org/abs/2206.14568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02212v1</id>
    <updated>2022-06-28T10:21:58Z</updated>
    <published>2022-06-28T10:21:58Z</published>
    <title>Combining Topic Modeling with Grounded Theory: Case Studies of Project
  Collaboration</title>
    <summary>  This paper proposes an Artificial Intelligence (AI) Grounded Theory for
management studies. We argue that this novel and rigorous approach that embeds
topic modelling will lead to the latent knowledge to be found. We illustrate
this abductive method using 51 case studies of collaborative innovation
published by Project Management Institute (PMI). Initial results are presented
and discussed that include 40 topics, 6 categories, 4 of which are core
categories, and two new theories of project collaboration.
</summary>
    <author>
      <name>Eyyub Can Odacioglu</name>
    </author>
    <author>
      <name>Lihong Zhang</name>
    </author>
    <author>
      <name>Richard Allmendinger</name>
    </author>
    <link href="http://arxiv.org/abs/2207.02212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02299v1</id>
    <updated>2022-07-03T11:58:29Z</updated>
    <published>2022-07-03T11:58:29Z</published>
    <title>Using Self Determination Theory to Design to Support Young Peoples
  Online Help-Seeking</title>
    <summary>  The application of Self-Determination Theory to understand online
help-seeking and the design of online help-seeking technologies presents an
interesting avenue for investigation. Improving motivation to engage in the
help-seeking process could be achieved using the Basic Psychological Needs
Theory as a structure to guide the design of online help-seeking technologies
and online mental health resources. Positive online help-seeking experiences
have an important role to play in sustained help-seeking and improved health
outcomes.
</summary>
    <author>
      <name>Claudette Pretorius</name>
    </author>
    <author>
      <name>David Coyle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, in ACM CHI22 Workshop on Self Determination Theory in HCI:
  Shaping a Research Agenda</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02310v1</id>
    <updated>2022-07-03T12:15:10Z</updated>
    <published>2022-07-03T12:15:10Z</published>
    <title>Personal Investigator: a Therapeutic 3D Game for Teenagers</title>
    <summary>  This position paper describes the implementation and initial findings of a
game called Personal Investigator (PI). PI is an online 3D detective game that
implements a model of Brief Solution Focused Therapy (BSFT). It aims to help
teenagers overcome mental health problems and engage with traditional mental
health care services. It is predicted that the combination of goal-oriented
gaming with a model of goal-oriented therapy will help to attract and sustain
the interest of teenagers, a group that therapists often have difficulty
engaging with. PI is the first game to integrate this established psychotherapy
approach into an engaging online 3D game.
</summary>
    <author>
      <name>David Coyle</name>
    </author>
    <author>
      <name>Mark Matthewas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, ACM CHI2004 Vienna. Presented at the Social Learning Through
  Gaming Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.03170v1</id>
    <updated>2022-07-07T08:59:48Z</updated>
    <published>2022-07-07T08:59:48Z</published>
    <title>AFFORCE: Actionable Framework for Designing Crowdsourcing Experiences
  for Older Adults</title>
    <summary>  In this article we propose a unique framework for designing attractive and
engaging crowdsourcing systems for older adults, which is called AFFORCE
(Actionable Framework For Crowdsourcing Experiences). We first categorize and
map mitigating factors and barriers to crowdsourcing for older adults to
finally discuss, present and combine system elements addressing them into an
actionable reference framework. This innovative framework is based on our
experience with the design of crowdsourcing systems for older adults in
exploratory cases and studies, related work, as well as our and related
research at the intersection of older adults' use of ICT, crowdsourcing and
citizen science.
</summary>
    <author>
      <name>Kinga Skorupska</name>
    </author>
    <author>
      <name>Radosław Nielek</name>
    </author>
    <author>
      <name>Wiesław Kopeć</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3486622.3494026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3486622.3494026" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, presented at the WI-IAT 2021 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.03170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.03170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.03999v1</id>
    <updated>2022-07-08T16:34:33Z</updated>
    <published>2022-07-08T16:34:33Z</published>
    <title>Incorporating Personality Traits in User Modeling for EUD</title>
    <summary>  Personality traits such as Need for Cognition, Locus of Control, Mindset and
Self-efficacy could impact the perception, acceptance and appreciation of
recommendations provided to support configuration tasks in the End User
Development (EUD) context. In this paper we describe the user model services we
have developed to measure such traits. These services can be accessed by users
through a simple web interface and can be queried by EUD applications by means
of REST API.
</summary>
    <author>
      <name>Federica Cena</name>
    </author>
    <author>
      <name>Cristina Gena</name>
    </author>
    <author>
      <name>Claudio Mattutino</name>
    </author>
    <author>
      <name>Michele Mioli</name>
    </author>
    <author>
      <name>Barbara Treccani</name>
    </author>
    <author>
      <name>Fabiana Vernero</name>
    </author>
    <author>
      <name>Massimo Zancanaro</name>
    </author>
    <link href="http://arxiv.org/abs/2207.03999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.03999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.07297v1</id>
    <updated>2022-07-15T05:14:46Z</updated>
    <published>2022-07-15T05:14:46Z</published>
    <title>Affective Computational Advertising Based on Perceptual Metrics</title>
    <summary>  We present \textbf{ACAD}, an \textbf{a}ffective \textbf{c}omputational
\textbf{ad}vertising framework expressly derived from perceptual metrics.
Different from advertising methods which either ignore the emotional nature of
(most) programs and ads, or are based on axiomatic rules, the ACAD formulation
incorporates findings from a user study examining the effect of within-program
ad placements on ad perception. A linear program formulation seeking to achieve
(a) \emph{{genuine}} ad assessments and (b) \emph{maximal} ad recall is then
proposed. Effectiveness of the ACAD framework is confirmed via a validational
user study, where ACAD-induced ad placements are found to be optimal with
respect to objectives (a) and (b) against competing approaches.
</summary>
    <author>
      <name>Soujanya Narayana</name>
    </author>
    <author>
      <name>Shweta Jain</name>
    </author>
    <author>
      <name>Harish Katti</name>
    </author>
    <author>
      <name>Roland Goecke</name>
    </author>
    <author>
      <name>Ramanathan Subramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.07297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.07478v1</id>
    <updated>2022-07-15T13:49:51Z</updated>
    <published>2022-07-15T13:49:51Z</published>
    <title>Yourfeed: Towards open science and interoperable systems for social
  media</title>
    <summary>  Existing social media platforms (SMPs) make it incredibly difficult for
researchers to conduct studies on social media, which in turn has created a
knowledge gap between academia and industry about the effects of platform
design on user behavior. To close the gap, we introduce Yourfeed, a research
tool for conducting ecologically valid social media research. We introduce the
platform architecture, as well key opportunities such as assessing the effects
of exposure of content on downstream beliefs and attitudes, measuring
attentional exposure via dwell time, and evaluating heterogeneous newsfeed
algorithms. We discuss the underlying philosophy of interoperability for social
media and future developments for the platform.
</summary>
    <author>
      <name>Ziv Epstein</name>
    </author>
    <author>
      <name>Hause Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2207.07478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.08967v1</id>
    <updated>2022-07-18T22:40:45Z</updated>
    <published>2022-07-18T22:40:45Z</published>
    <title>Low Cost Portable Touch Screen Technology Applied to University Teaching</title>
    <summary>  This article describes an implementation of low-cost portable touch screen
technology, applied to university teaching, using as a base the remote control
of the Nintendo Wii console (known as Wiimote), a normal projector, a computer
and free software. The purpose is to show the feasibility of such
implementation to improve teaching/learning processes, without incurring high
costs associated with unaffordable technological equipment, special
infrastructure in classrooms, or expensive computer programs. Also included is
a summary of a test of the system in two college courses.
</summary>
    <author>
      <name> Navas-López</name>
    </author>
    <author>
      <name>Eduardo Adam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Spanish language. Presented at "Congreso de Electr\'onica e
  Inform\'atica 2010", Universidad Centroamericana "Jos\'e Sime\'on Ca\~nas"</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.08967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.1; B.4.2; C.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.10828v1</id>
    <updated>2022-07-22T00:43:57Z</updated>
    <published>2022-07-22T00:43:57Z</published>
    <title>Tell Me How You Feel: Designing Emotion-Aware Voicebots to Ease Pandemic
  Anxiety In Aging Citizens</title>
    <summary>  The feeling of anxiety and loneliness among aging population has been
recently amplified by the COVID-19 related lockdowns. Emotion-aware multimodal
bot application combining voice and visual interface was developed to address
the problem in the group of older citizens. The application is novel as it
combines three main modules: information, emotion selection and psychological
intervention, with the aim of improving human well-being. The preliminary study
with target group confirmed that multimodality improves usability and that the
information module is essential for participating in a psychological
intervention. The solution is universal and can also be applied to areas not
directly related to COVID-19 pandemic.
</summary>
    <author>
      <name>W. Mieleszczenko-Kowszewicz</name>
    </author>
    <author>
      <name>K. Warpechowski</name>
    </author>
    <author>
      <name>K. Zieliński</name>
    </author>
    <author>
      <name>R. Nielek</name>
    </author>
    <author>
      <name>A. Wierzbicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.10828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.13626v1</id>
    <updated>2022-07-27T16:38:01Z</updated>
    <published>2022-07-27T16:38:01Z</published>
    <title>Towards Mapping and Assessing Sidewalk Accessibility Across
  Sociocultural and Geographic Contexts</title>
    <summary>  Despite the important role of sidewalks in supporting mobility,
accessibility, and public health, there is a lack of high-quality datasets and
corresponding analyses on sidewalk existence and condition. Our work explores a
twofold vision: first, to develop scalable mechanisms to locate and assess
sidewalks in cities across the world, and second, to use this data to support
new urban analyses and mobility tools. We report on two preliminary urban
science explorations enabled by our approach: exploring geo-spatial patterns
and key correlates of sidewalk accessibility and examining differences in
sidewalk infrastructure across regions.
</summary>
    <author>
      <name>Jon E. Froehlich</name>
    </author>
    <author>
      <name>Michael Saugstad</name>
    </author>
    <author>
      <name>Manaswi Saha</name>
    </author>
    <author>
      <name>Matthew Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop paper at the Data4Good workshop hosted as part of AVI2020.
  See https://sites.google.com/view/data4good/program</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.13626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.13626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.13904v1</id>
    <updated>2022-07-28T06:51:20Z</updated>
    <published>2022-07-28T06:51:20Z</published>
    <title>CaminAR: Supporting Walk-and-talk Experiences for Remote Dyads using
  Augmented Reality on Smart Glasses</title>
    <summary>  In this paper, we propose CaminAR, an augmented reality (AR) system for
remote social walking among dyads. CaminAR enables two people who are
physically away from one another to synchronously see and hear each other while
going on a walk. The system shows a partner's avatar superimposed onto the
physical world using smart glasses while walking and talking. Using a
combination of visual and auditory augments, CaminAR simulates the experience
of co-located walking when dyads are apart.
</summary>
    <author>
      <name>Victor Chu</name>
    </author>
    <author>
      <name>Andrés Monroy-Hernández</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, submitted to UIST Poster</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.13904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.13904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.14462v1</id>
    <updated>2022-07-29T03:43:28Z</updated>
    <published>2022-07-29T03:43:28Z</published>
    <title>Towards Reproducible Evaluations for Flying Drone Controllers in Virtual
  Environments</title>
    <summary>  Research attention on natural user interfaces (NUIs) for drone flights are
rising. Nevertheless, NUIs are highly diversified, and primarily evaluated by
different physical environments leading to hard-to-compare performance between
such solutions. We propose a virtual environment, namely VRFlightSim, enabling
comparative evaluations with enriched drone flight details to address this
issue. We first replicated a state-of-the-art (SOTA) interface and designed two
tasks (crossing and pointing) in our virtual environment. Then, two user
studies with 13 participants demonstrate the necessity of VRFlightSim and
further highlight the potential of open-data interface designs.
</summary>
    <author>
      <name>Zheng Li</name>
    </author>
    <author>
      <name>Yiming Huang</name>
    </author>
    <author>
      <name>Yui-Pan Yau</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <author>
      <name>Lik-Hang Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IROS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.14462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.14462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.14681v1</id>
    <updated>2022-07-29T13:56:15Z</updated>
    <published>2022-07-29T13:56:15Z</published>
    <title>Unfolding Values through Systematic Guidance: Conducting a
  Value-Centered Participatory Workshop for a Patient-Oriented Data Donation</title>
    <summary>  Routinely collected clinical patient data posits a valuable resource for
data-driven medical innovation. Such secondary data use for medical research
purposes is dependent on the patient's consent. To gain an understanding of the
patients' values and needs regarding medical data donations, we developed a
participatory workshop method, integrating approaches from value-sensitive and
reflective design to explore patients' values and translate them into
hypothetical, ideal design solutions. The data gathered in the workshop are
used to derive practicable design requirements for patient-oriented data
donation technologies. In this paper, we introduce the workshop process and
evaluate its application.
</summary>
    <author>
      <name>David Leimstädtner</name>
    </author>
    <author>
      <name>Peter Sörries</name>
    </author>
    <author>
      <name>Claudia Müller-Birn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3543758.3547560</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3543758.3547560" rel="related"/>
    <link href="http://arxiv.org/abs/2207.14681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.14681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00823v1</id>
    <updated>2022-07-29T03:01:06Z</updated>
    <published>2022-07-29T03:01:06Z</published>
    <title>Designing Programming Exercises from Board Games</title>
    <summary>  This paper introduces a collection of board games specifically chosen to
serve as a basis for programming exercises. We examine the attractiveness of
board games in this context as well as features that make a particular game a
good exercise. The collection is annotated across several dimensions to assist
choosing a game suitable for the target topic and student level. We discuss
possible changes into exercise tasks to make them more challenging and
introduce new topics. The work relies on established topics taxonomy and board
games resources which makes extending the current collection easy.
</summary>
    <author>
      <name>Maxim Mozgovoy</name>
    </author>
    <author>
      <name>Marina Purgina</name>
    </author>
    <link href="http://arxiv.org/abs/2208.00823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04936v1</id>
    <updated>2022-08-04T05:44:18Z</updated>
    <published>2022-08-04T05:44:18Z</published>
    <title>Young women's cognition of commercial digital signage in shopping malls:
  A situated action approach</title>
    <summary>  Existing literature on digital signage is growing but has not always
emphasized the cognitive processes of the audience. This research aims to
address this gap by studying how young women in Singapore cognize commercial
digital signage in shopping malls and what cause them to do so. Using cognitive
ethnography and taking the situated action approach, our findings suggest a
comprehensive list of factors, both external and internal, that influence young
women's cognition of commercial digital signage in both positive and negative
ways. The research's practical implications are discussed.
</summary>
    <author>
      <name>Becky Pham</name>
    </author>
    <author>
      <name>Weiyu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A previous version of the paper was presented in June 2016 at the
  66th Annual Conference of the International Communication Association,
  Fukuoka, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.04936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.05410v1</id>
    <updated>2022-08-10T16:00:53Z</updated>
    <published>2022-08-10T16:00:53Z</published>
    <title>TagTeam: Towards Wearable-Assisted, Implicit Guidance for Human--Drone
  Teams</title>
    <summary>  The availability of sensor-rich smart wearables and tiny, yet capable,
unmanned vehicles such as nano quadcopters, opens up opportunities for a novel
class of highly interactive, attention-shared human--machine teams. Reliable,
lightweight, yet passive exchange of intent, data and inferences within such
human--machine teams make them suitable for scenarios such as search-and-rescue
with significantly improved performance in terms of speed, accuracy and
semantic awareness. In this paper, we articulate a vision for such human--drone
teams and key technical capabilities such teams must encompass. We present
TagTeam, an early prototype of such a team and share promising demonstration of
a key capability (i.e., motion awareness).
</summary>
    <author>
      <name>Kasthuri Jayarajah</name>
    </author>
    <author>
      <name>Aryya Gangopadhyay</name>
    </author>
    <author>
      <name>Nicholas Waytowich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3556560.3560715</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3556560.3560715" rel="related"/>
    <link href="http://arxiv.org/abs/2208.05410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.05410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.06679v1</id>
    <updated>2022-08-13T16:19:19Z</updated>
    <published>2022-08-13T16:19:19Z</published>
    <title>Neural Encoding of Songs is Modulated by Their Enjoyment</title>
    <summary>  We examine user and song identification from neural (EEG) signals. Owing to
perceptual subjectivity in human-media interaction, music identification from
brain signals is a challenging task. We demonstrate that subjective differences
in music perception aid user identification, but hinder song identification. In
an attempt to address intrinsic complexities in music identification, we
provide empirical evidence on the role of enjoyment in song recognition. Our
findings reveal that considering song enjoyment as an additional factor can
improve EEG-based song recognition.
</summary>
    <author>
      <name>Gulshan Sharma</name>
    </author>
    <author>
      <name>Pankaj Pandey</name>
    </author>
    <author>
      <name>Ramanathan Subramanian</name>
    </author>
    <author>
      <name>Krishna. P. Miyapuram</name>
    </author>
    <author>
      <name>Abhinav Dhall</name>
    </author>
    <link href="http://arxiv.org/abs/2208.06679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09100v1</id>
    <updated>2022-08-19T00:26:23Z</updated>
    <published>2022-08-19T00:26:23Z</published>
    <title>User Experience of Symptom Checkers: A Systematic Review</title>
    <summary>  This review reports the user experience of symptom checkers, aiming to
characterize users studied in the existing literature, identify the aspects of
user experience of symptom checkers that have been studied, and offer design
suggestions. Our literature search resulted in 31 publications. We found that
(1) most symptom checker users are relatively young; (2) eight relevant aspects
of user experience have been explored, including motivation, trust,
acceptability, satisfaction, accuracy, usability, safety or security, and
functionality; (3) future symptom checkers should improve their accuracy,
safety, and usability.
</summary>
    <author>
      <name>Yue You</name>
    </author>
    <author>
      <name>Renkai Ma</name>
    </author>
    <author>
      <name>Xinning Gui</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10167v1</id>
    <updated>2022-08-22T09:28:54Z</updated>
    <published>2022-08-22T09:28:54Z</published>
    <title>Interdisciplinary Research with Older Adults in the area of ICT:
  Selected Ethical Considerations and Challenges</title>
    <summary>  In this paper we analyse, classify and discuss some ethical considerations
and challenges related to pursuing exploratory and interdisciplinary research
projects in the area of ICT, especially those involving older adults. First, we
identify spotlight areas, which are especially prominent in these fields. Next,
we explore possible pitfalls interdisciplinary researchers may stumble onto
when planning, conducting and presenting exploratory research activities.
Finally, some of these are selected and discussed more closely, while related
open questions are posed.
</summary>
    <author>
      <name>Kinga Skorupska</name>
    </author>
    <author>
      <name>Ewa Makowska</name>
    </author>
    <author>
      <name>Anna Jaskulska</name>
    </author>
    <link href="http://arxiv.org/abs/2208.10167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10918v1</id>
    <updated>2022-08-18T19:22:36Z</updated>
    <published>2022-08-18T19:22:36Z</published>
    <title>The DialPort tools</title>
    <summary>  The DialPort project http://dialport.org/, funded by the National Science
Foundation (NSF), covers a group of tools and services that aim at fulfilling
the needs of the dialog research community. Over the course of six years,
several offerings have been created, including the DialPort Portal and
DialCrowd. This paper describes these contributions, which will be demoed at
SIGDIAL, including implementation, prior studies, corresponding discoveries,
and the locations at which the tools will remain freely available to the
community going forward.
</summary>
    <author>
      <name>Jessica Huynh</name>
    </author>
    <author>
      <name>Shikib Mehri</name>
    </author>
    <author>
      <name>Cathy Jiao</name>
    </author>
    <author>
      <name>Maxine Eskenazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SIGDIAL 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.10918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10928v1</id>
    <updated>2022-07-18T09:58:07Z</updated>
    <published>2022-07-18T09:58:07Z</published>
    <title>Meta Avatar Robot Cafe: Linking Physical and Virtual Cybernetic Avatars
  to Provide Physical Augmentation for People with Disabilities</title>
    <summary>  Meta avatar robot cafe is a cafe that fuses cyberspace and physical space to
create new encounters with people. We create a place where people with
disabilities who have difficulty going out can freely switch between their
physical bodies and virtual bodies, and communicate their presence and warmth
to each other.
</summary>
    <author>
      <name>Yoichi Yamazaki</name>
    </author>
    <author>
      <name>Tsukuto Yamada</name>
    </author>
    <author>
      <name>Hiroki Nomura</name>
    </author>
    <author>
      <name>Nobuaki Hosoda</name>
    </author>
    <author>
      <name>Ryoma Kawamura</name>
    </author>
    <author>
      <name>Kazuaki Takeuchi</name>
    </author>
    <author>
      <name>Hiroaki Kato</name>
    </author>
    <author>
      <name>Ryuma Niiyama</name>
    </author>
    <author>
      <name>Kentaro Yoshifuji</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3532721.3546117</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3532721.3546117" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGGRAPH '22 Emerging Technologies, 2022, 2 Pages. Project page:
  https://bit.ly/metaavatarrobotcafe</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.10928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11898v2</id>
    <updated>2022-09-08T08:33:00Z</updated>
    <published>2022-08-25T07:06:36Z</published>
    <title>Embedding Privacy Into Design Through Software Developers: Challenges &amp;
  Solutions</title>
    <summary>  To make privacy a first-class citizen in software, we argue for equipping
developers with usable tools, as well as providing support from organizations,
educators, and regulators. We discuss the challenges with the successful
integration of privacy features and propose solutions for stakeholders to help
developers perform privacy-related tasks.
</summary>
    <author>
      <name>Mohammad Tahaei</name>
    </author>
    <author>
      <name>Kami Vaniea</name>
    </author>
    <author>
      <name>Awais Rashid</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSEC.2022.3204364</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSEC.2022.3204364" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in "IEEE Security &amp; Privacy: Special Issue on Usable
  Security for Security Workers" 11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.11898v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11898v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.12715v1</id>
    <updated>2022-08-26T15:17:31Z</updated>
    <published>2022-08-26T15:17:31Z</published>
    <title>ICEBOAT: An Interactive User Behavior Analysis Tool for Automotive User
  Interfaces</title>
    <summary>  In this work, we present ICEBOAT an interactive tool that enables automotive
UX experts to explore how users interact with In-vehicle Information Systems.
Based on large naturalistic driving data continuously collected from production
line vehicles, ICEBOAT visualizes drivers' interactions and driving behavior on
different levels of detail. Hence, it allows to easily compare different user
flows based on performance- and safety-related metrics.
</summary>
    <author>
      <name>Patrick Ebel</name>
    </author>
    <author>
      <name>Kim Julian Gülle</name>
    </author>
    <author>
      <name>Christoph Lingenfelder</name>
    </author>
    <author>
      <name>Andreas Vogelsang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3526114.3558739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3526114.3558739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Adjunct Publication of the 35th Annual ACM Symposium on User
  Interface Software and Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.12715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.12961v1</id>
    <updated>2022-08-27T08:51:22Z</updated>
    <published>2022-08-27T08:51:22Z</published>
    <title>Kuchibashi: 3D-Printed Tweezers Bioinspired by the New Caledonian Crow's
  Beak</title>
    <summary>  In this study we implemented Kuchibashi, the New Caledonian Crow beak-like
tweezers, and conducted a user study to evaluate the prototype's usability. We
proved that Kuchibashi is superior in interacting with large spherical objects
than hands and tweezers. Also, impressions of security and safeness were
perceived positively by the participants.
</summary>
    <author>
      <name>Takahito Murakami</name>
    </author>
    <author>
      <name>Maya Grace Torii</name>
    </author>
    <author>
      <name>Xanat Vargas Meza</name>
    </author>
    <author>
      <name>Yoichi Ochiai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3532719.3543254</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3532719.3543254" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2figures,ACM SIGGRAPH2022</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGGRAPH 2022. Posters Article 18. 1-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.12961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.13512v1</id>
    <updated>2022-08-29T11:28:14Z</updated>
    <published>2022-08-29T11:28:14Z</published>
    <title>Labeling of Cultural Heritage Collections on the Intersection of Visual
  Analytics and Digital Humanities</title>
    <summary>  Engaging in interdisciplinary projects on the intersection between
visualization and humanities research can be a challenging endeavor. Challenges
can be finding valuable outcomes for both domains, or how to apply
state-of-the-art visual analytics methods like supervised machine learning
algorithms. We discuss these challenges when working with cultural heritage
data. Further, there is a gap in applying these methods to intangible heritage.
To give a reflection on some interdisciplinary projects, we present three case
studies focusing on the labeling of cultural heritage collections, the problems
and challenges with the data, the participatory design process, and takeaways
for the visualization scholars from these collaborations.
</summary>
    <author>
      <name>Christofer Meinecke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VIS4DH57440.2022.00009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VIS4DH57440.2022.00009" rel="related"/>
    <link href="http://arxiv.org/abs/2208.13512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.13512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.13630v1</id>
    <updated>2022-08-29T14:26:16Z</updated>
    <published>2022-08-29T14:26:16Z</published>
    <title>A systematic review of research on the use and impact of technology for
  learning Chinese</title>
    <summary>  In light of technological development enforced by the pandemic, learning
Chinese has become more digitalised. Confucius institutes went online and now
follow 2021 to 2025 Action Plans for the Construction of Teaching Resources for
International Chinese Education and International Chinese Online Education. New
ways of learning Chinese emerged, such as educational games and intelligent
tutoring systems ITS, some of them based on artificial intelligence. The aim of
this systematic review is to examine recent research published in ScienceDirect
and Scopus databases on the use and impact of educational games and ITS in
Chinese language learning. A total of 29 selected studies were analysed.
</summary>
    <author>
      <name>Angelina Maksimova</name>
    </author>
    <link href="http://arxiv.org/abs/2208.13630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.13630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.00086v1</id>
    <updated>2022-08-31T19:44:58Z</updated>
    <published>2022-08-31T19:44:58Z</published>
    <title>In Alexa, We Trust. Or Do We? : An analysis of People's Perception of
  Privacy Policies</title>
    <summary>  Smart home devices have found their way through people's homes as well as
hearts. One such smart device is Amazon Alexa. Amazon Alexa is a
voice-controlled application that is rapidly gaining popularity. Alexa was
primarily used for checking weather forecasts, playing music, and controlling
other devices. This paper tries to explore the extent to which people are aware
of the privacy policies pertaining to the Amazon Alexa devices. We have
evaluated behavioral change towards their interactions with the device post
being aware of the adverse implications. Resulting knowledge will give
researchers new avenues of research and interaction designers new insights into
improving their systems.
</summary>
    <author>
      <name>Sanjana Gautam</name>
    </author>
    <link href="http://arxiv.org/abs/2209.00086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; H.5.2; K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.00836v2</id>
    <updated>2024-11-13T16:32:16Z</updated>
    <published>2022-09-02T06:22:03Z</published>
    <title>Information Visualization for Effective Altruism</title>
    <summary>  Effective altruism is a movement whose goal it to use evidence and reason to
figure out how to benefit others as much as possible. This movement is becoming
influential, but effective altruists still lack tools to help them understand
complex humanitarian trade-offs and make good decisions based on data.
Visualization-the study of computer-supported, visual representations of data
meant to support understanding, communication, and decision makingcan help
alleviate this issue. Conversely, effective altruism provides a powerful
thinking framework for visualization research that focuses on humanitarian
applications.
</summary>
    <author>
      <name>Pierre Dragicevic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2024 update</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.00836v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00836v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01846v1</id>
    <updated>2022-09-05T09:10:04Z</updated>
    <published>2022-09-05T09:10:04Z</published>
    <title>Evaluating Situated Visualization in AR with Eye Tracking</title>
    <summary>  Augmented reality (AR) technology provides means for embedding visualization
in a real-world context. Such techniques allow situated analyses of live data
in their spatial domain. However, as existing techniques have to be adapted for
this context and new approaches will be developed, the evaluation thereof poses
new challenges for researchers. Apart from established performance measures,
eye tracking has proven to be a valuable means to assess visualizations
qualitatively and quantitatively. We discuss the challenges and opportunities
of eye tracking for the evaluation of situated visualizations. We envision that
an extension of gaze-based evaluation methodology into this field will provide
new insights on how people perceive and interact with visualizations in
augmented reality.
</summary>
    <author>
      <name>Kuno Kurzhals</name>
    </author>
    <author>
      <name>Michael Becher</name>
    </author>
    <author>
      <name>Nelusa Pathmanathan</name>
    </author>
    <author>
      <name>Guido Reina</name>
    </author>
    <link href="http://arxiv.org/abs/2209.01846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01927v1</id>
    <updated>2022-09-05T12:12:25Z</updated>
    <published>2022-09-05T12:12:25Z</published>
    <title>Gather -- a better way to codehack online</title>
    <summary>  A virtual hands-on computer laboratory has been designed within the Gather
online meeting platform. Gather's features such as spatial audio, private
spaces and interactable objects offer scope for great improvements over
currently used platforms, especially for small-group based teaching. We
describe our experience using this virtual computer laboratory for a recent
'Python for Beginners' workshop held as part of the Software Sustainability
Institute's 2022 Research Software Camp.
</summary>
    <author>
      <name>Rika Kobayashi</name>
    </author>
    <author>
      <name>Sarah Jaffa</name>
    </author>
    <author>
      <name>Jiachen Dong</name>
    </author>
    <author>
      <name>Roger D. Amos</name>
    </author>
    <author>
      <name>Jeremy Cohen</name>
    </author>
    <author>
      <name>Emily F. Kerrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.01927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02204v1</id>
    <updated>2022-09-06T03:58:18Z</updated>
    <published>2022-09-06T03:58:18Z</published>
    <title>Exploiting and Guiding User Interaction in Interactive Machine Teaching</title>
    <summary>  Humans are talented with the ability to perform diverse interactions in the
teaching process. However, when humans want to teach AI, existing interactive
systems only allow humans to perform repetitive labeling, causing an
unsatisfactory teaching experience. My Ph.D. research studies Interactive
Machine Teaching (IMT), an emerging field of HCI research that aims to enhance
humans' teaching experience in the AI creation process. My research builds IMT
systems that exploit and guide user interaction and shows that such in-depth
integration of human interaction can benefit both AI models and user
experience.
</summary>
    <author>
      <name>Zhongyi Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">UIST 2022 Doctoral Symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02298v1</id>
    <updated>2022-09-06T08:51:08Z</updated>
    <published>2022-09-06T08:51:08Z</published>
    <title>You Are What You Use: Usage-based Profiling in IoT Environments</title>
    <summary>  Habit extraction is essential to automate services and provide appliance
usage insights in the smart home environment. However, habit extraction comes
with plenty of challenges in viewing typical start and end times for particular
activities. This paper introduces a novel way of identifying habits using an
ensemble of unsupervised clustering techniques. We use different clustering
algorithms to extract habits based on how static or dynamic they are.
Silhouette coefficients and a novel noise metric are utilized to extract habits
appropriately. Furthermore, we associate the extracted habits with time
intervals and a confidence score to denote how confident we are that a habit is
likely to occur at that time.
</summary>
    <author>
      <name>Manan Choksi</name>
    </author>
    <author>
      <name>Dipankar Chaki</name>
    </author>
    <author>
      <name>Abdallah Lakhdari</name>
    </author>
    <author>
      <name>Athman Bouguettaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02403v1</id>
    <updated>2022-09-01T09:45:08Z</updated>
    <published>2022-09-01T09:45:08Z</published>
    <title>Guidelines to Develop Trustworthy Conversational Agents for Children</title>
    <summary>  Conversational agents (CAs) embodied in speakers or chatbots are becoming
very popular in some countries, and despite their adult-centred design, they
have become part of children's lives, generating a need for children-centric
trustworthy systems. This paper presents a literature review to identify the
main opportunities, challenges and risks brought by CAs when used by children.
We then consider relevant ethical guidelines for AI and adapt them to this
particular system and population, using a Delphi methodology with a set of
experts from different disciplines. From this analysis, we propose specific
guidelines to help CAs developers improve their design towards trustworthiness
and children.
</summary>
    <author>
      <name>Marina Escobar-Planas</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>Carlos-D Martínez-Hinarejos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02579v1</id>
    <updated>2022-09-06T15:37:02Z</updated>
    <published>2022-09-06T15:37:02Z</published>
    <title>Contextualizing Large-Scale Domain Knowledge for Conceptual Modeling and
  Simulation</title>
    <summary>  We present an interactive modeling tool, VERA, that scaffolds the acquisition
of domain knowledge involved in conceptual modeling and agent-based
simulations. We describe the knowledge engineering process of contextualizing
large-scale domain knowledge. Specifically, we use the ontology of biotic
interactions in Global Biotic Interactions, and the trait data of species in
Encyclopedia of Life to facilitate the model construction. Learners can use
VERA to construct qualitative conceptual models of ecological phenomena, run
them as quantitative simulations, and review their predictions.
</summary>
    <author>
      <name>Sungeun An</name>
    </author>
    <author>
      <name>Spencer Rugaber</name>
    </author>
    <author>
      <name>Jennifer Hammock</name>
    </author>
    <author>
      <name>Ashok K. Goel</name>
    </author>
    <link href="http://arxiv.org/abs/2209.02579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.05224v1</id>
    <updated>2022-09-12T13:16:22Z</updated>
    <published>2022-09-12T13:16:22Z</published>
    <title>Toward Inclusion and Accessibility in Visualization Research:
  Speculations on Challenges, Solution Strategies, and Calls for Action
  (Position Paper)</title>
    <summary>  Inclusion and accessibility in visualization research have gained increasing
attention in recent years. However, many challenges still remain to be solved
on the road toward a more inclusive, shared-experience-driven visualization
design and evaluation process. In this position paper, we discuss challenges
and speculate about potential solutions, based on related work, our own
research, as well as personal experiences. The goal of this paper is to start
discussions on the role of accessibility and inclusion in visualization design
and evaluation.
</summary>
    <author>
      <name>Katrin Angerbauer</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, IEEE VIS BELIV 2022 workshop, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.05224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.05224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.05384v1</id>
    <updated>2022-09-12T16:36:59Z</updated>
    <published>2022-09-12T16:36:59Z</published>
    <title>Design heuristics: privacy and portability Regulation as a feature
  request</title>
    <summary>  The lack of user experience standards in regulations for data privacy and
data portability in the health sector increases the cost of leaving a network
provider while not protecting the patient's privacy, directly impacting
people's health. Furthermore, user in-app options for data sharing and
portability in the health sector's applications make it difficult to transfer
data between providers while facilitating privacy breaches. Moreover, it leaves
users unaware of occasional past unauthorized data access episodes. In this
article, we propose an extension for the traditional design heuristics to
increase privacy and portability controls for applications that deal with
users' personal information based on a benchmark in applications from different
sectors and a literature review.
</summary>
    <author>
      <name>Yasodara Cordova</name>
    </author>
    <link href="http://arxiv.org/abs/2209.05384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.05384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.07025v1</id>
    <updated>2022-09-15T03:32:10Z</updated>
    <published>2022-09-15T03:32:10Z</published>
    <title>Dynamic X-Ray Vision in Mixed Reality</title>
    <summary>  X-ray vision, a technique that allows users to see through walls and other
obstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality
(MR). In this paper, we demonstrate a dynamic X-ray vision window that is
rendered in real-time based on the user's current position and changes with
movement in the physical environment. Moreover, the location and transparency
of the window are also dynamically rendered based on the user's eye gaze. We
build this X-ray vision window for a current state-of-the-art MR Head-Mounted
Device (HMD) -- HoloLens 2 by integrating several different features: scene
understanding, eye tracking, and clipping primitive.
</summary>
    <author>
      <name>Hung-Jui Guo</name>
    </author>
    <author>
      <name>Jonathan Z. Bakdash</name>
    </author>
    <author>
      <name>Laura R. Marusich</name>
    </author>
    <author>
      <name>Balakrishnan Prabhakaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3562939.3565675</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3562939.3565675" rel="related"/>
    <link href="http://arxiv.org/abs/2209.07025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.07025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.08834v2</id>
    <updated>2022-09-24T08:15:22Z</updated>
    <published>2022-09-19T08:31:50Z</published>
    <title>NL2INTERFACE: Interactive Visualization Interface Generation from
  Natural Language Queries</title>
    <summary>  We develop NL2INTERFACE to explore the potential of generating usable
interactive multi-visualization interfaces from natural language queries. With
NL2INTERFACE, users can directly write natural language queries to
automatically generate a fully interactive multi-visualization interface
without any extra effort of learning a tool or programming language. Further,
users can interact with the interfaces to easily transform the data and quickly
see the results in the visualizations.
</summary>
    <author>
      <name>Yiru Chen</name>
    </author>
    <author>
      <name>Ryan Li</name>
    </author>
    <author>
      <name>Austin Mac</name>
    </author>
    <author>
      <name>Tianbao Xie</name>
    </author>
    <author>
      <name>Tao Yu</name>
    </author>
    <author>
      <name>Eugene Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, IEEE Visualization Conference NLVIZ Workshop 2022</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Visualization Conference NLVIZ Workshop 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.08834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.08834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12140v1</id>
    <updated>2022-09-25T04:22:01Z</updated>
    <published>2022-09-25T04:22:01Z</published>
    <title>Modie Viewer: Protein Beasts and How to View Them</title>
    <summary>  Understanding chemical modifications on proteins opens up further
possibilities for research on rare diseases. This work proposes visualization
approaches using two-dimensional (2D) and three-dimensional (3D) visual
representations to analyze and gain insights into protein modifications. In
this work, we present the application of Modie Viewer as an attempt to address
the Bio+MedVis Challenge at IEEE VIS 2022.
</summary>
    <author>
      <name>Huyen N. Nguyen</name>
    </author>
    <author>
      <name>Caleb Trujillo</name>
    </author>
    <author>
      <name>Tommy Dang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, Bio+MedVis Challenge @ IEEE VIS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; J.3; D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12792v1</id>
    <updated>2022-09-26T15:48:57Z</updated>
    <published>2022-09-26T15:48:57Z</published>
    <title>Towards Data-driven GIM tools: Two Prototypes</title>
    <summary>  Here we describe two approaches to improve group information management (GIM)
and draw on the results of prior works to implement them in software
prototypes. The first aids browsing and retrieving from large and unfamiliar
collections like shared drives by dynamically reducing and re-organising them.
The second supports the transfer and re-use of collections (e.g. to/by
successors, descendants, or curators) by integrating novel sorting and
annotation features. The prototypes' source code is shared online and
screenshots are presented in the accompanying poster.
</summary>
    <author>
      <name>Jesse David Dinneen</name>
    </author>
    <author>
      <name>Sascha Donner</name>
    </author>
    <author>
      <name>Helen Bubinger</name>
    </author>
    <author>
      <name>Jwen Fai Low</name>
    </author>
    <author>
      <name>Maja Krtalić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ASIST'22, final will appear in proceedings. Poster draft
  included at end of file</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.13498v1</id>
    <updated>2022-09-22T15:53:34Z</updated>
    <published>2022-09-22T15:53:34Z</published>
    <title>Characterizing Uncertainty in the Visual Text Analysis Pipeline</title>
    <summary>  Current visual text analysis approaches rely on sophisticated processing
pipelines. Each step of such a pipeline potentially amplifies any uncertainties
from the previous step. To ensure the comprehensibility and interoperability of
the results, it is of paramount importance to clearly communicate the
uncertainty not only of the output but also within the pipeline. In this paper,
we characterize the sources of uncertainty along the visual text analysis
pipeline. Within its three phases of labeling, modeling, and analysis, we
identify six sources, discuss the type of uncertainty they create, and how they
propagate.
</summary>
    <author>
      <name>Pantea Haghighatkhah</name>
    </author>
    <author>
      <name>Mennatallah El-Assady</name>
    </author>
    <author>
      <name>Jean-Daniel Fekete</name>
    </author>
    <author>
      <name>Narges Mahyar</name>
    </author>
    <author>
      <name>Carita Paradis</name>
    </author>
    <author>
      <name>Vasiliki Simaki</name>
    </author>
    <author>
      <name>Bettina Speckmann</name>
    </author>
    <link href="http://arxiv.org/abs/2209.13498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.13498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.14186v1</id>
    <updated>2022-09-28T16:00:19Z</updated>
    <published>2022-09-28T16:00:19Z</published>
    <title>How unitizing affects annotation of cohesion</title>
    <summary>  This paper investigates how unitizing affects external observers' annotation
of group cohesion. We compared unitizing techniques belonging to these
categories: interval coding, continuous coding, and a technique inspired by a
cognitive theory on event perception. We applied such techniques for sampling
coding units from a set of recordings of social interactions rich in behaviors
related to cohesion. Then, we compared the cohesion scores the observers
assigned to each coding unit. Results show that the three techniques can lead
to suitable ratings and that the technique inspired to cognitive theories leads
to scores reflecting variability in cohesion better than the other ones.
</summary>
    <author>
      <name>Eleonora Ceccaldi</name>
    </author>
    <author>
      <name>Nale Lehmann-Willenbrock</name>
    </author>
    <author>
      <name>Erica Volta</name>
    </author>
    <author>
      <name>Mohamed Chetouani</name>
    </author>
    <author>
      <name>Gualtiero Volpe</name>
    </author>
    <author>
      <name>Giovanna Varni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACII.2019.8925527</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACII.2019.8925527" rel="related"/>
    <link href="http://arxiv.org/abs/2209.14186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.14186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.15029v1</id>
    <updated>2022-09-29T18:09:16Z</updated>
    <published>2022-09-29T18:09:16Z</published>
    <title>Multimodal analogs to infer humanities visualization requirements</title>
    <summary>  Gaps and requirements for multi-modal interfaces for humanities can be
explored by observing the configuration of real-world environments and the
tasks of visitors within them compared to digital environments. Examples
include stores, museums, galleries, and stages with tasks similar to
visualization tasks such as overview, zoom and detail; multi-dimensional
reduction; collaboration; and comparison; with real-world environments offering
much richer interactions. Some of these capabilities exist with the technology
and visualization research, but not routinely available in implementations.
</summary>
    <author>
      <name>Richard Brath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures. Visualization for Digital Humanities 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.15029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.15029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.15041v1</id>
    <updated>2022-09-29T18:25:42Z</updated>
    <published>2022-09-29T18:25:42Z</published>
    <title>Summarizing text to embed qualitative data into visualizations</title>
    <summary>  Qualitative data can be conveyed with strings of text. Fitting longer text
into visualizations requires a) space to place the text inside the
visualization; and b) appropriate text to fit the space available. For
quantitative visualizations, space is available in area marks; or within
visualization layouts where the marks have an implied space (e.g. bar charts).
For qualitative visualizations, space is defined in common text layouts such as
prose paragraphs. To fit text within these layouts is a function for emerging
NLP capabilities such as summarization.
</summary>
    <author>
      <name>Richard Brath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, accepted at NLVIZ 2022: Exploring Research
  Opportunities for Natural Language, Text, and Data Visualization</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.15041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.15041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.00171v1</id>
    <updated>2022-10-01T02:41:14Z</updated>
    <published>2022-10-01T02:41:14Z</published>
    <title>PORTAL: Portal Widget for Remote Target Acquisition and Control in
  Immersive Virtual Environments</title>
    <summary>  This paper introduces PORTAL (POrtal widget for Remote Target Acquisition and
controL) that allows the user to interact with out-of-reach objects in a
virtual environment. We describe the PORTAL interaction technique for placing a
portal widget and interacting with target objects through the portal. We
conduct two formal user studies to evaluate PORTAL for selection and
manipulation functionalities. The results show PORTAL supports participants to
interact with remote objects successfully and precisely. Following that, we
discuss its potential and limitations, and future works.
</summary>
    <author>
      <name>Dongyun Han</name>
    </author>
    <author>
      <name>Donghoon Kim</name>
    </author>
    <author>
      <name>Isaac Cho</name>
    </author>
    <link href="http://arxiv.org/abs/2210.00171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.00725v1</id>
    <updated>2022-10-03T06:06:22Z</updated>
    <published>2022-10-03T06:06:22Z</published>
    <title>Unpacking Cultural Perceptions of Future Elder Care through Design
  Fiction</title>
    <summary>  We present a case using Design Fiction to unpack cultural perceptions of
future elder care rooted in the Asian context of Singapore. We created two
design fictions, addressing the tensions between filial piety and automated
care and the controversy of integrating elder care facilities into residential
communities. The design fictions took the visual forms of a shopping web page
and a petition site and the public were invited to make fictional decisions.
Received in total 109 responses, we identify the key tensions and value
conflicts and illustrate them through visual narratives. Further, we propose
the Asian perspective of positioning relationships as the protagonist in
creating elder care design fiction.
</summary>
    <author>
      <name>Tse Pei Ng</name>
    </author>
    <author>
      <name>Jung-Joo Lee</name>
    </author>
    <author>
      <name>Yiying Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IASDR conference 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.00725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.01362v1</id>
    <updated>2022-10-04T04:08:00Z</updated>
    <published>2022-10-04T04:08:00Z</published>
    <title>HapticLever: Kinematic Force Feedback using a 3D Pantograph</title>
    <summary>  HapticLever is a new kinematic approach for VR haptics which uses a 3D
pantograph to stiffly render large-scale surfaces using small-scale proxies.
The HapticLever approach does not consume power to render forces, but rather
puts a mechanical constraint on the end effector using a small-scale proxy
surface. The HapticLever approach provides stiff force feedback when the user
interacts with a static virtual surface, but allows the user to move their arm
freely when moving through free virtual space. We present the problem space,
the related work, and the HapticLever design approach.
</summary>
    <author>
      <name>Marcus Friedel</name>
    </author>
    <author>
      <name>Ehud Sharlin</name>
    </author>
    <author>
      <name>Ryo Suzuki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3526114.3558736</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3526114.3558736" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">UIST 2022 Poster</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.01362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.01362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04400v1</id>
    <updated>2022-10-10T02:14:43Z</updated>
    <published>2022-10-10T02:14:43Z</published>
    <title>Focus Plus: Detect Learner's Distraction by Web Camera in Distance
  Teaching</title>
    <summary>  Distance teaching has become popular these years because of the COVID-19
epidemic. However, both students and teachers face several challenges in
distance teaching, like being easy to distract. We proposed Focus+, a system
designed to detect learners' status with the latest AI technology from their
web camera to solve such challenges. By doing so, teachers can know students'
status, and students can regulate their learning experience. In this research,
we will discuss the expected model's design for training and evaluating the AI
detection model of Focus+.
</summary>
    <author>
      <name>Eason Chen</name>
    </author>
    <author>
      <name>Yuen Hsien Tseng</name>
    </author>
    <author>
      <name>Kuo-Ping Lo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 4 Figures, 2021 National Chair Professorship Academic
  Series: Teaching and Learning in Pandemic Era</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.04400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06915v1</id>
    <updated>2022-10-13T11:28:58Z</updated>
    <published>2022-10-13T11:28:58Z</published>
    <title>Adapting Behaviour Based On Trust In Human-Agent Ad Hoc Teamwork</title>
    <summary>  This work proposes a framework that incorporates trust in an ad hoc teamwork
scenario with human-agent teams, where an agent must collaborate with a human
to perform a task. During the task, the agent must infer, through interactions
and observations, how much the human trusts it and adapt its behaviour to
maximize the team's performance. To achieve this, we propose collecting data
from human participants in experiments to define different settings (based on
trust levels) and learning optimal policies for each of them. Then, we create a
module to infer the current setting (depending on the amount of trust).
Finally, we validate this framework in a real-world scenario and analyse how
this adaptable behaviour affects trust.
</summary>
    <author>
      <name>Ana Carrasco</name>
    </author>
    <link href="http://arxiv.org/abs/2210.06915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08175v1</id>
    <updated>2022-10-15T03:23:05Z</updated>
    <published>2022-10-15T03:23:05Z</published>
    <title>DataTV: Streaming Data Videos for Storytelling</title>
    <summary>  Data videos -- motion graphics that incorporate visualizations -- have been
recognized as an effective way to communicate ideas, but creating such video
requires both time and expertise, precluding them from being created and
streamed live. We introduce DataTV, a system for combining multiple media
sources in real time. We validate our work through an expert review involving
researchers using the DataTV prototype to create a one-minute data video for
their current project. Results show that the new method facilitates rapid
creation and enables users to focus on the narrative rather than mechanics of
video production.
</summary>
    <author>
      <name>Zhenpeng Zhao</name>
    </author>
    <author>
      <name>Niklas Elmqvist</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.08175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.11820v2</id>
    <updated>2022-11-07T13:23:09Z</updated>
    <published>2022-10-21T08:47:09Z</published>
    <title>A drag-and-drop proof tactic</title>
    <summary>  We explore the features of a user interface where formal proofs can be built
through gestural actions. In particular, we show how proof construction steps
can be associated to drag-and-drop actions. We argue that this can provide
quick and intuitive proof construction steps. This work builds on theoretical
tools coming from deep inference. It also resumes and integrates some ideas of
the former proof-by-pointing project.
</summary>
    <author>
      <name>Pablo Donato</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX, PARTOUT</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre-Yves Strub</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Werner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIX, PARTOUT</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3497775.3503692</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3497775.3503692" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CPP '22: 11th ACM SIGPLAN International Conference on Certified
  Programs and Proofs, Jan 2022, Philadelphia, United States. pp.197-209</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.11820v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11820v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12220v1</id>
    <updated>2022-10-21T19:59:27Z</updated>
    <published>2022-10-21T19:59:27Z</published>
    <title>Considerations for Visualizing Uncertainty in Clinical Machine Learning
  Models</title>
    <summary>  Clinician-facing predictive models are increasingly present in the healthcare
setting. Regardless of their success with respect to performance metrics, all
models have uncertainty. We investigate how to visually communicate uncertainty
in this setting in an actionable, trustworthy way. To this end, we conduct a
qualitative study with cardiac critical care clinicians. Our results reveal
that clinician trust may be impacted most not by the degree of uncertainty, but
rather by how transparent the visualization of what the sources of uncertainty
are. Our results show a clear connection between feature interpretability and
clinical actionability.
</summary>
    <author>
      <name>Caitlin F. Harrigan</name>
    </author>
    <author>
      <name>Gabriela Morgenshtern</name>
    </author>
    <author>
      <name>Anna Goldenberg</name>
    </author>
    <author>
      <name>Fanny Chevalier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Prepared for the CHI 2021 Workshop: Realizing AI in Healthcare:
  Challenges Appearing in the Wild
  https://dl.acm.org/doi/10.1145/3411763.3441347</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13151v1</id>
    <updated>2022-10-24T12:09:00Z</updated>
    <published>2022-10-24T12:09:00Z</published>
    <title>Good governance and national information transparency: A comparative
  study of 117 countries</title>
    <summary>  Information transparency is a major building block of responsible
governments. We explored factors influencing the information transparency of
117 world nations. After controlling for the effects of confounding variables
of wealth (GDP per capita), corruption rate, population density, human capital,
and telecommunication infrastructure, we found that the good governance indices
(democracy, economy, and management) were strong and stable predictors of
information transparency of world nations.
</summary>
    <author>
      <name>Mahmood Khosrowjerdi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-96957-8_14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-96957-8_14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2022. Lecture Notes in Computer Science, 13192, pp. 143-160</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.13151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13283v1</id>
    <updated>2022-10-24T14:24:26Z</updated>
    <published>2022-10-24T14:24:26Z</published>
    <title>Content Transfer Across Multiple Screens with Combined Eye-Gaze and
  Touch Interaction -- A Replication Study</title>
    <summary>  In this paper, we describe the results of replicating one of our studies from
two years ago which compares two techniques for transferring content across
multiple screens in VR. Results from the previous study have shown that a
combined gaze and touch input can outperform a bimanual touch-only input in
terms of task completion time, simulator sickness, task load and usability.
Except for the simulator sickness, these findings could be validated by the
replication. The difference with regards to simulator sickness and variations
in absolute scores of the other measures could be explained by a different set
of user with less VR experience.
</summary>
    <author>
      <name>Verena Biener</name>
    </author>
    <author>
      <name>Jens Grubert</name>
    </author>
    <link href="http://arxiv.org/abs/2210.13283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13820v1</id>
    <updated>2022-10-25T07:53:50Z</updated>
    <published>2022-10-25T07:53:50Z</published>
    <title>Supporting Electronics Learning through Augmented Reality</title>
    <summary>  Understanding electronics is a critical area in the maker scene. Many of the
makers' projects require electronics knowledge to connect microcontrollers with
sensors and actuators. Yet, learning electronics is challenging, as internal
component processes remain invisible, and students often fear personal harm or
component damage. Augmented Reality (AR) applications are developed to support
electronics learning and visualize complex processes. This paper reflects on
related work around AR and electronics that characterize open research
challenges around the four characteristics functionality, fidelity, feedback
type, and interactivity.
</summary>
    <author>
      <name>Thomas Kosch</name>
    </author>
    <author>
      <name>Julian Rasch</name>
    </author>
    <author>
      <name>Albrecht Schmidt</name>
    </author>
    <author>
      <name>Sebastian Feger</name>
    </author>
    <link href="http://arxiv.org/abs/2210.13820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00597v1</id>
    <updated>2022-10-29T01:40:04Z</updated>
    <published>2022-10-29T01:40:04Z</published>
    <title>Mixed Reality Interface for Digital Twin of Plant Factory</title>
    <summary>  An easier and intuitive interface architecture is necessary for digital twin
of plant factory. I suggest an immersive and interactive mixed reality
interface for digital twin models of smart farming, for remote work rather than
simulation of components. The environment is constructed with UI display and a
streaming background scene, which is a real time scene taken from camera device
located in the plant factory, processed with deformable neural radiance fields.
User can monitor and control the remote plant factory facilities with HMD or 2D
display based mixed reality environment. This paper also introduces detailed
concept and describes the system architecture to implement suggested mixed
reality interface.
</summary>
    <author>
      <name>Byunghyun Ban</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.00597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12434v1</id>
    <updated>2022-11-22T17:44:03Z</updated>
    <published>2022-11-22T17:44:03Z</published>
    <title>Expansive Participatory AI: Supporting Dreaming within Inequitable
  Institutions</title>
    <summary>  Participatory Artificial Intelligence (PAI) has recently gained interest by
researchers as means to inform the design of technology through collective's
lived experience. PAI has a greater promise than that of providing useful input
to developers, it can contribute to the process of democratizing the design of
technology, setting the focus on what should be designed. However, in the
process of PAI there existing institutional power dynamics that hinder the
realization of expansive dreams and aspirations of the relevant stakeholders.
In this work we propose co-design principals for AI that address institutional
power dynamics focusing on Participatory AI with youth.
</summary>
    <author>
      <name>Michael Alan Chang</name>
    </author>
    <author>
      <name>Shiran Dudy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, Human-Centered AI workshop</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Human-Centered AI workshop (HCAI) 2022, NEURIPS</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.12434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12660v1</id>
    <updated>2022-11-23T01:34:42Z</updated>
    <published>2022-11-23T01:34:42Z</published>
    <title>The Impact of Generative AI on the Future of Visual Content Marketing</title>
    <summary>  In today's world of marketing, it is necessary to have visually appealing
content. Visual material has become an essential area of focus for every
company as a result of the widespread availability of gadgets for mass
communication and extended visual advancements. Similarly, artificial
intelligence is also gaining ground and it is proving to be the most
revolutionary technological advancement thus far. The integration of visual
content with artificial intelligence is the key to acquiring and retaining
loyal customers; its absence from the overarching marketing strategy of any
production raises a red flag that could ultimately result in a smaller market
share for that company.
</summary>
    <author>
      <name>Shiva Mayahi</name>
    </author>
    <author>
      <name>Marko Vidrih</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.12660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16914v1</id>
    <updated>2022-11-30T11:24:58Z</updated>
    <published>2022-11-30T11:24:58Z</published>
    <title>A Design Philosophy for Agents in the Smart Home</title>
    <summary>  The home is often the most private space in people's lives, and not one in
which they expect to be surveilled. However, today's market for smart home
devices has quickly evolved to include products that monitor, automate, and
present themselves as human. After documenting some of the more unusual
emergent problems with contemporary devices, this body of work seeks to develop
a design philosophy for intelligent agents in the smart home that can act as an
alternative to the ways that these devices are currently built. This is then
applied to the design of privacy empowering technologies, representing the
first steps from the devices of the present towards a more respectful future.
</summary>
    <author>
      <name>William Seymour</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3334480.3375032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3334480.3375032" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Extended Abstracts of the 2020 CHI Conference on Human Factors
  in Computing Systems</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.16914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.01205v1</id>
    <updated>2022-12-02T14:34:27Z</updated>
    <published>2022-12-02T14:34:27Z</published>
    <title>Diver Interest via Pointing: Human-Directed Object Inspection for AUVs</title>
    <summary>  In this paper, we present the Diver Interest via Pointing (DIP) algorithm, a
highly modular method for conveying a diver's area of interest to an autonomous
underwater vehicle (AUV) using pointing gestures for underwater human-robot
collaborative tasks. DIP uses a single monocular camera and exploits human body
pose, even with complete dive gear, to extract underwater human pointing
gesture poses and their directions. By extracting 2D scene geometry based on
the human body pose and density of salient feature points along the direction
of pointing, using a low-level feature detector, the DIP algorithm is able to
locate objects of interest as indicated by the diver.
</summary>
    <author>
      <name>Chelsey Edge</name>
    </author>
    <author>
      <name>Junaed Sattar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under submission at ICRA23</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.01205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.01205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03452v1</id>
    <updated>2022-12-07T04:28:26Z</updated>
    <published>2022-12-07T04:28:26Z</published>
    <title>"just like therapy!": Investigating the Potential of Storytelling in
  Online Postpartum Depression Communities</title>
    <summary>  One in seven women encounter postpartum depression upon transitioning to
motherhood. Many of them frequently seek social support in online support
groups. We conducted a mixed-methods formative research to assess the potential
of digital storytelling on those online platforms. We observed that mothers
acquire social support from online groups through storytelling. We present
design recommendations for online postpartum depression (PPD) communities to
utilize storytelling in fostering emotional support and providing relevant
information and education through storytelling.
</summary>
    <author>
      <name>Farhat Tasnim Progga</name>
    </author>
    <author>
      <name>Sabirat Rubya</name>
    </author>
    <link href="http://arxiv.org/abs/2212.03452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.05588v1</id>
    <updated>2022-12-11T19:53:22Z</updated>
    <published>2022-12-11T19:53:22Z</published>
    <title>Towards a Learner-Centered Explainable AI: Lessons from the learning
  sciences</title>
    <summary>  In this short paper, we argue for a refocusing of XAI around human learning
goals. Drawing upon approaches and theories from the learning sciences, we
propose a framework for the learner-centered design and evaluation of XAI
systems. We illustrate our framework through an ongoing case study in the
context of AI-augmented social work.
</summary>
    <author>
      <name>Anna Kawakami</name>
    </author>
    <author>
      <name>Luke Guerdan</name>
    </author>
    <author>
      <name>Yang Cheng</name>
    </author>
    <author>
      <name>Anita Sun</name>
    </author>
    <author>
      <name>Alison Hu</name>
    </author>
    <author>
      <name>Kate Glazko</name>
    </author>
    <author>
      <name>Nikos Arechiga</name>
    </author>
    <author>
      <name>Matthew Lee</name>
    </author>
    <author>
      <name>Scott Carter</name>
    </author>
    <author>
      <name>Haiyi Zhu</name>
    </author>
    <author>
      <name>Kenneth Holstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Human-Centered Explainable AI Workshop at ACM CHI Conference on
  Human Factors in Computing Systems 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.05588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.05588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13299v1</id>
    <updated>2022-12-26T21:15:31Z</updated>
    <published>2022-12-26T21:15:31Z</published>
    <title>Leveraging Collaboration for Multifaceted Design and Product Teams: A
  Financial Perspective</title>
    <summary>  Collaboration is a key driving force for a team's success. In this case
study, we discuss the collaboration practices of the Design Team and a subset
of product teams at Chatham Financial -- a financial risk management advisory
and technology firm. The Design Team's collaboration workflow has four key
elements surrounding the structure, cross-team communication, onboarding, and
feedback that occurs both in team and cross-team collaborative partnerships.
Each of the key elements leads to a unique set of challenges and opportunities
for the Design Team. We analyze the current state of each element, their value
proposition, challenges, and initiatives undertaken to make the collaboration
practice more robust
</summary>
    <author>
      <name>Esha Shandilya</name>
    </author>
    <author>
      <name>Jacalyn DeFeo</name>
    </author>
    <link href="http://arxiv.org/abs/2212.13299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00003v1</id>
    <updated>2022-12-28T23:50:27Z</updated>
    <published>2022-12-28T23:50:27Z</published>
    <title>Emotion in Cognitive Architecture: Emergent Properties from Interactions
  with Human Emotion</title>
    <summary>  This document presents endeavors to represent emotion in a computational
cognitive architecture. The first part introduces research organizing with two
axes of emotional affect: pleasantness and arousal. Following this basic of
emotional components, the document discusses an aspect of emergent properties
of emotion, showing interaction studies with human users. With these past
author's studies, the document concludes that the advantage of the cognitive
human-agent interaction approach is in representing human internal states and
processes.
</summary>
    <author>
      <name>Junya Morita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at HAI'22 workshop on Cognitive Human-agent Interaction
  (https://sites.google.com/view/chai-workshop22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00987v3</id>
    <updated>2023-02-18T06:40:07Z</updated>
    <published>2023-01-03T07:41:29Z</published>
    <title>AI in HCI Design and User Experience</title>
    <summary>  In this chapter, we review and discuss the transformation of AI technology in
HCI/UX work and assess how AI technology will change how we do the work. We
first discuss how AI can be used to enhance the result of user research and
design evaluation. We then discuss how AI technology can be used to enhance
HCI/UX design. Finally, we discuss how AI-enabled capabilities can improve UX
when users interact with computing systems, applications, and services.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00987v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00987v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.02527v1</id>
    <updated>2023-01-06T14:38:35Z</updated>
    <published>2023-01-06T14:38:35Z</published>
    <title>Avatar-centred AR Collaborative Mobile Interaction</title>
    <summary>  Interaction with the physical environment and different users is essential to
foster a collaborative experience. For this, we propose an interaction based on
a central point represented by an Augmented Reality marker in which several
users can capture the attention and interact with a virtual avatar. The
interface provides different game modes, with various challenges, supporting a
collaborative mobile interaction. The system fosters various group interactions
with a virtual avatar and enables various tasks with playful and didactic
components.
</summary>
    <author>
      <name>Bianca Marques</name>
    </author>
    <author>
      <name>Rui Nóbrega</name>
    </author>
    <author>
      <name>Carmen Morgado</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, in Portuguese language, 4 figures, 1 table, accepted and
  presented at ICGI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.02527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05198v2</id>
    <updated>2023-01-13T09:57:26Z</updated>
    <published>2023-01-12T18:33:16Z</published>
    <title>The Keyword Explorer Suite: A Toolkit for Understanding Online
  Populations</title>
    <summary>  We have developed a set of Python applications that use large language models
to identify and analyze data from social media platforms relevant to a
population of interest. Our pipeline begins with using OpenAI's GPT-3 to
generate potential keywords for identifying relevant text content from the
target population. The keywords are then validated, and the content downloaded
and analyzed using GPT-3 embedding and manifold reduction. Corpora are then
created to fine-tune GPT-2 models to explore latent information via
prompt-based queries. These tools allow researchers and practitioners to gain
valuable insights into population subgroups online.
  Source code at https://github.com/pgfeldman/KeywordExplorer
</summary>
    <author>
      <name>Philip Feldman</name>
    </author>
    <author>
      <name>Shimei Pan</name>
    </author>
    <author>
      <name>James R. Foulds</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.05198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.1.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07509v1</id>
    <updated>2023-01-18T13:31:01Z</updated>
    <published>2023-01-18T13:31:01Z</published>
    <title>Coverage of Course Topics in Learnersourced SQL Exercises</title>
    <summary>  Learnersourcing is a common task in modern computing classrooms, where it is
used, for example, for the creation of educational resources such as
multiple-choice questions and programming exercises. One less studied type of
learnersourced artefact is SQL exercises. In this work, we explore how well
different SQL topics are covered by learnersourced SQL exercises. Covering most
course topics would allow students to practice the full content of the course
by completing learnersourced exercises. Our results suggest that
learnersourcing can be used to create a large pool of SQL exercises that cover
most of the topics of the course.
</summary>
    <author>
      <name>Nea Pirttinen</name>
    </author>
    <author>
      <name>Arto Hellas</name>
    </author>
    <author>
      <name>Juho Leinonen</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07687v1</id>
    <updated>2022-12-14T00:07:06Z</updated>
    <published>2022-12-14T00:07:06Z</published>
    <title>Maybe, Maybe Not: A Survey on Uncertainty in Visualization</title>
    <summary>  Understanding and evaluating uncertainty play a key role in decision-making.
When a viewer studies a visualization that demands inference, it is necessary
that uncertainty is portrayed in it. This paper showcases the importance of
representing uncertainty in visualizations. It provides an overview of
uncertainty visualization and the challenges authors and viewers face when
working with such charts. I divide the visualization pipeline into four parts,
namely data collection, preprocessing, visualization, and inference, to
evaluate how uncertainty impacts them. Next, I investigate the authors'
methodologies to process and design uncertainty. Finally, I contribute by
exploring future paths for uncertainty visualization.
</summary>
    <author>
      <name>Krisha Mehta</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.08835v1</id>
    <updated>2023-01-21T00:28:03Z</updated>
    <published>2023-01-21T00:28:03Z</published>
    <title>Extended Reality and Internet of Things for Hyper-Connected Metaverse
  Environments</title>
    <summary>  The Metaverse encompasses technologies related to the internet, virtual and
augmented reality, and other domains toward smart interfaces that are
hyper-connected, immersive, and engaging. However, Metaverse applications face
inherent disconnects between virtual and physical components and interfaces.
This work explores how an Extended Metaverse framework can be used to increase
the seamless integration of interoperable agents between virtual and physical
environments. It contributes an early theory and practice toward the synthesis
of virtual and physical smart environments anticipating future designs and
their potential for connected experiences.
</summary>
    <author>
      <name>Jie Guan</name>
    </author>
    <author>
      <name>Jay Irizawa</name>
    </author>
    <author>
      <name>Alexis Morris</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VRW55335.2022.00043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VRW55335.2022.00043" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 2022 IEEE Conference on Virtual Reality and 3D User
  Interfaces Abstracts and Workshops (VRW), Christchurch, New Zealand, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.08835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.08835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09406v1</id>
    <updated>2023-01-23T13:03:58Z</updated>
    <published>2023-01-23T13:03:58Z</published>
    <title>The Reasonable Effectiveness of Diverse Evaluation Data</title>
    <summary>  In this paper, we present findings from an semi-experimental exploration of
rater diversity and its influence on safety annotations of conversations
generated by humans talking to a generative AI-chat bot. We find significant
differences in judgments produced by raters from different geographic regions
and annotation platforms, and correlate these perspectives with demographic
sub-groups. Our work helps define best practices in model development --
specifically human evaluation of generative models -- on the backdrop of
growing work on sociotechnical AI evaluations.
</summary>
    <author>
      <name>Lora Aroyo</name>
    </author>
    <author>
      <name>Mark Diaz</name>
    </author>
    <author>
      <name>Christopher Homan</name>
    </author>
    <author>
      <name>Vinodkumar Prabhakaran</name>
    </author>
    <author>
      <name>Alex Taylor</name>
    </author>
    <author>
      <name>Ding Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.09406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11799v1</id>
    <updated>2023-01-25T03:40:25Z</updated>
    <published>2023-01-25T03:40:25Z</published>
    <title>Factors influencing to use of Bluezone</title>
    <summary>  This study aims to understand the main factors and their influence on the
behavioral intention of users about using Bluezone. Surveys are sent to users
through the Google Form tool. Experimental results through analysis of
exploratory factors on 224 survey subjects show that there are 4 main factors
affecting user behavior. Structural equation modeling indicates that trust,
performance expectations, effort expectations, and social influence have a
positive impact on behavioral intention of using Bluezone
</summary>
    <author>
      <name>Vinh T. Nguyen</name>
    </author>
    <author>
      <name>Anh T. Nguyen</name>
    </author>
    <author>
      <name>Tan H. Nguyen</name>
    </author>
    <author>
      <name>Dinh K. Luong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Vietnamese language</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13388v1</id>
    <updated>2023-01-31T03:37:59Z</updated>
    <published>2023-01-31T03:37:59Z</published>
    <title>Large Music Recommendation Studies for Small Teams</title>
    <summary>  Running live music recommendation studies without direct industry
partnerships can be a prohibitively daunting task, especially for small teams.
In order to help future researchers interested in such evaluations, we present
a number of struggles we faced in the process of generating our own such
evaluation system alongside potential solutions. These problems span the topics
of users, data, computation, and application architecture.
</summary>
    <author>
      <name>Kyle Robinson</name>
    </author>
    <author>
      <name>Dan Brown</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Late Breaking/Demo, Proc. of the 22nd Int. Society for Music
  Information Retrieval Conf., Online, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.13388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02050v1</id>
    <updated>2023-02-04T01:21:12Z</updated>
    <published>2023-02-04T01:21:12Z</published>
    <title>Location-based AR for Social Justice: Case Studies, Lessons, and Open
  Challenges</title>
    <summary>  Dear Visitor and Charleston Reconstructed were location-based augmented
reality (AR) experiences created between 2018 and 2020 dealing with two
controversial monument sites in the US. The projects were motivated by the
ability of AR to 1) link layers of context to physical sites in ways that are
otherwise difficult or impossible and 2) to visualize changes to physical
spaces, potentially inspiring changes to the spaces themselves. We discuss the
projects' motivations, designs, and deployments. We reflect on how physical
changes to the projects' respective sites radically altered their outcomes, and
we describe lessons for future work in location-based AR, particularly for
projects in contested spaces.
</summary>
    <author>
      <name>Hope Schroeder</name>
    </author>
    <author>
      <name>Rob Tokanel</name>
    </author>
    <author>
      <name>Kyle Qian</name>
    </author>
    <author>
      <name>Khoi Le</name>
    </author>
    <link href="http://arxiv.org/abs/2302.02050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02525v1</id>
    <updated>2023-02-06T01:48:59Z</updated>
    <published>2023-02-06T01:48:59Z</published>
    <title>Privacy concerns from variances in spatial navigability in VR</title>
    <summary>  Current Virtual Reality (VR) input devices make it possible to navigate a
virtual environment and record immersive, personalized data regarding the
user's movement and specific behavioral habits, which brings the question of
the user's privacy concern to the forefront. In this article, the authors
propose to investigate Machine Learning driven learning algorithms that try to
learn with human users co-operatively and can be used to countermand existing
privacy concerns in VR but could also be extended to Augmented Reality (AR)
platforms.
</summary>
    <author>
      <name>Aryabrata Basu</name>
    </author>
    <author>
      <name>Mohammad Jahed Murad Sunny</name>
    </author>
    <author>
      <name>Jayasri Sai Nikitha Guthula</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited Talk for IEEE VR 2023 Workshop on Security and Privacy for
  Immersive Virtual Worlds (SPIVW2023), Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.02525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02648v1</id>
    <updated>2023-02-06T09:43:25Z</updated>
    <published>2023-02-06T09:43:25Z</published>
    <title>First steps towards quantum machine learning applied to the
  classification of event-related potentials</title>
    <summary>  Low information transfer rate is a major bottleneck for brain-computer
interfaces based on non-invasive electroencephalography (EEG) for clinical
applications. This led to the development of more robust and accurate
classifiers. In this study, we investigate the performance of quantum-enhanced
support vector classifier (QSVC). Training (predicting) balanced accuracy of
QSVC was 83.17 (50.25) %. This result shows that the classifier was able to
learn from EEG data, but that more research is required to obtain higher
predicting accuracy. This could be achieved by a better configuration of the
classifier, such as increasing the number of shots.
</summary>
    <author>
      <name>Grégoire Cattan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PUT</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandre Quemy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PUT</arxiv:affiliation>
    </author>
    <author>
      <name>Anton Andreev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIPSA-Services</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French language</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.02648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02741v1</id>
    <updated>2023-02-06T12:37:44Z</updated>
    <published>2023-02-06T12:37:44Z</published>
    <title>Challenges and Opportunities of Content Optimization for Freeform User
  Interfaces</title>
    <summary>  While recent innovations on shape technologies allow for the creation of
displays with almost unlimited form factors, current graphical user interfaces
still rely on rectangular layouts and contents. This rectangular legacy hinders
the progress of freeform displays, which are particularly relevant for
pervasive scenarios to display interactive dynamic content where and when
needed. By challenging the prevailing layout tradition on rectangular displays,
freeform user interfaces raise design challenges which call for exploring the
interlink between computational approaches and user interface generation and
adaptation. In this position paper we report on previous work on content
optimization for freeform user interfaces and anticipate the upcoming
challenges and opportunities.
</summary>
    <author>
      <name>Aziz Niyazov</name>
    </author>
    <author>
      <name>Kaixing Zhao</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Nicolas Mellado</name>
    </author>
    <author>
      <name>Loic Barthe</name>
    </author>
    <author>
      <name>Marcos Serrano</name>
    </author>
    <link href="http://arxiv.org/abs/2302.02741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03915v1</id>
    <updated>2023-02-08T07:17:57Z</updated>
    <published>2023-02-08T07:17:57Z</published>
    <title>Exploring Affordances for AR in Laparoscopy</title>
    <summary>  This paper explores the possibilities of designing AR interfaces to be used
during laparoscopy surgery. It suggests that the laparoscopic video be
displayed on AR headsets and that surgeons can consult preoperative image data
on that display. Interaction with these elements is necessary, and no patterns
exist to design them. Thus the paper proposes a head-gaze and clicker approach
that is effective and minimalist. Finally, a prototype is presented, and an
evaluation protocol is briefly discussed.
</summary>
    <author>
      <name>Matheus Negrão</name>
    </author>
    <author>
      <name>Joaquim Jorge</name>
    </author>
    <author>
      <name>João Vissoci</name>
    </author>
    <author>
      <name>Regis Kopper</name>
    </author>
    <author>
      <name>Anderson Maciel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VRW58643.2023.00037</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VRW58643.2023.00037" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted for presentation at the 2nd XR Health
  workshop - XR Technologies for Healthcare and Wellbeing (XR Health)
  co-located with IEEE VR 2023, Shangai PRC
  https://ieeevr.org/2023/contribute/workshoppapers/#XRHealth</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.11577v2</id>
    <updated>2023-03-21T14:16:06Z</updated>
    <published>2023-01-25T10:34:38Z</published>
    <title>Explainable AI does not provide the explanations end-users are asking
  for</title>
    <summary>  Explainable Artificial Intelligence (XAI) techniques are frequently required
by users in many AI systems with the goal of understanding complex models,
their associated predictions, and gaining trust. While suitable for some
specific tasks during development, their adoption by organisations to enhance
trust in machine learning systems has unintended consequences. In this paper we
discuss XAI's limitations in deployment and conclude that transparency
alongside with rigorous validation are better suited to gaining trust in AI
systems.
</summary>
    <author>
      <name>Savio Rozario</name>
    </author>
    <author>
      <name>George Čevora</name>
    </author>
    <link href="http://arxiv.org/abs/2302.11577v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.11577v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.11722v1</id>
    <updated>2023-02-23T00:59:35Z</updated>
    <published>2023-02-23T00:59:35Z</published>
    <title>CrowDC: A Divide-and-Conquer Approach for Paired Comparisons in
  Crowdsourcing</title>
    <summary>  Ranking a set of samples based on subjectivity, such as the experience
quality of streaming video or the happiness of images, has been a typical
crowdsourcing task. Numerous studies have employed paired comparison analysis
to solve challenges since it reduces the workload for participants by allowing
them to select a single solution. Nonetheless, to thoroughly compare all target
combinations, the number of tasks increases quadratically. This paper presents
``CrowDC'', a divide-and-conquer algorithm for paired comparisons. Simulation
results show that when ranking more than 100 items, CrowDC can reduce 40-50% in
the number of tasks while maintaining 90-95% accuracy compared to the baseline
approach.
</summary>
    <author>
      <name>Ming-Hung Wang</name>
    </author>
    <author>
      <name>Chia-Yuan Zhang</name>
    </author>
    <author>
      <name>Jia-Ru Song</name>
    </author>
    <link href="http://arxiv.org/abs/2302.11722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.11722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.12834v1</id>
    <updated>2023-02-25T04:07:03Z</updated>
    <published>2023-02-25T04:07:03Z</published>
    <title>Leveraging Large Language Model and Story-Based Gamification in
  Intelligent Tutoring System to Scaffold Introductory Programming Courses: A
  Design-Based Research Study</title>
    <summary>  Programming skills are rapidly becoming essential for many educational paths
and career opportunities. Yet, for many international students, the traditional
approach to teaching introductory programming courses can be a significant
challenge due to the complexities of the language, the lack of prior
programming knowledge, and the language and cultural barriers. This study
explores how large language models and gamification can scaffold coding
learning and increase Chinese students sense of belonging in introductory
programming courses. In this project, a gamification intelligent tutoring
system was developed to adapt to Chinese international students learning needs
and provides scaffolding to support their success in introductory computer
programming courses.
</summary>
    <author>
      <name>Chen Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Doctoral consortium for IUI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.12834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.12834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06333v2</id>
    <updated>2023-03-25T01:40:34Z</updated>
    <published>2023-03-11T07:30:52Z</published>
    <title>Parachute: Evaluating Interactive Human-LM Co-writing Systems</title>
    <summary>  A surge of advances in language models (LMs) has led to significant interest
in using LMs to build co-writing systems, in which humans and LMs interactively
contribute to a shared writing artifact. However, there is a lack of studies
assessing co-writing systems in interactive settings. We propose a
human-centered evaluation framework, Parachute, for interactive co-writing
systems. Parachute showcases an integrative view of interaction evaluation,
where each evaluation aspect consists of categorized practical metrics.
Furthermore, we present Parachute with a use case to demonstrate how to
evaluate and compare co-writing systems using Parachute.
</summary>
    <author>
      <name>Hua Shen</name>
    </author>
    <author>
      <name>Tongshuang Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CHI'23 In2Writing Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.06333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08222v4</id>
    <updated>2025-01-12T00:43:58Z</updated>
    <published>2023-03-14T20:32:06Z</published>
    <title>Disconnected from Reality: Do the core concepts of the metaverse exclude
  disabled individuals?</title>
    <summary>  Commercially-driven metaverse development has been driven by philosophical
and science fiction concepts. Through translating these concepts into products,
the developers may have inadvertently excluded individuals with disabilities
from this new expanded reality. This ideologically-driven development is
presented in this paper through a brief background of what we see as the most
influential of these concepts, and explain how these might affect disabled
individuals wishing to engage with said products. It is our hope that these
ideas prompt conversation on future inclusivity access from the concept stage
of future metaverse development.
</summary>
    <author>
      <name>Mark Quinlan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TIAM23 Workshop at ACM CHI 23, Hamburg, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.08222v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08222v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.09304v1</id>
    <updated>2023-03-16T13:29:03Z</updated>
    <published>2023-03-16T13:29:03Z</published>
    <title>Thermal Feedback for Transparency in Human-Robot Interaction</title>
    <summary>  Robots can support humans in tedious tasks, as well as provide social
support. However, the decision-making and behavior of robots is not always
clear to the human interaction partner. In this work, we discuss the
opportunity of using thermal feedback as an additional modality to create
transparent interactions. We then present scenarios where thermal feedback is
incorporated into the interaction e.g. to unobtrusively communicate the
behavior of the robot. We highlight the limitations and challenges of
temperature-based feedback, which can be explored in future research.
</summary>
    <author>
      <name>Svenja Yvonne Schött</name>
    </author>
    <link href="http://arxiv.org/abs/2303.09304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.09304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10174v1</id>
    <updated>2023-03-10T03:19:25Z</updated>
    <published>2023-03-10T03:19:25Z</published>
    <title>Visual Studio Code in Introductory Computer Science Course: An
  Experience Report</title>
    <summary>  Involving integrated development environments (IDEs) in introductory-level
(CS1) programming courses is critical. However, it is difficult for instructors
to find a suitable IDE that is beginner friendly and supports strong
functionality. In this paper, we report the experience of using Visual Studio
Code (VS Code) in a CS1 programming course. We describe our motivation for
choosing VS Code and how we introduce it to students. We create comprehensive
guidance with hierarchical indexing to help students with diverse programming
backgrounds. We perform an experimental evaluation of students' programming
experience of using VS Code and validate the VS Code together with guidance as
a promising solution for CS1 programming courses.
</summary>
    <author>
      <name>Jialiang Tan</name>
    </author>
    <author>
      <name>Yu Chen</name>
    </author>
    <author>
      <name>Shuyin Jiao</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13522v1</id>
    <updated>2023-01-30T15:02:43Z</updated>
    <published>2023-01-30T15:02:43Z</published>
    <title>Online Assemblies: Civic Technologies Reshaping the Public Space</title>
    <summary>  Speaking or writing of political assemblies tends to evoke the action of
people gathering to deliberate, or the spaces in which this deliberation might
take place. One thing that is often overlooked, however, is the fact that these
spaces can be digital. Online assemblies have become more widespread in recent
years; from the first Web forums to civic technologies specifically designed to
host collective political debates. As digital services affect our possibilities
for political mobilization and participation, I will here attempt to define the
qualities specific to online assemblies, and to identify several patterns and
continuities in the design features of civic technologies offering online
spaces for debate.
</summary>
    <author>
      <name>Tallulah Frappier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3, CESSP</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2303.13522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13527v1</id>
    <updated>2023-02-07T03:57:56Z</updated>
    <published>2023-02-07T03:57:56Z</published>
    <title>Dataset for predicting cybersickness from a virtual navigation task</title>
    <summary>  This work presents a dataset collected to predict cybersickness in virtual
reality environments. The data was collected from navigation tasks in a virtual
environment designed to induce cybersickness. The dataset consists of many data
points collected from diverse participants, including physiological responses
(EDA and Heart Rate) and self-reported cybersickness symptoms. The paper will
provide a detailed description of the dataset, including the arranged
navigation task, the data collection procedures, and the data format. The
dataset will serve as a valuable resource for researchers to develop and
evaluate predictive models for cybersickness and will facilitate more research
in cybersickness mitigation.
</summary>
    <author>
      <name>Yuyang Wang</name>
    </author>
    <author>
      <name>Ruichen Li</name>
    </author>
    <author>
      <name>Jean-Rémy Chardonnet</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <link href="http://arxiv.org/abs/2303.13527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; J.0; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13535v1</id>
    <updated>2023-02-14T10:34:58Z</updated>
    <published>2023-02-14T10:34:58Z</published>
    <title>The Exploration and Evaluation of Generating Affective 360$^\circ$
  Panoramic VR Environments Through Neural Style Transfer</title>
    <summary>  Affective virtual reality (VR) environments with varying visual style can
impact users' valence and arousal responses. We applied Neural Style Transfer
(NST) to generate 360$^\circ$ VR environments that elicited users' varied
valence and arousal responses. From a user study with 30 participants, findings
suggested that generative VR environments changed participants' arousal
responses but not their valence levels. The generated visual features, e.g.,
textures and colors, also altered participants' affective perceptions. Our work
contributes novel insights about how users respond to generative VR
environments and provided a strategy for creating affective VR environments
without altering content.
</summary>
    <author>
      <name>Yanheng Li</name>
    </author>
    <author>
      <name>Long Bai</name>
    </author>
    <author>
      <name>Yaxuan Mao</name>
    </author>
    <author>
      <name>Xuening Peng</name>
    </author>
    <author>
      <name>Zehao Zhang</name>
    </author>
    <author>
      <name>Xin Tong</name>
    </author>
    <author>
      <name>Ray LC</name>
    </author>
    <link href="http://arxiv.org/abs/2303.13535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13544v1</id>
    <updated>2023-03-16T00:35:36Z</updated>
    <published>2023-03-16T00:35:36Z</published>
    <title>Empower Children in Nigeria to Design the Future of Artificial
  Intelligence (AI) through Writing</title>
    <summary>  This paper presents a new approach to engaging children in Nigeria to share
their views of AI. This approach is centered on an inclusive writing contest
for children in a secondary school in Abuja to write about AI to compete for
prizes and share their writings with others. A preliminary analysis of the
first 11 articles we received exhibits diverse gender and ethnic representation
that conveys cultural values and perspectives distinct from those of the
children in the western countries. This finding suggests future work to conduct
in-depth cross-cultural analysis of the articles and to replicate similar
writing contests to engage children in other underrepresented countries.
</summary>
    <author>
      <name>Cornelius Adejoro</name>
    </author>
    <author>
      <name>Luise Arn</name>
    </author>
    <author>
      <name>Larissa Schwartz</name>
    </author>
    <author>
      <name>Tom Yeh</name>
    </author>
    <link href="http://arxiv.org/abs/2303.13544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13546v1</id>
    <updated>2023-03-15T01:55:06Z</updated>
    <published>2023-03-15T01:55:06Z</published>
    <title>Challenges in Designing Racially Inclusive Language Technologies</title>
    <summary>  We take a critical lens toward the pursuit of racially inclusive language
technologies and identify several areas necessitating future work. We discuss
the potential harms of conversational technologies, outline three challenges
that arise in inclusive design, and lastly, argue that conversational user
interface designers and researchers should go beyond racially inclusive design
toward anti-racist and race positive designs.
</summary>
    <author>
      <name>Kimi Wenzel</name>
    </author>
    <author>
      <name>Geoff Kaufman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CUI@CHI: Inclusive Design of CUIs Across Modalities and Mobilities
  Workshop. In the Extended Abstracts of the 2023 CHI Conference on Human
  Factors in Computing Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.13546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13618v1</id>
    <updated>2023-03-23T19:10:22Z</updated>
    <published>2023-03-23T19:10:22Z</published>
    <title>Frankenstein's Toolkit: Prototyping Electronics Using Consumer Products</title>
    <summary>  In our practice as educators, researchers and designers we have found that
centering reverse engineering and reuse has pedagogical, environmental, and
economic benefits. Design decisions in the development of new hardware
tool-kits should consider how we can use e-waste at hand as integral components
of electronics prototyping. Dissection, extraction and modification can give
insights into how things are made at scale. Simultaneously, it can enable
prototypes that have greater fidelity or functionality than would otherwise be
cost-effective to produce.
</summary>
    <author>
      <name>Ilan Mandel</name>
    </author>
    <author>
      <name>Wendy Ju</name>
    </author>
    <link href="http://arxiv.org/abs/2303.13618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14071v1</id>
    <updated>2023-03-24T15:31:03Z</updated>
    <published>2023-03-24T15:31:03Z</published>
    <title>Improving Real-time Communication for Educational Metaverse by
  Alternative WebRTC SFU and Delegating Transmission of Avatar Transform</title>
    <summary>  Maintaining real-time communication quality in metaverse has always been a
challenge, especially when the number of participants increase. We introduce a
proprietary WebRTC SFU service to an open-source web-based VR platform, to
realize a more stable and reliable platform suitable for educational
communication of audio, video, and avatar transform. We developed the web-based
VR platform and conducted a preliminary validation on the implementation for
proof of concept, and high performance in both server and client sides are
confirmed, which may indicates better user experience in communication and
imply a solution to realize educational metaverse.
</summary>
    <author>
      <name>Yong-Hao Hu</name>
    </author>
    <author>
      <name>Kenichiro Ito</name>
    </author>
    <author>
      <name>Ayumi Igarashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE ICCE-TW 2023, waiting for notification of
  acceptance</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14174v1</id>
    <updated>2023-03-24T17:37:25Z</updated>
    <published>2023-03-24T17:37:25Z</published>
    <title>Experiential Futures In-the-wild to Inform Policy Design</title>
    <summary>  As technological innovation continues to shape our world at an accelerating
pace, policy makers struggle to keep up with the unintended consequences of
these new technologies. To address this policy-novelty gap, Responsible
Research Innovation (RRI) has been proposed as a way to drive science and
technology innovation towards socially desirable goals. This work suggests a
more active HCI's position in the materialisation of pluralistic future visions
and emphasizes the engagement between policy design and HCI for more agile and
responsive evaluation environments. It calls for both fields to engage in
questioning which and how futures are constructed, who they are benefiting, and
how the findings of these interventions are interpreted towards other futures.
</summary>
    <author>
      <name>Camilo Sanchez</name>
    </author>
    <author>
      <name>Felix A. Epp</name>
    </author>
    <link href="http://arxiv.org/abs/2303.14174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14336v1</id>
    <updated>2023-03-25T02:59:18Z</updated>
    <published>2023-03-25T02:59:18Z</published>
    <title>(Legal Design) Research through Litigation</title>
    <summary>  This paper proposes the concept of 'research through litigation', where a HCI
researcher would bring a claim in the legal system in order to understand
judicial attitudes towards technologies. Based on my seven years of experience
of bringing legal cases as a computer scientist in Tribunals, I demonstrate the
value of this approach by presenting multiple case studies, which illustrate
the counter-intuitive approach towards technology taken by Tribunals. This
exercise surfaced some serious (and somewhat surreal) concerns with the
operation of the justice system, as well as demonstrating how research through
litigation changed the law on several occasions. This work therefore makes
important methodological and practical contributions to the nascent topic of
legal (interaction) design, especially from a methodological standpoint.
</summary>
    <author>
      <name>Reuben Kirkham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop paper, to be presented at the CHI'23 Design x Policy
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14476v2</id>
    <updated>2024-03-20T06:49:32Z</updated>
    <published>2023-03-25T14:09:18Z</published>
    <title>A Spatial-Constraint Model for Manipulating Static Visualizations</title>
    <summary>  We propose a spatial-constraint approach for modeling spatial-based
interactions and enabling interactive visualizations, which involves the
manipulation of visualizations through selection, filtering, navigation,
arrangement, and aggregation. We proposes a system that activates static
visualizations by adding intelligent interactions, which is achieved by
associating static visual objects with forces. Our force-directed technique
facilitates smooth animated transitions of the visualizations between different
interaction states. We showcase the effectiveness of our technique through
usage scenarios that involve activating visualizations in real-world settings.
</summary>
    <author>
      <name>Can Liu</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Cong Wu</name>
    </author>
    <author>
      <name>Chen Li</name>
    </author>
    <author>
      <name>Xiaoru Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/2303.14476v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14476v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16723v1</id>
    <updated>2023-03-29T14:19:48Z</updated>
    <published>2023-03-29T14:19:48Z</published>
    <title>Prospects for the implementation of an affordable VR/AR-content
  management tool for Learning Management Systems</title>
    <summary>  The article discusses the prospects for introducing into educational practice
the designer of electronic training courses based on virtual and augmented
reality technologies for LMS Moodle. The requirements for the functions,
interface, appearance of the module-designer being developed, the formation of
VR/AR-content in terms of its use by unprepared users, such as teachers and
develop-ers of training courses, are formulated.
</summary>
    <author>
      <name>Anastasia Grigoreva</name>
    </author>
    <author>
      <name>Stanislav Grigorev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.16723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17709v1</id>
    <updated>2023-03-30T21:03:33Z</updated>
    <published>2023-03-30T21:03:33Z</published>
    <title>Teru Teru Bōzu: Defensive Raincloud Plots</title>
    <summary>  Univariate visualizations like histograms, rug plots, or box plots provide
concise visual summaries of distributions. However, each individual
visualization may fail to robustly distinguish important features of a
distribution, or provide sufficient information for all of the relevant tasks
involved in summarizing univariate data. One solution is to juxtapose or
superimpose multiple univariate visualizations in the same chart, as in Allen
et al.'s "raincloud plots." In this paper I examine the design space of
raincloud plots, and, through a series of simulation studies, explore designs
where the component visualizations mutually "defend" against situations where
important distribution features are missed or trivial features are given undue
prominence. I suggest a class of "defensive" raincloud plot designs that
provide good mutual coverage for surfacing distributional features of interest.
</summary>
    <author>
      <name>Michael Correll</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/cgf.14826</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/cgf.14826" rel="related"/>
    <link href="http://arxiv.org/abs/2303.17709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17978v1</id>
    <updated>2023-03-31T11:25:10Z</updated>
    <published>2023-03-31T11:25:10Z</published>
    <title>How Can Mixed Reality Benefit From Physiologically-Adaptive Systems?
  Challenges and Opportunities for Human Factors Applications</title>
    <summary>  Mixed Reality (MR) allows users to interact with digital objects in a
physical environment, but several limitations have hampered widespread
adoption. Physiologically adaptive systems detecting user's states can drive
interaction and address these limitations. Here, we highlight potential
usability and interaction limitations in MR and how physiologically adaptive
systems can benefit MR experiences and applications. We specifically address
potential applications for human factors and operational settings such as
healthcare, education, and entertainment. We further discuss benefits and
applications in light of ethical and privacy concerns. The use of
physiologically adaptive systems in MR has the potential to revolutionize
human-computer interactions and provide users with a more personalized and
engaging experience.
</summary>
    <author>
      <name>Francesco Chiossi</name>
    </author>
    <author>
      <name>Sven Mayer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational UI Workshop CHI2023 HomeSchedule</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.17978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.18038v1</id>
    <updated>2023-03-31T13:15:06Z</updated>
    <published>2023-03-31T13:15:06Z</published>
    <title>Designing for Affective Augmentation: Assistive, Harmful, or Unfamiliar?</title>
    <summary>  In what capacity are affective augmentations helpful to humans, and what
risks (if any) do they pose? In this position paper, we outline three works on
affective augmentation systems, where our studies suggest these systems have
the ability to influence our cognitive, affective, and (social) bodily
perceptions in perhaps unusual ways. We provide considerations on whether these
systems, outside clinical settings, are assistive, harmful, or as of now
largely unfamiliar to users.
</summary>
    <author>
      <name>Abdallah El Ali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Assistive Augmentation Workshop 2023 held at the
  Augmented Humans 2023 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.18038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.18038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.00266v2</id>
    <updated>2023-04-27T10:29:41Z</updated>
    <published>2023-04-01T09:13:36Z</published>
    <title>Hidden Layer Interaction: A Co-Creative Design Fiction for Generative
  Models</title>
    <summary>  This paper presents a speculation on a fictive co-creation scenario that
extends classical interaction patterns with generative models. While existing
interfaces are restricted to the input and output layers, we suggest hidden
layer interaction to extend the horizonal relation at play when co-creating
with a generative model's design space. We speculate on applying feature
visualization to manipulate neurons corresponding to features ranging from
edges over textures to objects. By integrating visual representations of a
neural network's hidden layers into co-creation, we aim to provide humans with
a new means of interaction, contributing to a phenomenological account of the
model's inner workings during generation.
</summary>
    <author>
      <name>Imke Grabe</name>
    </author>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2304.00266v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.00266v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.01157v1</id>
    <updated>2023-04-03T17:24:35Z</updated>
    <published>2023-04-03T17:24:35Z</published>
    <title>Promoting Bright Patterns</title>
    <summary>  User experience designers are facing increasing scrutiny and criticism for
creating harmful technologies, leading to a pushback against unethical design
practices. While clear-cut harmful practices such as dark patterns have
received attention, trends towards automation, personalization, and
recommendation present more ambiguous ethical challenges. To address potential
harm in these "gray" instances, we propose the concept of "bright patterns" -
persuasive design solutions that prioritize user goals and well-being over
their desires and business objectives. The ambition of this paper is threefold:
to define the term "bright patterns", to provide examples of such patterns, and
to advocate for the adoption of bright patterns through policymaking.
</summary>
    <author>
      <name>Hauke Sandhaus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For associated website, see https://brightpatterns.org/. Published to
  the CHI '23 Workshop: Designing Technology and Policy Simultaneously</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.01157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.01157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.02861v1</id>
    <updated>2023-04-06T04:43:26Z</updated>
    <published>2023-04-06T04:43:26Z</published>
    <title>Replicability and Transparency for the Creation of Public Human User
  Video Game Datasets</title>
    <summary>  Replicability is absent in games research; a lack of transparency in protocol
detail hinders scientific consensus and willingness to publish public datasets,
impacting the application of these techniques in video games research. To
combat this, we propose and give an example of the use of a set of experimental
considerations, such as games and materials choice. This work promotes the
communication of research protocols when publishing datasets, benefiting
researchers when designing experiments.
</summary>
    <author>
      <name>Emma J. Pretty</name>
    </author>
    <author>
      <name>Renan Guarese</name>
    </author>
    <author>
      <name>Haytham M. Fayek</name>
    </author>
    <author>
      <name>Fabio Zambetta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for submission at the Data4XR Workshop at IEEEVR2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.02861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08748v3</id>
    <updated>2023-04-29T09:10:16Z</updated>
    <published>2023-04-18T06:05:46Z</published>
    <title>Option-Driven Design: Context, Tradeoffs, and Considerations for
  Accessibility</title>
    <summary>  In Option-Driven Design, users must interact with options and settings for
systems to adapt to their needs. This approach places the burden on both the
user and the system to make the interaction between user and system fit. The
user must know and find which options they need and then adjust them. In
addition, the system must be capable of robust change, similar to system change
in ability-based design. In this micro-paper I outline the context for
option-driven design, followed by several design negotiations, tradeoffs, and
suggestions worth considering with this approach.
</summary>
    <author>
      <name>Frank Elavsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, 1 page of references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Micro-papers, arXiv release, 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.08748v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08748v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08901v1</id>
    <updated>2023-04-18T11:09:02Z</updated>
    <published>2023-04-18T11:09:02Z</published>
    <title>Multimodal Group Activity Dataset for Classroom Engagement Level
  Prediction</title>
    <summary>  We collected a new dataset that includes approximately eight hours of
audiovisual recordings of a group of students and their self-evaluation scores
for classroom engagement. The dataset and data analysis scripts are available
on our open-source repository. We developed baseline face-based and
group-activity-based image and video recognition models. Our image models yield
45-85% test accuracy with face-area inputs on person-based classification task.
Our video models achieved up to 71% test accuracy on group-level prediction
using group activity video inputs. In this technical report, we shared the
details of our end-to-end human-centered engagement analysis pipeline from data
collection to model development.
</summary>
    <author>
      <name>Alpay Sabuncuoglu</name>
    </author>
    <author>
      <name>T. Metin Sezgin</name>
    </author>
    <link href="http://arxiv.org/abs/2304.08901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09394v1</id>
    <updated>2023-04-19T03:08:15Z</updated>
    <published>2023-04-19T03:08:15Z</published>
    <title>Revitalizing Endangered Languages: AI-powered language learning as a
  catalyst for language appreciation</title>
    <summary>  According to UNESCO, there are nearly 7,000 languages spoken worldwide, of
which around 3,000 languages are in danger of disappearing before the end of
the century. With roughly 230 languages having already become extinct between
the years 1950-2010, collectively this represents a significant loss of
linguistic and cultural diversity. This position paper aims to explore the
potential of AI-based language learning approaches that promote early exposure
and appreciation of languages to ultimately contribute to the preservation of
endangered languages, thereby addressing the urgent need to protect linguistic
and cultural diversity.
</summary>
    <author>
      <name>Dinesh Kumar Nanduri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Information Studies, University of Maryland</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth M. Bonsignore</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Information Studies, University of Maryland</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.09394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09477v1</id>
    <updated>2023-04-19T07:55:52Z</updated>
    <published>2023-04-19T07:55:52Z</published>
    <title>Towards Co-Creative Generative Adversarial Networks for Fashion
  Designers</title>
    <summary>  Originating from the premise that Generative Adversarial Networks (GANs)
enrich creative processes rather than diluting them, we describe an ongoing PhD
project that proposes to study GANs in a co-creative context. By asking How can
GANs be applied in co-creation, and in doing so, how can they contribute to
fashion design processes? the project sets out to investigate co-creative GAN
applications and further develop them for the specific application area of
fashion design. We do so by drawing on the field of mixed-initiative
co-creation. Combined with the technical insight into GANs' functioning, we aim
to understand how their algorithmic properties translate into interactive
interfaces for co-creation and propose new interactions.
</summary>
    <author>
      <name>Imke Grabe</name>
    </author>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at GenAICHI, CHI 2022 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.09477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09866v1</id>
    <updated>2023-04-18T17:24:14Z</updated>
    <published>2023-04-18T17:24:14Z</published>
    <title>Towards Designing a ChatGPT Conversational Companion for Elderly People</title>
    <summary>  Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns.
</summary>
    <author>
      <name>Abeer Alessa</name>
    </author>
    <author>
      <name>Hend Al-Khalifa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 Figures, Workshop paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.09866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09873v1</id>
    <updated>2023-04-19T13:35:23Z</updated>
    <published>2023-04-19T13:35:23Z</published>
    <title>ChatGPT as a Therapist Assistant: A Suitability Study</title>
    <summary>  This paper proposes using ChatGPT, an innovative technology with various
applications, as an assistant for psychotherapy. ChatGPT can serve as a patient
information collector, a companion for patients in between therapy sessions,
and an organizer of gathered information for therapists to facilitate treatment
processes. The research identifies five research questions and discovers useful
prompts for fine-tuning the assistant, which shows that ChatGPT can participate
in positive conversations, listen attentively, offer validation and potential
coping strategies without providing explicit medical advice, and help
therapists discover new insights from multiple conversations with the same
patient. Using ChatGPT as an assistant for psychotherapy poses several
challenges that need to be addressed, including technical as well as
human-centric challenges which are discussed.
</summary>
    <author>
      <name>Mahshid Eshghie</name>
    </author>
    <author>
      <name>Mojtaba Eshghie</name>
    </author>
    <link href="http://arxiv.org/abs/2304.09873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.10546v1</id>
    <updated>2023-04-14T13:48:08Z</updated>
    <published>2023-04-14T13:48:08Z</published>
    <title>Introducing Vibration for use in Interaction Designs to support Human
  Performance: A Pilot Study</title>
    <summary>  While vibration is a well-used output signal in HCI as part of haptic
interaction, vibration outside HCI is used in many other modes to support human
performance, from rehabilitation to cognition. In this late breaking work, we
present preliminary positive results of a novel protocol that informs how
vibration might be used to enrich HCI interventions for aspects of both health
and intellectual performance. We also present a novel apparatus specifically
designed to help HCI researchers explore different vibration amplitudes and
frequencies for such applications.
</summary>
    <author>
      <name>Alexander Dawid Bincalar</name>
    </author>
    <author>
      <name>M. C. Schraefel</name>
    </author>
    <author>
      <name>Christopher Freeman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures; pilot study report</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.10546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.10546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.10555v1</id>
    <updated>2023-04-20T13:35:31Z</updated>
    <published>2023-04-20T13:35:31Z</published>
    <title>Exploring Low Cost Non-Contact Detection of Biosignals for HCI</title>
    <summary>  In an effort to make biosignal integration more accessible to explore for
more HCI researchers, this paper presents our investigation of how well a
standard, near ubiquitous webcam can support remote sensing of heart rate and
respiration rate across skin tone ranges. The work contributes: how the webcam
can be used for this purpose, its limitations, and how to mitigate these
limitations affordably, including how the skin tone range affect the estimation
results.
</summary>
    <author>
      <name>Christoph Tremmel</name>
    </author>
    <author>
      <name>Indu Bodala</name>
    </author>
    <author>
      <name>M. C. Schraefel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.10555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.10555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.10612v1</id>
    <updated>2023-04-07T19:18:16Z</updated>
    <published>2023-04-07T19:18:16Z</published>
    <title>Halcyon -- A Pathology Imaging and Feature analysis and Management
  System</title>
    <summary>  Halcyon is a new pathology imaging analysis and feature management system
based on W3C linked-data open standards and is designed to scale to support the
needs for the voluminous production of features from deep-learning feature
pipelines. Halcyon can support multiple users with a web-based UX with access
to all user data over a standards-based web API allowing for integration with
other processes and software systems. Identity management and data security is
also provided.
</summary>
    <author>
      <name>Erich Bremer</name>
    </author>
    <author>
      <name>Tammy DiPrima</name>
    </author>
    <author>
      <name>Joseph Balsamo</name>
    </author>
    <author>
      <name>Jonas Almeida</name>
    </author>
    <author>
      <name>Rajarsi Gupta</name>
    </author>
    <author>
      <name>Joel Saltz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures. arXiv admin note: text overlap with
  arXiv:2005.06469</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.10612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.10612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11413v1</id>
    <updated>2023-04-22T14:07:57Z</updated>
    <published>2023-04-22T14:07:57Z</published>
    <title>Three-dimensional hand guidance by midair haptic display</title>
    <summary>  Guiding human movements using tactile information is one of the promising
applications of haptics. Using midair ultrasonic haptic stimulation, it is
possible to guide a hand without visual information.However, the information of
movement shown by conventional methods was partial. It has not been shown a
method to guide a hand to an arbitrary point in three dimensional space. In
this study, we propose a method of guiding the hand to the top of a virtual
cone presented haptically and evaluate the effectiveness of the method through
experiments. As a result, the method guided the participant's hand to the goal
in a 30 cm cube workspace with an error of 64.34 mm
</summary>
    <author>
      <name>Koya Hiura</name>
    </author>
    <author>
      <name>Shun Suzuki</name>
    </author>
    <author>
      <name>Tao Morisaki</name>
    </author>
    <author>
      <name>Masahiro Fujiwara</name>
    </author>
    <author>
      <name>Yasutoshi Makino</name>
    </author>
    <author>
      <name>Hiroyuki Shinoda</name>
    </author>
    <link href="http://arxiv.org/abs/2304.11413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.12781v1</id>
    <updated>2023-04-25T13:04:46Z</updated>
    <published>2023-04-25T13:04:46Z</published>
    <title>SAPHIR: A Pluricultural Authoring Tool to Produce Resources in Support
  of Education for Sustainable Development</title>
    <summary>  In this paper, we present SAPHIR, a multilingual authoring tool producing a
Progressive Web App, usable on computers, tablets, and smartphones, online or
offline. We presented our design process, the architecture of the system, the
model on which it is based, and its main parts: SAPHIR it-self is the main
software proposing activities to children to learn and play; MINE is the
authoring tool used by pedagogical designers and resources translators to
create and translate resources without requiring any programming skills; TAILLE
is dedicated to teachers to whom he provides educational explanations to use
SAPHIR with their learners. The different parts were used with both pedagogical
designers and students.
</summary>
    <author>
      <name>Stéphanie Jean-Daubias</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRIS, TWEAK</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CSEDU 2023, Apr 2023, Prague, Czech Republic</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.12781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.12781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03375v1</id>
    <updated>2023-05-05T09:08:27Z</updated>
    <published>2023-05-05T09:08:27Z</published>
    <title>Towards Feminist Intersectional XAI: From Explainability to
  Response-Ability</title>
    <summary>  This paper follows calls for critical approaches to computing and
conceptualisations of intersectional, feminist, decolonial HCI and AI design
and asks what a feminist intersectional perspective in HCXAI research and
design might look like. Sketching out initial research directions and
implications for explainable AI design, it suggests that explainability from a
feminist perspective would include the fostering of response-ability - the
capacity to critically evaluate and respond to AI systems - and would centre
marginalised perspectives.
</summary>
    <author>
      <name>Goda Klumbyte</name>
    </author>
    <author>
      <name>Hannah Piehl</name>
    </author>
    <author>
      <name>Claude Draude</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop Human-Centered Explainable AI (HCXAI), ACM Conference on
  Human Factors in Computing Systems CHI 23, April 23-28, 2023, Hamburg,
  Germany, 9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.03375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.0; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03852v1</id>
    <updated>2023-05-05T21:25:35Z</updated>
    <published>2023-05-05T21:25:35Z</published>
    <title>CHAI-DT: A Framework for Prompting Conversational Generative AI Agents
  to Actively Participate in Co-Creation</title>
    <summary>  This paper explores the potential for utilizing generative AI models in
group-focused co-creative frameworks to enhance problem solving and ideation in
business innovation and co-creation contexts, and proposes a novel prompting
technique for conversational generative AI agents which employ methods inspired
by traditional 'human-to-human' facilitation and instruction to enable active
contribution to Design Thinking, a co-creative framework. Through experiments
using this prompting technique, we gather evidence that conversational
generative transformers (i.e. ChatGPT) have the capability to contribute
context-specific, useful, and creative input into Design Thinking activities.
We also discuss the potential benefits, limitations, and risks associated with
using generative AI models in co-creative ideation and provide recommendations
for future research.
</summary>
    <author>
      <name>Brandon Harwood</name>
    </author>
    <link href="http://arxiv.org/abs/2305.03852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.04242v1</id>
    <updated>2023-05-07T10:35:06Z</updated>
    <published>2023-05-07T10:35:06Z</published>
    <title>Dynamic Scene Adjustment for Player Engagement in VR Game</title>
    <summary>  Virtual reality (VR) produces a highly realistic simulated environment with
controllable environment variables. This paper proposes a Dynamic Scene
Adjustment (DSA) mechanism based on the user interaction status and
performance, which aims to adjust the VR experiment variables to improve the
user's game engagement. We combined the DSA mechanism with a musical rhythm VR
game. The experimental results show that the DSA mechanism can improve the
user's game engagement (task performance).
</summary>
    <author>
      <name>Zhitao Liu</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Ning Xie</name>
    </author>
    <author>
      <name>YouTeng Fan</name>
    </author>
    <author>
      <name>Haolan Tang</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2305.04242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.04242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.04368v1</id>
    <updated>2023-05-07T20:11:24Z</updated>
    <published>2023-05-07T20:11:24Z</published>
    <title>Towards a More Inclusive Metaverse via Designing Tools That Support
  Collaborative Virtual World Building by Users With and Without Disabilities</title>
    <summary>  Research has found social VR to bring various benefits to users with and
without disabilities. Given the success of social VR applications that support
user-created worlds, it is important to consider how we can empower users in
building inclusive virtual worlds by investigating how tools for world building
can be built to better support collaborations between users with and without
disabilities. As such, this position paper provides a brief discussion of
existing research into important factors that should be considered during such
collaborations, and possible future research directions.
</summary>
    <author>
      <name>Ken Jen Lee</name>
    </author>
    <author>
      <name>Edith Law</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, ACM CHI 2023 Workshop: Towards an Inclusive and Accessible
  Metaverse</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.04368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.04368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.04375v1</id>
    <updated>2023-05-07T20:37:12Z</updated>
    <published>2023-05-07T20:37:12Z</published>
    <title>Value Reflection: Motivation, Examples and The Role of Technology</title>
    <summary>  Findings from existing research provide possible evidence that values are an
important construct that should be part of the self-reflection process.
However, many questions about value reflection remain unexplored. As such, this
position paper aims to provide an overview of relevant research and frameworks,
example studies on value reflection pursued by the authors, and design aspects
that might both improve value reflection processes and provide a brief mapping
of where and how technology could be of help.
</summary>
    <author>
      <name>Ken Jen Lee</name>
    </author>
    <author>
      <name>Edith Law</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, ACM CHI 2023 Workshop: Integrating Individual and
  Social Contexts into Self-Reflection Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.04375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.04375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.05607v1</id>
    <updated>2023-05-09T16:57:19Z</updated>
    <published>2023-05-09T16:57:19Z</published>
    <title>Refining Human-Centered Autonomy Using Side Information</title>
    <summary>  Data-driven algorithms for human-centered autonomy use observed data to
compute models of human behavior in order to ensure safety, correctness, and to
avoid potential errors that arise at runtime. However, such algorithms often
neglect useful a priori knowledge, known as side information, that can improve
the quality of data-driven models. We identify several key challenges in
human-centered autonomy, and identify possible approaches to incorporate side
information in data-driven models of human behavior.
</summary>
    <author>
      <name>Adam J. Thorpe</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HCPS 2023 Workshop on Humans in Cyber-Physical Systems (HCPS
  2023), part of CPS-IoT Week</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.05607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.05607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09875v1</id>
    <updated>2023-05-17T01:15:46Z</updated>
    <published>2023-05-17T01:15:46Z</published>
    <title>Augmenting Learning with Augmented Reality: Exploring the Affordances of
  AR in Supporting Mastery of Complex Psychomotor Tasks</title>
    <summary>  This research seeks to explore how Augmented Reality (AR) can support
learning psychomotor tasks that involve complex manipulation and reasoning
processes. The AR prototype was created using Unity and used on HoloLens 2
headsets. Here, we explore the potential of AR as a training or assistive tool
for spatial tasks and the need for intelligent mechanisms to enable adaptive
and personalized interactions between learners and AR. The paper discusses how
integrating AR with Artificial Intelligence (AI) can adaptably scaffold the
learning of complex tasks to accelerate the development of expertise in
psychomotor domains.
</summary>
    <author>
      <name>Dong Woo Yoo</name>
    </author>
    <author>
      <name>Sakib Reza</name>
    </author>
    <author>
      <name>Nicholas Wilson</name>
    </author>
    <author>
      <name>Kemi Jona</name>
    </author>
    <author>
      <name>Mohsen Moghaddam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Demo Paper - ISLS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.09875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10349v1</id>
    <updated>2023-05-17T16:32:40Z</updated>
    <published>2023-05-17T16:32:40Z</published>
    <title>Interactive Learning of Hierarchical Tasks from Dialog with GPT</title>
    <summary>  We present a system for interpretable, symbolic, interactive task learning
from dialog using a GPT model as a conversational front-end. The learned tasks
are represented as hierarchical decompositions of predicate-argument structures
with scoped variable arguments. By using a GPT model to convert interactive
dialog into a semantic representation, and then recursively asking for
definitions of unknown steps, we show that hierarchical task knowledge can be
acquired and re-used in a natural and unrestrained conversational environment.
We compare our system to a similar architecture using a more conventional
parser and show that our system tolerates a much wider variety of linguistic
variance.
</summary>
    <author>
      <name>Lane Lawley</name>
    </author>
    <author>
      <name>Christopher J. MacLellan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.10349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.11874v1</id>
    <updated>2023-05-03T17:37:51Z</updated>
    <published>2023-05-03T17:37:51Z</published>
    <title>User Experience Considered Harmful (for the Planet)</title>
    <summary>  Great user experience is killing us (more or less)! My argument in this
provocation is that the excessive focus on user experience (UX) by the tech
industry and academic community has a negative impact on the sustainability of
ICT devices. I will argue based on two examples, that we need new metrics or
extend current UX metrics to also include third order effects and
sustainability perspectives. Lastly, I would like us - the (Sustainable) HCI
community - to increase our focus on solving the problems that result from our
very own creations.
</summary>
    <author>
      <name>Markus Löchtefeld</name>
    </author>
    <link href="http://arxiv.org/abs/2305.11874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.11874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.11877v1</id>
    <updated>2023-05-05T14:02:33Z</updated>
    <published>2023-05-05T14:02:33Z</published>
    <title>Development of a Metaverse Platform for Tourism Promotion in Apulia</title>
    <summary>  Metaverse is an engaging way to recreate in a digital environment the real
world. It allows people to connect not by just browsing a website, but by using
headsets and virtual reality techniques. The metaverse is actually in a rapid
development phase, thanks to the advances in different topics. This paper
proposes a smart tourism platform in which tourists can interact with guides
and different kinds of suppliers, without the need to phisically visit the city
they are in. We propose some techniques to scan the real world and transpose it
in a metaverse platform, using the recreation of an Italian city, Bari, as a
real life scenario.
</summary>
    <author>
      <name>Enrico Carmine Ciliberti</name>
    </author>
    <author>
      <name>Marco Fiore</name>
    </author>
    <author>
      <name>Marina Mongiello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MetaCom57706.2023.00123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MetaCom57706.2023.00123" rel="related"/>
    <link href="http://arxiv.org/abs/2305.11877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.11877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.12513v1</id>
    <updated>2023-05-21T17:07:08Z</updated>
    <published>2023-05-21T17:07:08Z</published>
    <title>Balancing the Digital and the Physical: Discussing Push and Pull Factors
  for Digital Well-being</title>
    <summary>  This position paper discusses the negative effects of excessive smartphone
usage on mental health and well-being. Despite efforts to limit smartphone
usage, users become desensitized to reminders and limitations. The paper
proposes the use of Push &amp; Pull Factors to contextualize intervention
strategies promoting digital well-being. Further, alternative metrics to
measure the effectiveness of intervention strategies will be discussed.
</summary>
    <author>
      <name>Luca-Maxim Meinhardt</name>
    </author>
    <author>
      <name>Jan-Henry Belz</name>
    </author>
    <author>
      <name>Michael Rietzler</name>
    </author>
    <author>
      <name>Enrico Rukzio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.12513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.12513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.16700v1</id>
    <updated>2023-05-26T07:41:56Z</updated>
    <published>2023-05-26T07:41:56Z</published>
    <title>Applying Interdisciplinary Frameworks to Understand Algorithmic
  Decision-Making</title>
    <summary>  We argue that explanations for "algorithmic decision-making" (ADM) systems
can profit by adopting practices that are already used in the learning
sciences. We shortly introduce the importance of explaining ADM systems, give a
brief overview of approaches drawing from other disciplines to improve
explanations, and present the results of our qualitative task-based study
incorporating the "six facets of understanding" framework. We close with
questions guiding the discussion of how future studies can leverage an
interdisciplinary approach.
</summary>
    <author>
      <name>Timothée Schmude</name>
    </author>
    <author>
      <name>Laura Koesten</name>
    </author>
    <author>
      <name>Torsten Möller</name>
    </author>
    <author>
      <name>Sebastian Tschiatschek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.16700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.16700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01113v1</id>
    <updated>2023-06-01T19:55:34Z</updated>
    <published>2023-06-01T19:55:34Z</published>
    <title>Cross-Reality for Extending the Metaverse: Designing Hyper-Connected
  Immersive Environments with XRI</title>
    <summary>  The Metaverse comprises technologies to enable virtual twins of the real
world, via mixed reality, internet of things, and others. As it matures unique
challenges arise such as a lack of strong connections between virtual and
physical worlds. This work presents design frameworks for cross-reality hybrid
spaces. Contributions include: i) clarifying the metaverse "disconnect", ii)
extended metaverse design frameworks, iii) prototypes, and iv) discussions
toward new metaverse smart environments.
</summary>
    <author>
      <name>Jie Guan</name>
    </author>
    <author>
      <name>Alexis Morris</name>
    </author>
    <author>
      <name>Jay Irizawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VRW58643.2023.00071</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VRW58643.2023.00071" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2023 IEEE Conference on Virtual Reality and 3D User Interfaces
  Abstracts and Workshops (VRW)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.01113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01615v1</id>
    <updated>2023-06-02T15:26:46Z</updated>
    <published>2023-06-02T15:26:46Z</published>
    <title>Enough With "Human-AI Collaboration"</title>
    <summary>  Describing our interaction with Artificial Intelligence (AI) systems as
'collaboration' is well-intentioned, but flawed. Not only is it misleading, but
it also takes away the credit of AI 'labour' from the humans behind it, and
erases and obscures an often exploitative arrangement between AI producers and
consumers. The AI 'collaboration' metaphor is merely the latest episode in a
long history of labour appropriation and credit reassignment that
disenfranchises labourers in the Global South. I propose that viewing AI as a
tool or an instrument, rather than a collaborator, is more accurate, and
ultimately fairer.
</summary>
    <author>
      <name>Advait Sarkar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544549.3582735</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544549.3582735" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIn Extended Abstracts of the 2023 CHI Conference on Human Factors
  in Computing Systems (CHI EA '23). Association for Computing Machinery, New
  York, NY, USA, Article 415, 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.01615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.03195v1</id>
    <updated>2023-06-05T19:13:44Z</updated>
    <published>2023-06-05T19:13:44Z</published>
    <title>Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time
  Light Patterns</title>
    <summary>  We introduce NightPulse, an interactive tool for Night-time light (NTL) data
visualization and analytics, which enables researchers and stakeholders to
explore and analyze NTL data with a user-friendly platform. Powered by
efficient system architecture, NightPulse supports image segmentation,
clustering, and change pattern detection to identify urban development and
sprawl patterns. It captures temporal trends of NTL and semantics of cities,
answering questions about demographic factors, city boundaries, and unusual
differences.
</summary>
    <author>
      <name>Jakob Hederich</name>
    </author>
    <author>
      <name>Shreya Ghosh</name>
    </author>
    <author>
      <name>Zeyu He</name>
    </author>
    <author>
      <name>Prasenjit Mitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures. Accepted in ECML PKDD Demo track</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.03195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.03195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.03714v2</id>
    <updated>2023-06-07T13:12:28Z</updated>
    <published>2023-06-06T14:23:06Z</published>
    <title>DashQL -- Complete Analysis Workflows with SQL</title>
    <summary>  We present DashQL, a language that describes complete analysis workflows in
self-contained scripts. DashQL combines SQL, the grammar of relational database
systems, with a grammar of graphics in a grammar of analytics. It supports
preparing and visualizing arbitrarily complex SQL statements in a single
coherent language. The proximity to SQL facilitates holistic optimizations of
analysis workflows covering data input, encoding, transformations, and
visualizations. These optimizations use model and query metadata for
visualization-driven aggregation, remote predicate pushdown, and adaptive
materialization. We introduce the DashQL language as an extension of SQL and
describe the efficient and interactive processing of text-based analysis
workflows.
</summary>
    <author>
      <name>André Kohn</name>
    </author>
    <author>
      <name>Dominik Moritz</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/2306.03714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.03714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.04332v1</id>
    <updated>2023-06-07T10:57:46Z</updated>
    <published>2023-06-07T10:57:46Z</published>
    <title>A Systematic Study Of Various Fingertip Detection Techniques For Air
  Writing Using Machine Learning</title>
    <summary>  The recent advancement in technology breaks the barriers to communication
between users and computers. The communication between humans and computers
includes emotion and gesture recognition. Emotions can be recognized on the
face of humans whereas gesture recognition includes hand and body gesture
recognition. Fingertip detection is also part of it. Gesture recognition is the
way of interaction that is used in air writing. Users can control the devices
with simple gestures without touching them. It is how computers can understand
human language which will reduce the interaction barriers between them. This
paper discusses the different techniques that can be used for fingertip
detection in air writing using machine learning
</summary>
    <author>
      <name> Heena</name>
    </author>
    <author>
      <name>Sandeep Ranjan</name>
    </author>
    <link href="http://arxiv.org/abs/2306.04332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.04332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07011v1</id>
    <updated>2023-06-12T10:31:00Z</updated>
    <published>2023-06-12T10:31:00Z</published>
    <title>Deepfake in the Metaverse: An Outlook Survey</title>
    <summary>  We envision deepfake technologies, which synthesize realistic fake images and
videos, will play an important role in the future metaverse. While enhancing
users' immersion and experience with synthesized virtual characters and scenes,
deepfake can cause serious consequences if used for fraud, impersonation, and
dissemination of fake information. In this paper, we introduce the principles,
applications, and risks of deepfake technology, and propose some
countermeasures to help users and developers in the metaverse deal with the
challenges brought by deepfake technologies. Further, we provide an outlook on
the future development of deepfake in the metaverse.
</summary>
    <author>
      <name>Haojie Wu</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <author>
      <name>Pengyuan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2306.07011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07519v1</id>
    <updated>2023-06-13T03:25:04Z</updated>
    <published>2023-06-13T03:25:04Z</published>
    <title>Decoding Brain Motor Imagery with various Machine Learning techniques</title>
    <summary>  Motor imagery (MI) is a well-documented technique used by subjects in BCI
(Brain Computer Interface) experiments to modulate brain activity within the
motor cortex and surrounding areas of the brain. In our term project, we
conducted an experiment in which the subjects were instructed to perform motor
imagery that would be divided into two classes (Right and Left). Experiments
were conducted with two different types of electrodes (Gel and POLiTag) and
data for individual subjects was collected. In this paper, we will apply
different machine learning (ML) methods to create a decoder based on offline
training data that uses evidence accumulation to predict a subject's intent
from their modulated brain signals in real-time.
</summary>
    <author>
      <name>Giovanni Jana</name>
    </author>
    <author>
      <name>Corey Karnei</name>
    </author>
    <author>
      <name>Shuvam Keshari</name>
    </author>
    <link href="http://arxiv.org/abs/2306.07519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12973v1</id>
    <updated>2023-06-22T15:35:33Z</updated>
    <published>2023-06-22T15:35:33Z</published>
    <title>From ontology design to user-centred interfaces for music heritage</title>
    <summary>  In this article we investigate the bridge between ontology design and UI/UX
design methodologies to assist designers in prototyping web applications for
information seeking purposes. We briefly review the state of the art in
ontology design and UI/UX methodologies, then we illustrate our approach
applied to a case study in the music heritage domain.
</summary>
    <author>
      <name>Giulia Renda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bologna</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Grasso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bologna</arxiv:affiliation>
    </author>
    <author>
      <name>Marilena Daquino</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bologna</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, part of "XII Convegno Annuale AIUCD 2023
  Proceedings"</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.12973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13381v1</id>
    <updated>2023-06-23T09:03:16Z</updated>
    <published>2023-06-23T09:03:16Z</published>
    <title>Co-creating a globally interpretable model with human input</title>
    <summary>  We consider an aggregated human-AI collaboration aimed at generating a joint
interpretable model. The model takes the form of Boolean decision rules, where
human input is provided in the form of logical conditions or as partial
templates. This focus on the combined construction of a model offers a
different perspective on joint decision making. Previous efforts have typically
focused on aggregating outcomes rather than decisions logic. We demonstrate the
proposed approach through two examples and highlight the usefulness and
challenges of the approach.
</summary>
    <author>
      <name>Rahul Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper at Artificial Intelligence &amp; Human-Computer Interaction
  Workshop at ICML 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.13381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.16344v2</id>
    <updated>2025-03-26T11:38:12Z</updated>
    <published>2023-06-28T16:20:06Z</published>
    <title>Simulating vibration transmission and comfort in automated driving
  integrating models of seat, body, postural stabilization and motion
  perception</title>
    <summary>  To enhance motion comfort in (automated) driving we present biomechanical
models and demonstrate their ability to capture vibration transmission from
seat to trunk and head. A computationally efficient full body model is
presented, able to operate in real time while capturing translational and
rotational motion of trunk and head with fore-aft, lateral and vertical seat
motion. Sensory integration models are presented predicting motion perception
and motion sickness accumulation using the head motion as predicted by
biomechanical models.
</summary>
    <author>
      <name>Riender Happee</name>
    </author>
    <author>
      <name>Raj Desai</name>
    </author>
    <author>
      <name>Georgios Papaioannou</name>
    </author>
    <link href="http://arxiv.org/abs/2306.16344v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.16344v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.00348v2</id>
    <updated>2023-12-22T05:54:57Z</updated>
    <published>2023-07-01T14:09:55Z</published>
    <title>Lottery and Sprint: Generate a Board Game with Design Sprint Method on
  AutoGPT</title>
    <summary>  In this paper, we present a novel approach using the Auto GPT system
alongside Design Sprint methodology to facilitate board game creation for
inexperienced users. We introduce the implementation of Auto GPT for generating
diverse board games and the subsequent optimization process through a
customized Design Sprint. A user study is conducted to investigate the
playability and enjoyment of the generated games, revealing both successes and
challenges in employing systems like Auto GPT for board game design. Insights
and future research directions are proposed to overcome identified limitations
and enhance computational-driven game creation.
</summary>
    <author>
      <name>Maya Grace Torii</name>
    </author>
    <author>
      <name>Takahito Murakami</name>
    </author>
    <author>
      <name>Yoichi Ochiai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3573382.3623706</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3573382.3623706" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, author's version</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.00348v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.00348v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.02590v1</id>
    <updated>2023-07-05T18:38:17Z</updated>
    <published>2023-07-05T18:38:17Z</published>
    <title>Homo-Loggatus. The anthropological condition of historians in the
  digital world</title>
    <summary>  Computerization has created a digital ecological niche where humans live in a
state of interconnection that modifies their Epigenetics. Within this
hyper-datafied virtual space, the logged-in agent enhances their intellectual
and rational abilities, giving rise to a new cognitive entity. Humans are
evolving towards a new anthropological status that shifts the terms of the
Digital History debate from History to the historian, compelling the latter to
reflect on the positions of Fichte and Schelling regarding the mind-body-world
relationship (ecological niche). This reflection leads to the possibility of
overcoming the crisis of History imposed by presentism and the necessity of
redefining the research methodology based on the new vision of the
interconnection between the mind and the digital niche as an investigative
tool.
</summary>
    <author>
      <name>Salvatore Spina</name>
    </author>
    <link href="http://arxiv.org/abs/2307.02590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.02590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.02819v1</id>
    <updated>2023-07-06T07:24:38Z</updated>
    <published>2023-07-06T07:24:38Z</published>
    <title>Trends in Machine Learning and Electroencephalogram (EEG): A Review for
  Undergraduate Researchers</title>
    <summary>  This paper presents a systematic literature review on Brain-Computer
Interfaces (BCIs) in the context of Machine Learning. Our focus is on
Electroencephalography (EEG) research, highlighting the latest trends as of
2023. The objective is to provide undergraduate researchers with an accessible
overview of the BCI field, covering tasks, algorithms, and datasets. By
synthesizing recent findings, our aim is to offer a fundamental understanding
of BCI research, identifying promising avenues for future investigations.
</summary>
    <author>
      <name>Nathan Koome Murungi</name>
    </author>
    <author>
      <name>Michael Vinh Pham</name>
    </author>
    <author>
      <name>Xufeng Dai</name>
    </author>
    <author>
      <name>Xiaodong Qu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure, HCI International 2023 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.02819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.02819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.04462v1</id>
    <updated>2023-07-10T10:20:09Z</updated>
    <published>2023-07-10T10:20:09Z</published>
    <title>Utilising Explanations to Mitigate Robot Conversational Failures</title>
    <summary>  This paper presents an overview of robot failure detection work from HRI and
adjacent fields using failures as an opportunity to examine robot explanation
behaviours. As humanoid robots remain experimental tools in the early 2020s,
interactions with robots are situated overwhelmingly in controlled
environments, typically studying various interactional phenomena. Such
interactions suffer from real-world and large-scale experimentation and tend to
ignore the 'imperfectness' of the everyday user. Robot explanations can be used
to approach and mitigate failures, by expressing robot legibility and
incapability, and within the perspective of common-ground. In this paper, I
discuss how failures present opportunities for explanations in interactive
conversational robots and what the potentials are for the intersection of HRI
and explainability research.
</summary>
    <author>
      <name>Dimosthenis Kontogiorgos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM International Conference on Human-Robot Interaction
  (Workshops) 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2307.04462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.04462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.14206v1</id>
    <updated>2023-07-26T14:12:16Z</updated>
    <published>2023-07-26T14:12:16Z</published>
    <title>AI and Education: An Investigation into the Use of ChatGPT for Systems
  Thinking</title>
    <summary>  This exploratory study investigates the potential of the artificial
intelligence tool, ChatGPT, to support systems thinking (ST) in various
subjects. Using both general and subject specific prompts, the study assesses
the accuracy, helpfulness, and reliability of ChatGPT's responses across
different versions of the tool. The results indicate that ChatGPT can provide
largely correct and very helpful responses in various subjects, demonstrating
its potential as a tool for enhancing ST skills. However, occasional
inaccuracies highlight the need for users to remain critical of ChatGPT's
responses. Despite some limitations, this study suggests that with careful use
and attention to its idiosyncrasies, ChatGPT can be a valuable tool for
teaching and learning ST.
</summary>
    <author>
      <name>Holger Arndt</name>
    </author>
    <link href="http://arxiv.org/abs/2307.14206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.14206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15452v1</id>
    <updated>2023-07-28T10:10:46Z</updated>
    <published>2023-07-28T10:10:46Z</published>
    <title>From OECD to India: Exploring cross-cultural differences in perceived
  trust, responsibility and reliance of AI and human experts</title>
    <summary>  AI is getting more involved in tasks formerly exclusively assigned to humans.
Most of research on perceptions and social acceptability of AI in these areas
is mainly restricted to the Western world. In this study, we compare trust,
perceived responsibility, and reliance of AI and human experts across OECD and
Indian sample. We find that OECD participants consider humans to be less
capable but more morally trustworthy and more responsible than AI. In contrast,
Indian participants trust humans more than AI but assign equal responsibility
for both types of experts. We discuss implications of the observed differences
for algorithmic ethics and human-computer interaction.
</summary>
    <author>
      <name>Vishakha Agrawal</name>
    </author>
    <author>
      <name>Serhiy Kandul</name>
    </author>
    <author>
      <name>Markus Kneer</name>
    </author>
    <author>
      <name>Markus Christen</name>
    </author>
    <link href="http://arxiv.org/abs/2307.15452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.00682v1</id>
    <updated>2023-08-01T17:37:24Z</updated>
    <published>2023-08-01T17:37:24Z</published>
    <title>TimePool: Visually Answer "Which and When" Questions On Univariate Time
  Series</title>
    <summary>  When exploring time series datasets, analysts often pose "which and when"
questions. For example, with world life expectancy data over one hundred years,
they may inquire about the top 10 countries in life expectancy and the time
period when they achieved this status, or which countries have had longer life
expectancy than Ireland and when. This paper proposes TimePool, a new
visualization prototype, to address this need for univariate time series
analysis. It allows users to construct interactive "which and when" queries and
visually explore the results for insights.
</summary>
    <author>
      <name>Tinghao Feng</name>
    </author>
    <author>
      <name>Yueqi Hu</name>
    </author>
    <author>
      <name>Jing Yang</name>
    </author>
    <author>
      <name>Tom Polk</name>
    </author>
    <author>
      <name>Ye Zhao</name>
    </author>
    <author>
      <name>Shixia Liu</name>
    </author>
    <author>
      <name>Zhaocong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2308.00682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.00682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.01010v1</id>
    <updated>2023-08-02T08:32:43Z</updated>
    <published>2023-08-02T08:32:43Z</published>
    <title>Point Anywhere: Directed Object Estimation from Omnidirectional Images</title>
    <summary>  One of the intuitive instruction methods in robot navigation is a pointing
gesture. In this study, we propose a method using an omnidirectional camera to
eliminate the user/object position constraint and the left/right constraint of
the pointing arm. Although the accuracy of skeleton and object detection is low
due to the high distortion of equirectangular images, the proposed method
enables highly accurate estimation by repeatedly extracting regions of interest
from the equirectangular image and projecting them onto perspective images.
Furthermore, we found that training the likelihood of the target object in
machine learning further improves the estimation accuracy.
</summary>
    <author>
      <name>Nanami Kotani</name>
    </author>
    <author>
      <name>Asako Kanezaki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3588028.3603650</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3588028.3603650" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SIGGRAPH 2023 Poster. Project page:
  https://github.com/NKotani/PointAnywhere</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.01010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.01010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.04861v1</id>
    <updated>2023-08-09T10:49:38Z</updated>
    <published>2023-08-09T10:49:38Z</published>
    <title>Behavioural analysis of interaction between individuals and a robot in
  the window (cAESAR2023 workshop)</title>
    <summary>  The aim of the current research is to analyse and discover, in a real
context, behaviours, reactions and modes of interaction of social actors
(people) with the humanoid robot Pepper. Indeed, we wanted to observe in a
real, highly frequented context, the reactions and interactions of people with
Pepper, placed in a shop window, through a systematic observation approach. The
most interesting aspects of this research will be illustrated, bearing in mind
that this is a preliminary analysis, therefore, not yet definitively concluded.
</summary>
    <author>
      <name>Federica Bertel</name>
    </author>
    <author>
      <name>Cristina Gena</name>
    </author>
    <author>
      <name>Matteo Nazzario</name>
    </author>
    <author>
      <name>Irene Borgini</name>
    </author>
    <link href="http://arxiv.org/abs/2308.04861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.04861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.06358v1</id>
    <updated>2023-08-11T19:27:45Z</updated>
    <published>2023-08-11T19:27:45Z</published>
    <title>CA2: Cyber Attacks Analytics</title>
    <summary>  The VAST Challenge 2020 Mini-Challenge 1 requires participants to identify
the responsible white hat groups behind a fictional Internet outage. To address
this task, we have created a visual analytics system named CA2: Cyber Attacks
Analytics. This system is designed to efficiently compare and match subgraphs
within an extensive graph containing anonymized profiles. Additionally, we
showcase an iterative workflow that utilizes our system's capabilities to
pinpoint the responsible group.
</summary>
    <author>
      <name>Luyu Cheng</name>
    </author>
    <author>
      <name>Bairui Su</name>
    </author>
    <author>
      <name>Yumeng Xue</name>
    </author>
    <author>
      <name>Xiaoyu Liu</name>
    </author>
    <author>
      <name>Yunhai Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Visual Analytics Science and Technology (VAST)
  Challenge Workshop 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.06358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.06358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.08576v1</id>
    <updated>2023-08-16T02:55:19Z</updated>
    <published>2023-08-16T02:55:19Z</published>
    <title>Artistic control over the glitch in AI-generated motion capture</title>
    <summary>  Artificial intelligence (AI) models are prevalent today and provide a
valuable tool for artists. However, a lesser-known artifact that comes with AI
models that is not always discussed is the glitch. Glitches occur for various
reasons; sometimes, they are known, and sometimes they are a mystery. Artists
who use AI models to generate art might not understand the reason for the
glitch but often want to experiment and explore novel ways of augmenting the
output of the glitch. This paper discusses some of the questions artists have
when leveraging the glitch in AI art production. It explores the unexpected
positive outcomes produced by glitches in the specific context of motion
capture and performance art.
</summary>
    <author>
      <name>Jamal Knight</name>
    </author>
    <author>
      <name>Andrew Johnston</name>
    </author>
    <author>
      <name>Adam Berry</name>
    </author>
    <link href="http://arxiv.org/abs/2308.08576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.08576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.09153v1</id>
    <updated>2023-08-17T18:50:02Z</updated>
    <published>2023-08-17T18:50:02Z</published>
    <title>Alternatives to Contour Visualizations for Power Systems Data</title>
    <summary>  Electrical grids are geographical and topological structures whose voltage
states are challenging to represent accurately and efficiently for visual
analysis. The current common practice is to use colored contour maps, yet these
can misrepresent the data. We examine the suitability of four alternative
visualization methods for depicting voltage data in a geographically dense
distribution system -- Voronoi polygons, H3 tessellations, S2 tessellations,
and a network-weighted contour map. We find that Voronoi tessellations and
network-weighted contour maps more accurately represent the statistical
distribution of the data than regular contour maps.
</summary>
    <author>
      <name>Isaiah Lyons-Galante</name>
    </author>
    <author>
      <name>Morteza Karimzadeh</name>
    </author>
    <author>
      <name>Samantha Molnar</name>
    </author>
    <author>
      <name>Graham Johnson</name>
    </author>
    <author>
      <name>Kenny Gruchalla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Vis 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.09153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.09586v1</id>
    <updated>2023-08-18T14:29:23Z</updated>
    <published>2023-08-18T14:29:23Z</published>
    <title>Explaining the Arts: Toward a Framework for Matching Creative Tasks with
  Appropriate Explanation Mediums</title>
    <summary>  Although explainable computational creativity seeks to create and sustain
computational models of creativity that foster a collaboratively creative
process through explainability, there remains little to no work in supporting
designers when exploring the explanation medium. While explainable artificial
intelligence methods tend to support textual, visual, and numerical
explanations, within the arts, interaction mediums such as auditorial, tactile,
and olfactoral may offer more salient communication within the creative process
itself. Through this research, I propose a framework to assist designers of
explainable user interfaces in modeling the type of interaction they wish to
create using explanations.
</summary>
    <author>
      <name>Michael Clemens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, XAIxArts2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.09586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.09877v1</id>
    <updated>2023-08-19T01:53:43Z</updated>
    <published>2023-08-19T01:53:43Z</published>
    <title>User-centric AIGC products: Explainable Artificial Intelligence and AIGC
  products</title>
    <summary>  Generative AI tools, such as ChatGPT and Midjourney, are transforming
artistic creation as AI-art integration advances. However, Artificial
Intelligence Generated Content (AIGC) tools face user experience challenges,
necessitating a human-centric design approach. This paper offers a brief
overview of research on explainable AI (XAI) and user experience, examining
factors leading to suboptimal experiences with AIGC tools. Our proposed
solution integrates interpretable AI methodologies into the input and
adjustment feedback stages of AIGC products. We underscore XAI's potential to
enhance the user experience for ordinary users and present a conceptual
framework for improving AIGC user experience.
</summary>
    <author>
      <name>Hanjie Yu</name>
    </author>
    <author>
      <name>Yan Dong</name>
    </author>
    <author>
      <name>Qiong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2308.09877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.13539v1</id>
    <updated>2023-08-19T02:44:35Z</updated>
    <published>2023-08-19T02:44:35Z</published>
    <title>Redefining Computer Science Education: Code-Centric to Natural Language
  Programming with AI-Based No-Code Platforms</title>
    <summary>  This paper delves into the evolving relationship between humans and computers
in the realm of programming. Historically, programming has been a dialogue
where humans meticulously crafted communication to suit machine understanding,
shaping the trajectory of computer science education. However, the advent of
AI-based no-code platforms is revolutionizing this dynamic. Now, humans can
converse in their natural language, expecting machines to interpret and act.
This shift has profound implications for computer science education. As
educators, it's imperative to integrate this new dynamic into curricula. In
this paper, we've explored several pertinent research questions in this
transformation, which demand continued inquiry and adaptation in our
educational strategies.
</summary>
    <author>
      <name>David Y. J. Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.13539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.13539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.13552v1</id>
    <updated>2023-08-23T21:21:16Z</updated>
    <published>2023-08-23T21:21:16Z</published>
    <title>A Lens to Pandemic Stay at Home Attitudes</title>
    <summary>  We describe the design process and the challenges we met during a rapid
multi-disciplinary pandemic project related to stay-at-home orders and social
media moral frames. Unlike our typical design experience, we had to handle a
steeper learning curve, emerging and continually changing datasets, as well as
under-specified design requirements, persistent low visual literacy, and an
extremely fast turnaround for new data ingestion, prototyping, testing and
deployment. We describe the lessons learned through this experience.
</summary>
    <author>
      <name>Andrew Wentzel</name>
    </author>
    <author>
      <name>Lauren Levine</name>
    </author>
    <author>
      <name>Vipul Dhariwal</name>
    </author>
    <author>
      <name>Zahra Fatemi</name>
    </author>
    <author>
      <name>Barbara Di Eugenio</name>
    </author>
    <author>
      <name>Andrew Rojecki</name>
    </author>
    <author>
      <name>Elena Zheleva</name>
    </author>
    <author>
      <name>G. Elisabeta Marai</name>
    </author>
    <link href="http://arxiv.org/abs/2308.13552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.13552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.13953v1</id>
    <updated>2023-08-26T20:28:20Z</updated>
    <published>2023-08-26T20:28:20Z</published>
    <title>Investigating Psychological Ownership in a Shared AR Space: Effects of
  Human and Object Reality and Object Controllability</title>
    <summary>  Augmented reality (AR) provides users with a unique social space where
virtual objects are natural parts of the real world. The users can interact
with 3D virtual objects and virtual humans projected onto the physical
environment. This work examines perceived ownership based on the reality of
objects and partners, as well as object controllability in a shared AR setting.
Our formal user study with 28 participants shows a sense of possession,
control, separation, and partner presence affect their perceived ownership of a
shared object. Finally, we discuss the findings and present a conclusion.
</summary>
    <author>
      <name>Dongyun Han</name>
    </author>
    <author>
      <name>Donghoon Kim</name>
    </author>
    <author>
      <name>Kangsoo Kim</name>
    </author>
    <author>
      <name>Isaac Cho</name>
    </author>
    <link href="http://arxiv.org/abs/2308.13953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.13953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.14247v1</id>
    <updated>2023-08-28T01:27:12Z</updated>
    <published>2023-08-28T01:27:12Z</published>
    <title>Draco 2: An Extensible Platform to Model Visualization Design</title>
    <summary>  Draco introduced a constraint-based framework to model visualization design
in an extensible and testable form. It provides a way to abstract design
guidelines from theoretical and empirical studies and applies the knowledge in
automated design tools. However, Draco is challenging to use because there is
limited tooling and documentation. In response, we present Draco 2, the
successor with (1) a more flexible visualization specification format, (2) a
comprehensive test suite and documentation, and (3) flexible and convenient
APIs. We designed Draco 2 to be more extensible and easier to integrate into
visualization systems. We demonstrate these advantages and believe that they
make Draco 2 a platform for future research.
</summary>
    <author>
      <name>Junran Yang</name>
    </author>
    <author>
      <name>Péter Ferenc Gyarmati</name>
    </author>
    <author>
      <name>Zehua Zeng</name>
    </author>
    <author>
      <name>Dominik Moritz</name>
    </author>
    <link href="http://arxiv.org/abs/2308.14247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.14247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.16025v1</id>
    <updated>2023-08-30T13:30:57Z</updated>
    <published>2023-08-30T13:30:57Z</published>
    <title>A Critical Analysis of the What3Words Geocoding Algorithm</title>
    <summary>  What3Words is a geocoding application that uses triples of words instead of
alphanumeric coordinates to identify locations. What3Words has grown rapidly in
popularity over the past few years and is used in logistical applications
worldwide, including by emergency services. What3Words has also attracted
criticism for being less reliable than claimed, in particular that the chance
of confusing one address with another is high. This paper investigates these
claims and shows that the What3Words algorithm for assigning addresses to grid
boxes creates many pairs of confusable addresses, some of which are quite close
together. The implications of this for the use of What3Words in critical or
emergency situations is discussed.
</summary>
    <author>
      <name>Rudy Arthur</name>
    </author>
    <link href="http://arxiv.org/abs/2308.16025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.16025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.16377v1</id>
    <updated>2023-08-31T00:39:33Z</updated>
    <published>2023-08-31T00:39:33Z</published>
    <title>Science Communications for Explainable Artificial Intelligence</title>
    <summary>  Artificial Intelligence (AI) has a communication problem. XAI methods have
been used to make AI more understandable and helped resolve some of the
transparency issues that inhibit AI's broader usability. However, user
evaluation studies reveal that the often numerical explanations provided by XAI
methods have not always been effective for many types of users of AI systems.
This article aims to adapt the major communications models from Science
Communications into a framework for practitioners to understand, influence, and
integrate the context of audiences both for their communications supporting AI
literacy in the public and in designing XAI systems that are more adaptive to
different users.
</summary>
    <author>
      <name>Simon Hudson</name>
    </author>
    <author>
      <name>Matija Franklin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the IJCAI-23 Workshop on XAI held at the Thirty-Second
  International Joint Conference on Artificial Intelligence (IJCAI-23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.16377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.16377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00256v1</id>
    <updated>2023-09-01T05:20:35Z</updated>
    <published>2023-09-01T05:20:35Z</published>
    <title>Beyond Screens: Supporting Co-located Augmented Reality Experiences with
  Smart Home Devices</title>
    <summary>  We introduce Spooky Spirits, an AR game that makes novel use of everyday
smart home devices to support co-located play. Recent exploration of co-located
AR experiences consists mainly of digital visual augmentations on mobile or
head-mounted screens. In this work, we leverage widely adopted smart lightbulbs
to expand AR capabilities beyond the digital and into the physical world,
further leveraging the physicality of users' shared environment.
</summary>
    <author>
      <name>Ava Robinson</name>
    </author>
    <author>
      <name>Yu Jiang Tham</name>
    </author>
    <author>
      <name>Rajan Vaish</name>
    </author>
    <author>
      <name>Andrés Monroy-Hernández</name>
    </author>
    <link href="http://arxiv.org/abs/2309.00256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00718v1</id>
    <updated>2023-09-01T19:51:31Z</updated>
    <published>2023-09-01T19:51:31Z</published>
    <title>A Multisensory Approach to Virtual Reality Stress Reduction</title>
    <summary>  Forest bathing is a nature immersion practice that reduces stress, restores
mental resources, and has a wide variety of use cases in the treatment of
mental illnesses. Since many people who need the benefits of forest bathing
have little access to nature, virtual reality (VR) is being explored as a tool
for delivering accessible immersive nature experiences via virtual nature
environments (VNE's). Research on VNE's mainly utilizes the audiovisual
capabilities of VR, but since forest bathing is a fully multisensory
experience, further investigations into the integration of other sensory
technologies, namely smell and temperature, are essential for the future of VNE
research.
</summary>
    <author>
      <name>Rachel Masters</name>
    </author>
    <author>
      <name>Francisco Ortega</name>
    </author>
    <author>
      <name>Victoria Interrante</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00819v1</id>
    <updated>2023-09-02T04:12:30Z</updated>
    <published>2023-09-02T04:12:30Z</published>
    <title>Mixed Reality: The Interface of the Future</title>
    <summary>  The world is slowly moving towards everything being simulated digitally and
virtually. Mixed Reality (MR) is the amalgam of the real world with virtual
stimuli. It has great prospects in the future in terms of various applications
additionally with some challenges. This paper focuses on how Mixed Reality
could be used in the future along with the challenges that could arise. Several
application areas along with the potential benefits are studied in this
research. Three research questions are proposed, analyzed, and concluded
through the experiments. While the availability of MR devices could introduce a
lot of potential, specific challenges need to be scrutinized by the developers
and manufacturers. Overall, MR technology has a chance to enhance personalized,
supportive, and interactive experiences for human lives.
</summary>
    <author>
      <name>Dipesh Gyawali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.01763v1</id>
    <updated>2023-09-04T18:47:22Z</updated>
    <published>2023-09-04T18:47:22Z</published>
    <title>Visualizing the Weird and the Eerie</title>
    <summary>  In this brief essay, I reflect on how Mark Fisher's definitions of the weird
and the eerie could be applied in communicative data visualization. I ask how
visualization designers might elicit these two impressions when a viewer is
engaging with multimodal representations of data. I argue that there are
situations in which viewers should feel uncertain or suspicious of unseen
forces that account for the presence or absence of audiovisual patterns.
Finally, I conclude that the ability to appreciate the weird and the eerie in
data is particularly important at this moment in history, one marked by
significant ecological and economic disruption.
</summary>
    <author>
      <name>Matthew Brehmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of alt.VIS 2023 (https://altvis.github.io/), an IEEE VIS
  Workshop on Monday, October 23 in Melbourne, Australia</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.01763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.01763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.02945v1</id>
    <updated>2023-09-06T12:24:55Z</updated>
    <published>2023-09-06T12:24:55Z</published>
    <title>Designing Situated Dashboards: Challenges and Opportunities</title>
    <summary>  Situated Visualization is an emerging field that unites several areas -
visualization, augmented reality, human-computer interaction, and
internet-of-things, to support human data activities within the ubiquitous
world. Likewise, dashboards are broadly used to simplify complex data through
multiple views. However, dashboards are only adapted for desktop settings, and
requires visual strategies to support situatedness. We propose the concept of
AR-based situated dashboards and present design considerations and challenges
developed over interviews with experts. These challenges aim to propose
directions and opportunities for facilitating the effective designing and
authoring of situated dashboards.
</summary>
    <author>
      <name>Anika Sayara</name>
    </author>
    <author>
      <name>Benjamin Lee</name>
    </author>
    <author>
      <name>Carlos Quijano-Chavez</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ISA: 2nd Workshop on Immersive and Situated
  Analytics @ ISMAR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.02945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.02945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03201v1</id>
    <updated>2023-05-29T07:21:38Z</updated>
    <published>2023-05-29T07:21:38Z</published>
    <title>Disappearing frameworks explained</title>
    <summary>  The web is the most prominent application platform globally, thanks to its
vast user base. It started as a site platform in the 90s and morphed into an
application one over time as interactive web applications became a reality.
So-called single-page applications (SPAs) represent the current mainstream way
of developing web applications but they come with their drawbacks. Although
SPAs provide good developer experience, there is a cost for the users due to
the underlying technical assumptions. Disappearing frameworks question these
assumptions and provide a fresh start. The purpose of this short book is to
give a quick introduction to disappearing frameworks and show their meaning as
an emerging topic within the space of web application development.
</summary>
    <author>
      <name>Juho Vepsäläinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 7 figures, short book</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03233v1</id>
    <updated>2023-09-05T06:57:52Z</updated>
    <published>2023-09-05T06:57:52Z</published>
    <title>Scale-Score: Food Label to Support Nutritious and Sustainable Online
  Grocery Shopping</title>
    <summary>  To empower online grocery shoppers in making nutritionally and
environmentally informed decisions, we investigate the efficacy of the
Scale-Score, a label combining nutritional and environmental information to
highlight a product's benefit to both the consumer's and the planet's health,
without obscuring either information. We conducted an experimental study in a
mock online grocery environment, and assessed label efficacy. We find that the
Scale-Score supports nutritious purchases, yet needs improving regarding
sustainability support. Our research shows first insights into design
considerations and performance of a combined yet disjoint food label.
</summary>
    <author>
      <name>Marco Druschba</name>
    </author>
    <author>
      <name>Gözel Shakeri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICT4S 2023; extended abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.04543v1</id>
    <updated>2023-09-08T18:19:51Z</updated>
    <published>2023-09-08T18:19:51Z</published>
    <title>A Study on Workload Assessment and Usability of Wind-Aware User
  Interface for Small Unmanned Aircraft System Remote Operations</title>
    <summary>  This study evaluates pilots' cognitive workload and situational awareness
during remote small unmanned aircraft system operations in different wind
conditions. To complement the urban air mobility concept that envisions safe,
sustainable, and accessible air transportation, we conduct multiple experiments
in a realistic wind-aware simulator-user interface pipeline. Experiments are
performed with basic and wind-aware displays in several wind conditions to
assess how complex wind fields impact pilots' cognitive resources. Post-hoc
analysis reveals that providing pilots with real-time wind information improves
situational awareness while decreasing cognitive workload.
</summary>
    <author>
      <name>Asma Tabassum</name>
    </author>
    <author>
      <name>He Bai</name>
    </author>
    <author>
      <name>Nicoletta Fala</name>
    </author>
    <link href="http://arxiv.org/abs/2309.04543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.04543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.04722v1</id>
    <updated>2023-09-09T08:52:20Z</updated>
    <published>2023-09-09T08:52:20Z</published>
    <title>TECVis: A Visual Analytics Tool to Compare People's Emotion Feelings</title>
    <summary>  Twitter is one of the popular social media platforms where people share news
or reactions towards an event or topic using short text messages called
"tweets". Emotion analysis in these tweets can play a vital role in
understanding peoples' feelings towards the underlying event or topic. In this
work, we present our visual analytics tool, called TECVis, that focuses on
providing comparison views of peoples' emotion feelings in tweets towards an
event or topic. The comparison is done based on geolocations or timestamps.
TECVis provides several interaction and filtering options for navigation and
better exploration of underlying tweet data for emotion feelings comparison.
</summary>
    <author>
      <name>Ilya Nemtsov</name>
    </author>
    <author>
      <name>MST Jasmine Jahan</name>
    </author>
    <author>
      <name>Chuting Yan</name>
    </author>
    <author>
      <name>Shah Rukh Humayoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.04722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.04722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.04724v1</id>
    <updated>2023-09-09T09:01:33Z</updated>
    <published>2023-09-09T09:01:33Z</published>
    <title>A Visual Analytic Environment to Co-locate Peoples' Tweets with City
  Factual Data</title>
    <summary>  Social Media platforms (e.g., Twitter, Facebook, etc.) are used heavily by
public to provide news, opinions, and reactions towards events or topics.
Integrating such data with the event or topic factual data could provide a more
comprehensive understanding of the underlying event or topic. Targeting this,
we present our visual analytics tool, called VC-FaT, that integrates peoples'
tweet data regarding crimes in San Francisco city with the city factual crime
data. VC-FaT provides a number of interactive visualizations using both data
sources for better understanding and exploration of crime activities happened
in the city during a period of five years.
</summary>
    <author>
      <name>Snehal Patil</name>
    </author>
    <author>
      <name>Shah Rukh Humayoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.04724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.04724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.05669v1</id>
    <updated>2023-09-08T07:49:23Z</updated>
    <published>2023-09-08T07:49:23Z</published>
    <title>Implications of Edge Computing for Static Site Generation</title>
    <summary>  Static site generation (SSG) is a common technique in the web development
space to create performant websites that are easy to host. Numerous SSG tools
exist, and the approach has been complemented by newer approaches, such as
Jamstack, that extend its usability. Edge computing represents a new option to
extend the usefulness of SSG further by allowing the creation of dynamic sites
on top of a static backdrop, providing dynamic resources close to the user. In
this paper, we explore the impact of the recent developments in the edge
computing space and consider its implications for SSG.
</summary>
    <author>
      <name>Juho Vepsäläinen</name>
    </author>
    <author>
      <name>Arto Hellas</name>
    </author>
    <author>
      <name>Petri Vuorimaa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, 1 table, approved for WEBIST 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.05669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.05676v1</id>
    <updated>2023-09-09T08:55:22Z</updated>
    <published>2023-09-09T08:55:22Z</published>
    <title>MultiCaM-Vis: Visual Exploration of Multi-Classification Model with High
  Number of Classes</title>
    <summary>  Visual exploration of multi-classification models with large number of
classes would help machine learning experts in identifying the root cause of a
problem that occurs during learning phase such as miss-classification of
instances. Most of the previous visual analytics solutions targeted only a few
classes. In this paper, we present our interactive visual analytics tool,
called MultiCaM-Vis, that provides \Emph{overview+detail} style parallel
coordinate views and a Chord diagram for exploration and inspection of
class-level miss-classification of instances. We also present results of a
preliminary user study with 12 participants.
</summary>
    <author>
      <name>Syed Ahsan Ali Dilawer</name>
    </author>
    <author>
      <name>Shah Rukh Humayoun</name>
    </author>
    <link href="http://arxiv.org/abs/2309.05676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.06227v1</id>
    <updated>2023-09-12T13:46:29Z</updated>
    <published>2023-09-12T13:46:29Z</published>
    <title>On the Injunction of XAIxArt</title>
    <summary>  The position paper highlights the range of concerns that are engulfed in the
injunction of explainable artificial intelligence in art (XAIxArt). Through a
series of quick sub-questions, it points towards the ambiguities concerning
'explanation' and the postpositivist tradition of 'relevant explanation'.
Rejecting both 'explanation' and 'relevant explanation', the paper takes a
stance that XAIxArt is a symptom of insecurity of the anthropocentric notion of
art and a nostalgic desire to return to outmoded notions of authorship and
human agency. To justify this stance, the paper makes a distinction between an
ornamentation model of explanation to a model of explanation as sense-making.
</summary>
    <author>
      <name>Cheshta Arora</name>
    </author>
    <author>
      <name>Debarun Sarkar</name>
    </author>
    <link href="http://arxiv.org/abs/2309.06227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.06227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09781v1</id>
    <updated>2023-09-18T13:59:03Z</updated>
    <published>2023-09-18T13:59:03Z</published>
    <title>Talking to Data Visualizations: Opportunities and Challenges</title>
    <summary>  Speech is one of the interaction modalities that we increasingly come across
in natural user interfaces. However, its use in collaborative scenarios has not
yet been thoroughly investigated. In this reflection statement, we discuss the
opportunities and challenges of integrating speech interaction in multimodal
solutions for collaborative work with data visualizations. We discuss related
findings from other research communities and how we could build upon their work
to explore and make use of speech interaction for data visualizations in
co-located, hybrid, and remote settings.
</summary>
    <author>
      <name>Gabriela Molina León</name>
    </author>
    <author>
      <name>Petra Isenberg</name>
    </author>
    <author>
      <name>Andreas Breiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the MERCADO workshop at IEEE VIS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.09781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.11234v2</id>
    <updated>2023-09-22T11:19:41Z</updated>
    <published>2023-09-20T11:53:48Z</published>
    <title>Towards medhub: A Self-Service Platform for Analysts and Physicians</title>
    <summary>  Combining clinical and omics data can improve both daily clinical routines
and research to gain more insights into complex medical procedures. We present
the results of our first phase in a multi-year collaboration with analysts and
physicians aiming at improved inter-disciplinary biomarker identification. We
also outline our user-centered approach along its challenges, describe the
intermediate technical artifacts that serve as a basis for summative and
formative evaluation for the second project phase. Finally, we sketch the road
ahead and how we intend to combine visualization research with user-centered
design through problem-based prioritization.
</summary>
    <author>
      <name>Markus Höhn</name>
    </author>
    <author>
      <name>Hendrik Lücke-Tieke</name>
    </author>
    <author>
      <name>Jan Burmeister</name>
    </author>
    <author>
      <name>Jörn Kohlhammer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 + 1 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.11234v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.11234v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.12538v1</id>
    <updated>2023-09-21T23:32:37Z</updated>
    <published>2023-09-21T23:32:37Z</published>
    <title>Hanstreamer: an Open-source Webcam-based Live Data Presentation System</title>
    <summary>  We present Hanstreamer, a free and open-source system for webcam-based data
presentation. The system performs real-time gesture recognition on the user's
webcam video stream to provide interactive data visuals. Apart from the
standard chart and map visuals, Hanstreamer is the first such video data
presentation system to support network visualisation and interactive
DimpVis-style time-series data exploration. The system is ready for use with
popular online meeting software such as Zoom and Microsoft Teams.
</summary>
    <author>
      <name>Adrian Kristanto</name>
    </author>
    <author>
      <name>Maxime Cordeil</name>
    </author>
    <author>
      <name>Benjamin Tag</name>
    </author>
    <author>
      <name>Nathalie Henry Riche</name>
    </author>
    <author>
      <name>Tim Dwyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop MERCADO: Multimodal Experiences for Remote Communication
  Around Data Online, IEEE Visualization Conference 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2309.12538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.12538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.12583v1</id>
    <updated>2023-09-22T02:23:44Z</updated>
    <published>2023-09-22T02:23:44Z</published>
    <title>Using ChatGPT in HCI Research -- A Trioethnography</title>
    <summary>  This paper explores the lived experience of using ChatGPT in HCI research
through a month-long trioethnography. Our approach combines the expertise of
three HCI researchers with diverse research interests to reflect on our daily
experience of living and working with ChatGPT. Our findings are presented as
three provocations grounded in our collective experiences and HCI theories.
Specifically, we examine (1) the emotional impact of using ChatGPT, with a
focus on frustration and embarrassment, (2) the absence of accountability and
consideration of future implications in design, and raise (3) questions around
bias from a Global South perspective. Our work aims to inspire critical
discussions about utilizing ChatGPT in HCI research and advance equitable and
inclusive technological development.
</summary>
    <author>
      <name>Smit Desai</name>
    </author>
    <author>
      <name>Tanusree Sharma</name>
    </author>
    <author>
      <name>Pratyasha Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3571884.3603755</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3571884.3603755" rel="related"/>
    <link href="http://arxiv.org/abs/2309.12583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.12583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.13151v1</id>
    <updated>2023-09-22T19:27:03Z</updated>
    <published>2023-09-22T19:27:03Z</published>
    <title>A Survey of Brain Computer Interface Using Non-Invasive Methods</title>
    <summary>  Research on Brain-Computer Interface (BCI) began in the 1970s and has
increased in volume and diversified significantly since then. Today BCI is
widely used for applications like assistive devices for physically challenged
users, mental state monitoring, input devices for hands-free applications,
marketing, education, security, games and entertainment. This article explores
the advantages and disadvantages of invasive and non-invasive BCI technologies
and focuses on use cases of several non-invasive technologies, namely
electroencephalogram (EEG), functional Magnetic Resonance Imaging (fMRI), Near
Infrared Spectroscopy (NIRs) and hybrid systems.
</summary>
    <author>
      <name>Ritam Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/2309.13151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.13151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14634v1</id>
    <updated>2023-09-26T03:28:09Z</updated>
    <published>2023-09-26T03:28:09Z</published>
    <title>Synchronizing Full-Body Avatar Transforms with WebRTC DataChannel on
  Educational Metaverse</title>
    <summary>  Full-body avatars are suggested to be beneficial for communication in virtual
environments, and consistency between users' voices and gestures is considered
essential to ensure communication quality. This paper propose extending the
functionality of a web-based VR platform to support the use of full-body
avatars and delegated avatar transforms synchronization to WebRTC DataChannel
to enhance the consistency between voices and gestures. Finally, we conducted a
preliminary validation to confirm the consistency.
</summary>
    <author>
      <name>Yong-Hao Hu</name>
    </author>
    <author>
      <name>Kenichiro Ito</name>
    </author>
    <author>
      <name>Ayumi Igarashi</name>
    </author>
    <link href="http://arxiv.org/abs/2309.14634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14877v2</id>
    <updated>2023-09-27T11:40:43Z</updated>
    <published>2023-09-26T12:20:18Z</published>
    <title>Explainable Sustainability for AI in the Arts</title>
    <summary>  AI is becoming increasingly popular in artistic practices, but the tools for
informing practitioners about the environmental impact (and other
sustainability implications) of AI are adapted for other contexts than creative
practices -- making the tools and sustainability implications of AI not
accessible for artists and creative practitioners. In this position paper, I
describe two empirical studies that aim to develop environmental sustainability
reflection systems for AI Arts, and discuss and introduce Explainable
Sustainability in for AI Arts.
</summary>
    <author>
      <name>Petra Jääskeläinen</name>
    </author>
    <link href="http://arxiv.org/abs/2309.14877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.16305v1</id>
    <updated>2023-09-28T09:56:55Z</updated>
    <published>2023-09-28T09:56:55Z</published>
    <title>Does Explanation Matter? An Exploratory Study on the Effects of Covid 19
  Misinformation Warning Flags on Social Media</title>
    <summary>  We investigate whether adding specific explanations from fact checking
websites enhances trust in these flags. We experimented with 348 American
participants, exposing them to a randomised order of true and false news
headlines related to COVID 19, with and without warning flags and explanation
text. Our findings suggest that warning flags, whether alone or accompanied by
explanatory text, effectively reduce the perceived accuracy of fake news and
the intent to share such headlines. Interestingly, our study also suggests that
incorporating explanatory text in misinformation warning systems could
significantly enhance their trustworthiness, emphasising the importance of
transparency and user comprehension in combating fake news on social media.
</summary>
    <author>
      <name>Dipto Barman</name>
    </author>
    <author>
      <name>Owen Conlan</name>
    </author>
    <link href="http://arxiv.org/abs/2309.16305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.16305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00215v1</id>
    <updated>2023-09-30T01:34:03Z</updated>
    <published>2023-09-30T01:34:03Z</published>
    <title>Implicit collaboration with a drawing machine through dance movements</title>
    <summary>  In this demonstration, we exhibit the initial results of an ongoing body of
exploratory work, investigating the potential for creative machines to
communicate and collaborate with people through movement as a form of implicit
interaction. The paper describes a Wizard-of-Oz demo, where a hidden wizard
controls an AxiDraw drawing robot while a participant collaborates with it to
draw a custom postcard. This demonstration aims to gather perspectives from the
computational fabrication community regarding how practitioners of fabrication
with machines experience interacting with a mixed-initiative collaborative
machine.
</summary>
    <author>
      <name>Itay Grinberg</name>
    </author>
    <author>
      <name>Alexandra Bremers</name>
    </author>
    <author>
      <name>Louisa Pancoast</name>
    </author>
    <author>
      <name>Wendy Ju</name>
    </author>
    <link href="http://arxiv.org/abs/2310.00215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00768v1</id>
    <updated>2023-10-01T19:22:25Z</updated>
    <published>2023-10-01T19:22:25Z</published>
    <title>Anywhere &amp; Everywhere: A Mobile, Immersive, and Ubiquitous Vision for
  Data Analytics</title>
    <summary>  Data is collected everywhere in our increasingly instrumented world and
people are increasingly wanting to access this data from anywhere in it. This
kind of anywhere &amp; everywhere data present new challenges and opportunities for
data-driven sensemaking and decision-making that will require leveraging novel
mobile, immersive, and ubiquitous technologies undergirded by recent advances
in human cognition. In this paper, we examine these emerging forms of analytics
that are transforming how data analysis will be conducted in the future: in an
ecosystem of connected devices, interactive visualizations, and collaborating
users with vast amounts of data and analytical mechanisms available at their
fingertips.
</summary>
    <author>
      <name>Niklas Elmqvist</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications of the ACM, December 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.00768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.04737v1</id>
    <updated>2023-10-07T08:34:28Z</updated>
    <published>2023-10-07T08:34:28Z</published>
    <title>Super Synthesis Pros., or why CHI PLAY needs research synthesis</title>
    <summary>  Games user research is a-booming -- or maybe a-goomba-ing -- with a boundless
parade of papers popping up from every nook and pipe. We may need a super power
-- or super method -- from another world. I outline three motivations for
jump-starting research synthesis in games user research. I argue that: research
synthesis will validate this field of study and enrich primary research
(meta-scholarship); we must level up both primary and secondary research
(education); and we should reflect this epistemological stance in community
structures and adopt established tools and protocols (standardization). I offer
power-ups to get the toads rolling.
</summary>
    <author>
      <name>Katie Seaborn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3573382.3616051</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3573382.3616051" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CHI PLAY '23</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Companion Proceedings of the Annual Symposium on Computer-Human
  Interaction in Play, 235-237 (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.04737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.04737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.07458v1</id>
    <updated>2023-10-11T13:03:23Z</updated>
    <published>2023-10-11T13:03:23Z</published>
    <title>RealityDrop: A Multimodal Mixed Reality Framework to Manipulate Virtual
  Content between Cross-system Displays</title>
    <summary>  In this poster, we present RealityDrop, a novel multimodal framework that
uses Mixed Reality (MR) technology to manipulate, display, and transfer virtual
content across different display systems. Employing MR as the centre of
control, RealityDrop affords concise information dissemination among diverse
collaborators, through varied representations that best fit each display
system's unique features using `superhuman' gaze and gesture interactions.
Three multimodal interaction techniques, a customised content interpreter, and
two cross-system interfaces are incorporated for fluent content manipulation
and presentation.
</summary>
    <author>
      <name>Jeremy McDade</name>
    </author>
    <author>
      <name>Allison Jing</name>
    </author>
    <author>
      <name>Andrew Cunningham</name>
    </author>
    <link href="http://arxiv.org/abs/2310.07458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.07458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.10988v3</id>
    <updated>2023-11-12T02:31:41Z</updated>
    <published>2023-10-17T04:22:26Z</published>
    <title>HCI in e-Government and e-Democracy</title>
    <summary>  This chapter introduces the application of HCI design processes and design
principles in e-government and e-democracy. We elaborate on HCI design
processes and six HCI design principles in the context of e-government and
e-democracy, including citizen-centered design, usability, accessibility,
access to information, transaction efficiency, and security and privacy. Then,
we present two cases to demonstrate the value of applying the HCI processes and
design principles in developing and deploying e-government and e-democracy.
Finally, we highlight the challenges faced by e-government and e-democracy as
well as the future trends. In conclusion, HCI can help the success of
e-government and e-democracy and their future growth.
</summary>
    <author>
      <name>Tianmu Zhu</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2310.10988v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.10988v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.11521v1</id>
    <updated>2023-10-17T18:38:12Z</updated>
    <published>2023-10-17T18:38:12Z</published>
    <title>DataGarden: Exploring our Community in a VR Data Visualization</title>
    <summary>  As our society is becoming increasingly data-dependent, more and more people
rely on charts and graphs to understand and communicate complex data. While
such visualizations effectively reveal meaningful trends, they unavoidably
aggregate data into points and bars that are overly simplified depictions of
ourselves and our communities. We present DataGarden, a system that supports
embodied interactions with humane data representations in an immersive VR
environment. Through the system, we explore ways to rethink the traditional
visualization approach and allow people to empathize more deeply with the
people behind the data.
</summary>
    <author>
      <name>Joy Kondo</name>
    </author>
    <author>
      <name>Justin Park</name>
    </author>
    <author>
      <name>Josiah Kondo</name>
    </author>
    <author>
      <name>Nam Wook Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.11521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.11521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13182v1</id>
    <updated>2023-10-19T22:31:59Z</updated>
    <published>2023-10-19T22:31:59Z</published>
    <title>Show Me My Users: A Dashboard Visualizing User Interaction Logs</title>
    <summary>  This paper describes the design of a dashboard and analysis pipeline to
monitor users of visualization tools in the wild. Our pipeline describes how to
extract analytical KPIs from extensive log event data involving a mix of user
types. The resulting three-page dashboard displays live KPIs, helping analysts
understand users, detect exploratory behaviors, plan education interventions,
and improve tool features. We propose this case study as a motivation to use
the dashboard approach for a more `casual' monitoring of users and building
carer mindsets for visualization tools.
</summary>
    <author>
      <name>Jinrui Wang</name>
    </author>
    <author>
      <name>Mashael AlKadi</name>
    </author>
    <author>
      <name>Benjamin Bach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the IEEE VIS conference (Melbourne, VIC), 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.13182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13700v1</id>
    <updated>2023-09-28T16:36:25Z</updated>
    <published>2023-09-28T16:36:25Z</published>
    <title>Augmenting Heritage: An Open-Source Multiplatform AR Application</title>
    <summary>  AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.
</summary>
    <author>
      <name>Corrie Green</name>
    </author>
    <link href="http://arxiv.org/abs/2310.13700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13709v1</id>
    <updated>2023-10-09T13:28:18Z</updated>
    <published>2023-10-09T13:28:18Z</published>
    <title>Redefining Access to Large Audiovisual Archives through Embodied
  Experiences in Immersive Environments: Creativity &amp; Cognition 2022 --
  Graduate Student Symposium</title>
    <summary>  Audiovisual archives are the mnemonic archives of the 21st century, with
important cultural institutions increasingly digitizing their video
collections. However, these remain mostly inaccessible, due to the sheer amount
of content combined with the lack of innovative forms of engagement through
compelling frameworks for their exploration. The present research therefore
aims at redefining access to large video collections through embodied
experiences in immersive environments. The author claims that, once users are
empowered to be actors of the experience rather than mere spectators, their
creativity is stimulated and narrative can emerge.
</summary>
    <author>
      <name>Giacomo Alliata</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3527927.3533735</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3527927.3533735" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 14th Conference on Creativity and Cognition</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.13709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13718v1</id>
    <updated>2023-10-17T09:28:43Z</updated>
    <published>2023-10-17T09:28:43Z</published>
    <title>A Workflow Approach to Visualization-Based Storytelling with Cultural
  Heritage Data</title>
    <summary>  Stories are as old as human history - and a powerful means for the engaging
communication of information, especially in combination with visualizations.
The InTaVia project is built on this intersection and has developed a platform
which supports the workflow of cultural heritage experts to create compelling
visualization-based stories: From the search for relevant cultural objects and
actors in a cultural knowledge graph, to the curation and visual analysis of
the selected information, and to the creation of stories based on these data
and visualizations, which can be shared with the interested public.
</summary>
    <author>
      <name>Johannes Liem</name>
    </author>
    <author>
      <name>Jakob Kusnick</name>
    </author>
    <author>
      <name>Samuel Beck</name>
    </author>
    <author>
      <name>Florian Windhager</name>
    </author>
    <author>
      <name>Eva Mayr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VIS4DH 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.13718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.03385v1</id>
    <updated>2023-11-03T16:51:44Z</updated>
    <published>2023-11-03T16:51:44Z</published>
    <title>Intelligent Stress Assessment for e-Coaching</title>
    <summary>  This paper considers the adaptation of the e-coaching concept at times of
emergencies and disasters, through aiding the e-coaching with intelligent tools
for monitoring humans' affective state. The states such as anxiety, panic,
avoidance, and stress, if properly detected, can be mitigated using the
e-coaching tactic and strategy. In this work, we focus on a stress monitoring
assistant tool developed on machine learning techniques. We provide the results
of an experimental study using the proposed method.
</summary>
    <author>
      <name>Kenneth Lai</name>
    </author>
    <author>
      <name>Svetlana Yanushkevich</name>
    </author>
    <author>
      <name>Vlad Shmerko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Symposium Series on Computational Intelligence.
  arXiv admin note: substantial text overlap with arXiv:2105.11437</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.03385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.03385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.04328v1</id>
    <updated>2023-11-07T20:19:13Z</updated>
    <published>2023-11-07T20:19:13Z</published>
    <title>A Pilot Study of a Human-Readable Robotic Process Automation Language</title>
    <summary>  In this paper, we explore the usability of a custom eXtensible Robotic
Language (XRL) we proposed. To evaluate the user experience and the interaction
with the potential XRL-based software robot, we conducted an exploratory study
comparing the notation of three business processes using our XRL language and
two languages used by the leading RPA solutions. The results of our exploratory
study show that the currently used XML-based formats perform worse in terms of
conciseness and readability. Our new XRL language is promising in terms of
increasing the readability of the language, thus reducing the time needed to
automate business processes.
</summary>
    <author>
      <name>Piotr Gago</name>
    </author>
    <author>
      <name>Daniel Jabłoński</name>
    </author>
    <author>
      <name>Anna Voitenkova</name>
    </author>
    <author>
      <name>Ihor Debelyi</name>
    </author>
    <author>
      <name>Kinga Skorupska</name>
    </author>
    <author>
      <name>Maciej Grzeszczuk</name>
    </author>
    <author>
      <name>Wiesław Kopeć</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.04328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.04328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.05449v1</id>
    <updated>2023-11-09T15:38:05Z</updated>
    <published>2023-11-09T15:38:05Z</published>
    <title>Understanding emotions in the context of IT-based self-monitoring</title>
    <summary>  This study explores the intersection of information technology-based
self-monitoring (ITSM) and emotional responses in chronic care. It critiques
the lack of theoretical depth in current ITSM research and proposes a dynamic
emotion process theory to understand ITSM's impact on users' emotions.
Utilizing computational grounded theory and machine learning analysis of
hypertension app reviews, the research seeks to extend emotion theory by
examining ITSM stimuli and their influence on emotional episodes, moving beyond
discrete emotion models towards a continuous, nuanced understanding of
emotional responses.
</summary>
    <author>
      <name>Danielly de Paula</name>
    </author>
    <author>
      <name>Florian Borchert</name>
    </author>
    <author>
      <name>Ariane Sasso</name>
    </author>
    <author>
      <name>Falk Uebernickel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 5 Figures, 5 Tables, 2 Appendixes</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.05449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.05449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.06695v1</id>
    <updated>2023-11-12T00:22:09Z</updated>
    <published>2023-11-12T00:22:09Z</published>
    <title>Conversational Data Exploration: A Game-Changer for Designing Data
  Science Pipelines</title>
    <summary>  This paper proposes a conversational approach implemented by the system
Chatin for driving an intuitive data exploration experience. Our work aims to
unlock the full potential of data analytics and artificial intelligence with a
new generation of data science solutions. Chatin is a cutting-edge tool that
democratises access to AI-driven solutions, empowering non-technical users from
various disciplines to explore data and extract knowledge from it.
</summary>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <author>
      <name>Tania Cerquitelli</name>
    </author>
    <author>
      <name>Javier A. Espinosa-Oviedo</name>
    </author>
    <author>
      <name>François Cheval</name>
    </author>
    <author>
      <name>Anthelme Buchaille</name>
    </author>
    <author>
      <name>Luca Polgar</name>
    </author>
    <link href="http://arxiv.org/abs/2311.06695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.06695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.09244v1</id>
    <updated>2023-11-12T20:53:08Z</updated>
    <published>2023-11-12T20:53:08Z</published>
    <title>Quality Evaluation of Projection-Based VR Displays</title>
    <summary>  We present a collection of heuristics and simple tests for evaluating the
quality of a projection-based virtual reality display. A typical VR system
includes numerous potential sources of error. By understanding the
characteristics of a correctly working system, and the types of errors that are
likely to occur, users can quickly determine if their display is inaccurate and
what components may need correction.
</summary>
    <author>
      <name>Dave Pape</name>
    </author>
    <author>
      <name>Dan Sandin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 4th International Immersive Projection Technology
  Workshop (Ames, Iowa, 19-20 June 2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.09244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.09244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10303v1</id>
    <updated>2023-11-17T03:25:53Z</updated>
    <published>2023-11-17T03:25:53Z</published>
    <title>Virtual Heritage at iGrid 2000</title>
    <summary>  As part of the iGrid Research Demonstration at INET 2000, we created two
Virtual Cultural Heritage environments - "Virtual Harlem" and "Shared Miletus".
The purpose of these applications was to explore possibilities in using the
combination of high-speed international networks and virtual reality (VR)
displays for cultural heritage education. Our ultimate goal is to enable the
construction of tele-immersive museums and classes. In this paper we present an
overview of the infrastructure used for these applications, and some details of
their construction.
</summary>
    <author>
      <name>Dave Pape</name>
    </author>
    <author>
      <name>Josephine Anstey</name>
    </author>
    <author>
      <name>Bryan Carter</name>
    </author>
    <author>
      <name>Jason Leigh</name>
    </author>
    <author>
      <name>Maria Roussou</name>
    </author>
    <author>
      <name>Tim Portlock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, INET 2001 (Stockholm, Sweden, 5-8 June 2001)</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.10303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10833v2</id>
    <updated>2025-04-28T15:25:12Z</updated>
    <published>2023-11-17T19:24:39Z</published>
    <title>Generative AI has lowered the barriers to computational social sciences</title>
    <summary>  Generative artificial intelligence (AI) has revolutionized the field of
computational social science (CSS), unleashing new possibilities for collecting
and analyzing multimodal data, especially for scholars who may not have
extensive programming expertise. This breakthrough carries profound
implications for social scientists. First, generative AI can significantly
enhance the productivity of social scientists by automating the generation,
annotation, and debugging of code. Second, it empowers researchers to delve
into sophisticated data analysis through the innovative use of prompt
engineering. Last, the educational sphere of CSS stands to benefit immensely
from these tools, given their exceptional ability to annotate and elucidate
complex codes for learners, thereby simplifying the learning process and making
the technology more accessible.
</summary>
    <author>
      <name>Yongjun Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2311.10833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.12037v1</id>
    <updated>2023-11-17T04:03:53Z</updated>
    <published>2023-11-17T04:03:53Z</published>
    <title>The ImmersaDesk3 -- Experiences With A Flat Panel Display for Virtual
  Reality</title>
    <summary>  In this paper we discuss the design and implementation of a plasma display
panel for a wide field of view desktop virtual reality environment. Present
commercial plasma displays are not designed with virtual reality in mind,
leading to several problems in generating stereo imagery and obtaining good
tracking information. Although we developed solutions for a number of these
problems, the limitations of the system preclude its current use in practical
applications, and point to issues that must be resolved for flat panel displays
to be useful for VR.
</summary>
    <author>
      <name>Dave Pape</name>
    </author>
    <author>
      <name>Josephine Anstey</name>
    </author>
    <author>
      <name>Mike Bogucki</name>
    </author>
    <author>
      <name>Greg Dawe</name>
    </author>
    <author>
      <name>Tom DeFanti</name>
    </author>
    <author>
      <name>Andy Johnson</name>
    </author>
    <author>
      <name>Dan Sandin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, 3rd International Immersive Projection Technology
  Workshop (Stuttgart, Germany, 10-11 May 1999)</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.12037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.12654v1</id>
    <updated>2023-11-21T14:56:55Z</updated>
    <published>2023-11-21T14:56:55Z</published>
    <title>PARK: Parkinson's Analysis with Remote Kinetic-tasks</title>
    <summary>  We present a web-based framework to screen for Parkinson's disease (PD) by
allowing users to perform neurological tests in their homes. Our web framework
guides the users to complete three tasks involving speech, facial expression,
and finger movements. The task videos are analyzed to classify whether the
users show signs of PD. We present the results in an easy-to-understand manner,
along with personalized resources to further access to treatment and care. Our
framework is accessible by any major web browser, improving global access to
neurological care.
</summary>
    <author>
      <name>Md Saiful Islam</name>
    </author>
    <author>
      <name>Sangwu Lee</name>
    </author>
    <author>
      <name>Abdelrahman Abdelkader</name>
    </author>
    <author>
      <name>Sooyong Park</name>
    </author>
    <author>
      <name>Ehsan Hoque</name>
    </author>
    <link href="http://arxiv.org/abs/2311.12654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.13894v1</id>
    <updated>2023-11-23T10:25:51Z</updated>
    <published>2023-11-23T10:25:51Z</published>
    <title>A first step towards an ecosystem meta-model for humancentered design in
  case of disabled users</title>
    <summary>  The involvement of the ecosystem or social environment of the disabled user
is considered as very useful and even essential for the human-centered design
of assistive technologies. In the era of model-based approaches, the modeling
of the ecosystem is therefore to be considered. The first version of a
metamodel of ecosystem is proposed. It is illustrated through a first case
study. It concerns a project aiming at a communication aid for people with
cerebral palsy. A conclusion and research perspectives end this paper.
</summary>
    <author>
      <name>Christophe Kolski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMIH</arxiv:affiliation>
    </author>
    <author>
      <name>Nadine Vigouroux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT-ELIPSE, CNRS, GREYC</arxiv:affiliation>
    </author>
    <author>
      <name>Yohan Guerrier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMIH</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Vella</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT-ELIPSE, CNRS</arxiv:affiliation>
    </author>
    <author>
      <name>Marine Guffroy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Disab2023 Engineering Interactive Computing Systems for People
  with Disabilities, Jun 2023, Swansea, United Kingdom</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.13894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.13894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14726v1</id>
    <updated>2023-11-20T11:09:59Z</updated>
    <published>2023-11-20T11:09:59Z</published>
    <title>Visual Guitar Tab Comparison</title>
    <summary>  We designed a visual interface for comparing different guitar tablature (tab)
versions of the same piece. By automatically aligning the bars of these
versions and visually encoding different metrics, our interface helps determine
similarity, difficulty, and correctness. During our design, we collected and
integrated feedback from musicians and finally conducted a qualitative
evaluation with five guitarists. Results confirm that our interface effectively
supports comparison and helps musicians choose a version appropriate for their
personal skills and tastes.
</summary>
    <author>
      <name>Frank Heyen</name>
    </author>
    <author>
      <name>Alejandro Gabino Diaz Mendoza</name>
    </author>
    <author>
      <name>Quynh Quang Ngo</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Late-breaking demo for ISMIR 2023
  https://ismir2023program.ismir.net/lbd_357.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.00408v1</id>
    <updated>2023-12-01T08:05:22Z</updated>
    <published>2023-12-01T08:05:22Z</published>
    <title>Beyond the Screen: Reshaping the Workplace with Virtual and Augmented
  Reality</title>
    <summary>  Although extended reality technologies have enjoyed an explosion in
popularity in recent years, few applications are effectively used outside the
entertainment or academic contexts. This work consists of a literature review
regarding the effective integration of such technologies in the workplace. It
aims to provide an updated view of how they are being used in that context.
First, we examine existing research concerning virtual, augmented, and
mixed-reality applications. We also analyze which have made their way to the
workflows of companies and institutions. Furthermore, we circumscribe the
aspects of extended reality technologies that determined this applicability.
</summary>
    <author>
      <name>Nuno Verdelho Trindade</name>
    </author>
    <author>
      <name>Alfredo Ferreira</name>
    </author>
    <author>
      <name>João Madeiras Pereira</name>
    </author>
    <link href="http://arxiv.org/abs/2312.00408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.00408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.02358v1</id>
    <updated>2023-12-04T21:36:58Z</updated>
    <published>2023-12-04T21:36:58Z</published>
    <title>Peer attention enhances student learning</title>
    <summary>  Human visual attention is susceptible to social influences. In education,
peer effects impact student learning, but their precise role in modulating
attention remains unclear. Our experiment (N=311) demonstrates that displaying
peer visual attention regions when students watch online course videos enhances
their focus and engagement. However, students retain adaptability in following
peer attention cues. Overall, guided peer attention improves learning
experiences and outcomes. These findings elucidate how peer visual attention
shapes students' gaze patterns, deepening understanding of peer influence on
learning. They also offer insights into designing adaptive online learning
interventions leveraging peer attention modelling to optimize student
attentiveness and success.
</summary>
    <author>
      <name>Songlin Xu</name>
    </author>
    <author>
      <name>Dongyin Hu</name>
    </author>
    <author>
      <name>Ru Wang</name>
    </author>
    <author>
      <name>Xinyu Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2312.02358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.02358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03193v1</id>
    <updated>2023-12-05T23:56:05Z</updated>
    <published>2023-12-05T23:56:05Z</published>
    <title>Conceptualizing the Relationship between AI Explanations and User Agency</title>
    <summary>  We grapple with the question: How, for whom and why should explainable
artificial intelligence (XAI) aim to support the user goal of agency? In
particular, we analyze the relationship between agency and explanations through
a user-centric lens through case studies and thought experiments. We find that
explanation serves as one of several possible first steps for agency by
allowing the user convert forethought to outcome in a more effective manner in
future interactions. Also, we observe that XAI systems might better cater to
laypersons, particularly "tinkerers", when combining explanations and user
control, so they can make meaningful changes.
</summary>
    <author>
      <name>Iyadunni Adenuga</name>
    </author>
    <author>
      <name>Jonathan Dodge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2023 Workshop: Human-Centered Explainable AI (HCXAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.03193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03411v1</id>
    <updated>2023-12-06T10:49:17Z</updated>
    <published>2023-12-06T10:49:17Z</published>
    <title>Automated clustering of video games into groups with distinctive names</title>
    <summary>  When doing a study on a large number of video games, it may be difficult to
cluster them into coherent groups to better study them. In this paper, we
introduce a novel algorithm, that takes as input any set of games S that are
released on Steam and an integer k, and cluster S into k groups. Each group is
then assigned a distinctive name in the form of a Steam tag. We believe our
tool to be valuable for gaining deeper insights into the video game market. We
show that our algorithm maximises an objective function that we introduce, the
naming score, which assesses the quality of a clustering and how distinctive
its name is.
</summary>
    <author>
      <name>Nicolas Grelier</name>
    </author>
    <author>
      <name>Stéphane Kaufmann</name>
    </author>
    <link href="http://arxiv.org/abs/2312.03411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.06570v1</id>
    <updated>2023-12-11T17:57:08Z</updated>
    <published>2023-12-11T17:57:08Z</published>
    <title>Preserving the Artifacts of the Early Digital Era: A Study of What, Why
  and How?</title>
    <summary>  In this article, we report the pilot results of a survey study (N=1036)
related to social attitudes towards the early digital heritage. On the basis of
the answers, we consider what constitutes early digital artifacts (EDA) and
outline how knowledge about them can be useful. We explore attitudes toward the
historical and cultural importance of various EDAs and chart the surveyed
requirements for their successful and sustainable preservation for current and
future generations.
</summary>
    <author>
      <name>Maciej Grzeszczuk</name>
    </author>
    <author>
      <name>Kinga Skorupska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 2 tables. To be published in 11th Machine
  Intelligence and Digital Interaction MIDI Conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.06570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.06570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.11144v1</id>
    <updated>2023-12-18T12:35:16Z</updated>
    <published>2023-12-18T12:35:16Z</published>
    <title>LSDvis: Hallucinatory Data Visualisations in Real World Environments</title>
    <summary>  We propose the concept of "LSDvis": the (highly exaggerated) visual blending
of situated visualisations and the real-world environment to produce data
representations that resemble hallucinations. Such hallucinatory visualisations
incorporate elements of the physical environment, twisting and morphing their
appearance such that they become part of the visualisation itself. We
demonstrate LSDvis in a ``proof of proof of concept'', where we use Stable
Diffusion to modify images of real environments with abstract data
visualisations as input. We conclude by discussing considerations of LSDvis. We
hope that our work promotes visualisation designs which deprioritise saliency
in favour of quirkiness and ambience.
</summary>
    <author>
      <name>Ari Kouts</name>
    </author>
    <author>
      <name>Lonni Besançon</name>
    </author>
    <author>
      <name>Michael Sedlmair</name>
    </author>
    <author>
      <name>Benjamin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the alt.VIS workshop at IEEE VIS 2023:
  https://altvis.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.11144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.11144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.11817v1</id>
    <updated>2023-12-19T03:20:41Z</updated>
    <published>2023-12-19T03:20:41Z</published>
    <title>Users Approach on Providing Feedback for Smart Home Devices Phase I</title>
    <summary>  Smart home technology is part of our everyday lives, and this technology is
fast-evolving compared to other technologies. The user's feedback is gathered
in this paper by conducting expert interviews on how collecting the feedback
from the smart home devices will be helpful to improve the devices. We are yet
to know about the feedback system of the smart home devices and how provided
feedback will support increasing the devices' requirements. Today, we present
our analysis from our exploratory interview method with the student of a
certain group, and we try to study the attitude of providing feedback. The
results suggested that the users are ready to give their feedback very actively
to better their usage as every user has their own needs to fulfill.
</summary>
    <author>
      <name>Santhosh Pogaku</name>
    </author>
    <link href="http://arxiv.org/abs/2312.11817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.11817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.12473v2</id>
    <updated>2023-12-21T02:25:41Z</updated>
    <published>2023-12-19T10:24:53Z</published>
    <title>A Study on Social Robot Behavior in Group Conversation</title>
    <summary>  Recently, research in human-robot interaction began to consider a robot's
influence at the group level. Despite the recent growth in research
investigating the effects of robots within groups of people, our overall
understanding of what happens when robots are placed within groups or teams of
people is still limited. This paper investigates several key problems for
social robots that manage conversations in a group setting, where the number of
participants is more than two. In a group setting, the conversation dynamics
are a lot more complicated than the conventional one-to-one conversation, thus,
there are more challenges need to be solved.
</summary>
    <author>
      <name>Tung Nguyen</name>
    </author>
    <author>
      <name>Eric Nichols</name>
    </author>
    <author>
      <name>Randy Gomez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.12473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.12473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.13734v1</id>
    <updated>2023-12-21T10:57:52Z</updated>
    <published>2023-12-21T10:57:52Z</published>
    <title>Dialogue System of Team NTT-EASE for DRC2023</title>
    <summary>  We developed a dialogue system as a team NTT-EASE in the Dialogue Robot
Competition 2023 (DRC2023). We introduce a dialogue system (EASE-DRCBot)
constructed for DRC2023. EASE-DRCBot incorporates a manually defined dialogue
flow. The conditions for system utterances are based on keyword extraction,
example-based method, and sentiment analysis. For answering a user's question,
EASE-DRCBot utilizes GPT-3.5 to generate responses. We analyze the results of
the preliminary round and explain future works.
</summary>
    <author>
      <name>Yuki Kubo</name>
    </author>
    <author>
      <name>Tomoya Yamashita</name>
    </author>
    <author>
      <name>Masanori Yamada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is part of the proceedings of the Dialogue Robot
  Competition 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.13734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.13734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.13787v1</id>
    <updated>2023-12-21T12:23:14Z</updated>
    <published>2023-12-21T12:23:14Z</published>
    <title>User-adaptive Tourist Information Dialogue System with Yes/No Classifier
  and Sentiment Estimator</title>
    <summary>  We introduce our system developed for Dialogue Robot Competition 2023
(DRC2023). First, rule-based utterance selection and utterance generation using
a large language model (LLM) are combined. We ensure the quality of system
utterances while also being able to respond to unexpected user utterances.
Second, dialogue flow is controlled by considering the results of the
BERT-based yes/no classifier and sentiment estimator. These allow the system to
adapt state transitions and sightseeing plans to the user.
</summary>
    <author>
      <name>Ryo Yanagimoto</name>
    </author>
    <author>
      <name>Yunosuke Kubo</name>
    </author>
    <author>
      <name>Miki Oshio</name>
    </author>
    <author>
      <name>Mikio Nakano</name>
    </author>
    <author>
      <name>Kenta Yamamoto</name>
    </author>
    <author>
      <name>Kazunori Komatani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is part of the proceedings of the Dialogue Robot
  Competition 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.13787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.13787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.17708v1</id>
    <updated>2023-12-29T17:52:13Z</updated>
    <published>2023-12-29T17:52:13Z</published>
    <title>The six ways to build trust and reduce privacy concern in a Central Bank
  Digital Currency (CBDC)</title>
    <summary>  Central Bank Digital Currencies (CBDCs) have been implemented by only a
handful of countries, but they are being explored by many more. CBDCs are
digital currencies issued and backed by a central bank. Consumer trust can
encourage or discourage the adoption of this currency, which is also a payment
system and a technology. This research attempts to understand consumer trust in
CBDCs so that the development and adoption stages are more effective and
satisfying for all the stakeholders.
</summary>
    <author>
      <name>Alex Zarifis</name>
    </author>
    <author>
      <name>Xusen Cheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-33665-2_6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-33665-2_6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Zarifis A., Ktoridou D., Efthymiou L. &amp; Cheng X. (ed.) Business
  digital transformation: Selected cases from industry leaders (2023), London:
  Palgrave Macmillan</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2312.17708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.17708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.00338v1</id>
    <updated>2023-12-30T22:17:59Z</updated>
    <published>2023-12-30T22:17:59Z</published>
    <title>A Rapid Scoping Review and Conceptual Analysis of the Educational
  Metaverse in the Global South: Socio-Technical Perspectives</title>
    <summary>  This paper presents a conceptual insight into the Design of the Metaverse to
facilitate educational transformation in selected developing nations within the
Global South regions, e.g., India. These regions are often afflicted with
socio-economic challenges but rich in cultural diversity. By utilizing a
socio-technical design approach, this study explores the specific needs and
opportunities presented by these diverse settings. A rapid scoping review of
the scant existing literature is conducted to provide fundamental insights. A
novel design methodology was formulated that utilized ChatGPT for ideation,
brainstorming, and literature survey query generation. This paper aims not only
to shed light on the educational possibilities enabled by the Metaverse but
also to highlight design considerations unique to the Global South.
</summary>
    <author>
      <name>Anmol Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/2401.00338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.00338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.04114v1</id>
    <updated>2023-12-21T23:00:45Z</updated>
    <published>2023-12-21T23:00:45Z</published>
    <title>Timeline-based Process Discovery</title>
    <summary>  A key concern of automatic process discovery is to provide insights into
performance aspects of business processes. Waiting times are of particular
importance in this context. For that reason, it is surprising that current
techniques for automatic process discovery generate directly-follows graphs and
comparable process models, but often miss the opportunity to explicitly
represent the time axis. In this paper, we present an approach for
automatically constructing process models that explicitly align with a time
axis. We exemplify our approach for directly-follows graphs. Our evaluation
using two BPIC datasets and a proprietary dataset highlight the benefits of
this representation in comparison to standard layout techniques.
</summary>
    <author>
      <name>Harleen Kaur</name>
    </author>
    <author>
      <name>Jan Mendling</name>
    </author>
    <author>
      <name>Christoffer Rubensson</name>
    </author>
    <author>
      <name>Timotheus Kampik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.04114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.04114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.06686v1</id>
    <updated>2024-01-08T10:23:52Z</updated>
    <published>2024-01-08T10:23:52Z</published>
    <title>Exploring Conversational Agents as an Effective Tool for Measuring
  Cognitive Biases in Decision-Making</title>
    <summary>  Heuristics and cognitive biases are an integral part of human
decision-making. Automatically detecting a particular cognitive bias could
enable intelligent tools to provide better decision-support. Detecting the
presence of a cognitive bias currently requires a hand-crafted experiment and
human interpretation. Our research aims to explore conversational agents as an
effective tool to measure various cognitive biases in different domains. Our
proposed conversational agent incorporates a bias measurement mechanism that is
informed by the existing experimental designs and various experimental tasks
identified in the literature. Our initial experiments to measure framing and
loss-aversion biases indicate that the conversational agents can be effectively
used to measure the biases.
</summary>
    <author>
      <name>Stephen Pilli</name>
    </author>
    <link href="http://arxiv.org/abs/2401.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08682v1</id>
    <updated>2024-01-12T13:48:16Z</updated>
    <published>2024-01-12T13:48:16Z</published>
    <title>Visualizing the genealogy of video game specifications of video game
  genres -- through a case study of the raising up game genre</title>
    <summary>  Although several methodologies for identifying the genealogy of video game
genres and showing their relationships have been proposed in existing research,
there have been few attempts to visualize the genealogy of a genre in a
quantitative and qualitative manner. In this study, we propose a methodology to
identify the scope of a specific game genre and to show how game specifications
specific to that genre are related to each other.
</summary>
    <author>
      <name>Akito Inoue</name>
    </author>
    <author>
      <name>Hitomi Mohri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">93 pages, Japanese</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.08682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08964v1</id>
    <updated>2024-01-17T04:36:06Z</updated>
    <published>2024-01-17T04:36:06Z</published>
    <title>Evidence-centered Assessment for Writing with Generative AI</title>
    <summary>  We propose a learning analytics-based methodology for assessing the
collaborative writing of humans and generative artificial intelligence. Framed
by the evidence-centered design, we used elements of knowledge-telling,
knowledge transformation, and cognitive presence to identify assessment claims;
we used data collected from the CoAuthor writing tool as potential evidence for
these claims; and we used epistemic network analysis to make inferences from
the data about the claims. Our findings revealed significant differences in the
writing processes of different groups of CoAuthor users, suggesting that our
method is a plausible approach to assessing human-AI collaborative writing.
</summary>
    <author>
      <name>Yixin Cheng</name>
    </author>
    <author>
      <name>Kayley Lyons</name>
    </author>
    <author>
      <name>Guanliang Chen</name>
    </author>
    <author>
      <name>Dragan Gasevic</name>
    </author>
    <author>
      <name>Zachari Swiecki</name>
    </author>
    <link href="http://arxiv.org/abs/2401.08964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
